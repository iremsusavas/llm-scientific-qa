{"id": "2507.23773v1_chunk0", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "SIMURA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model Mingkai Deng⋄,†∗Jinyu Hou⋄,†∗Yilin Shen‡Hongxia Jin‡ Graham Neubig†Zhiting Hu⋄,▲Eric Xing⋄,† ⋄Institute of Foundation Models, Mohamed bin Zayed University of Artificial Intelligence †School of Computer Science, Carnegie Mellon University ‡Samsung Research ▲Halıcıo ˘glu Data Science Institute, UC San Diego {mingkaid, jinyuhou}@cs.cmu.edu, eric.xing@mbzuai.ac.ae Abstract AI agents built on large language models (LLMs) hold enormous promise, but current practice focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also suffers from the fundamental limitations of autoregressive LLMs. On the other hand, humans are general agents who reason by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SIMURA, a goal-oriented architecture for generalized agentic reason"}
{"id": "2507.23773v1_chunk1", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "on by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SIMURA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of optimal agent in any environment, SIMURAovercomes the limitations of au- toregressive reasoning by introducing a world model for planning via simulation. The generalized world model is implemented using LLM, which can flexibly plan in a wide range of environments using the concept-rich latent space of natural lan- guage. Experiments on difficult web browsing tasks show that SIMURAimproves the success of flight search from 0% to 32.2%. World-model-based planning, in particular, shows consistent advantage of up to 124% over autoregressive planning, demonstrating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently "}
{"id": "2507.23773v1_chunk2", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "strating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently in all environments. To start, we make REASONER AGENT -WEB, a web-browsing agent built on SIMURAwith pretrained LLMs, available as a research demo for public testing. 1 Introduction AI agents powered by large language models (LLMs) hold tremendous potential for handling tasks that require flexible decision making. Recently, there have been great advancements in agents specialized in web and computer automation [ 1,2,3,4], internet research [ 5,6,7], social simulation [8], software development [ 9,10], scientific research [ 11,12], and so on. Despite the promise, current LLMs often prove insufficient for solving complex agentic tasks, suffering from issues such as hallucination, repetitions, or failure at complex planning [ 13,14]. To address these issues, many approaches focus on creating "}
{"id": "2507.23773v1_chunk3", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "icient for solving complex agentic tasks, suffering from issues such as hallucination, repetitions, or failure at complex planning [ 13,14]. To address these issues, many approaches focus on creating agents tailored to specific tasks like the above examples. However, this strategy have some inherent drawbacks. Economically, redesigning custom agents for every task is ∗Co-first author Preprint. Under review.arXiv:2507.23773v1 [cs.AI] 31 Jul 2025Flight Searching : “I’d like to find a round -trip ticket from Chicago to Dubai next month, traveling in economy class, with non -stop flights only, departing after 8 AM, and returning within two weeks. ” Online Shopping : “I want to buy a black mattress. Can you look at Amazon, eBay, and Mattress Firm and give me one good option from each? ” News Research : “Find and summarize one article from each of these local news outlets on Trump’s inauguration: Times of San Diego, The Tennessee Tribune, and MinnPost ?”Go to Google Flights Fill in origin an"}
{"id": "2507.23773v1_chunk4", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "search : “Find and summarize one article from each of these local news outlets on Trump’s inauguration: Times of San Diego, The Tennessee Tribune, and MinnPost ?”Go to Google Flights Fill in origin and destination Fill in departure and return dates Filter search result and respond Go to Amazon website Search for mattress on eBay Go on Mattress Firm website Search and compose response Go on Times of San Diego Browse The Tennessee Tribune Search on MinnPost Review and compose responseFigure 1: Demo of tasks performed using a web-browsing agent built on SIMURAwith simulative planning using a LLM-based world model. costly and not scalable from a business standpoint. Intellectually, narrowly focused solutions offer no clear path towards general and transferrable intelligence. [ 15] Technically, autoregressive LLMs rely on linear, step-by-step reasoning that often leads to errors that propagate through their thought trajectories [ 16,17]. Humans, in contrast, are generalist problem-solvers t"}
{"id": "2507.23773v1_chunk5", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " autoregressive LLMs rely on linear, step-by-step reasoning that often leads to errors that propagate through their thought trajectories [ 16,17]. Humans, in contrast, are generalist problem-solvers that can reason and plan to achieve goals in diverse environments. Using a single cognitive system, we adapt to different tasks not only by linear reasoning, but also by imagining potential outcomes, simulating possibilities using a mental world model, and planning accordingly. [18] Moving towards a more general and powerful AI agent, we introduce SIMURA(Simulative Reasoning Architecture), a goal-oriented architecture for generalized agentic reasoning. SIMURAmitigates the limitations of LLM autoregressive reasoning by introducing world model as the engine for planning via simulation. Specifically, a policy module first proposes a few potential actions, aimed at achieving specific goals based on agent identity and environment. Then, the world model simulates the outcomes of those proposed ac"}
{"id": "2507.23773v1_chunk6", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ly, a policy module first proposes a few potential actions, aimed at achieving specific goals based on agent identity and environment. Then, the world model simulates the outcomes of those proposed actions. Finally, a critic module evaluates these outcomes against the initial goals in order to select the best action from the candidates. Because simulating the full details of the world is infeasible and unnecessary for planning, we extract only the relevant information using natural language as a compact but complete representation, and simulate the next world in this latent space. To ensure robustness from observation noise and distracting execution details, we further propose a hierarchical architecture that isolates perception, simulative planning, and action selection which ensures adaptability and consistency across diverse tasks. Experiments on a range of web browsing tasks show SIMURAimproving substantially compared to baselines, increasing the success rate of flight search from "}
{"id": "2507.23773v1_chunk7", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "tability and consistency across diverse tasks. Experiments on a range of web browsing tasks show SIMURAimproving substantially compared to baselines, increasing the success rate of flight search from 0%to32.2% , with reasoning by WM simulation outperforming LLM autoregressive reasoning by up to 124% . Figure 1 shows examples of the agent performing multi-website, long-range task such as flight searching, online shopping, and news research. For evaluation and demonstration purposes, we implemented SIMURAas an open-source library available via LLM Reasoners [ 19].2The resulting web agent, REASONER AGENT -WEB[20], is 2https: //github.com/maitrix-org/llm-reasoners/tree/main/examples/ReasonerAgent-Web 2available as a research preview.3We are actively expanding the system to address broader challenges and to further demonstrate its generality across a wider range of task domains. 2 Related Work LLM-Based Agents LLM-based agents have rapidly evolved into versatile systems capable of autonomou"}
{"id": "2507.23773v1_chunk8", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "lenges and to further demonstrate its generality across a wider range of task domains. 2 Related Work LLM-Based Agents LLM-based agents have rapidly evolved into versatile systems capable of autonomous behavior across a range of environments. One major approach to build such systems focuses on data collection in the targeted environment followed by model training. Notable examples include AutoWebGLM [ 21], AgentQ [ 22], UI-TARS [ 23], etc. Prompt-based workflows, on the other hand, have also shown strong potential when equipped with carefully designed modules, as demonstrated by recent work such as AWM [ 24], VOYAGER [ 25], and so on. SIMURAis built on prompt-based workflows but can leverage observation data for targeted improvement of its world model [ 26], leading to reduced reliance on human demonstration and strong generalizability to new tasks [18], which is an exciting next step. World-Model-Based Agents Model-based planning for agents have long been frequently discussed and stud"}
{"id": "2507.23773v1_chunk9", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "human demonstration and strong generalizability to new tasks [18], which is an exciting next step. World-Model-Based Agents Model-based planning for agents have long been frequently discussed and studied. Early work demonstrated the success of this approach by testing in classic games like go, chess, shogi and Atari. [ 27,28]. Later on, world model was used for policy optimization and experimented on control tasks. [ 29,30] In recent years, with the boost in foundation model’s capabilities, world-model-based planning was applied to more complex problems like math reasoning [31], playing Minecraft [ 32], and web browsing [ 33]. However, these world models typically represent and predict the world states using holistic continuous embeddings, which suffer from noise and high variability which detracts from robust and stable decision-making [ 34].SIMURAinstead adopts natural language as a discrete, concept-based latent space for consistent representation and prediction, which shows more ge"}
{"id": "2507.23773v1_chunk10", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " detracts from robust and stable decision-making [ 34].SIMURAinstead adopts natural language as a discrete, concept-based latent space for consistent representation and prediction, which shows more general applicability across tasks in practice. Web Browsing Agents Web browsing and navigation were chosen to evaluate SIMURAdue to their realism and the complex decision-making they demand across diverse, dynamic interfaces. Recent years have seen the emergence of several prominent web-browsing agents, from proprietary ones such as OpenAI’s Operator [ 1], Anthropic’s Computer Use [ 6], and Google-DeepMind’s Project Mariner [2], and open-source ones including OpenHand’s BrowsingAgent [ 35], WebV oyager [ 36], CogAgent [37] and WebAgent [ 38]. These agents are typically built on simple ReAct-based [ 17] autoregressive reasoning which have difficulty recovering from previous mistakes; their often specialized design also preclude these approaches from generalizing to other task domains like so"}
{"id": "2507.23773v1_chunk11", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ed [ 17] autoregressive reasoning which have difficulty recovering from previous mistakes; their often specialized design also preclude these approaches from generalizing to other task domains like social interactions and the physical world. Numerous benchmarks have been introduced to evaluate these web agents, including WebArena [ 3], WebV oyager [ 36] MiniWoB++ [ 39], Mind2Web [ 40], and WebShop [ 41]. Despite wide adoption, these benchmarks are usually either built in simulated and simplified environments, based on outdated questions, or lacks convincing method of measuring task completion, which detract from the goal of evaluating practically useful web agents. To address these challenges, we build FlightQA, an new dataset for evaluating agent ability in real-time complex website navigation. More details are included in Section 4.1. Generalist Agents There have been various attempts of building generalist agents recently. One major approach focuses on creating a multi-agent system "}
{"id": "2507.23773v1_chunk12", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ation. More details are included in Section 4.1. Generalist Agents There have been various attempts of building generalist agents recently. One major approach focuses on creating a multi-agent system that consists of a unified interface on top of a few specialist agents that collaborates to decompose and complete complex tasks. [ 42,43,44,45] Although this approach could lead to impressive performance on benchmarks, it has a few inherent limitations. First of all, tasks in reality could be versatile and may constantly require new specialist agents to be added to the system to achieve optimal performance, which is not efficient. Moreover, independently trained specialist agents for different domains are unable to leverage shared experience in the way that world model training enables. Finally, error propagation along the interaction trajectory remains an open challenge and is further complicated by the presence of multiple agents. Another popular approach utilizes framework similar to t"}
{"id": "2507.23773v1_chunk13", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ly, error propagation along the interaction trajectory remains an open challenge and is further complicated by the presence of multiple agents. Another popular approach utilizes framework similar to the CodeActAgent [ 46]. These agents [ 35,47,48] suffer from inaccurate code plans and have limited ability to revise or correct prior errors as well. SIMURA, on the other hand, is able to avoid these limitations by working as a monolithic architecture in which world model act as a central planning component. 3https://easyweb.maitrix.org/ 3𝜋 Agent 𝑎𝑡𝑠𝑡+1 Goal World State 𝑔 𝑠𝑡 𝑇 𝜇 Universe Critic 𝑉Figure 2: A possible definition of an optimal agent 3 S IMURA: Generalized Architecture for Optimal Goal-Oriented Agent 3.1 Formulation of Agent-Environment Model We first present our formulation of an optimal goal-oriented agent following the agent-environment model presented in [ 49]: We consider an agent πwith identity i(e.g., name, description) and goal g acting in environment µ(e.g., web brows"}
{"id": "2507.23773v1_chunk14", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " optimal goal-oriented agent following the agent-environment model presented in [ 49]: We consider an agent πwith identity i(e.g., name, description) and goal g acting in environment µ(e.g., web browser, physical world, the entire universe) with action space Aand state space S. Formally, at each time step t, the agent πtakes the current state st∈ S and outputs the next action at∈ A following a policy distribution pπ(at|st), while the environment µ takes the current state stand action at, and outputs the next state st+1∈ S based on the distribution pµ(st+1|st, at). We can thus denote the distribution of the interaction trajectory up to timestep T, or (at, st+1, . . . , a T−1, sT)given the current state stas below: pπ µ(at, st+1, . . . , a T−1, sT|st) =T−1Y k=tpπ(ak|sk)|{z} agentpµ(sk+1|sk, ak)| {z } environment(1) In each state st, the agent also receives a reward r(g, st)based on its goal g. We evaluate the agent by its discounted cumulative reward, denoted asP∞ k=tγkr(g, sk)(with the "}
{"id": "2507.23773v1_chunk15", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ak)| {z } environment(1) In each state st, the agent also receives a reward r(g, st)based on its goal g. We evaluate the agent by its discounted cumulative reward, denoted asP∞ k=tγkr(g, sk)(with the discount parameter γt decaying to zero with time, i.e., limt→∞γt= 0). Note that this reward function can be dense (e.g., gaming scores), but perhaps frequently sparse (e.g., curing a disease). The agent’s long-term success can thus be measured by its expected future discounted reward, also known as value function [50], which satisfies the following recurrence: Vg π,µ(st):=Eπ,µ\"∞X k=tγkr(g, sk) st# = lim T→∞X (at,st+1,...,s T)TX k=tγkr(g, sk)pπ µ(at, st+1, . . . , s T|st) =X (at,st+1,...,s T) T−1X k=tγkr(g, sk) +γTVg π,µ(sT) | {z } goal progress! pπ µ(at, st+1, . . . , s T|st) | {z } trajectory,(2) Which indicates that the value function in state stcan be expressed in terms of the value function at possible future states sTweighted by their probabilities. 3.2 Definition of Optimal Agent Bas"}
{"id": "2507.23773v1_chunk16", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": ",(2) Which indicates that the value function in state stcan be expressed in terms of the value function at possible future states sTweighted by their probabilities. 3.2 Definition of Optimal Agent Based on Equations 1 and 2, we can define the optimal agent π∗ µin environment µas one that maximizes the value function, written formally as below: π∗ µ:= arg max πVg π,µ. (3) 4Some simple derivation will show that the optimal agent in state stwill follow the following decision ruleπ∗ µwhen planning for actions at:T−1: π∗ µ(st) = arg max at:T−1|{z} possible actionsX st+1:T T−1X k=tγkr(g, sk) +γTVg π,µ(sT) | {z } goal progress!T−1Y i=tpµ(si+1|si, ai)|{z } universe response(4) In practice, agents often samples promising action candidates using a policy function ˜πthrough the distribution p˜π(at|st). Building the optimal agent thus requires capabilities for proposing possible actions ( ˜π), predicting their outcomes ( µ), and evaluating goal progress ( r, V), respectively. Note that typical rea"}
{"id": "2507.23773v1_chunk17", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "). Building the optimal agent thus requires capabilities for proposing possible actions ( ˜π), predicting their outcomes ( µ), and evaluating goal progress ( r, V), respectively. Note that typical reactive agents that output the next action directly can be seen as taking the first sample from ˜π(similar to “System 1” in humans which makes fast, instinctive reactions [ 51]), without simulating and evaluating the outcomes using µandV(similar to “System 2” responsible for deliberate decision- making). In terms of LLM-based agents, this can also be seen as the agent generating a plan using autoregressive LLMs, which has no way of correcting errors during the sampling process. 3.3 World Model for Generalized Simulative Reasoning Note that the optimal decision-making process defined in Equation 4 requires the agent to have access to the ground-truth world state sand the environment µto experience and optimize over. However, these are often not available aside from simple scenarios like Go an"}
{"id": "2507.23773v1_chunk18", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "4 requires the agent to have access to the ground-truth world state sand the environment µto experience and optimize over. However, these are often not available aside from simple scenarios like Go and Chess games [ 52,53] – imagine building an spacecraft to land on Mars, or simply a humanoid robot relying on noisy sensors in daily environments. World Model (WM) thus arises as a crucial component for predicting any environment’s response to a general agent. Specifically, a WM foperates on an internal representation of the world state, denoted as a belief state ˆst, which is derived from sensory inputs otvia an Encoder h(unlike the optimal agent described in §3.2 which has direct access to the true world state st). Given proposed action at, the WM predicts the next belief state ˆst+1according to the distribution pf(ˆst+1|ˆst, at). The predicted belief state then allows the agent to propose the next action, continuing the cycle of prediction and action up to the desired time horizon T. T"}
{"id": "2507.23773v1_chunk19", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ng to the distribution pf(ˆst+1|ˆst, at). The predicted belief state then allows the agent to propose the next action, continuing the cycle of prediction and action up to the desired time horizon T. Thus, a WM here essentially functions as a generative model of possible future world states, which enables simulative reasoning, or “thought experiments\". Formally, for the optimal agent π∗ fequipped with WMfin belief state ˆst, we define the simulation-based decision rule in Equation 6 as follows: π∗ f(ˆst) = arg max at:T−1|{z} possible actionsX ˆst+1:T T−1X k=tγkr(g,ˆsk) +γTVg π,f(ˆsT) | {z } goal progress!T−1Y i=tpf(ˆsi+1|ˆsi, ai)|{z} simulation with world model(5) A general-purpose WM fhere enables simulation of diverse possibilities across a wide range of domains, enabling agents to reason about outcomes without direct interaction with the environment. 3.4 Design of Simulative Reasoning Agent Using LLM-Based World Model In this subsection, we present our design of a generalizable simul"}
{"id": "2507.23773v1_chunk20", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " about outcomes without direct interaction with the environment. 3.4 Design of Simulative Reasoning Agent Using LLM-Based World Model In this subsection, we present our design of a generalizable simulative reasoning agent using large language models (LLMs) as building blocks due to the latter’s strength in a wide range of capabilities such as summarization, commonsense knowledge, reflection, and tool use, which are gained from large-scale pretraining and instruction tuning. In particular, we provide detailed discussion on design decisions that enable robust and general applicability across environments and tasks. Discrete, Hierarchical State Representation via Natural Language The dominant approach to encoding observation ot(e.g., webpages, video streams) has been to directly pass all input tokens into an LLM to form continuous embeddings ˆsz t. While technically preserving all information, real- world sensory readings often suffer from inherent noise and high variability (e.g., ads on"}
{"id": "2507.23773v1_chunk21", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "t tokens into an LLM to form continuous embeddings ˆsz t. While technically preserving all information, real- world sensory readings often suffer from inherent noise and high variability (e.g., ads on a webpage, varying weather and lighting conditions in video), which can make them brittle for reasoning over. Human cognition, on the other hand, has evolved to counter this variability by categorizing raw perception into discrete concepts [34], which are often encoded in language, symbols or structured thoughts. Indeed, natural language is inherently hierarchical, capable of encoding concepts from concrete ones (e.g., apple) to highly abstract ones (e.g., religion). Discrete representations are also complete in general [ 49], which ensures no information is necessarily lost in the compression process. 5𝜇inaccessible𝜋 Agent 𝑎𝑡′Ƹ𝑠𝑡+1′Goal Critic𝑓 𝑉Belief 𝑔 𝑇𝑜𝑡World Model Ƹ𝑠𝑡 Encoder ℎ agent estimation of world state Observation𝑇′ 𝑎𝑡continuous or discrete internal representations sensory da"}
{"id": "2507.23773v1_chunk22", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "n process. 5𝜇inaccessible𝜋 Agent 𝑎𝑡′Ƹ𝑠𝑡+1′Goal Critic𝑓 𝑉Belief 𝑔 𝑇𝑜𝑡World Model Ƹ𝑠𝑡 Encoder ℎ agent estimation of world state Observation𝑇′ 𝑎𝑡continuous or discrete internal representations sensory data contains incomplete informationactual actionsimulative reasoning to find the best action UniverseFigure 3: An agent in real world where groundtruth world state and universe are unavailable to experience or experiment, so world model is crucial for simulation. As discussed in §3.4, separation of simulated actions a′ tfor planning and concrete actions atfor execution facilitates transfer and hierarchical planning, leading to more diverse and grounded actions which lead to better task success. Implementing this form of perception, we propose to represent the world state ˆstusing a discrete natural language summary ˆsc tgenerated by an encoder LLM h, formally expressed as below: ph(ˆst|ot) =NtY i=1ph(ˆst,i|ˆst,<i, ot), (6) Where each ˆst,iis a natural language token. Likewise, we also imple"}
{"id": "2507.23773v1_chunk23", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ural language summary ˆsc tgenerated by an encoder LLM h, formally expressed as below: ph(ˆst|ot) =NtY i=1ph(ˆst,i|ˆst,<i, ot), (6) Where each ˆst,iis a natural language token. Likewise, we also implement the WM fusing an LLM which predicts the next state ˆst+1as a natural language sequence ˆsc t+1, formally as below: pf(ˆst+1|ˆst, at) =Nt+1Y i=1ph(ˆst+1,i|ˆst+1,<i,ˆst, at) (7) Such a concept-based representation allows the other modules like policy to operate on a more structured latent space, which we find empirically to reduce hallucination and enable more robust planning, leading to better task performance in practice. Hierarchical Planning via Simulated Actions The customary approach to decision-making with world models has been to perform simulations or rollouts based on the specific action space A(π) afforded to the agent. While this approach indeed captures all the execution details, the specific idiosyncracies of individual action spaces (e.g., parameter ordering, format, and "}
{"id": "2507.23773v1_chunk24", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " action space A(π) afforded to the agent. While this approach indeed captures all the execution details, the specific idiosyncracies of individual action spaces (e.g., parameter ordering, format, and scale) may distract from transferring knowledge across different action spaces, environments, and tasks for generalizable reasoning. The real world may contain a richer range of intentions than what a particular action space offers (e.g., clicking on a flight may mean either exploring the pricing or committing to the option). Last but not least, the sequential roll-out over atomic actions can be inefficient and increase opportunities for error accumulation across multi-step, low-level predictions (e.g., swooshing of liquids with each muscle twitch), when higher-level dynamics over more abstract actions (e.g., spilling water due to tilting the glass) remain stable and predictable. To close this gap, we adopt a hierarchical architecture which separates high-level, flexible planning from low-"}
{"id": "2507.23773v1_chunk25", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "t actions (e.g., spilling water due to tilting the glass) remain stable and predictable. To close this gap, we adopt a hierarchical architecture which separates high-level, flexible planning from low-level, rigorous execution [ 54]. As illustrated in Figure 3, the agent’s policy p˜π(a′ t|ˆst)and world model pf(ˆst+1|ˆst, a′ t)operate over simulated actions a′ tfrom a separate action space A′, while another actor pν(at|a′ t,ˆst)is responsible for selecting the concrete action at∈ A conditioned on the selected simulated action a′ t. This divide-and-conquer approach allows for more generalized reasoning disentangled from the exact details of the concrete action space and enables representation of a richer set of intentions. Furthermore, each simulated action a′ tmay represent multiple execution steps in the environment (e.g., “explore the website” vs “click on the link”), which shortens the number of rollout steps for higher efficiency and fewer chances for error accumulation. In practice"}
{"id": "2507.23773v1_chunk26", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "on steps in the environment (e.g., “explore the website” vs “click on the link”), which shortens the number of rollout steps for higher efficiency and fewer chances for error accumulation. In practice, we represent simulated actions a′ tusing natural language due to its generality and expressivity, and find it results in more diverse and grounded action proposals, leading to better task success. Having discussed our major designs, we proceed to describe the full decision process of the SIMURA architecture: As illustrated in Figure 3, given observation ot(e.g., webpage screenshots and/or accessibility tree), SIMURAfirst infers the world state ˆstusing the encoder h, and then selects the 6 Observation Encoder Actor Belief State Planner Simulated Action Action Plan Candidates Next State Predictions State Values Optimize Policy World Model Critic Identity Goal Agent Simulative Reasoning Environment Figure 4: Optimal agent architecture design with conditional probability annotation best sim"}
{"id": "2507.23773v1_chunk27", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ctions State Values Optimize Policy World Model Critic Identity Goal Agent Simulative Reasoning Environment Figure 4: Optimal agent architecture design with conditional probability annotation best simulated action a′∗ tthrough the planner. Inside the planner, the architecture performs simulative reasoning by proposing actions a′ tusing policy ˜πand predicting the next state ˆst+1using the world model f, and evaluating goal progressPT′−1 k=tγkr(g,ˆsk)+γT′Vg π,f(ˆsT′)using critic vupon reaching state ˆsT′at the planning horizon T′. This can repeat multiple times until the planner selects the action sequence a′∗ t:T′−1with the highest expected success and passes the first step a∗ tto actor ν which finally outputs the concrete action at. Formally, SIMURAcan be seen as solving the following multi-level optimization problem: ˆst= arg max ˆsph(ˆs|ot)|{z} encoder(Perception) a′∗ t:T′−1= arg max a′ t:T−1|{z} sampled from policy ˜πX ˆst+1:T′v(ˆsT′)|{z} criticT′−1Y k=tpf(ˆsk+1|ˆsk, a′ k)|{z } wor"}
{"id": "2507.23773v1_chunk28", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ti-level optimization problem: ˆst= arg max ˆsph(ˆs|ot)|{z} encoder(Perception) a′∗ t:T′−1= arg max a′ t:T−1|{z} sampled from policy ˜πX ˆst+1:T′v(ˆsT′)|{z} criticT′−1Y k=tpf(ˆsk+1|ˆsk, a′ k)|{z } world model(Planning) (8) at= arg max apν(a|ˆst, a′∗ t)|{z} actor(Acting) In practice, we implement each of these components by zero-shot prompting pretrained LLMs. While these LLMs alone are often insufficient for many complex agentic tasks, SIMURA’s divide-and- conquer approach combines existing LLM strengths like instruction-following, summarization, reflec- tion, and tool use to allow agentic behavior to emerge. Benefiting from massive web-scale pretraining on next-token prediction p(xt|x<t), which is formally akin to world modeling, LLMs possess significant potential to serve as world models with natural-language state and action spaces [ 31,55]. We approximately infer the world state ˆstand action atby sampling from the LLM-based encoder and actor distributions phandpν, respectively. Fo"}
{"id": "2507.23773v1_chunk29", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " with natural-language state and action spaces [ 31,55]. We approximately infer the world state ˆstand action atby sampling from the LLM-based encoder and actor distributions phandpν, respectively. For planning, we optimize over the sampled actions a′ t:T′−1using readily available tree search algorithms like Depth-First Search (DFS) and Monte-Carlo Tree Search (MCTS). 4 Experiments Our proposed SIMURAarchitecture is generally applicable to various environments and tasks. As our first step, we evaluate our implementation on web browsing as an example due to both its practical value and its technical challenge. Web browser is an indispensable portal for individuals to perform 7Goal Observation \"Find the cheapest one-way flight from Boston to Los Angeles next Wednesday\" \"The current webpage is a Google flight interface that has ...\"\" Encoder Actor Environment Identity \"An information and automation assistant who navigates ...\" Memory \"... We filled out flight information and clicked on th"}
{"id": "2507.23773v1_chunk30", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "s a Google flight interface that has ...\"\" Encoder Actor Environment Identity \"An information and automation assistant who navigates ...\" Memory \"... We filled out flight information and clicked on the 'Search' botton ...\" User Belief State \"Select the 'Cheapest' tab\" Planner \"click('128')\" Simulated Action ActionFigure 5: LLM-based implementation of our proposed agent model for web-related tasks (e.g. multi- website QA, flight search, etc). Planner is where we implement our proposed world-model-based planning. We also implement a baseline that simply samples the plan from a language model (i.e., autoregressive planning). many tasks in real life (e.g., gather information, book travels, and submit applications). Whereas many existing products do access the internet [ 56,57, etc.], they typically use specialized tools (e.g., search engines and data APIs) to capture a subset of web browser capabilities (i.e., reading) while falling short of the full functionality (e.g., access content not"}
{"id": "2507.23773v1_chunk31", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "cally use specialized tools (e.g., search engines and data APIs) to capture a subset of web browser capabilities (i.e., reading) while falling short of the full functionality (e.g., access content not exposed to search engines or predefined APIs like flight and hotel databases). We argue that an agent that takes advantage of the full browser will push the envelope in AI’s abilities to serve human needs. Despite the richness and flexibility, the web browser is a highly challenging environment for agentic reasoning due to its immense complexity, long-horizon nature, partial observability, and multimodal- ity [3,58]. We evaluate our architecture in 3 types of web browsing tasks: 1) complex website navigation, 2) multi-hop, multi-website QA, and 3) general web automation. For the baselines, we compare against: 1.BrowsingAgent from OpenHands [ 35], a representative open-web agent which generates chain-of-thought before selecting an action 2.SIMURA(our architecture) with autoregressive plann"}
{"id": "2507.23773v1_chunk32", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " compare against: 1.BrowsingAgent from OpenHands [ 35], a representative open-web agent which generates chain-of-thought before selecting an action 2.SIMURA(our architecture) with autoregressive planning (i.e., commit to the first sample from our policy module) instead of our proposed simulation-based planning with world model. Formally, the planning process is simplified to the following: a′∗ t= arg max a′ tp˜π(a′ t|ˆst) Implementation for Web Browsing Figure 5 presents our implementation when applied to web browsing. We use prompts tailored to the web environments in this example, but plan to extend to other environments and move towards training a single agent model that can act optimally in all environments, which is an exciting next step. At each step t, the agent receives the observation otas the HTML-based accessibility tree visible through the browser’s viewport (an example is provided in 8Figure 6: Overview of performance comparison between SIMURAand baselines. The full archit"}
{"id": "2507.23773v1_chunk33", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "tion otas the HTML-based accessibility tree visible through the browser’s viewport (an example is provided in 8Figure 6: Overview of performance comparison between SIMURAand baselines. The full architecture shows clear advantage over the baseline BrowsingAgent, improving the performance on complex website navigation from 0% to 32.2%. Our proposed world model reasoning for planning also consistently improves over simple planning with autoregressive LLM by up to 124%. Appendix A). The agent then uses encoder LLM hto summarizes the observation as ˜st∼ph(· |ot), and then add it to a selective memory of past summaries and simulated actions {m(˜sk, a′∗ k)}t−1 k=1to form the estimated world state ˆst= [m(˜s1, a′∗ 1), . . . , m (˜st−1, a′∗ t−1),˜st]for planning. During planning, we sample Msimulated actions a′ tfrom the policy ˜π, cluster them into distinct actions, and use the world model fto predict the next summary as ˜st+1∼pf(· |ˆst, a′ t)to form the next state ˆst+1= [m(˜s1, a′∗ 1), . . ."}
{"id": "2507.23773v1_chunk34", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ed actions a′ tfrom the policy ˜π, cluster them into distinct actions, and use the world model fto predict the next summary as ˜st+1∼pf(· |ˆst, a′ t)to form the next state ˆst+1= [m(˜s1, a′∗ 1), . . . , m (˜st, a′ t),˜st+1]; this repeats until the planning horizon T. To evaluate the terminal state ˆsTwith critic v, we prompt the LLM to generate qualitative answers and convert them into numerical scores (e.g., “success” receives a score of 1), and repeat for Ntimes to capture the fine-grained differences between states. Following previous work [ 59,33], we set M=N= 20 and T=t+ 1, and use DFS as the search algorithm. We implement the planning process using LLM Reasoners [ 19], a library for LLM-based complex reasoning using advanced algorithms. After the planner selects the simulated action a′∗ t, we update the memory with m(˜st, a′∗ t). For the actor ν, we additionally include the observation text otin the prompt to ensure the action grounding. All the prompts are included in Appendix B"}
{"id": "2507.23773v1_chunk35", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "a′∗ t, we update the memory with m(˜st, a′∗ t). For the actor ν, we additionally include the observation text otin the prompt to ensure the action grounding. All the prompts are included in Appendix B. Overview of Results An overview of our results is presented in Figure 6. Across all 3 categories of tasks, our architecture shows a clear advantage over the baseline BrowsingAgent, specifically increase the success rate on complex website navigation from 0% to 32.2%. Our proposed world model reasoning for planning also consistently improves over simple planning with autoregressive LLMs by up to 124%. In the subsections below, we will introduce the evaluation settings and discuss the results in more detail. 4.1 Complex Website Navigation A distinguishing feature of web agents is the ability to gather live information (e.g., flights, stock prices, social media) not present in the training data of foundation models due to its rapid update [ 60]. For many questions (e.g., the price of the ea"}
{"id": "2507.23773v1_chunk36", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "o gather live information (e.g., flights, stock prices, social media) not present in the training data of foundation models due to its rapid update [ 60]. For many questions (e.g., the price of the earliest flight tomorrow), LLMs without such grounding often result in hallucinations (see Figure 7 for an example). In practice, however, obtaining the information is challenging, as many websites are very complex and difficult to navigate (e.g., execute a flight search query on a travel website and filter through the results), which calls for superb reasoning skills on the part of the agent. Dataset Due to the lack of benchmarks for complex website navigation on the open internet, we created our own dataset using flight search as an example, since it is an important part of travel planning, a major use case for web agents. We formulate flight search as the user asking for a flight satisfying a list of constraints (e.g., one-way, from New York to Los Angeles), and the agent operating a flig"}
{"id": "2507.23773v1_chunk37", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "major use case for web agents. We formulate flight search as the user asking for a flight satisfying a list of constraints (e.g., one-way, from New York to Los Angeles), and the agent operating a flight search engine to find such a flight and returning it to the user. To evaluate the agent’s reasoning 9Figure 7: Faced with the question “What is the earliest-arriving flight tomorrow from Pittsburgh to Zurich?” ChatGPT-4o browsed the frontpage of Kayak.com and hallucinated a flight that arrives at 10:45am on the following day as the answer ( left). Performing the search on Kayak.com, however, shows that the earliest-arriving flight lands in Zurich at 6:10am on the next day ( right ). The question was asked on December 17th, 2024. Constraint Generation Question Generation ['round-trip', 'San Francisco to Paris', 'departing in two weeks'] Constraint Extension Number of Constraints: 3 ['round-trip', 'San Francisco to Paris', 'departing in two weeks', 'returning after a 10-day stay'] Could y"}
{"id": "2507.23773v1_chunk38", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "Francisco to Paris', 'departing in two weeks'] Constraint Extension Number of Constraints: 3 ['round-trip', 'San Francisco to Paris', 'departing in two weeks', 'returning after a 10-day stay'] Could you find a round-trip flight from San Francisco to Paris departing in two weeks? Could you find a round-trip flight from San Francisco to Paris departing in two weeks and returning after a 10-day stay? Repeat K Times Repeat N Times FlightQA Dataset (N x (K+1) Questions) Figure 8: Illustration of the data generation process for the FlightQA dataset. We first prompt a LLM to generate Nlists of Cstarting constraints (Constraint Generation). Then, we prompt the LLM to iteratively add constraints to the lists one by one, repeating for Ktimes (Constraint Extension). Finally, we prompt the LLM to convert each constraint list into a question in natural language (Question Generation). ability, we further produce questions with varying number of constraints by iteratively adding to the list, which en"}
{"id": "2507.23773v1_chunk39", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ert each constraint list into a question in natural language (Question Generation). ability, we further produce questions with varying number of constraints by iteratively adding to the list, which enables a counterfactual analysis that controls for the confounding effect of specific constraint configurations (e.g., an agent with perfect reasoning should still be able to answer the same question with one more constraint; an agent of rote memorization will likely fail when the question changes slightly). We illustrate our data collection process in Figure 8. To ensure scalability and controllability, we prompt a LLM to first generate a list of Cstarting constraints, repeating for Ntimes. After that, we prompt the LLM to iteratively add constraints to the lists one at a time, repeating for Ktimes. Finally, we prompt the LLM to convert each constraint list into a question in natural language. In practice, we set C= 3,N= 15 , and K= 5, which results in FlightQA, a dataset consisting of 90 "}
{"id": "2507.23773v1_chunk40", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "times. Finally, we prompt the LLM to convert each constraint list into a question in natural language. In practice, we set C= 3,N= 15 , and K= 5, which results in FlightQA, a dataset consisting of 90 questions with 15 sequences of constraint lists where the number of constraints increases from 3 to 8. We use gpt-4o to perform all the data generation steps. The initial question generation and question expansion prompts are included in Appendix C Evaluation Because FlightQA involves querying live information from the open internet, it is impossible to establish ground truth answers due to the constantly evolving flight pricing and availability. Inspired by previous work on evaluation for generated text [ 61], we propose to evaluate the agent response based on two quality aspects: groundedness for whether the response is supported by the interaction history and relevance for whether the response satisfies user constraints to the 10Performance (%) Outcomes (%) Method Correct Grounded Relev"}
{"id": "2507.23773v1_chunk41", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "s for whether the response is supported by the interaction history and relevance for whether the response satisfies user constraints to the 10Performance (%) Outcomes (%) Method Correct Grounded RelevantResponse ReturnedBrowser CrashedMax Steps ReachedRepetitive ActionsAction Errors OpenHands BrowsingAgent 0.0 0.0 0.0 0.0 3.3 3.3 0.0 93.3 SIMURA(Ours) Autoregressive Planning 14.4 15.6 14.4 16.7 0.0 37.8 44.4 1.1 – with o1†1.1 1.1 1.1 1.1 11.1 40.0 37.8 10.0 – with o3-mini†3.3 4.4 3.3 4.4 3.3 51.1 32.2 8.9 World Model Planning 32.2**36.7 32.2 38.9 1.1 40.0 18.9 1.1 Table 1: Performance and outcome statistics for the FlightQA dataset. Our architecture increases the correct rate from 0% in OpenHands BrowsingAgent to 32.2%. Reasoning by world model simulation also clearly outperforms autoregressive reasoning by 124%. ** indicates being significantly higher than the second-best method at the statistical significance level of 0.01 ( p <0.01) based on pairwise t-test.†We implement the autoreg"}
{"id": "2507.23773v1_chunk42", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ssive reasoning by 124%. ** indicates being significantly higher than the second-best method at the statistical significance level of 0.01 ( p <0.01) based on pairwise t-test.†We implement the autoregressive planner with o1 and o3-mini, respectively. extent allowed by the results (e.g., if the search results do not include any flight that satisfies all user constraints). Due to the strong ability of LLMs in evaluating generated text [ 62], we prompt LLMs to assess the two quality aspects of the agent response. Specifically, we include all browser observations in the agent’s trajectory over Tsteps (o1, o2, . . . o T), the constraint list, the question, and the agent response, and ask the LLM to provide judgment on the groundedness and relevance of the response. We further define an answer to be correct when it is both grounded and relevant. We also include the evaluation prompt in Appendix C. Experiment Setup We ran the experiments and evaluation using gpt-4o between November 24th, 2024"}
{"id": "2507.23773v1_chunk43", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "o be correct when it is both grounded and relevant. We also include the evaluation prompt in Appendix C. Experiment Setup We ran the experiments and evaluation using gpt-4o between November 24th, 2024 and December 9th, 2024. For the environment, we use BrowserGym [ 63], a popular open-source browser sandbox. We stop each run when the agent provides a response or after the agent takes 30 actions, whichever comes first. We also mark the run as failed when the agent repeats the same action for 3 times consecutively or when the agent causes more than 3 errors while interacting with the browser. Results We present our Complex Website Navigation results in Table 1. Compared to Browsin- gAgent which fails completely in this task, our full architecture improves the correct rate from 0% to 32.2%. Within our architecture, our proposed world-model-based planning shows superior performance over autoregressive reasoning with a 124% improvement (significant at the 0.01 level). The other components i"}
{"id": "2507.23773v1_chunk44", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ithin our architecture, our proposed world-model-based planning shows superior performance over autoregressive reasoning with a 124% improvement (significant at the 0.01 level). The other components in our architecture, which communicate using the concept-based latent space of model-generated language (e.g., observation summary and selective memory), also result in more coherent behavior by reducing the action error rate in BrowsingAgent from 93.3% to 1.1%. How- ever, the autoregressive reasoning still results in frequent repetitions, which is mitigated by world model-based planning (44.4% →18.9%). Analysis of Reasoning Ability To compare the reasoning abilities of autoregressive and world- model planners within our architecture, we visualize the percentage of correct responses vs number of constraints in Figure 9. As the questions in FlightQA are generated based on iteratively expanded constraint lists, this analysis should faithfully reflect the effect of increasing constraints while"}
{"id": "2507.23773v1_chunk45", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "f constraints in Figure 9. As the questions in FlightQA are generated based on iteratively expanded constraint lists, this analysis should faithfully reflect the effect of increasing constraints while controlling for other confounders such as specific constraint sets. Based on our data samples, world model planning shows consistent advantage over autoregressive planning as we increase the number of constraints, showing signs of improved reasoning ability. The performance for both methods decreases with more constraints initially but then increases sharply at 7 constraints before dropping again, which may reflect memorization in the backend LLM or implicit constraints in questions with fewer explicit constraints. 4.2 Multi-Hop, Multi-Website QA Another type of challenging questions for web agents is those that require gathering information about multiple entities over multiple websites. For instance, given the question “ What are the 11Figure 9: % correct and % response returned vs. num"}
{"id": "2507.23773v1_chunk46", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "agents is those that require gathering information about multiple entities over multiple websites. For instance, given the question “ What are the 11Figure 9: % correct and % response returned vs. number of constraints for FlightQA. Based on our data samples, world model planning consistently outperforms autoregressive planning as we increase the number of constraints, showing signs of improved reasoning ability. availabilities of the top-10 restaurants in Paris for a dinner next week? ”, an agent must first find the top-10 restaurants in Paris, then look up the availability of each restaurant, and finally compile the information into a response to the user. Whereas complex website navigation stresses the depth of individual websites, multi-hop, multi-website QA concerns the breadth of websites to navigate over long-horizon interactions. Dataset To evaluate agent abilities for multi-hop, multi-website QA, we adopt the FanOutQA [ 64] dataset, which consists of questions of exactly this "}
{"id": "2507.23773v1_chunk47", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "sites to navigate over long-horizon interactions. Dataset To evaluate agent abilities for multi-hop, multi-website QA, we adopt the FanOutQA [ 64] dataset, which consists of questions of exactly this nature. Due to resource constraints, we evaluate on the first 100 examples of the dev set. As the results show, however, the smaller sample size is sufficient to show statistically significant differences between methods. Experiment Setup We ran the experiments using gpt-4o-2024-05-13 between November 10th, 2024 and December 8th, 2024. We noticed that our architecture with world-model-based planning deteriorates in performance when using the newer versions of gpt-4o , which may be due to additional training which changed the model’s response patterns to the same prompts. We operate the browser using the same rules as in experiments for Complex Website Navigation. Results We present our results on Multi-Hop, Multi-Website QA in Table 2. Again, our method increases the accuracy from 17.0% to"}
{"id": "2507.23773v1_chunk48", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " using the same rules as in experiments for Complex Website Navigation. Results We present our results on Multi-Hop, Multi-Website QA in Table 2. Again, our method increases the accuracy from 17.0% to 29.8% and world model planning improves over autoregressive planning by 48.6% (p-value = 0.011). BrowsingAgent achieves fair performance even though it cannot memorize information from different websites, often due to some questions in the dataset being answerable based on information from a single Wikipedia page (e.g., What are the publication dates for all of the Harry Potter books? ). Despite this, our architecture improves over BrowsingAgent even without world model planning by dramatically reducing action errors (43% →10%). Browser crashes make a sizable contribution to agent failures (24% for our architecture), indicating room for improvement in the tooling for open-web navigation. 4.3 General Web Automation Last but not least, web agents are often tasked with performing various wor"}
{"id": "2507.23773v1_chunk49", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "4% for our architecture), indicating room for improvement in the tooling for open-web navigation. 4.3 General Web Automation Last but not least, web agents are often tasked with performing various work tedious to human users (e.g., online shopping, managing social media). These tasks often require the ability to interact with a range of websites of moderate complexity. As an example, given the question “ Summarize customer reviews for Amazon Echo Dot 3rd generation ,” the agent should navigate a shopping website to locate and go over all the customer reviews of said product before summarizing the content for the user. 12Performance (%) Outcomes (%) Method Acc. Acc. (Strict)Response ReturnedBrowser CrashedMax Steps ReachedRepetitive ActionsAction ErrorParsing Error OpenHands BrowsingAgent 17.0 4.0 32.0 17.0 8.0 0.0 43.0 0.0 SIMURA(Ours) Autoregressive Planning 20.2 3.0 37.0 24.0 10.0 18.0 10.0 1.0 World Model Planning 29.8* 4.0 55.0 24.0 12.0 8.0 1.0 0.0 Table 2: Performance and outcome"}
{"id": "2507.23773v1_chunk50", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "7.0 4.0 32.0 17.0 8.0 0.0 43.0 0.0 SIMURA(Ours) Autoregressive Planning 20.2 3.0 37.0 24.0 10.0 18.0 10.0 1.0 World Model Planning 29.8* 4.0 55.0 24.0 12.0 8.0 1.0 0.0 Table 2: Performance and outcome statistics for the FanOutQA dataset. Acc. (Strict) refers to the percentage of responses that exactly match the groundtruth. Our architecture clearly outperforms the baseline BrowsingAgent. Reasoning by world model increases the response rate and fact-level accuracy vs. autoregressive planning by 48.6% and 47.5%, respectively. * indicates being significantly higher than the second-best method at the 0.05 level based on pairwise t-test. Dataset To evaluate general web automation capabilities, we adopt the WebArena [ 3] benchmark, a standard environment for testing web agents which features a range of simulated websites including a Reddit-like social forum, a shopping site, a GitLab-based code management platform, a map, and a Wikipedia-like encyclopedia. Following the evaluation for Multi-"}
{"id": "2507.23773v1_chunk51", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ange of simulated websites including a Reddit-like social forum, a shopping site, a GitLab-based code management platform, a map, and a Wikipedia-like encyclopedia. Following the evaluation for Multi-Hop, Multi-Website QA, we take a random subset of 100 examples. Experiment Setup We run the experiments using gpt-4o over BrowserGym accessed via the OpenHands platform which provides a uniform evaluation procedure. Because WebArena demands a specific response format for evaluation, we rewrote the agent description to steer the agent answer format accordingly (Appendix B.1). We keep all other environment rules the same as previous experiments, except for setting the maximum allowed steps to 15 which is consistent with the default setting of WebArena. Results We present our results on General Web Automation in Table 3. Continuing the patterns from previous experiments, our proposed architecture improves over BrowsingAgent by up to 91.7%, while within our architecture, world model reasoning "}
{"id": "2507.23773v1_chunk52", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "eb Automation in Table 3. Continuing the patterns from previous experiments, our proposed architecture improves over BrowsingAgent by up to 91.7%, while within our architecture, world model reasoning improves over autoregressive reasoning by 21.1%, highlighting the comparative advantage under the given experimental setup. Method Success Rate (%) OpenHands BrowsingAgent 12.0 SIMURA(Ours) Autoregressive Planning 19.0 Ours (World Model Planning) 23.0 Table 3: Results on a random 100-sample subset of WebArena. Our architecture improves over BrowsingAgent by up to 91.7%, while world model planning improves over autoregressive planning by 21.1%. 5 Limitations Due to the modular pipeline and thorough exploration of multiple plans in world model planning, the current agent takes longer than typical LLM agents to run. Speeding up world-model-based reasoning with appropriate caching and parallelization strategies is an important part of our future work. Agent capabilities can be limited by the t"}
{"id": "2507.23773v1_chunk53", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "l LLM agents to run. Speeding up world-model-based reasoning with appropriate caching and parallelization strategies is an important part of our future work. Agent capabilities can be limited by the tooling. For example, with open-source browser environments, web agents are often blocked by Captcha or anti-scraping tools from certain websites. Deeper integration with user browser can help solve this issue. As agent-based automation become more integrated into browsing and computer-use workflows, we also encourage conversations around fair use and protocols around agent access of certain websites. We are currently only using the text portion of the webpage observations, which can miss information like images and layout information (e.g., occlusions). While existing work are experimenting with visual-based web browsing, it is still 13challenging to combine multimodal perception and planning, which we are excited to keep working on. 6 Conclusion In this paper, we have presented SIMURA, a "}
{"id": "2507.23773v1_chunk54", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "th visual-based web browsing, it is still 13challenging to combine multimodal perception and planning, which we are excited to keep working on. 6 Conclusion In this paper, we have presented SIMURA, a general goal-oriented architecture for optimal agent decision-making. Empowered by simulation-based planning using world model and modeling of agent internal belief space activities using natural language as latent representation, we see significant and strong improvements on a range of tasks in web browsing experiments, with world model-based reasoning showing improved reasoning capacity compared to LLM autoregressive reasoning. We are very excited about the possibilities for a single, general, superintelligent agent, but are also keenly aware of the risks for individuals and societies. On the capability side, we aim to test on more types of environments (e.g., software development) and continue developing functional components that strengthen the agent (e.g., multi-agent interaction and "}
{"id": "2507.23773v1_chunk55", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "e capability side, we aim to test on more types of environments (e.g., software development) and continue developing functional components that strengthen the agent (e.g., multi-agent interaction and long-term memory). On the safety / alignment side, we look forward to engaging the community in discussions about how to ensure such an agent stays aligned with our shared values, priorities, and welfare. Acknowledgement This work was supported in part by the Samsung GRO Project “Efficient Designs for Generative and Agent LLM Development.” We thank Zhoujun Cheng, Shibo Hao, and Xinyu Pi from MixLab; Han Guo, Nicholas Ho, and Bowen Tan from SAILING Lab; Li Erran Li from AWS, Zora Wang from NeuLab; and Sarah Cheah and Hector Ren from MBZUAI for their insightful feedback and discussions. We are also grateful for their helpful suggestions throughout the project. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily"}
{"id": "2507.23773v1_chunk56", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "lso grateful for their helpful suggestions throughout the project. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Samsung. References [1]OpenAI. Computer -using agent (cua). https://openai.com/index/ computer-using-agent/ , January 2025. Research preview of “Operator”, published January 23, 2025. [2]DeepMind. Project mariner. https://deepmind.google/models/project-mariner/ , 2024. Accessed: 2025-07-16. [3]Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854 , 2023. [4]Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Oswo"}
{"id": "2507.23773v1_chunk57", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, 2024. [5]OpenAI. Introducing Deep Research. https://openai.com/index/ introducing-deep-research/ , February 2025. Deep Research agent release an- nouncement. [6]Anthropic. Introducing computer use, a new claude 3.5 sonnet, and claude 3.5 haiku. https: //www.anthropic.com/news/3-5-models-and-computer-use , October 22 2024. Public beta “computer use” feature for Claude 3.5 Sonnet. [7]Google. Gemini Deep Research: Your Personal Research Assistant. https://gemini. google/overview/deep-research/?hl=en , December 2024. Overview of Gemini Deep Research agent feature. [8]Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactiv"}
{"id": "2507.23773v1_chunk58", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " 2024. Overview of Gemini Deep Research agent feature. [8]Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior, 2023. 14[9]Anysphere Inc. Cursor: The ai code editor. https://cursor.com , 2025. Accessed: 2025-07- 16. [10] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for ai software developers as generalist agents, 2025. [11] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, A"}
{"id": "2507.23773v1_chunk59", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "u, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, Yossi Matias, Andrew Carroll, Kavita Kulkarni, Nenad Tomasev, Yuan Guan, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Tiago R D Costa, José R Penadés, Gary Peltz, Yunhan Xu, Annalisa Pawlosky, Alan Karthikesalingam, and Vivek Natarajan. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864 , February 2025. URL: https://storage.googleapis.com/coscientist_paper/ai_coscientist.pdf . [12] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search, 2025. [13] Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, Yuheng Cheng, Suyuchen Wang, Xiao"}
{"id": "2507.23773v1_chunk60", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "very via agentic tree search, 2025. [13] Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, Yuheng Cheng, Suyuchen Wang, Xiaoqiang Wang, Yuyu Luo, Haibo Jin, Peiyan Zhang, Ollie Liu, Jiaqi Chen, Huan Zhang, Zhaoyang Yu, Haochen Shi, Boyan Li, Dekun Wu, Fengwei Teng, Xiaojun Jia, Jiawei Xu, Jinyu Xiang, Yizhang Lin, Tianming Liu, Tongliang Liu, Yu Su, Huan Sun, Glen Berseth, Jianyun Nie, Ian Foster, Logan Ward, Qingyun Wu, Yu Gu, Mingchen Zhuge, Xiangru Tang, Haohan Wang, Jiaxuan You, Chi Wang, Jian Pei, Qiang Yang, Xiaoliang Qi, and Chenglin Wu. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems, 2025. [14] Trishna Chakraborty, Udita Ghosh, Xiaopan Zhang, Fahim Faisal Niloy, Yue Dong, Jiachen Li, Amit K. Roy-Chowdhury, and Chengyu Song. Heal: An empirical study on hallucinations in embodied agents driven by large language models, 2025. "}
{"id": "2507.23773v1_chunk61", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " Xiaopan Zhang, Fahim Faisal Niloy, Yue Dong, Jiachen Li, Amit K. Roy-Chowdhury, and Chengyu Song. Heal: An empirical study on hallucinations in embodied agents driven by large language models, 2025. [15] Cewu Lu and Shiquan Wang. The general-purpose intelligent agent. Engineering , 6(3):221–226, 2020. [16] Jacob Andreas. Language models as agent models, 2022. [17] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations , 2023. [18] Yann LeCun. A path towards autonomous machine intelligence. OpenReview preprint, June 2022. Version 0.9.2, June 27 2022. [19] Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221 ,"}
{"id": "2507.23773v1_chunk62", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221 , 2024. [20] Brandon Chiou, Mason Choey, Mingkai Deng, Jinyu Hou, Jackie Wang, Ariel Wu, Frank Xu, Zhiting Hu, Hongxia Jin, Li Erran Li, Graham Neubig, Yilin Shen, and Eric P. Xing. Reasoneragent: A fully open source, ready-to-run agent that does research in a web browser and answers your queries, February 2025. [21] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: A large language model-based web navigating agent, 2024. [22] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents, 2024. 15[23] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda"}
{"id": "2507.23773v1_chunk63", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "n, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents, 2024. 15[23] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. Ui-tars: Pioneering automated gui interaction with native agents, 2025. [24] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory, 2024. [25] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. V oyager: An open-ended embodied agent with large language models, 2023. [26] Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, "}
{"id": "2507.23773v1_chunk64", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " and Anima Anandkumar. V oyager: An open-ended embodied agent with large language models, 2023. [26] Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232 , 2024. [27] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action- conditional video prediction using deep networks in atari games, 2015. [28] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. Nature , 588(7839):604–609, December 2020. [29] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization, 2021. [30] Nicklas Hansen, Xiaolon"}
{"id": "2507.23773v1_chunk65", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ature , 588(7839):604–609, December 2020. [29] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization, 2021. [30] Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive control, 2022. [31] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model, 2023. [32] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models, 2024. [33] Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, and Yu Su. Is your llm secretly a world model of the internet? model-based planning for web agents, 2025. [34] Lisa Feldman Barrett. How emotions are made: The secret life of the brain . Pan Macmillan, 2017. [35] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge,"}
{"id": "2507.23773v1_chunk66", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "agents, 2025. [34] Lisa Feldman Barrett. How emotions are made: The secret life of the brain . Pan Macmillan, 2017. [35] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024. [36] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models, 2024. [37] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2024. [38] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdar"}
{"id": "2507.23773v1_chunk67", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2024. [38] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis, 2024. 16[39] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In International Conference on Learning Representations (ICLR) , 2018. [40] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web, 2023. [41] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents, 2023. [42] Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Q"}
{"id": "2507.23773v1_chunk68", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ebshop: Towards scalable real-world web interaction with grounded language agents, 2023. [42] Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, and Guohao Li. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation, 2025. [43] Xun Jiang, Feng Li, Han Zhao, Jiahao Qiu, Jiaying Wang, Jun Shao, Shihao Xu, Shu Zhang, Weiling Chen, Xavier Tang, Yize Chen, Mengyue Wu, Weizhi Ma, Mengdi Wang, and Tianqiao Chen. Long term memory: The foundation of ai self-evolution, 2025. [44] Wentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorchestra: A hierarchical multi-agent framework for general-purpose task solving, 2025. [45] Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang, Zhu, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter C"}
{"id": "2507.23773v1_chunk69", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "sk solving, 2025. [45] Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang, Zhu, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter Chang, Ricky Loynd, Robert West, Victor Dibia, Ahmed Awadallah, Ece Kamar, Rafah Hosn, and Saleema Amershi. Magentic-one: A generalist multi-agent system for solving complex tasks, 2024. [46] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents, 2024. [47] Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunismäki. ‘smolagents‘: a smol library to build great agentic systems. https://github. com/huggingface/smolagents , 2025. [48] Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, Xing Zhou, Dongrui Liu, Ling Yang, Yue Wu, Kaixuan Huang, Shilong Liu, Hongru Wang, and Mengdi Wang. Alita: Generalist agent ena"}
{"id": "2507.23773v1_chunk70", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "an, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, Xing Zhou, Dongrui Liu, Ling Yang, Yue Wu, Kaixuan Huang, Shilong Liu, Hongru Wang, and Mengdi Wang. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution, 2025. [49] Eric Xing, Mingkai Deng, Jinyu Hou, and Zhiting Hu. Critiques of world models. arXiv preprint arXiv:2507.05169 , 2025. [50] Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction , volume 1. MIT press Cambridge, 1998. [51] Daniel Kahneman. Thinking, fast and slow . macmillan, 2011. [52] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess- che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas- tering the game of go with deep neural networks and tree search. nature , 529(7587):484–489, 2016. [53] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez"}
{"id": "2507.23773v1_chunk71", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " tering the game of go with deep neural networks and tree search. nature , 529(7587):484–489, 2016. [53] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815 , 2017. [54] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin , 2(4):160–163, 1991. [55] Zhiting Hu and Tianmin Shu. Language models, agent models, and world models: The law for machine reasoning and planning. arXiv preprint arXiv:2312.05230 , 2023. 17[56] OpenAI. Introducing chatgpt search. https://openai.com/index/ introducing-chatgpt-search/ , 2024. Accessed: 2024-12-19. [57] Perplexity. Getting started with perplexity. https://www.perplexity.ai/hub/blog/ getting-started-with-perplexity , 2024. Accessed: 2024-12-19. [58] Yu Gu, Boyuan Zheng"}
{"id": "2507.23773v1_chunk72", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "2024. Accessed: 2024-12-19. [57] Perplexity. Getting started with perplexity. https://www.perplexity.ai/hub/blog/ getting-started-with-perplexity , 2024. Accessed: 2024-12-19. [58] Yu Gu, Boyuan Zheng, Boyu Gou, Kai Zhang, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, and Yu Su. Is your llm secretly a world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559 , 2024. [59] Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents. arXiv preprint arXiv:2407.01476 , 2024. [60] Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. Assistantbench: Can web agents solve realistic and time-consuming tasks? arXiv preprint arXiv:2407.15711 , 2024. [61] Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P Xing, and Zhiting Hu. Compression, transduction, and creation: A unified framework for evaluating natural language generation. arXiv preprint arXiv:210"}
{"id": "2507.23773v1_chunk73", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P Xing, and Zhiting Hu. Compression, transduction, and creation: A unified framework for evaluating natural language generation. arXiv preprint arXiv:2109.06379 , 2021. [62] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G- eval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 2511–2522, Singapore, December 2023. Association for Computational Linguistics. [63] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web agents at solving common knowledge work tasks? In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International"}
{"id": "2507.23773v1_chunk74", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ommon knowledge work tasks? In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning , volume 235 of Proceedings of Machine Learning Research , pages 11642–11662. PMLR, 21–27 Jul 2024. [64] Andrew Zhu, Alyssa Hwang, Liam Dugan, and Chris Callison-Burch. FanOutQA: A multi-hop, multi-document question answering benchmark for large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 18–37, Bangkok, Thailand, August 2024. Association for Computational Linguistics. 18A Details on Web Browsing Environment Example Observation URL https://www.google.com/travel/flights Scroll Position: 0, Window Height: 720, Webpage Height: 3024, Remaining Pixels: 2304, Scrolling Progress: 23.8% RootWebArea ’Google Flights -"}
{"id": "2507.23773v1_chunk75", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "mple Observation URL https://www.google.com/travel/flights Scroll Position: 0, Window Height: 720, Webpage Height: 3024, Remaining Pixels: 2304, Scrolling Progress: 23.8% RootWebArea ’Google Flights - Find Cheap Flight Options & Track Prices’ [149] banner ’’ [160] button ’Main menu’, clickable, expanded=False [161] image ’’ [168] link ’Google’, clickable StaticText ’Skip to main content’ StaticText ’Accessibility feedback’ [186] navigation ’’ [189] link ’Travel’ [193] image ’’ [197] link ’Explore’ [201] image ’’ [205] link ’Flights’ [209] image ’’ [213] link ’Hotels’ [217] image ’’ [221] link ’Vacation rentals’ [225] image ’’ [235] button ’Change appearance’, hasPopup=’menu’, expanded=False [240] image ’’ [249] button ’Google apps’, clickable, expanded=False [250] image ’’ [251] link ’Sign in’, clickable [342] image ’’ StaticText ’Flights’ [346] search ’Flight’ [355] combobox ’Change ticket type. \\u200bRound trip’, live=’polite’, relevant=’additions text’, hasPopup=’listbox’, expanded="}
{"id": "2507.23773v1_chunk76", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " in’, clickable [342] image ’’ StaticText ’Flights’ [346] search ’Flight’ [355] combobox ’Change ticket type. \\u200bRound trip’, live=’polite’, relevant=’additions text’, hasPopup=’listbox’, expanded=False, controls=’i9’ [364] image ’’ [399] button ’1 passenger, change number of passengers.’, hasPopup=’dialog’ [404] image ’’ [406] image ’’ [522] combobox ’Change seating class. \\u200bEconomy’, live=’polite’, relevant=’additions text’, hasPopup=’listbox’, expanded=False, controls=’i22’ [529] image ’’ [576] combobox ’Where from?’ value=’Pittsburgh’, clickable, autocomplete=’inline’, hasPopup=’menu’, expanded=False [580] image ’’ [628] button ’Swap origin and destination.’, disabled=True [631] image ’’ [638] combobox ’Where to?’, clickable, focused, autocomplete=’inline’, hasPopup=’menu’, expanded=False [641] image ’’ generic ’’, hidden=True [690] image ’’ [691] textbox ’Departure’, clickable, describedby=’i32’ [712] textbox ’Return’, clickable, describedby=’i32’ generic ’’, hidden=True [8"}
{"id": "2507.23773v1_chunk77", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "nded=False [641] image ’’ generic ’’, hidden=True [690] image ’’ [691] textbox ’Departure’, clickable, describedby=’i32’ [712] textbox ’Return’, clickable, describedby=’i32’ generic ’’, hidden=True [857] button ’Explore destinations’ [866] Section ’’ [867] heading ’Find cheap flights from Pittsburgh to anywhereMore information on suggested flights.’ [871] button ’More information on suggested flights.’, hasPopup=’menu’ [873] image ’’ [904] list ’’, clickable [905] listitem ’’ StaticText ’Pittsburgh’ [907] listitem ’’ [908] button ’Cleveland’ [909] listitem ’’ [910] button ’Columbus’ [911] listitem ’’ [912] button ’Akron’ StaticText ’San Francisco’ StaticText ’\\$128’ StaticText ’Jan 9 — Jan 16’ StaticText ’1 stop’ StaticText ’ ·’ StaticText ’10 hr 30 min’ StaticText ’New York’ StaticText ’\\$68’ StaticText ’Dec 7 — Dec 14’ 19B Prompts for Web Browsing Implementation Prompt for Agent Identity # Name: Web Browsing Agent # Description: An information and automation assistant who responds to"}
{"id": "2507.23773v1_chunk78", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "$68’ StaticText ’Dec 7 — Dec 14’ 19B Prompts for Web Browsing Implementation Prompt for Agent Identity # Name: Web Browsing Agent # Description: An information and automation assistant who responds to user instructions by browsing the internet. The assistant strives to answer each question accurately, thoroughly, efficiently, and politely, and to be forthright when it is impossible to answer the question or carry out the instruction. The assistant will end the task once it sends a message to the user. # Observation Space: The text representation and screenshot of the part of webpage visible in the viewport of a browser. Here is an abstract description of the information available in the webpage text representation: - Identification Information: - URL: The web address that specifies the location of the webpage. -Document Properties: Attributes such as scroll position and viewport dimensions that describe the current viewing context. - Structural Hierarchy: -Root Element: The primary con"}
{"id": "2507.23773v1_chunk79", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ation of the webpage. -Document Properties: Attributes such as scroll position and viewport dimensions that describe the current viewing context. - Structural Hierarchy: -Root Element: The primary container for the webpage, indicating its overall theme or purpose. -Nested Elements: A hierarchy of sections, containers, and components that organize content logically (e.g., headers, footers, sidebars). - Interactive Components: -Links: Elements that can be clicked to navigate to other pages or sections, often labeled descriptively. -Buttons: Interactive controls that trigger actions (e.g., submitting forms, opening menus). - Content Types: -Text: Main content, headings, and subheadings that provide information and context. -Images and Media: Visual elements that enhance the understanding or appeal of the content. -Forms and Inputs: Fields for user input, including text boxes, dropdowns, and checkboxes. - Functional Areas: -Navigation Menus: Organized sets of links that allow users to expl"}
{"id": "2507.23773v1_chunk80", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ppeal of the content. -Forms and Inputs: Fields for user input, including text boxes, dropdowns, and checkboxes. - Functional Areas: -Navigation Menus: Organized sets of links that allow users to explore different sections of the site. -Search Interface: Components that enable users to search for content within the site, including input fields and associated buttons. - State Information: -Visibility and Expand/Collapse States: Indicators showing whether certain elements are active, visible, or in a collapsed state, impacting user interaction. -Focus States: Information on which elements are currently focused, important for keyboard navigation and accessibility. 20Prompt for Agent Identity (Continued) - Accessibility Features: -Role and Description Information: Metadata that provides context about the purpose of elements, useful for screen readers and assistive technologies. - General User Considerations: - Navigation: Recognizing how to traverse the webpage using links and buttons. - I"}
{"id": "2507.23773v1_chunk81", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "t about the purpose of elements, useful for screen readers and assistive technologies. - General User Considerations: - Navigation: Recognizing how to traverse the webpage using links and buttons. - Interactivity: Understanding how to engage with forms, search fields, and dynamic components. -Content Engagement: Identifying and interpreting various content types to glean necessary information. # Action Space: 13 different types of actions are available. noop(wait_ms: float = 1000) Examples: noop() noop(500) send_msg_to_user(text: str) Examples: send_msg_to_user(’Based on the results of my search, the city was built in 1751.’) scroll(delta_x: float, delta_y: float) Examples: scroll(0, 200) scroll(-50.2, -100.5) fill(bid: str, value: str) Examples: fill(’237’, ’example value’) fill(’45’, ’multi-line\\nexample’) fill(’a12’, ’example with \"quotes\"’) select_option(bid: str, options: str | list[str]) Examples: select_option(’a48’, ’blue’) select_option(’c48’, [’red’, ’green’, ’blue’]) click(b"}
{"id": "2507.23773v1_chunk82", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "lti-line\\nexample’) fill(’a12’, ’example with \"quotes\"’) select_option(bid: str, options: str | list[str]) Examples: select_option(’a48’, ’blue’) select_option(’c48’, [’red’, ’green’, ’blue’]) click(bid: str, button: Literal[’left’, ’middle’, ’right’] = ’left’, modifiers: list[typing.Literal[’Alt’, ’Control’, ’Meta’, ’Shift’]] = []) Examples: click(’a51’) click(’b22’, button=’right’) click(’48’, button=’middle’, modifiers=[’Shift’]) dblclick(bid: str, button: Literal[’left’, ’middle’, ’right’] = ’left’, modifiers: list[typing.Literal[’Alt’, ’Control’, ’Meta’, ’Shift’]] = []) Examples: dblclick(’12’) dblclick(’ca42’, button=’right’) dblclick(’178’, button=’middle’, modifiers=[’Shift’]) 21Prompt for Agent Identity (Continued) hover(bid: str) Examples: hover(’b8’) press(bid: str, key_comb: str) Examples: press(’88’, ’Backspace’) press(’a26’, ’Control+a’) press(’a61’, ’Meta+Shift+t’) focus(bid: str) Examples: focus(’b455’) clear(bid: str) Examples: clear(’996’) drag_and_drop(from_bid: str,"}
{"id": "2507.23773v1_chunk83", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "r) Examples: press(’88’, ’Backspace’) press(’a26’, ’Control+a’) press(’a61’, ’Meta+Shift+t’) focus(bid: str) Examples: focus(’b455’) clear(bid: str) Examples: clear(’996’) drag_and_drop(from_bid: str, to_bid: str) Examples: drag_and_drop(’56’, ’498’) upload_file(bid: str, file: str | list[str]) Examples: upload_file(’572’, ’my_receipt.pdf’) upload_file(’63’, [’/home/bob/Documents/image.jpg’, ’/home/bob/Documents/- file.zip’]) Only a single action can be provided at once. Example: fill(’a12’, ’example with \"quotes\"’) # Instruction: {user_instruction} # Current Date and Time: {current_datetime} Prompt for Encoder # Observation: {observation} # State: Describe all the elements in the current webpage observation. Note any dialogs, progress indicators, or error messages. Include any interactive elements and their values or if they are blank. Note any detailed information such as facts, entities, or data that are relevant to the task. Report any error messages like whether the last action wa"}
{"id": "2507.23773v1_chunk84", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " elements and their values or if they are blank. Note any detailed information such as facts, entities, or data that are relevant to the task. Report any error messages like whether the last action was correct. Try to be as comprehensive and detailed as possible. Wrap your response in the tag <state> and </state>. 22Prompt for Policy {memory} # Current State: {state} # Intent: Describe the action the assistant should take next to carry out the user’s instruction. Avoid using phrases such as \"To accomplish the goal,\" \"I will,\" \"To proceed.\". Avoid ending with phrases like \"to execute the search.\" Describe one action at a time and avoid combining multiple steps. Refrain from mentioning specific element IDs as they may change during execution. Limit your response to one phrase and include any details that help select the correct action. Be creative and propose novel methods to achieve the goal. Avoid creating accounts without user permission or providing personal information. Concrete exa"}
{"id": "2507.23773v1_chunk85", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " details that help select the correct action. Be creative and propose novel methods to achieve the goal. Avoid creating accounts without user permission or providing personal information. Concrete example would be \"Go to the home page of Google Flights.\" and \"Click on the ’Search’ button.\" Wrap your response in the following format: <think> Your thoughts and reasoning process </think> <intent> Description of the action to perform next </intent> Prompt for World Model {memory} # Current State: {state} # Memory Update: {memory_update} # Action Intent: {plan} # Next State: Describe all the elements in the webpage after the agent attempts to carry out the intent. Note that the execution may not be successful, so you will have to infer the result of the action. Note any dialogs, progress indicators, or error messages. Include any interactive elements and their values or if they are blank. Note any detailed information such as facts, entities, or data that are relevant to the task. Report an"}
{"id": "2507.23773v1_chunk86", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "tors, or error messages. Include any interactive elements and their values or if they are blank. Note any detailed information such as facts, entities, or data that are relevant to the task. Report any error messages displayed. Try to be as comprehensive and detailed as possible. Wrap your response in the following format: <next_state> Follow the format of the current state description. Use present tense. Avoid starting phrases like \"Based on the interaction history, current state, and current intent\". </next_state> 23Prompt for Critic {memory} # Final State: {state} # Task Success and Progress: Your task is to evaluate the performance of the agent. Given the agent’s instruction, interaction history, the final state of the webpage, and the agent’s responses to the user if any, your goal is to decide whether the agent’s execution is successful or not. If the current state is a failure but it looks like the agent is on the right track towards success, you should also output as such. Wrap"}
{"id": "2507.23773v1_chunk87", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " to decide whether the agent’s execution is successful or not. If the current state is a failure but it looks like the agent is on the right track towards success, you should also output as such. Wrap your response in the following format: <think> Your thoughts and reasoning process </think> <status> \"success\" or \"failure\" </status> <on_the_right_track> \"yes\" or \"no\" </on_the_right_track> Prompt for Memory Update {memory} # State: {state} # Action Intent: {plan} # Memory Update: Summarize the changes in the webpage observation that should be remembered for achieving your goal and for predicting the next state. Note any new elements, any elements no longer visible, or any changes in the content of existing elements. Also note if there is no change. Include any other inferred information that may help you decide the next action, such as whether an action intent is successful, or whether progress has been made or reversed. Do not include your next planned actions. Revise your belief from "}
{"id": "2507.23773v1_chunk88", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "t may help you decide the next action, such as whether an action intent is successful, or whether progress has been made or reversed. Do not include your next planned actions. Revise your belief from previous history if the current state contradicts it. Wrap your response in the tag <memory_update> and </memory_update>. 24Prompt for Actor {memory} # Observation: {observation} # Current State: {state} # Current Intent: {plan} # Action: Choose an API call that will carry out the intent when executed in the webpage. Use only one action at a time. You must not enclose bid inputs in [brackets] but instead in ’single quotes’. Interact only with elements in the current step observation. Your response will be executed as a Python function call, so ensure it adheres to the format and argument data type specifications defined in the action space. Wrap your response in the tag <action> and </action>. 25Prompt for Action Clustering Here is the action space for a browser agent to navigate in a webp"}
{"id": "2507.23773v1_chunk89", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "type specifications defined in the action space. Wrap your response in the tag <action> and </action>. 25Prompt for Action Clustering Here is the action space for a browser agent to navigate in a webpage: 16 different types of actions are available: noop(wait_ms: float = 1000) send_msg_to_user(text: str) scroll(delta_x: float, delta_y: float) fill(bid: str, value: str) select_option(bid: str, options: str | list[str]) click(bid: str, button: Literal[’left’, ’middle’, ’right’] = ’left’, modifiers: list[typing.Literal[’Alt’, ’Control’, ’Meta’, ’Shift’]] = []) dblclick(bid: str, button: Literal[’left’, ’middle’, ’right’] = ’left’, modifiers: list[typing.Literal[’Alt’, ’Control’, ’Meta’, ’Shift’]] = []) hover(bid: str) press(bid: str, key_comb: str) focus(bid: str) clear(bid: str) drag_and_drop(from_bid: str, to_bid: str) upload_file(bid: str, file: str | list[str]) go_back() go_forward() goto(url: str) Only a single action can be provided at once. Example: fill(’a12’, ’example with \"quote"}
{"id": "2507.23773v1_chunk90", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "p(from_bid: str, to_bid: str) upload_file(bid: str, file: str | list[str]) go_back() go_forward() goto(url: str) Only a single action can be provided at once. Example: fill(’a12’, ’example with \"quotes\"’) Below, you will find lists of intents, or natural language descriptions of actions that, when executed, will translate to one of the function calls above. The intents will be provided in the following JSON format: ‘‘‘json { \"intent_id\": \"intent description\" } ‘‘‘ Your task is to cluster list of intents into semantically equivalent groups, where each group represents intents that lead to the same action when executed (i.e., navigating to the Google homepage is translated to goto(’https://www.google.com’)) and would therefore correspond to the same API call in a Playwright browser. Intents that use different wording but convey the same action should be grouped together. Try to minimize the number of clusters. 26Prompt for Action Clustering (Continued) Represent the clustering results us"}
{"id": "2507.23773v1_chunk91", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "that use different wording but convey the same action should be grouped together. Try to minimize the number of clusters. 26Prompt for Action Clustering (Continued) Represent the clustering results using a JSON object where each cluster has a unique identifier, and each identifier maps to a list of actions in that cluster. See below for an abstract example: ‘‘‘json { \"cluster_id\": { \"intent\": \"representative intent name for this cluster\", \"candidates\": [ \"<list of intent ids that belong to this cluster> ] } } ‘‘‘ Concrete Example 1: Dictionary of Intents: ‘‘‘json { \"0\": \"Navigate to the Google homepage by entering its URL.\", \"1\": \"Go to the Google homepage.\", \"2\": \"Go to the Google homepage\", \"3\": \"Go to the Google homepage by navigating to ’https://www.google.com’\", \"4\": \"Go to the home page of Google\" } ‘‘‘ [\"Navigate to the Google homepage by entering its URL.\", \"Go to the Google home- page.\", \"Go to the Google homepage\", \"Go to the Google homepage by navigating to ¨https://www.goog"}
{"id": "2507.23773v1_chunk92", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "age of Google\" } ‘‘‘ [\"Navigate to the Google homepage by entering its URL.\", \"Go to the Google home- page.\", \"Go to the Google homepage\", \"Go to the Google homepage by navigating to ¨https://www.google.com ¨\", \"Go to the home page of Google\"] Clustering Results: ‘‘‘json { \"cluster_1\": { \"intent\": \"Navigate to the Google homepage\", \"candidates\": [0, 1, 2, 3, 4] } } ‘‘‘ Concrete Example 2: Dictionary of Intents: {action_candidate_json} Clustering Results: 27B.1 Adaptation for WebArena Evaluation Agent Description for WebArena Evaluation An information and automation assistant that interacts with the browser and responds to user instructions. The response follows the following rules: 1. When the intent is a question, and a complete answer to the question has been found, then send the answer to the user; 2. the intent wants to locate specific information or navigate to a particular section of a site, and the current page satisfies, then stop and tell the user you found the required inform"}
{"id": "2507.23773v1_chunk93", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "to the user; 2. the intent wants to locate specific information or navigate to a particular section of a site, and the current page satisfies, then stop and tell the user you found the required information; 3. the intent want to conduct an operation, and has been done, then stop and tell the user the operation has been completed. The assistant should try to achieve the goal in the current site without navigating to sites like Google. Be forthright when it is impossible to answer the question or carry out the task. The assistant will end the task once it sends a message to the user. 28C Prompts for Generating and Evaluating on the FlightQA Dataset Prompt for Generating Initial Constraints and Questions System : You are a creative writer who is an expert at crafting questions to help train assistants who answer user queries. Current date and time: {current_datetime} Instruction : Your task is to create a robust benchmark for evaluating an AI’s ability to search for flights through a plat"}
{"id": "2507.23773v1_chunk94", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ssistants who answer user queries. Current date and time: {current_datetime} Instruction : Your task is to create a robust benchmark for evaluating an AI’s ability to search for flights through a platform like Google Flights. To ensure the dataset effectively represents real-world use cases. Here are some important factors to consider: 1. Diversity of Queries - Range of Destinations: Include both common and obscure destinations to test how well the model handles varying levels of demand. - Dates and Duration: Include different date ranges, including last-minute flights, peak travel dates (like holidays), and extended trips. Ensure there’s a variety in trip duration as well. - Passenger Variability: Include solo travelers, families, and group travel (e.g., one adult vs. two adults and two children) since these change the search parameters and price results. - Class and Preference: Vary preferences like cabin class (economy, business, first) and filters (non-stop, one stop, preferred air"}
{"id": "2507.23773v1_chunk95", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "hildren) since these change the search parameters and price results. - Class and Preference: Vary preferences like cabin class (economy, business, first) and filters (non-stop, one stop, preferred airlines, etc.). - Budget Constraints: Test price sensitivity by setting different budget limits to see how well the AI handles trade-offs. 2. Complexity of Requirements - Multi-Leg Flights: Add queries for multi-city trips or those requiring complex layovers. - Dynamic Constraints: Include queries with dynamic constraints, such as “find the cheapest flight but depart between 8-10 AM,” to see if the model can adapt its search within specific time frames. - Conditional Preferences: Test cases where users might want options based on multiple conditions, like “either find the cheapest non-stop or the shortest two-stop option.” In practice, the questions typically vary in the following dimensions: - Ticket type (round-trip, one-way, etc.) - Routes (origin and destination) - Layover location(s) - "}
{"id": "2507.23773v1_chunk96", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "e shortest two-stop option.” In practice, the questions typically vary in the following dimensions: - Ticket type (round-trip, one-way, etc.) - Routes (origin and destination) - Layover location(s) - Dates (departure and/or return) - Flight time (departure and arrival) - Total flight time - Airlines - Cabin class (economy, business, etc.) - Aircraft - Eco-friendly options (CO2 Emissions) Given a number of constraints, you should first provide a list of constraints, with the number of constraints equal to the specification. After that, you will generate a question a typical user will ask which imposes those constraints. You should re- peat this for at least 7 times to generate a set of questions with simple language. Make sure that the number of constraints in the question matches the number of constraints specified. Do not include constraints about the number of passengers. If the constraint is a date, you can use relative dates (e.g., \"tomorrow\", \"next month\", \"after 8 PM\", etc.). Avo"}
{"id": "2507.23773v1_chunk97", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "r of constraints specified. Do not include constraints about the number of passengers. If the constraint is a date, you can use relative dates (e.g., \"tomorrow\", \"next month\", \"after 8 PM\", etc.). Avoid using specific dates like \"December 25th\" to ensure the questions are relevant throughout the year. Your response should follow the JSON format below: 29Prompt for Generating Initial Constraints and Questions (Continued) Number of Constraints: <num_constraints> { \"num_constraints\": <num_constraints>, \"questions\": [ { \"constraints\": [<constraints>], \"question\": <question> }, ... ] } Below is a concrete example: Number of Constraints: 3 { \"num_constraints\": 3, \"questions\": [ { \"constraints\": [\"one-way\", \"New York to London\", \"departing next Friday\"], \"question\": \"Can you find a one-way flight from New York to London departing next Friday?\" }, ... ] } 30Prompt for Iteratively Expanding Constraints and Questions System : [Same as above] Instruction : [Same as above until “Your response shou"}
{"id": "2507.23773v1_chunk98", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " from New York to London departing next Friday?\" }, ... ] } 30Prompt for Iteratively Expanding Constraints and Questions System : [Same as above] Instruction : [Same as above until “Your response should follow”] Your response should follow the JSON format below: Maximum number of constraints: <max_constraints> Starting constraints and questions: { \"num_constraints\": <num_constraints>, \"constraints\": [<constraints>], \"question\": <question> } Questions with increasing complexity: { \"questions\": [ { \"num_constraints\": <starting num_constraints + 1>, \"constraints\": [<previous constraints with 1 additional>], \"question\": <question> }, { \"num_constraints\": <starting num_constraints + 2>, \"constraints\": [<previous constraints with 2 additional>], \"question\": <question> }, ... (continue until reaching the maximum number of constraints) ] } Your Response: Maximum number of constraints: {max_num_constraints} Starting constraints and questions: {starting_constraint_questions} Questions with incre"}
{"id": "2507.23773v1_chunk99", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "hing the maximum number of constraints) ] } Your Response: Maximum number of constraints: {max_num_constraints} Starting constraints and questions: {starting_constraint_questions} Questions with increasing complexity: 31Prompt for Evaluation # Interaction Date and Time: {interaction_datetime} # Interaction History: [Concatenation of observations from all steps] Above are the webpages an assistant interacted with while trying to answer the user’s query. The user is looking for flights with the following constraints: {constraints} Here is the exact query provided by the user: {goal} Here is the assistant’s response: {message} Your task is to evaluate two aspects of the response: 1) Whether the assistant’s response is supported by the interaction history, and 2) Whether the assistant’s response satisfies the user constraints to the extent al- lowed by the results. Some Context: - To determine the seating class of a flight being returned, refer to the value of the \"Change seating class\" co"}
{"id": "2507.23773v1_chunk100", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "se satisfies the user constraints to the extent al- lowed by the results. Some Context: - To determine the seating class of a flight being returned, refer to the value of the \"Change seating class\" combobox. - It is not always possible to satisfy all the user constraints. In this case, examine whether the response is as close to the user constraints as possible. Answer in the following format: <think> Your thoughts and reasoning. </think> <grounding> Your assessment of whether the response is supported by the interaction history. Answer \"yes\" or \"no\" </grounding> <relevance> Your assessment of whether the response satisfies the user constraints to the extent allowed by the results. Answer \"yes\" or \"no\" </relevance> 32"}
{"id": "2507.23735v1_chunk0", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "1 Distributed AI Agents for Cognitive Underwater Robot Autonomy Markus Buchholz, Ignacio Carlucho, Michele Grimaldi, and Yvan R. Petillot Abstract —Achieving robust cognitive autonomy in robots navi- gating complex, unpredictable environments remains a fundamen- tal challenge in robotics. This paper presents Underwater Robot Self-Organizing Autonomy (UROSA), a groundbreaking architec- ture leveraging distributed Large Language Model AI agents inte- grated within the Robot Operating System 2 (ROS 2) framework to enable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA decentralises cognition into specialised AI agents responsible for multimodal perception, adaptive reasoning, dynamic mission planning, and real-time decision-making. Central innovations include flexible agents dynamically adapting their roles, retrieval-augmented generation utilising vector databases for efficient knowledge management, reinforcement learning- driven behavioural optimisation, and aut"}
{"id": "2507.23735v1_chunk1", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " agents dynamically adapting their roles, retrieval-augmented generation utilising vector databases for efficient knowledge management, reinforcement learning- driven behavioural optimisation, and autonomous on-the-fly ROS 2 node generation for runtime functional extensibility. Extensive empirical validation demonstrates UROSA’s promising adaptability and reliability through realistic underwater missions in simulation and real-world deployments, showing significant advantages over traditional rule-based architectures in handling unforeseen scenarios, environmental uncertainties, and novel mission objectives. This work not only advances underwater autonomy but also establishes a scalable, safe, and versatile cognitive robotics framework capable of generalising to a diverse array of real-world applications.1 I. I NTRODUCTION The pursuit of truly autonomous robots capable of effec- tively navigating and interacting within complex, unstructured environments remains a grand challenge in the"}
{"id": "2507.23735v1_chunk2", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ld applications.1 I. I NTRODUCTION The pursuit of truly autonomous robots capable of effec- tively navigating and interacting within complex, unstructured environments remains a grand challenge in the field of robotics [1]. For decades, the dominant paradigm has been rooted in traditional robotic systems, which rely on modular architectures and meticulously pre-defined, rule-based algorithms [2]. While these systems have demonstrated notable success in controlled and predictable settings such as factory automation, they increasingly reveal their limitations when confronted with the inherent dynamism and uncertainty of real-world scenarios [3]. Legacy approaches often struggle with novel or unforeseen situations, demanding extensive manual reprogramming for even minor environmental changes and fundamentally lacking the adaptability needed for open-ended tasks [4]. This inherent inflexibility prevents them from truly operating in a self-playing manner, where the system autonomously adapt"}
{"id": "2507.23735v1_chunk3", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "and fundamentally lacking the adaptability needed for open-ended tasks [4]. This inherent inflexibility prevents them from truly operating in a self-playing manner, where the system autonomously adapts and solves problems without continuous human intervention. Within this evolving landscape, the rapid advancement of Large Language Models (LLMs) and Vision Language Models (VLMs) offers a compelling pathway toward more flexible, adaptive, and robust robot autonomy. Unlike conventional M. Buchholz, I. Carlucho, M. Grimaldi, and Y .R. Petillot are with the School of Engineering & Physical Sciences, Heriot-Watt University, Edinburgh, UK. e-mail: m.buchholz@hw.ac.uk. 1UROSA project webpage: https://markusbuchholz.github.io/urosa.htmlsoftware, which executes a rigid set of programmed instructions, AI agents are designed to understand complex information, reason about it, and generate a sequence of actions to perform specific tasks [5], [6]. This approach fundamentally shifts the development p"}
{"id": "2507.23735v1_chunk4", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": ", AI agents are designed to understand complex information, reason about it, and generate a sequence of actions to perform specific tasks [5], [6]. This approach fundamentally shifts the development paradigm from imperative programming, where every step of a task is explicitly coded (e.g., move forward 1 meter, turn left 90 degrees ), to declarative goal setting, allowing the agent to autonomously determine how to fulfill its mission (e.g., inspect the valve ). By leveraging vast pre-trained knowledge repositories, AI agents exhibit emergent reasoning capabilities that can adapt to new tasks and conditions without requiring exhaustive manual reprogramming [7], [8]. However, the integration of these agents in robotic applications introduces new challenges, such as ensuring robustness and verifiability of the decision made and protecting against possible hallucinations [9], [10]. The increasing capabilities of advanced AI systems also bring to the forefront critical considerations relate"}
{"id": "2507.23735v1_chunk5", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rifiability of the decision made and protecting against possible hallucinations [9], [10]. The increasing capabilities of advanced AI systems also bring to the forefront critical considerations related to AI safety and verification, encompassing complex issues such as alignment of the mental models of the operators and the AI system [11]–[13]. UROSA is inspired by ROSA [11], which is used as a parser from high level natural language to generate actions through ROS messages. Unlike ROSA, our work integrates agentic AI at multiple levels of the autonomy architecture, able to make decisions as well as communicate through ROS. This is a novel and highly capable autonomy framework that fundamentally replaces the traditional paradigm of a human-governed main program with a distributed network of specialised and dynamically adaptable AI agents . This distributed architecture represents a radical shift towards true autonomy, where once a mission is initiated with high-level commands, the syste"}
{"id": "2507.23735v1_chunk6", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " specialised and dynamically adaptable AI agents . This distributed architecture represents a radical shift towards true autonomy, where once a mission is initiated with high-level commands, the system operates with minimal human interaction and oversight. The distributed AI agents communicate and solve problems together, without needing human intervention or the development of coding or extra programmatic loops. This allows for emergent system-level intelligence, where human interaction primarily involves providing high-level commands through natural language. Each agent is responsible for a specific aspect of the robot’s operational workflow, ranging from multimodal perception (e.g., vision, depth, sonar) to high- level strategic planning. Crucially, the AI agents within UROSA are not merely generative; they are designed as agentic AI entities , capable of autonomous decision-making and action- taking within their environment without requiring continuous human intervention in the loo"}
{"id": "2507.23735v1_chunk7", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rely generative; they are designed as agentic AI entities , capable of autonomous decision-making and action- taking within their environment without requiring continuous human intervention in the loop [14], [15]. The realisation of this distributed cognitive architecture is founded upon the following key innovations that form the core of our UROSA framework:arXiv:2507.23735v1 [cs.RO] 31 Jul 20252 •Decoupled Reasoning and Environmental Adaptabil- ity: The framework achieves architectural flexibility by replacing traditional code-based logic with pretrained AI agents that handle system functions. Domain experts, for example a Remotely Operated Vehicle (ROV) pilot, can build systems using these existing agents without extensive reengineering. The system is adaptable in near-real time to a large variety of changes in the environment and internal states, and can access a much larger set of data through its ability to understand descriptions in natural language accessible from a variety of "}
{"id": "2507.23735v1_chunk8", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rge variety of changes in the environment and internal states, and can access a much larger set of data through its ability to understand descriptions in natural language accessible from a variety of sources, e.g., live meteorological ocean data, vehicle design. •Behaviour adaptation and lifelong learning : The system is also able to learn and adapt in real-time to improve existing agentic behaviour or generate new ones, based on prompts from operators or other agents. It can also improve over time by using previous experiences and operator feedback. This flexibility is supported by a Vector Database (VDB) that stores and retrieves past experiences, observational data, simulator outcomes and external knowledge. This comprehensive data forms the foundation for Retrieval-Augmented Generation (RAG), facilitating context-driven and informed decision-making. •Autonomous On-the-Fly Function Extension : The sys- tem can generate code on the fly to deal with unforeseen circumstances to extend "}
{"id": "2507.23735v1_chunk9", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " (RAG), facilitating context-driven and informed decision-making. •Autonomous On-the-Fly Function Extension : The sys- tem can generate code on the fly to deal with unforeseen circumstances to extend its functionality dynamically based on real-time requirements identified autonomously by the Planning Agent. This means that if a new functional component is needed for the mission, the Planning Agent can request its generation, testing, and integration without human code intervention. These capabilities can be further refined through online, agent-driven instructional tuning, where agents learn optimal response strategies through iterative, interactive feedback. The new code can be validated in simulation before being deployed on the real platform. This enables unprecedented adaptability in response to evolving mission demands and environmental contingencies. •Dynamic, Predictive System Diagnostics : The system is able to perform diagnostics of its internal state and environmental conditi"}
{"id": "2507.23735v1_chunk10", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "esponse to evolving mission demands and environmental contingencies. •Dynamic, Predictive System Diagnostics : The system is able to perform diagnostics of its internal state and environmental conditions, without the requirement of a predefined static fault tree or a fixed set of unit tests. •Inherent Safety and Control : UROSA explicitly ad- dresses aspects of AI safety and control. Through the introduction of several robust ROS 2 mechanisms and architectural constraints, such as a dedicated safety parser and a layered design, we introduce fine-grained control over the agents’ outputs and behaviours. This framework aims to mitigate hallucinations and increase the likelihood of actions aligning with predefined safety protocols, thereby contributing to the critical goal of aligning AI with human values in autonomous operations. While UROSA leverages contemporary LLMs and VLMs, its foundational architecture is inherently model-agnostic and designed with a forward-looking perspective. Rec"}
{"id": "2507.23735v1_chunk11", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "human values in autonomous operations. While UROSA leverages contemporary LLMs and VLMs, its foundational architecture is inherently model-agnostic and designed with a forward-looking perspective. Recognising the rapid advancements in artificial intelligence its modular design allows for the seamless integration of future, more advancedAI paradigms as they emerge. We aim to propose a general concept for distributed cognitive autonomy that can evolve and benefit from future breakthroughs in AI, ensuring its continued relevance and adaptability. We validate each of the key innovations previously outlined through a series of demanding use-case scenarios. These empirical tests provide concrete evidence of the system’s performance in areas such as real-time adaptation, autonomous code generation, continuous learning, and advanced diagnostics. To further illustrate these capabilities, this paper is accompanied by a supplementary video2demonstrating our key experiments in both simulation and "}
{"id": "2507.23735v1_chunk12", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "n, continuous learning, and advanced diagnostics. To further illustrate these capabilities, this paper is accompanied by a supplementary video2demonstrating our key experiments in both simulation and real-world deployments. Finally, we analyse these results to address the critical challenges of AI safety and verifiability, and discuss the future trajectory of distributed cognitive architectures like UROSA in an era of rapidly advancing artificial intelligence. II. L ITERATURE OVERVIEW : THEASCENT OF COGNITIVE AUTONOMY IN ROBOTICS The pursuit of cognitive autonomy has historically navigated a path between symbolic reasoning and reactive, embodied in- telligence. Early endeavours, influenced by symbolic AI, sought to replicate cognition through formal logic and planning [2], [16], [17], but these top-down systems often proved unstable when faced with the uncertainty and dynamism of the real world [3], [18], [19]. In response, a bottom-up approach emerged with behaviour-based robotics and"}
{"id": "2507.23735v1_chunk13", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "these top-down systems often proved unstable when faced with the uncertainty and dynamism of the real world [3], [18], [19]. In response, a bottom-up approach emerged with behaviour-based robotics and the principle of embodied, situated cognition, which emphasised that intelligence arises from the direct sensorimotor interaction between an agent and its environment [20]–[23]. While architectures like subsumption produced remarkably robust reactive behaviours [4], they lacked the capacity for abstract reasoning and complex planning. Cognitive architectures such as SOAR [7] and ACT-R [8], [24] attempted to bridge this gap by integrating symbolic processing with more plausible cognitive mechanisms, yet often struggled with the knowledge acquisition bottleneck and extensive hand- engineering required for novel domains [25]. The recent advent of LLMs has been a paradigm shift, offer- ing powerful new avenues for high-level robotic cognition [26]. Trained on vast datasets, models like GPT-4 "}
{"id": "2507.23735v1_chunk14", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "equired for novel domains [25]. The recent advent of LLMs has been a paradigm shift, offer- ing powerful new avenues for high-level robotic cognition [26]. Trained on vast datasets, models like GPT-4 have demonstrated remarkable emergent capabilities in reasoning and natural language understanding [5], [6], [27], [28], making them potent tools for robotics [29]. Research has rapidly demonstrated their efficacy in translating high-level natural language commands into actionable plans, as seen in works like SayCan [30], [31]. This has been extended by grounding language in rich perceptual data using VLMs [32]–[36], generating executable robot code from text [37], [38], and leveraging interactive reasoning paradigms such as ReAct [39] for complex, embodied problem-solving. However, integrating LLMs effectively and safely into robotic systems introduces a new set of research challenges, moving the frontier from single-model planning to robust, multi-agent architectures. The concept of a si"}
{"id": "2507.23735v1_chunk15", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "g LLMs effectively and safely into robotic systems introduces a new set of research challenges, moving the frontier from single-model planning to robust, multi-agent architectures. The concept of a single, monolithic robot brain gives way to distributed cognitive architectures, which promise greater modularity, fault tolerance, and emergent 2The video is available at: https://markusbuchholz.github.io/urosa.html3 intelligence from the coordination of multiple specialised agents [40]–[43]. Realising this vision of true cognitive autonomy [44]– [46] requires overcoming significant hurdles. These include ensuring robustness against model hallucinations [9], [10], [47], achieving real-time performance on resource-constrained hardware [48], and addressing the critical challenges of AI safety, verification, and ethical alignment in safety-critical applications [49]–[52]. This paper introduces UROSA, a novel distributed AI agent architecture designed to directly address this new frontier. Our "}
{"id": "2507.23735v1_chunk16", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rification, and ethical alignment in safety-critical applications [49]–[52]. This paper introduces UROSA, a novel distributed AI agent architecture designed to directly address this new frontier. Our work moves beyond using a single LLM for planning and instead proposes a collaborative ecosystem of specialised agentic nodes integrated deeply within the ROS 2 framework. By decentralising cognition, UROSA enhances robustness and scalability. It explicitly tackles the challenges of reliability and safety through mechanisms such as RAG for factual grounding, per-agent safety parsers to validate outputs, and dynamic, on-the-fly function generation to adapt to unforeseen circumstances. Through the design and extensive empirical validation of this framework, we demonstrate a concrete and scalable pathway toward more capable, adaptable, and reliable cognitive autonomy in real-world robotic systems. III. A RCHITECTURE The UROSA framework fundamentally reimagines robotic autonomy by replacing a "}
{"id": "2507.23735v1_chunk17", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " pathway toward more capable, adaptable, and reliable cognitive autonomy in real-world robotic systems. III. A RCHITECTURE The UROSA framework fundamentally reimagines robotic autonomy by replacing a single, monolithic control program with a distributed cognitive architecture, as depicted in the overview in Fig. 1. This design is rooted in the principles of embodied and situated cognition [21]–[23], where intelligence emerges from the dynamic interaction between multiple spe- cialised agents and their environment. At its core, UROSA de- centralizes high-level reasoning across a network of AI agents, each functioning as an intelligent, collaborative unit. These agents leverage the robust communication backbone of ROS 2 to coordinate their actions and share knowledge. This hybrid architecture follows a clear separation: deterministic, low-level controllers handle time-critical tasks like stabilization, while the distributed AI agents perform the high-level cognitive functions of reasonin"}
{"id": "2507.23735v1_chunk18", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "e follows a clear separation: deterministic, low-level controllers handle time-critical tasks like stabilization, while the distributed AI agents perform the high-level cognitive functions of reasoning, planning, and adapting to novel situations. The fundamental building block of this architecture is the Agentic ROS 2 Node . As detailed at the top right of Fig. 1, this is not a standard ROS 2 node but a composite entity. It fuses the high-level reasoning of an AI Agent with the robust communication of its ROS 2 Node Implementation. Each of these nodes encapsulates the core AI Reasoner , a critical Safety Parser to validate all outputs, and standard Publishers/Subscribers for system-wide communication. This modular design is a core innovation of UROSA, allowing specialised intelligence to be embedded directly within the robotics framework. A. The Set of Cognitive Intelligent Agents The cognitive core of UROSA is not a single brain but a distributed team of agents with distinct roles, al"}
{"id": "2507.23735v1_chunk19", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "e embedded directly within the robotics framework. A. The Set of Cognitive Intelligent Agents The cognitive core of UROSA is not a single brain but a distributed team of agents with distinct roles, all communicating over ROS 2.a) The Commander AI Agent: This agent serves as the central cognitive orchestrator, akin to a vessel’s captain. Upon receiving a high-level mission description, it utilises techniques inspired by Chain-of-Thought prompting to decompose the goal into a coherent sequence of sub-tasks for the specialist agents [53]. Its primary role is to maintain cognitive consistency by employing a reflection and revision cycle, similar to the self-correction processes described in state-of-the-art agent architectures [54]. b) The Autonomous Crew (Specialist Agents): A suite of specialist agents, analogous to an expert crew, handles specific functional domains. The agents shown in Fig. 1 are key examples, including the Perception & Scene Reasoning Agent , theMotion Planning Agent "}
{"id": "2507.23735v1_chunk20", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "list agents, analogous to an expert crew, handles specific functional domains. The agents shown in Fig. 1 are key examples, including the Perception & Scene Reasoning Agent , theMotion Planning Agent , the Autonomous Code Synthesis Agent , the Predictive Diagnostics Agent , the Capability Assessment Agent , and the Digital Twin Curator Agent . This set is not fixed; the modularity of the framework allows for the seamless integration of new agents as mission complexity demands. c) Shared Resources (ROS 2 & VDB): All agents are connected by the ROS 2 framework, which serves as the distributed communication backbone for the entire system. Furthermore, the VDB serves as a distributed, long-term memory accessible by all agents, enabling them to learn and improve their performance over time through RAG. B. Operational Flow: Digital Twin and Real-World Interaction A typical mission unfolds in two interconnected loops: 1. Cognitive Control Loop: TheCommander Agent receives a mission and direct"}
{"id": "2507.23735v1_chunk21", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ugh RAG. B. Operational Flow: Digital Twin and Real-World Interaction A typical mission unfolds in two interconnected loops: 1. Cognitive Control Loop: TheCommander Agent receives a mission and directs the specialist agents to generate plans and actions. For instance, the Motion Planning Agent generates a trajectory, which is passed to the low-level Controls & Interfaces to be executed by the robot in the Real World . 2. Digital Twin Loop: Concurrently, the robot’s Sensors provide real-time Meta data to the Digital Twin Curator Agent . This agent reasons about the real-world state and sends Fidelity Injections -updates and corrections-to the Simulator . The simulator’s Virtual State , now synchronised with reality, is used to create a high-fidelity Digital Twin for predictive analysis and proactive planning. IV. M ECHANISMS This section provides a detailed description of UROSA’s main mechanisms, which were initially introduced in Section I. Section V will then present different use cas"}
{"id": "2507.23735v1_chunk22", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "proactive planning. IV. M ECHANISMS This section provides a detailed description of UROSA’s main mechanisms, which were initially introduced in Section I. Section V will then present different use cases that demonstrate the performance of each mechanism. A. Decoupled Reasoning and Environmental Adaptability A primary innovation of UROSA is its ability to decouple high-level mission goals from low-level code implementation. This is achieved not just by distributing agents, but by fundamentally replacing traditional programmatic logic with a structured, pre-configured reasoning process embedded within each AI agent. The other innovation is the adaptability that these agents allow. The main engine behind UROSA’s adaptability is the4 Fig. 1: The UROSA Cognitive Architecture. The system is comprised of three main parts: (left) the real-world vehicle and its low-level controls; (top) the Digital Twin environment, which is continuously updated by a dedicated curator agent; and (center) the co"}
{"id": "2507.23735v1_chunk23", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rised of three main parts: (left) the real-world vehicle and its low-level controls; (top) the Digital Twin environment, which is continuously updated by a dedicated curator agent; and (center) the cognitive core of the framework, a distributed set of intelligent AI agents. The Commander AI Agent receives the high-level mission description and acts as a central orchestrator, ensuring cognitive consistency. It directs a crew of Specialist Agents , each responsible for a specific domain like planning, perception, or diagnostics. All agents communicate over the ROS 2 bus and access a shared, distributed VDB for long-term memory and experiential learning. The Digital Twin Curator Agent observes real-world sensor data Meta data and injects updates Fidelity Injections into the Simulator , whose Virtual State is fused with real-world data to form the comprehensive Digital Twin. The inset (top right) details the composition of each Agentic Node , which fuses the high-level AI Agent with its RO"}
{"id": "2507.23735v1_chunk24", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "Virtual State is fused with real-world data to form the comprehensive Digital Twin. The inset (top right) details the composition of each Agentic Node , which fuses the high-level AI Agent with its ROS 2 Node Implementation , encapsulating the AI Reasoner , a Safety Parser , and communication interfaces. pre-deployment tuning of each agent for its specific role. This is achieved not by changing model weights, but by engineering a detailed set of instructions within the SYSTEM prompt of itsModelfile . This prompt acts as the agent’s core cognitive and behavioural model, containing: •A Core Directive: Defining its primary function and role within the multi-agent system (e.g., You are a motion planner orYou are a follower agent that must coordinate with a master agent ). •Domain Knowledge and Physical Models: Textual descriptions of vehicle dynamics, environmental forces, and physical constraints like tether properties. •Reasoning Guidelines: Instructions on how to approach a problem, cov"}
{"id": "2507.23735v1_chunk25", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " Physical Models: Textual descriptions of vehicle dynamics, environmental forces, and physical constraints like tether properties. •Reasoning Guidelines: Instructions on how to approach a problem, covering both individual and cooperative logic (e.g., You must compute a collision-free path that yields to the master agent’s trajectory ). •A Strict Output Format: A rigid template for its response, as detailed in our safety strategy (Section V-D) In operation, the agent’s ROS 2 node subscribes to relevant topics and passes this multi-modal information to the tuned AI core. The AI agent then executes its embedded reasoning process to formulate a solution. This entire thinking process , guided by the initial SYSTEM prompt , replaces thousandsof lines of explicit code, allowing for unprecedented flexibility. B. Behaviour adaptation and lifelong learning UROSA is designed for continuous learning, where agents adapt their behaviour based on new information and past experiences. We enable this c"}
{"id": "2507.23735v1_chunk26", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "flexibility. B. Behaviour adaptation and lifelong learning UROSA is designed for continuous learning, where agents adapt their behaviour based on new information and past experiences. We enable this capability through two distinct, complementary mechanisms: (1) learning using a VDB with RAG for knowledge-grounding, and (2) online policy refine- ment using a novel Teacher-Student model for behavioural shaping. The primary mechanism for grounding and learning from past data is the VDB. At the start of a mission, a Flexible AI Agent is instantiated with a specific behaviour defined in itsSYSTEM prompt . This initial prompt instructs the agent on its core task and, crucially, how to utilise information from the VDB. During operation, the VDB is dynamically updated with valuable, curated data. In each operational cycle, the agent queries the VDB to retrieve contextually relevant past experiences. This RAG process grounds the agent’s reasoning, dramatically improving decision quality and acc"}
{"id": "2507.23735v1_chunk27", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "In each operational cycle, the agent queries the VDB to retrieve contextually relevant past experiences. This RAG process grounds the agent’s reasoning, dramatically improving decision quality and accelerating performance by leveraging a growing repository of relevant experience.5 To facilitate more direct online behavioural refinement, we introduce a novel Teacher-Student Instructional Tuning mechanism. This approach enables autonomous adaptation of a Student agent’s core policy through direct interaction with a Teacher agent. The process unfolds over episodes: the Student performs an action (e.g., generates a description). The Teacher agent analyses the response and its corrective action is to generate a new, modified SYSTEM prompt for the Student. This prompt acts as a rich, instructive policy guidance signal. This new prompt is then used to create a new, re-tuned instance of the Student agent for the next episode, creating a powerful meta-learning loop where the Teacher actively sh"}
{"id": "2507.23735v1_chunk28", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " policy guidance signal. This new prompt is then used to create a new, re-tuned instance of the Student agent for the next episode, creating a powerful meta-learning loop where the Teacher actively shapes the Student’s core reasoning policy from one episode to the next. C. Autonomous On-the-Fly Function Extensibility A key innovation of UROSA is its ability to autonomously generate, test, and integrate new software components at runtime, extending its own functionality without direct human intervention. This capacity for self-extension allows the system to adapt its software architecture to unforeseen challenges or mission requirements, effectively enabling runtime self-repair . This mechanism is enabled by the Autonomous Code Synthesis AI Agent , operating under the direction of the Commander AI Agent . The process begins when the Comman- derautonomously identifies a functional gap in the system, for instance, determining that a specific data filter or a novel planning algorithm is re"}
{"id": "2507.23735v1_chunk29", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "nder AI Agent . The process begins when the Comman- derautonomously identifies a functional gap in the system, for instance, determining that a specific data filter or a novel planning algorithm is required to handle the current situation. It then formulates a set of high-level requirements and sends them as a natural language request to the Code Synthesis agent. It is crucial to note that the AI’s role is strictly that of a powerful code synthesiser; it does not perform the requested task itself. Upon receiving the requirements, the agent: (1) Synthesises Code: Generates the source code for a complete ROS 2 node in a suitable language like Python; (2) Generates Tests: Simultaneously creates a suite of unit tests to validate the new code’s functional correctness and safety; (3) Saves and Deploys: If the automated tests pass, the generated source code is saved as a new executable file. This new node is then seamlessly launched and integrated into the live ROS 2 computation graph. This e"}
{"id": "2507.23735v1_chunk30", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "eploys: If the automated tests pass, the generated source code is saved as a new executable file. This new node is then seamlessly launched and integrated into the live ROS 2 computation graph. This entire workflow (from the identification of need to the deployment of a validated new capability) is performed on-the-fly. D. Dynamic, Predictive System Diagnostics UROSA features an advanced diagnostics capability that moves beyond static fault trees, using an AI agent to reason about the system’s health based on live data. This allows for the detection of complex, emergent failures that are not defined by simple error codes. This capability is enabled by a dedicated Diagnostic AI Agent whose behaviour is defined by a highly structured, task-specific reasoning process embedded within its core SYSTEM prompt . This transforms the generic LLM into a specialised diagnostic expert. The prompt instructs the agent to:(1)Perform a multi-step time-series analysis over a sliding window of 10 consecu"}
{"id": "2507.23735v1_chunk31", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "SYSTEM prompt . This transforms the generic LLM into a specialised diagnostic expert. The prompt instructs the agent to:(1)Perform a multi-step time-series analysis over a sliding window of 10 consecutive JSON status messages from thevehicle. (2)Utilise an embedded domain-specific physical model-the vehicle’s thruster allocation matrix-to determine the expected thruster behaviour for any commanded movement. (3)Compare the expected PWM values against the observed data for each thruster in the time window, using defined logic to classify distinct fault types (e.g., dead orout-of-range ). (4)Aggregate these findings to identify drifts, asymmetries, or other anomalies. (5)Report its findings in a rigid, three- line format: a) Issue Identification, c) Status Summary, and b) Suggested Action. This entire diagnostic procedure, governed by the initial prompt, allows the agent to use its holistic understanding of the vehicle’s expected behaviour to diagnose issues without relying on a predefine"}
{"id": "2507.23735v1_chunk32", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "is entire diagnostic procedure, governed by the initial prompt, allows the agent to use its holistic understanding of the vehicle’s expected behaviour to diagnose issues without relying on a predefined list of faults. E. Inherent Safety and Control Mechanisms Finally, UROSA explicitly addresses AI safety and control through its architecture, aiming to ensure that the agents’ actions are verifiable and aligned with human intent. This is achieved through a multi-layered safety strategy, implemented from the agent’s creation to its final output. Safety in UROSA is not a single component but an integrated, three-tiered approach: 1. Proactive Behavioural Scaffolding: The first and most critical layer of control is established during each agent’s creation via its Modelfile . This involves embedding a com- plete, task-specific reasoning process and strict output format directly into the agent’s core SYSTEM prompt . By providing detailed instructions, domain knowledge, physical models, and rea"}
{"id": "2507.23735v1_chunk33", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ing a com- plete, task-specific reasoning process and strict output format directly into the agent’s core SYSTEM prompt . By providing detailed instructions, domain knowledge, physical models, and reasoning guidelines, we transform the generic LLM into a highly specialised and predictable, task-oriented engine. The detailed implementation of this principle for the Diagnostic Agent , as described in Section IV-D , is a prime example of this technique. This same scaffolding approach is applied to all agents in the UROSA framework to ensure reliable and verifiable behaviour. 2. Dynamic Contextual Grounding: The second layer is provided by the heavy reliance on RAG, orchestrated by the Brain Agent . While the SYSTEM prompt defines how an agent should reason, RAG ensures that what it reasons about is grounded in a verified knowledge base and real-world data from the VDB. This provides the necessary context to make its structured output factually relevant to the current mission, further miti"}
{"id": "2507.23735v1_chunk34", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "out is grounded in a verified knowledge base and real-world data from the VDB. This provides the necessary context to make its structured output factually relevant to the current mission, further mitigating the risk of ungrounded decisions. 3. Reactive Output Validation: As a final, reactive backstop, every agentic node contains a Safety Parser . This component validates all LLM outputs before they are published as ROS 2 messages. It acts as a critical final checkpoint, checking for correct syntax and ensuring the command adheres to predefined operational safety rules (e.g., maximum speed or depth). Any command that fails this validation is blocked. Furthermore, the distributed architecture itself provides inherent fault tolerance, as the failure of one specialist agent does not necessarily compromise the entire system. V. R ESULTS To substantiate the claims made in this paper, we conducted a comprehensive evaluation strategy designed to validate each6 TABLE I: Dynamic Avoidance: AI Ag"}
{"id": "2507.23735v1_chunk35", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "y compromise the entire system. V. R ESULTS To substantiate the claims made in this paper, we conducted a comprehensive evaluation strategy designed to validate each6 TABLE I: Dynamic Avoidance: AI Agent vs. Enhanced A* Mission ID Error Delta (AI - A*) (m) AI Agent Success (%) 1 2.56 80% 2 3.78 80% 3 4.23 80% 4 6.37 60% 5 4.68 60% of UROSA’s core innovations. The experiments were both in simulation, using in a high-fidelity simulation environment, and in real-time robotic platforms. We utilise an Autonomous Surface Vehicle (ASV) and an Autonomous Underwater Vehicle (AUV) [55], in a water tank (3.5 m x 3.0 m x 2.5 m) available in our laboratories. The following subsections are structured to address each innovation introduced in Section I and described in detail in Section IV , presenting empirical validation through targeted use-case scenarios. A. Decoupled Reasoning and Environmental Adaptability We validated this through several complex scenarios, cover- ing both multi-agent coordinat"}
{"id": "2507.23735v1_chunk36", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "pirical validation through targeted use-case scenarios. A. Decoupled Reasoning and Environmental Adaptability We validated this through several complex scenarios, cover- ing both multi-agent coordination and single-robot manipula- tion, with quantitative results presented for each to demonstrate performance and reliability. 1) Constrained Multi-Robot Coordination With Obstacles: In this simulation, a tethered ASV-AUV system (with a 10 m tether) was tasked with achieving a position goal while avoiding an obstacle, as shown in Fig. 2. This required the AI agent to reason about the complex system dynamics, including the tether’s catenary shape, to maintain a valid formation throughout the maneuver. To benchmark UROSA’s performance, we compared it against a traditional motion planner based on an enhanced A* algorithm [56], [57]. A critical distinction in this evaluation is the source of obstacle information: the traditional A* planner was provided with the ground-truth position of the obst"}
{"id": "2507.23735v1_chunk37", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "an enhanced A* algorithm [56], [57]. A critical distinction in this evaluation is the source of obstacle information: the traditional A* planner was provided with the ground-truth position of the obstacle, whereas the UROSA framework had to rely on a Perception & Scene Reasoning Agent to detect and track the obstacle from a live camera feed. The validation was performed across 5 distinct mission configurations, each featuring a unique map with different start positions, goal locations, and obstacle layouts to test the system’s adaptability. For each of these configurations, we conducted 5 trials to account for any stochasticity in the perception and planning process. The key comparative metrics- the additional positioning error introduced by the AI agent compared to the A* baseline, and the agent’s task success rate-are presented in Table I. The results show an expected trade-off: the AI agent’s positioning error is higher because it operates on a live perception feed, unlike the A* pl"}
{"id": "2507.23735v1_chunk38", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "e agent’s task success rate-are presented in Table I. The results show an expected trade-off: the AI agent’s positioning error is higher because it operates on a live perception feed, unlike the A* planner, which used perfect information. Nevertheless, achieving a success rate of up to 80% on this difficult perception-in-the-loop task is a significant result, demonstrating that the framework can close the loop from raw perception to complex multi-robot coordination using only emergent reasoning. 2) Map-Based Path Planning for Multi-Robot Systems: To test the ability to interpret abstract data, we tasked the AI system with planning a path for the tethered ASV-AUV system using only a 2D map image and a text-based goal (Fig. 2). We again compared the AI-driven approach to the enhanced A* Fig. 2: Illustration of UROSA’s flexible, multi-modal planning architecture. The system’s AI agents act as a cognitive hub, integrating pre-defined mission data or dynamic, real-time perceptions to enable"}
{"id": "2507.23735v1_chunk39", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " 2: Illustration of UROSA’s flexible, multi-modal planning architecture. The system’s AI agents act as a cognitive hub, integrating pre-defined mission data or dynamic, real-time perceptions to enable seamless switching between goal-driven and perception-driven behaviors. In the map-based planning scenario shown, the agent must generate a collision-free path from the blue circle (start position) to the red circle (goal position), treating the yellow circles as obstacles. TABLE II: Map-Based Planning: AI Agent vs. Enhanced A* Map ID Error Delta (AI - A*) (m) AI Agent Success (%) 1 3.34 80% 2 2.98 80% 3 7.81 60% 4 8.13 60% 5 6.26 40% planner [56], [57]. The averaged results over 5 trials for each map layout are compared in Table II. The results in Table II demonstrate the trade-offs of an end-to-end reasoning system. The higher positioning error and variable success rates are an expected consequence of the task’s complexity; the AI agent must first interpret the raw map image, identify o"}
{"id": "2507.23735v1_chunk40", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "d-to-end reasoning system. The higher positioning error and variable success rates are an expected consequence of the task’s complexity; the AI agent must first interpret the raw map image, identify obstacles, and then plan a path, a process with more inherent uncertainty than the A* planner’s, which operates on pre-defined maps. The key achievement is that the system can succeed in this complex task at all, reaching success rates of up to 80% on. This is a significant leap in autonomy because the UROSA framework reasons directly from raw images and text to create a plan, bypassing the time-consuming manual setup required by traditional planners. 3) Flexible Motion Planning for a UVMS: We further validated this principle on an Underwater Vehicle Manipulator System (UVMS) [58], [59], focusing on autonomous motion planning driven by textual commands (Fig. 4). The experiment, conducted in a real-world tank, was designed to test the Commander AI Agent ’s ability to interpret nuanced natura"}
{"id": "2507.23735v1_chunk41", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ng on autonomous motion planning driven by textual commands (Fig. 4). The experiment, conducted in a real-world tank, was designed to test the Commander AI Agent ’s ability to interpret nuanced natural language instructions. To simplify the test’s scope and focus purely on command interpretation and planning, the coordinates for key locations like goal 1 andgoal 2 were predefined within the agent’s SYSTEM prompt . To provide a quantitative benchmark, we compared its7 TABLE III: Manipulator Planning & Interpretation Performance Metric AI Agent Naive A*+PRM Planning Time (s) 1.3 0.05 Interpretation Success (%) 90% N/A Planning Success (%) 100% 100% performance against a baseline traditional planner using a naive A* search on a Probabilistic Roadmap (PRM) [60]. To test the reliability of the AI’s language understanding, we conducted 10 trials using prompts with varied phrasing and intent. These included explicit instructions for avoidance (e.g., Go to goal 1 and check the obstacles on the"}
{"id": "2507.23735v1_chunk42", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "AI’s language understanding, we conducted 10 trials using prompts with varied phrasing and intent. These included explicit instructions for avoidance (e.g., Go to goal 1 and check the obstacles on the way ) as well as more ambiguous phrasing (e.g., Inspect goal 2 and check for any boxes on your way ). Crucially, the test also included direct commands that omitted any mention of obstacle avoidance, such as Inspect goal 2 . In these cases, to test literal command adherence, the Commander AI Agent intentionally disregarded perceptual data about obstacles and commanded a direct path, leading to a planned collision. The averaged results of these trials are summarized in Table III. The results highlight the unique capabilities of the UROSA approach. The system achieved a 90% Interpretation Success rate, correctly understanding the user’s intent in 9 out of the 10 trials. Crucially, for every prompt that was correctly understood, the subsequent Planning Success rate for generating a valid tra"}
{"id": "2507.23735v1_chunk43", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ss rate, correctly understanding the user’s intent in 9 out of the 10 trials. Crucially, for every prompt that was correctly understood, the subsequent Planning Success rate for generating a valid trajectory was 100%. This clearly distinguishes between the agent’s highly robust (but not infallible) natural language understanding and its near-perfect planning capability once the goal is known—a distinction the traditional method (N/A) cannot address. This showcases the framework’s power in translating high-level, human-like instructions directly into complex robot actions. B. Behaviour Adaptation and Lifelong Learning We evaluated these adaptive learning capabilities through targeted experiments demonstrating both experiential learning via RAG and online policy refinement. 1) Visual Positioning with Experiential Learning: To vali- date how the framework’s use of experience improves resilience against external disturbances, we conducted simulation tests focused on visual positioning. In "}
{"id": "2507.23735v1_chunk44", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ning with Experiential Learning: To vali- date how the framework’s use of experience improves resilience against external disturbances, we conducted simulation tests focused on visual positioning. In this scenario, an AUV was tasked with maintaining a stable position and orientation relative to a static feature on the seabed (a pipeline segment), as shown in the simulated views in Fig. 5. This capability is enabled by a Flexible AI Agent that uses a VDB as a short-term visual-temporal memory. This allows the agent to reason not just about its current state, but about the dynamics of recent events. To isolate the impact of this experience buffer, we bench- marked the agent’s performance in two conditions: (1) with VDB access disabled, and (2) with full VDB access. The test involved letting the AUV follow a pipeline and then applying a simulated external force to induce a specific lateral deviation. To evaluate the robustness of the agent’s response, we tested three magnitudes of absolut"}
{"id": "2507.23735v1_chunk45", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "the AUV follow a pipeline and then applying a simulated external force to induce a specific lateral deviation. To evaluate the robustness of the agent’s response, we tested three magnitudes of absolute deviation (1.0 m, 1.5 m, and 2.5 m), applying the force to induce errors to both the left and the right. We then measured the Recovery Time —the time takenTABLE IV: Disturbance Recovery with Experiential Learning DeviationRecovery Time (w/o VDB, s)Recovery Time (w/ VDB, s) 1.0 m 5.9 2.6 1.5 m 9.0 3.2 2.5 m 12.6 4.4 TABLE V: Averaged Online Policy Refinement via Teacher- Student Loop Episode Avg. Response Length (Words) Avg. Info. Relevance (%) 1 45 5% 3 18 65% 6 5 100% for the agent to guide the AUV back to the pipe centerline. Each of these six conditions was run 3 times, and the results, averaged across both left and right directions for each deviation magnitude, are presented in Table IV. The results demonstrate the significant value of experiential learning. The agent with VDB access"}
{"id": "2507.23735v1_chunk46", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "veraged across both left and right directions for each deviation magnitude, are presented in Table IV. The results demonstrate the significant value of experiential learning. The agent with VDB access recovered much faster because it leveraged the visual history to infer the disturbance dynamics and issue a proactive correction command. In contrast, the baseline agent could only react to its instantaneous error, proving less effective. The key hardware-independent finding is that the VDB provides the temporal context required for more resilient control. 2) Online Behavioural Tuning via Teacher-Student Interac- tion: To demonstrate more direct online behavioural adaptation, we validated our novel Teacher-Student Instructional Tuning mechanism . In this paradigm, a Teacher agent guides a Student agent’s policy by generating a new, more restrictive SYSTEM prompt as a form of instructive feedback, creating a powerful meta-learning loop. We conducted a series of real- world experiments in a"}
{"id": "2507.23735v1_chunk47", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "dent agent’s policy by generating a new, more restrictive SYSTEM prompt as a form of instructive feedback, creating a powerful meta-learning loop. We conducted a series of real- world experiments in a cluttered tank environment (Fig. 4) where the Commander AI Agent acted as the Teacher and the Perception & Scene Reasoning AI Agent was the Student . The Teacher’s objective was to guide the Student’s policy from providing verbose descriptions of the whole scene to reporting only the presence and location of a specific target. We ran a total of 20 trials, comprising 5 trials for each of four distinct target types: red balls, pink buoys, a fishing net, and other submerged obstacles. At each episode in a trial, theStudent generated a description, and the Teacher provided corrective feedback by generating a new SYSTEM prompt to re-tune the Student’s behaviour for the next episode. The learning process was quantified over three episodes, and Table V shows the averaged performance across all 2"}
{"id": "2507.23735v1_chunk48", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "generating a new SYSTEM prompt to re-tune the Student’s behaviour for the next episode. The learning process was quantified over three episodes, and Table V shows the averaged performance across all 20 trials. The results demonstrate a consistent and rapid convergence on the desired behaviour across all target types. On average, the Student’s policy quickly shifted from general scene description to specific, targeted reporting, confirming the mechanism’s ability to perform targeted, online behavioural shaping for a variety of perceptual goals using structured, linguistic guidance. 3) Validation: We evaluated this mechanism’s capacity for runtime software adaptation and self-repair using the controlled test setup shown in Fig. 6. For this evaluation, we simulated an autonomous request from the Commander AI Agent by manually publishing a ROS 2 topic containing the high-level8 Fig. 3: The AI agent workflow for the UVMS manipulation task. A high-level textual command from the User terminal"}
{"id": "2507.23735v1_chunk49", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "the Commander AI Agent by manually publishing a ROS 2 topic containing the high-level8 Fig. 3: The AI agent workflow for the UVMS manipulation task. A high-level textual command from the User terminal is interpreted by the Commander AI Agent , which then directs the Motion Planning andPerception agents to execute the task. Fig. 4: A direct Before and After comparison of the Teacher-Student instructional tuning mechanism. (Left) Without the Teacher enabled, the Student agent observers the scene (1) and defaults to a verbose scene description as seen in (2). (Right) With the Teacher enabled, the Teacher gives precise feedback (3) to the Student over several episodes, resulting in a converged output that is concise (4) and provides only the information requested by the Teacher Fig. 5: The control loop for visually-guided disturbance rejection. The AUV’s camera feeds a continuous stream of images into a VDB, which provides visual-temporal memory. ThePerception & Scene Reasoning AI Agent us"}
{"id": "2507.23735v1_chunk50", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "trol loop for visually-guided disturbance rejection. The AUV’s camera feeds a continuous stream of images into a VDB, which provides visual-temporal memory. ThePerception & Scene Reasoning AI Agent uses this historical context to calculate a precise positional error, which the AUV’s control system then uses to issue corrective motion commands. natural language requirements for a new node. A trial was considered a success only if the agent completed the full end- to-end process: correctly interpreting the request, generating valid Python code, creating and passing its own unit tests, and integrating the new node into the live system. A failure was defined as any case where the generated code did not compile, failed its unit tests, or was functionally incorrect. We conducted 10 trials for each of the following field- relevant scenarios: The field-relevant scenarios included gen- erating: (1)a stateful noise-reduction filter to average sensor values over a 10-sample window; (2)a standard "}
{"id": "2507.23735v1_chunk51", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ch of the following field- relevant scenarios: The field-relevant scenarios included gen- erating: (1)a stateful noise-reduction filter to average sensor values over a 10-sample window; (2)a standard sensor fusionTABLE VI: Performance of Autonomous Node Generation Generated Node Avg. Gen. Time (s) Success Rate Stateful Averaging Filter 1.5 80% Kalman Filter (2x Odom) 5.8 80% Kalman Filter (DVL+Compass) 5.9 70% node implementing a Kalman filter for two odometry topics; and(3)an emergency navigation repair node. This primary evaluation scenario simulated a critical Inertial Navigation System (INS) failure, requiring the system to synthesize a new Kalman filter to fuse compass and Doppler Velocity Log (DVL) data to mitigate navigational drift. The performance of the autonomous generation process is summarized in Table VI. Most significantly, in the navigation failure scenario, the dynamically generated Kalman filter node, once integrated, successfully fused the compass and DVL data, reduc"}
{"id": "2507.23735v1_chunk52", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ess is summarized in Table VI. Most significantly, in the navigation failure scenario, the dynamically generated Kalman filter node, once integrated, successfully fused the compass and DVL data, reducing the navigational drift rate by an estimated 70% compared to dead-reckoning with DVL alone. This provides powerful evidence of the system’s ability to perform runtime self-repair of a critical capability. Most significantly, in the navigation failure scenario, the dynamically generated Kalman filter node, once integrated, successfully fused the compass and DVL data, reducing the navigational drift rate by an estimated 70% compared to dead- reckoning with DVL alone. C. Dynamic, Predictive System Diagnostics This capability was evaluated through a series of proof- of-concept experiments where we simulated various hardware failure modes. For each test case, we programmatically disabled9 Fig. 6: The evaluation workflow for autonomous node gen- eration. 1) A user gives a query. 2) The Comman"}
{"id": "2507.23735v1_chunk53", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " we simulated various hardware failure modes. For each test case, we programmatically disabled9 Fig. 6: The evaluation workflow for autonomous node gen- eration. 1) A user gives a query. 2) The Commander AI Agent generates the requirements and passes them over to theAutonomous Code Synthesis Agent . 3) The Autonomous Code Synthesis Agent generates, tests, and deploys the new ROS 2 node. 4) For this evaluation, feedback on the new node’s performance was routed to the user terminal. It is important to note that in a fully autonomous mission, the Commander AI Agent can make the starting request and the feedback would be sent back to it to inform subsequent decisions. Fig. 7: Excerpt of the code generated. For full details please check the website and videos. specific thrusters via the vehicle’s control interface. The ArduPilot system [61], observing the lack of response, would then generate an updated vehicle status JSON reflecting the fault. The Diagnostic AI Agent’s task was to monitor "}
{"id": "2507.23735v1_chunk54", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "’s control interface. The ArduPilot system [61], observing the lack of response, would then generate an updated vehicle status JSON reflecting the fault. The Diagnostic AI Agent’s task was to monitor this stream and correctly identify which thrusters were malfunctioning. We ran 5 trials for each fault configuration, and the agent correctly diagnosed the system state with 100% accuracy across all tests, as summarised in Table VII. To further demonstrate the agent’s ability to track a dynamically changing system state, weTABLE VII: Summary of Diagnostic Test Cases and Results Test Case Simulated Fault Diagnosis Accuracy 1 All Thrusters OK 100% (5/5) 2 Thruster 2 Disabled 100% (5/5) 3 Thruster 6 Disabled 100% (5/5) 4 Thrusters 2 & 3 Disabled 100% (5/5) 5 Thrusters 2, 3, 6, 7 Disabled 100% (5/5) Fig. 8: Validation of the Predictive Diagnostics Agent during a simulated two-thruster failure. The plots on the right show the raw PWM signals from the ROS 2 interface, clearly indicating that thr"}
{"id": "2507.23735v1_chunk55", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "5) Fig. 8: Validation of the Predictive Diagnostics Agent during a simulated two-thruster failure. The plots on the right show the raw PWM signals from the ROS 2 interface, clearly indicating that thrusters 2 and 3 are unresponsive (flatlined) while the others are active. The Diagnostic Agent continuously analyzes this time-series data, reasons about the discrepancy between commanded and actual behavior, and autonomously generates the correct high-level diagnosis shown in the terminal output (top left). This demonstrates the agent’s ability to translate low-level signal anomalies into a precise, human-readable fault report without relying on pre-programmed error codes. conducted a specific transition test. First, we disabled thrusters 2 and 3 to induce a fault. After the agent correctly diagnosed this failure, we re-enabled the thrusters in the interface. The agent’s subsequent report immediately and accurately reflected the system’s return to a healthy state. Figure 8 shows the agent’"}
{"id": "2507.23735v1_chunk56", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "agnosed this failure, we re-enabled the thrusters in the interface. The agent’s subsequent report immediately and accurately reflected the system’s return to a healthy state. Figure 8 shows the agent’s real-time terminal output, capturing these precise system faults. D. Inherent Safety and Control Mechanisms To validate the safety of the agent, we conduct a complex evaluation of multi-agent behaviour. Two AUVs, each con- trolled by an independent AI agent, navigated a maze-like environment toward a shared goal (Fig. 9). The agents must then coordinate to avoid each other, but must do so via emergent coordination, without preplanning. The key to this capability was embedding not only a negotiation protocol but also the vehicle’s physical dimensions into each agent’s behavioural constitution ( SYSTEM prompt ). This allowed each agent to reason spatially about its own footprint and the other’s intended path. The evaluation involved 8 test runs with increasingly complex trajectory conflict"}
{"id": "2507.23735v1_chunk57", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " ( SYSTEM prompt ). This allowed each agent to reason spatially about its own footprint and the other’s intended path. The evaluation involved 8 test runs with increasingly complex trajectory conflicts, where paths crossed or required agents to pass each other in tight corridors. Each test case was run 5 times to ensure repeatability. During operation, the agents continuously share their intended trajectories via ROS 2. Each10 Fig. 9: Iustration of the decentralised collision avoidance scenario, where two AI agents negotiate a solution via ROS 2. Agent A, marked as Yellow, and Agent B, marked with green. Each has a starting position and a goal point, with conflicting goals. TABLE VIII: Performance of Decentralised Negotiation (Avg. over 5 runs) Test ID Negotiation Time (s) Min. Safe Distance (m) 1 0.05 1.2 2 0.05 1.4 3 0.06 2.1 4 0.12 0.21 5 0.13 0.22 6 0.06 2.32 7 0.14 0.32 8 0.13 0.76 agent then predicts the future states along both paths, calculat- ing the minimum distance between t"}
{"id": "2507.23735v1_chunk58", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ce (m) 1 0.05 1.2 2 0.05 1.4 3 0.06 2.1 4 0.12 0.21 5 0.13 0.22 6 0.06 2.32 7 0.14 0.32 8 0.13 0.76 agent then predicts the future states along both paths, calculat- ing the minimum distance between their vehicle’s bounding volumes. If this predicted distance fell below a predefined safety threshold, a collision was considered imminent, triggering a human-independent negotiation dialogue. Based on its pre- configured reasoning process, one agent would identify the conflict and autonomously compute a new path yielding to the other. Table VIII summarises the averaged performance, focusing on the efficiency of the negotiation and the safety of the resulting manoeuvre. Across all 40 trials, the agent pair successfully negotiated a collision-free path every time. The data in Table VIII shows that the negotiation was exceptionally efficient, with a resolution consistently found in well under 0.2 seconds. It should be noted that this time is presented as a reference, as it is dependent on the"}
{"id": "2507.23735v1_chunk59", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "at the negotiation was exceptionally efficient, with a resolution consistently found in well under 0.2 seconds. It should be noted that this time is presented as a reference, as it is dependent on the specific AI model and GPU hardware used. Critically, even in the most constrained scenarios where the vehicles had to pass with very tight clearances (e.g., a minimum distance of just 0.21 m in Test ID 4), a positive safe distance was always maintained. This consistent success in difficult configurations confirms that an effective, emergent deconfliction strategy canbe derived directly from high-level textual instructions about vehicle geometry. VI. C ONCLUSION This paper presented UROSA, a distributed AI agent architecture that replaces monolithic programmatic control with a collaborative ecosystem of agentic entities. By embedding a structured reasoning process into each agent via behavioural constitutions, UROSA decouples high-level mission goals from low-level code implementation, ena"}
{"id": "2507.23735v1_chunk60", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "osystem of agentic entities. By embedding a structured reasoning process into each agent via behavioural constitutions, UROSA decouples high-level mission goals from low-level code implementation, enabling a new level of cognitive flexibility in the underwater domain. Through empirical validation, we demonstrated that this framework effectively handles multi-robot coordination from abstract inputs, improves performance through experiential learning via RAG, autonomously synthesises new software modules at runtime, and performs predictive diagnostics without static fault trees. These capabilities, protected by a multi-layered safety strategy, proved both reliable and effective across demanding scenarios. While these findings are significant, challenges in deploying such cognitive systems remain. The engineering of effective behavioural constitutions is a complex new skill, and the formal verification and real-time performance of every AI-driven decision require further research. Future "}
{"id": "2507.23735v1_chunk61", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "main. The engineering of effective behavioural constitutions is a complex new skill, and the formal verification and real-time performance of every AI-driven decision require further research. Future work will address these challenges by exploring two frontiers: automating the generation of agent constitutions, and enabling runtime self- reconfiguration. This latter capability would allow an agent, upon receiving a diagnosis of hardware failure, to autonomously rewrite and deploy a new control model—such as a modified thruster allocation matrix—to ensure the system continues its mission with gracefully degraded performance. This research provides a scalable and safe framework for developing more adaptable and resilient autonomous systems, moving beyond pre-programmed tools towards reasoning partners capable of overcoming both environmental and internal, unforeseen challenges. REFERENCES [1]S. J. Russell and P. Norvig, Artificial intelligence: A modern approach . Pearson Education, 2020"}
{"id": "2507.23735v1_chunk62", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "g partners capable of overcoming both environmental and internal, unforeseen challenges. REFERENCES [1]S. J. Russell and P. Norvig, Artificial intelligence: A modern approach . Pearson Education, 2020. [2]N. J. Nilsson, “Shakey the robot,” SRI International Menlo Park CA Artificial Intelligence Center, Tech. Rep., 1984. [3]R. A. Brooks, “Intelligence without representation,” Artificial Intelligence , vol. 47, no. 1-3, pp. 139–159, 1991. [4] R. C. Arkin, Behavior-based robotics . MIT Press, 1998. [5]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, others, and D. Amodei, “Language models are few-shot learners,” in Advances in Neural Information Processing Systems , vol. 33, 2020, pp. 1877–1901. [6]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, others, and I. Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems , 2017, pp. 5998–6008. [7] J. E. Laird, The Soar cognitive architecture . MIT Press, 2012. [8]J"}
{"id": "2507.23735v1_chunk63", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rs, and I. Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems , 2017, pp. 5998–6008. [7] J. E. Laird, The Soar cognitive architecture . MIT Press, 2012. [8]J. R. Anderson, E. H. Bothell, M. D. Byrne, S. Douglass, C. Lebiere, and Y . Qin, “An integrated theory of the mind,” Psychological Review , vol. 111, no. 4, p. 1036, 2004. [9]Z. Ji, N. Lee, R. Fries, T. Yu, D. Su, Y . Xu, others, and A. Madotto, “Survey of hallucination in natural language generation,” ACM Computing Surveys , 2023. [10] L. Huang, D. Y . Zhou, H. Edwards, and W. Zhou, “A survey on hallucination in large language models,” arXiv preprint arXiv:2305.13565, 2023.11 [11] R. Royce, M. Kaufmann, J. Becktor, S. Moon, K. Carpenter, K. Pak, A. Towler, R. Thakker, and S. Khattak, “Enabling novel mission operations and interactions with rosa: The robot operating system agent,” NASA/JPL-Caltech Technical Report, 2025. [12] Anthropic, “Alignment research,” https://www.anthropic.com/resea"}
{"id": "2507.23735v1_chunk64", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "g novel mission operations and interactions with rosa: The robot operating system agent,” NASA/JPL-Caltech Technical Report, 2025. [12] Anthropic, “Alignment research,” https://www.anthropic.com/research/ alignment, 2024. [13] OpenAI, “Alignment and safety,” https://openai.com/safety, 2024. [14] EY , “The future of autonomous systems: agentic ai in consumer products,” EY Insights , March 2025. [15] Endava, “Agentic ai: Definition, types, applications,” 2025. [16] J. McCarthy, “Programs with common sense,” in Mechanisation of Thought Processes , vol. 1, 1959, pp. 77–84. [17] R. E. Fikes and N. J. Nilsson, “Strips: A new approach to theorem proving applied to problem solving,” Artificial Intelligence , vol. 2, no. 3-4, pp. 189–208, 1971. [18] J. McCarthy and P. J. Hayes, “Some philosophical problems from the standpoint of artificial intelligence,” Machine Intelligence , vol. 4, pp. 463–502, 1969. [19] P. E. Agre and D. Chapman, “Pengi: An implementation of a theory of activity,” in AAAI "}
{"id": "2507.23735v1_chunk65", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "problems from the standpoint of artificial intelligence,” Machine Intelligence , vol. 4, pp. 463–502, 1969. [19] P. E. Agre and D. Chapman, “Pengi: An implementation of a theory of activity,” in AAAI , 1987, pp. 268–272. [20] R. A. Brooks, “A robust layered control system for a mobile robot,” IEEE Journal of Robotics and Automation , vol. 2, no. 1, pp. 14–23, 1986. [21] R. Pfeifer and C. Scheier, Understanding intelligence . MIT Press, 1999. [22] A. Clark, Being there: Putting brain, body, and world together again . MIT Press, 1997. [23] M. Wilson, “Six views of embodied cognition,” Psychonomic Bulletin & Review , vol. 9, no. 4, pp. 625–636, 2002. [24] J. R. Anderson, How can the mind occur in the body? Oxford University Press, 2007. [25] R. J. Brachman and H. J. Levesque, “The tractability of subsumption in frame-based description languages,” in AAAI , 1984, pp. 34–37. [26] P. Kumar, “Large language models (LLMs): survey, technical frameworks, and future challenges,” Artif. Intell. Re"}
{"id": "2507.23735v1_chunk66", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "y of subsumption in frame-based description languages,” in AAAI , 1984, pp. 34–37. [26] P. Kumar, “Large language models (LLMs): survey, technical frameworks, and future challenges,” Artif. Intell. Rev. , vol. 57, no. 10, pp. 1–51, Aug. 2024. [27] OpenAI, “Gpt-4 technical report,” arXiv preprint arXiv:2303.08774, 2023. [28] S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, F. Kamani, others, and Y . Zhang, “Sparks of artificial general intelligence: Early experiments with gpt-4,” arXiv preprint arXiv:2303.12712, 2023. [29] N. Mirchev, S. Jiang, S. Shah, and A. Garg, “Large language models as general-purpose policies for robots,” arXiv preprint arXiv:2305.05042, 2023. [30] M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, G. Danihelka, others, and R. Tanwani, “Do as i can, not as i say: Grounding language in robotic affordances,” arXiv preprint arXiv:2204.01691, 2022. [31] A. Brohan, N. Brown, W. L. Brohan, Y . C. Chen, Y . Chebotar, J. Castro, others, and S. Levine, “"}
{"id": "2507.23735v1_chunk67", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " not as i say: Grounding language in robotic affordances,” arXiv preprint arXiv:2204.01691, 2022. [31] A. Brohan, N. Brown, W. L. Brohan, Y . C. Chen, Y . Chebotar, J. Castro, others, and S. Levine, “Rt-2: Vision-language-action models transfer web knowledge to robotic control,” arXiv preprint arXiv:2207.05736, 2023. [32] A. Radford, J. W. Kim, C. Xu, G. Xu, G. Chen, G. Brockman, others, and I. Sutskever, “Learning transferable visual models from natural language supervision,” in International Conference on Machine Learning . PMLR, 2021, pp. 8748–8763. [33] A. K. Gupta, A. Yan, J. Guo, Y . Cheng, L. Yang, L. H. Chen, others, and K. Keutzer, “Visual grounding for language-guided navigation,” in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , 2023, pp. 3562–3572. [34] D. Driess, A. Zeng, S. Cabi, M. S. Sajjadi, D. Allen, C. Lynch, others, and M. Toussaint, “Palm-e: An embodiment-aware language model for instruction following with visual and tactile feed"}
{"id": "2507.23735v1_chunk68", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "–3572. [34] D. Driess, A. Zeng, S. Cabi, M. S. Sajjadi, D. Allen, C. Lynch, others, and M. Toussaint, “Palm-e: An embodiment-aware language model for instruction following with visual and tactile feedback,” arXiv preprint arXiv:2203.16939, 2023. [35] C. Jia, Y . T. Chen, Z. Lu, S. Tunyasuvunakool, N. De Freitas, and D. Tarlow, “Scaling up visual and vision-language representation learning with noisy text supervision,” in International Conference on Machine Learning . PMLR, 2021, pp. 4615–4625. [36] K. Zhu, C. Gan, L. Wang, Y . Fang, X. Dai, and S. Han, “Vision-language models are zero-shot reward function approximators,” arXiv preprint arXiv:2303.02896, 2023. [37] J. Liang, W. Zeng, G. Mu, S. Yang, T. L. Griffiths, Y . Zhu, others, and D. Song, “Code as policies: Language model-based discrete action policies for embodied ai,” arXiv preprint arXiv:2209.07753, 2023. [38] A. Zeng, S. Song, C. Lee, N. Rodriguez-Ruiz, T. Van-Hove, R. S. Fearing, others, and C. Lynch, “Socratic models: Compo"}
{"id": "2507.23735v1_chunk69", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ete action policies for embodied ai,” arXiv preprint arXiv:2209.07753, 2023. [38] A. Zeng, S. Song, C. Lee, N. Rodriguez-Ruiz, T. Van-Hove, R. S. Fearing, others, and C. Lynch, “Socratic models: Composing zero-shot multimodality with language,” in International Conference on Machine Learning . PMLR, 2023, pp. 26 944–26 967. [39] S. Yao, W. Zhao, J. Wang, Y . Cao, S. Narasimhan, and D. Zhao, “React: Synergizing reasoning and acting in language models for task solving,” arXiv preprint arXiv:2210.03629, 2023.[40] Y . U. Cao, A. S. Fukunaga, and A. B. Kahng, “Cooperative mobile robotics: Antecedents and directions,” Autonomous Robots , vol. 4, no. 1, pp. 7–27, 1997. [41] P. Stone, Ed., Multiagent systems: A modern approach to distributed artificial intelligence . MIT Press, 2000. [42] M. Wooldridge, An introduction to multiagent systems . John Wiley & Sons, 2009. [43] R. Olfati-Saber, J. A. Fax, and R. M. Murray, “Consensus and cooperation in networked multi-agent systems,” Proceedings of "}
{"id": "2507.23735v1_chunk70", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ridge, An introduction to multiagent systems . John Wiley & Sons, 2009. [43] R. Olfati-Saber, J. A. Fax, and R. M. Murray, “Consensus and cooperation in networked multi-agent systems,” Proceedings of the IEEE , vol. 95, no. 1, pp. 215–233, 2007. [44] J. M. Bradshaw, R. R. Hoffman, M. Johnson, and P. J. Feltovich, “Beyond human-centered autonomy: Collaboration and reciprocal adaptation in human-agent teams,” IEEE Intelligent Systems , vol. 32, no. 3, pp. 70–78, 2017. [45] M. Longo, L. Rathenau, and J. Weber, Ethical autonomy in intelligent agents: Embedding ethics into artificial intelligence . Springer Nature, 2023. [46] R. D. Beer, M. Randall, and P. Fitch, “Evolving dynamical neural networks for adaptive behavior,” Adaptive Behavior , vol. 22, no. 1, pp. 3–27, 2014. [47] J. Maynez, S. Narayan, L. Lokhande, and R. Reddy, “On faithfulness and hallucination in abstractive summarization,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , 2020, p"}
{"id": "2507.23735v1_chunk71", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rayan, L. Lokhande, and R. Reddy, “On faithfulness and hallucination in abstractive summarization,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , 2020, pp. 1883–1896. [48] M. Verhelst and E. Moons, “What edge computing can do for deep learning,” in 2017 IEEE International Conference on Computer Commu- nications Workshops (INFOCOM WKSHPS) . IEEE, 2017, pp. 1–6. [49] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and I. Sutskever, “Concrete ai safety problems,” arXiv preprint arXiv:1606.06565, 2016. [50] P. Koopman and M. Wagner, “Challenges in autonomous vehicle verification and validation,” SAE International Journal of Transportation Safety , vol. 5, no. 1, pp. 19–27, 2017. [51] P. Lin, K. Abney, and R. Jenkins, “Robot ethics: Mapping the issues for computer scientists,” AI Magazine , vol. 32, no. 1, p. 15, 2011. [52] W. Wallach and C. Allen, Moral machines: Teaching robots right from wrong . Oxford University Press, 2008. "}
{"id": "2507.23735v1_chunk72", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "Mapping the issues for computer scientists,” AI Magazine , vol. 32, no. 1, p. 15, 2011. [52] W. Wallach and C. Allen, Moral machines: Teaching robots right from wrong . Oxford University Press, 2008. [53] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning in large language models,” arXiv preprint arXiv:2201.11903, 2022. [54] A. Madaan, N. Tandon, P. Gupta, K. Hallinan, L. Gao, S. Wiegreffe, U. Alon, P. Cair, Y . Yang, A. Cohan et al. , “Self-refine: Iterative refinement with self-feedback,” arXiv preprint arXiv:2303.17651, 2023. [55] J. S. Willners, I. Carlucho, S. Katagiri, C. Lemoine, J. Roe, D. Stephens, T.Łuczy ´nski, S. Xu, Y . Carreno, `E. Pairet et al. , “From market-ready rovs to low-cost auvs,” in OCEANS 2021: San Diego–Porto . IEEE, 2021, pp. 1–7. [56] M. Buchholz, I. Carlucho, Z. Huang, M. Grimaldi, P. Nicolay, S. Tun c ¸ay, and Y . R. Petillot, “Framework for robust motion planning of tether"}
{"id": "2507.23735v1_chunk73", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "OCEANS 2021: San Diego–Porto . IEEE, 2021, pp. 1–7. [56] M. Buchholz, I. Carlucho, Z. Huang, M. Grimaldi, P. Nicolay, S. Tun c ¸ay, and Y . R. Petillot, “Framework for robust motion planning of tethered multi-robot systems in marine environments,” in Proceedings of the IEEE/MTS OCEANS Conference . Brest, France: IEEE, May 2025. [57] M. Buchholz, I. Carlucho, M. Grimaldi, and Y . R. Petillot, “Tethered multi-robot systems in marine environments,” in Proceedings of the ICRA 2025 Workshop on Marine Robotics , New Orleans, USA, 2025. [58] M. Buchholz, I. Carlucho, M. Grimaldi, M. Koskinopoulou, and Y . R. Petillot, “Context-aware behavior learning with heuristic motion memory for underwater manipulation,” 2025. [Online]. Available: https://arxiv.org/abs/2507.14099 [59] E. Morgan, I. Carlucho, W. Ard, and C. Barbalata, “Autonomous underwater manipulation: Current trends in dynamics, control, planning, perception, and future directions,” Current Robotics Reports , vol. 3, no. 4, pp. 187–198,"}
{"id": "2507.23735v1_chunk74", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "W. Ard, and C. Barbalata, “Autonomous underwater manipulation: Current trends in dynamics, control, planning, perception, and future directions,” Current Robotics Reports , vol. 3, no. 4, pp. 187–198, 2022. [60] M. Buchholz, I. Carlucho, M. Grimaldi, and Y . R. Petillot, “Context- aware behavior learning with heuristic motion memory for underwater manipulation,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) . New Orleans, USA: IEEE, 2025. [61] The ArduPilot Development Team, “ArduPilot: Open source autopilot software suite,” https://ardupilot.org, 2025, accessed: 5-July-2025."}
{"id": "2507.23734v1_chunk0", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping Dongming Wu1, Yanping Fu2⋆, Saike Huang3, Yingfei Liu3†, Fan Jia3, Nian Liu4, Feng Dai2, Tiancai Wang3, Rao Muhammad Anwer4, Fahad Shahbaz Khan4, Jianbing Shen5‡ 1The Chinese University of Hong Kong,2Institute of Computing Technology, Chinese Academy of Sciences, 3Dexmal,4Mohamed bin Zayed University of Artificial Intelligence,5SKL-IOTSC, CIS, University of Macau General Grasping Large -scale Reasoning -based Affordance Segmentation Dataset You can grip its handle . I need grip a hammer. Sure, you can utilize the hammer by gripping its handle . …I need a tool to drive a nail into the wall. … Figure 1. Large-scale benchmark for reasoning-based affordance segmentation , which sources from various embodied domains: wild, robot, ego-centric indoor, and simulation. By leveraging the extensive high-quality dataset for training, our model AffordanceNet exhibits remarkable open-world generalization c"}
{"id": "2507.23734v1_chunk1", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "mbodied domains: wild, robot, ego-centric indoor, and simulation. By leveraging the extensive high-quality dataset for training, our model AffordanceNet exhibits remarkable open-world generalization capabilities, steering further towards robust general-purpose object grasping. Abstract General robotic grasping systems require accurate object affordance perception in diverse open-world scenarios fol- lowing human instructions. However, current studies suffer from the problem of lacking reasoning-based large-scale af- fordance prediction data, leading to considerable concern about open-world effectiveness. To address this limitation, we build a large-scale grasping-oriented affordance seg- mentation benchmark with human-like instructions, named RAGNet. It contains 273k images, 180 categories, and 26k reasoning instructions. The images cover diverse embod- ied data domains, such as wild, robot, ego-centric, and even ⋆The work is done during the internship at Dexmal. †Project lead. ‡Corres"}
{"id": "2507.23734v1_chunk2", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": ", and 26k reasoning instructions. The images cover diverse embod- ied data domains, such as wild, robot, ego-centric, and even ⋆The work is done during the internship at Dexmal. †Project lead. ‡Corresponding author. This work was supported in part by the Science and Technology Development Fund of Macau SAR (FDCT) under grants 0102/2023/RIA2 and 0154/2022/A3 and 001/2024/SKL, the Jiangyin Hi- tech Industrial Development Zone under the Taihu Innovation Scheme (EF2025-00003-SKL-IOTSC), and the University of Macau SRG2022- 00023-IOTSC grant.simulation data. They are carefully annotated with an affor- dance map, while the difficulty of language instructions is largely increased by removing their category name and only providing functional descriptions. Furthermore, we pro- pose a comprehensive affordance-based grasping frame- work, named AffordanceNet, which consists of a VLM pre- trained on our massive affordance data and a grasping network that conditions an affordance map to grasp the ta"}
{"id": "2507.23734v1_chunk3", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "affordance-based grasping frame- work, named AffordanceNet, which consists of a VLM pre- trained on our massive affordance data and a grasping network that conditions an affordance map to grasp the target. Extensive experiments on affordance segmentation benchmarks and real-robot manipulation tasks show that our model has a powerful open-world generalization abil- ity. Our data and code is available at this link. 1. Introduction Affordance prediction is a foundational research topic that significantly contributes to diverse practical applications, including robotic manipulation [16, 18, 19, 39, 53] and human-object interaction [3, 21, 23, 52, 58, 59]. It requiresarXiv:2507.23734v1 [cs.CV] 31 Jul 2025Dataset Images Categories Wild Robot Ego-centric Simulation Reasoning Output UMD [38] ICRA2015 10k 17 ✓ ✓ Seg AGD20k [36] CVPR2020 20k 50 ✓ Seg HANDAL [15] IROS2023 200k 17 ✓ Seg/Box AED [29] Arxiv2024 - 21 ✓ ✓ Seg 3DOI [43] ICCV2023 10k - ✓ ✓ Seg AffordanceLLM [45] CVPR2024 20k - ✓ Seg Man"}
{"id": "2507.23734v1_chunk4", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "015 10k 17 ✓ ✓ Seg AGD20k [36] CVPR2020 20k 50 ✓ Seg HANDAL [15] IROS2023 200k 17 ✓ Seg/Box AED [29] Arxiv2024 - 21 ✓ ✓ Seg 3DOI [43] ICCV2023 10k - ✓ ✓ Seg AffordanceLLM [45] CVPR2024 20k - ✓ Seg ManipVQA [17] IROS2024 84k - ✓ ✓ ✓ Seg RAGNet (Ours) 273k 180 ✓ ✓ ✓ ✓ ✓ Seg Table 1. Comparisons between previous affordance data and our collection. “(11k)” represents the number of video clips. “-” means unavailable data. “Reasoning” refers to reasoning instructions. comprehensively understanding the geometry and function of an object ( e.g., a wok affords to hold, a microwave door affords to pull) for further detecting graspable regions. Nonetheless, this task faces two additional primary difficul- ties in the realm of open-world generalization concerning vision and language instructions: 1) The capability to gen- eralize in a broad range of unseen object categories and im- age domains. 2) The alignment of commands that mimic human-like high-level instructions. Initial studies often specia"}
{"id": "2507.23734v1_chunk5", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "he capability to gen- eralize in a broad range of unseen object categories and im- age domains. 2) The alignment of commands that mimic human-like high-level instructions. Initial studies often specialize in particular domains, as depicted in Table 1. For instance, UMD [38] offers 10,000 RGB-D image pairs from three cluttered scenes with pixel- level affordance annotations. This dataset is categorized as robot data , characterized by a distinct operational table and a relatively stable background. Despite the abundance of public robot datasets [4, 42, 57], they often lack meticu- lous fine-grained labeling. Ego-centric data , which is cap- tured from a first-person or egocentric viewpoint, is the most prevalent type of data [8, 14]. Nevertheless, this data is often collected in indoor kitchens and workshops, result- ing in a lack of diversity in the data category distribution. Wild data , another prevalent type, is gathered in various sit- uations, which usually cover diverse categorie"}
{"id": "2507.23734v1_chunk6", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "hens and workshops, result- ing in a lack of diversity in the data category distribution. Wild data , another prevalent type, is gathered in various sit- uations, which usually cover diverse categories [36, 47, 54]. However, certain categories within this data, like bicycles, motorcycles, sofas and etc., are not suitable for robotic ma- nipulation. In addition, these data often fail to generalize to unseen domains and novel objects for affordance prediction. To enhance open-world generalization, Vision Language Model (VLM) with massive image-text training [30, 33] has become an important increment in various visual prediction tasks [26, 48]. Inspired by this, VLM also has an increasing interest in affordance prediction along with complex rea- soning, and several recent works are attempting to predict affordance regions [17, 28, 31, 45, 61, 64]. For example, ManipVQA [17] proposes tool detection, affordance recog- nition, and a broader understanding of physical concepts in a unified fra"}
{"id": "2507.23734v1_chunk7", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": " to predict affordance regions [17, 28, 31, 45, 61, 64]. For example, ManipVQA [17] proposes tool detection, affordance recog- nition, and a broader understanding of physical concepts in a unified framework. However, these works employ a fixed format of language prompt and a limited data scale, which brings some concerns about open-world generalization and complex reasoning. Overall, current efforts are either con- strained by insufficiently diverse datasets or by the absence of effective reasoning mechanisms, which hampers the abil- ity to perform open-world grasping.To address these challenges, we first build large-scale af- fordance segmentation data from various image sources, in- cluding wild, robot, ego-centric, and even simulated data, named RAGNet . It has 273k images and 180 categories. We design a set of affordance annotation tools for label- ing the regions of objects that are amenable to grasping. Furthermore, we leverage Large Language Models (LLMs) to generate a vast arra"}
{"id": "2507.23734v1_chunk8", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "ories. We design a set of affordance annotation tools for label- ing the regions of objects that are amenable to grasping. Furthermore, we leverage Large Language Models (LLMs) to generate a vast array of reasoning-based instructions, to- taling 26k distinct expressions. Here, we create two types of reasoning-based instructions beyond the template-based. One includes the name of the object and the other omits it. Take a knife as an example: “Please provide a knife” versus “I want something to slice the bread”. This approach closely mirrors real-life human interactions. In summary, we have constructed a massive-scale database with domains, object categories, and complex reasoning instructions. Furthermore, we introduce an affordance-based grasp- ing framework, titled AffordanceNet . It presents a deploy- able and general grasping pipeline, which stands out from prior MLLM-based affordance prediction methods [17, 45] that have not yet demonstrated real-robot deployment. Our model include"}
{"id": "2507.23734v1_chunk9", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "ents a deploy- able and general grasping pipeline, which stands out from prior MLLM-based affordance prediction methods [17, 45] that have not yet demonstrated real-robot deployment. Our model includes two crucial components, AffordanceVLM and Pose Generator. The AffordanceVLM transforms RGB images and human instruction into an accurate affordance map, while the pose generator uses the 2D affordance with a depth image to produce 3D grasper pose. In this work, we conduct extensive experiments to assess its open-world gen- eralization and reasoning capabilities. First, we create two distinct validation datasets to assess open-world generaliza- tion: one for zero-shot categoty recognition and another for out-of-domain affordance prediction. Second, to test rea- soning ability, we implement an affordance segmentation validation based on instructions that do not contain any tar- get category names. Thirdly, we carry out a range of close- loop real-robot grasping tasks in an entirely out-of-"}
{"id": "2507.23734v1_chunk10", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": " affordance segmentation validation based on instructions that do not contain any tar- get category names. Thirdly, we carry out a range of close- loop real-robot grasping tasks in an entirely out-of-domain setting. Last but not least, we test several representative simulation tasks from RLBench. All experiments show that our proposed method has great generalization and reasoning ability. In summary, our contributions are three-fold: • We present a large-scale reasoning-based affordance seg- mentation benchmark, RAGNet, for general grasping. It is collected from diverse sources and carefully annotated with affordance mask and reasoning instructions.VLM Mask Decoder Please prepare tools suitable for cooking. [AFF] Token Depth Filter Pose Generator Pose Project Mask Pseudo Point CloudManipulationSource Data 3DOI HANDAL EgoObjectsGraspNet Open -XRLBench Data Annotation Annotation Pipeline Instruction Building SAM2 VLPart Florence2 Human ImageCategoryPlease segment the affordance map of <c"}
{"id": "2507.23734v1_chunk11", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "ationSource Data 3DOI HANDAL EgoObjectsGraspNet Open -XRLBench Data Annotation Annotation Pipeline Instruction Building SAM2 VLPart Florence2 Human ImageCategoryPlease segment the affordance map of <category > in image ! Please give me a hammer ! We need to drive a nail into wood ! GPT4Template HardEasy ReasoningRAGNetFigure 2. Overview of our data annotation pipeline and manipulation model. We collect data from several public datasets, including HANDAL, GraspNet, Open-X, etc. A variety of models and manual annotation are utilized to annotate affordance masks. Subsequently, we refine textual instructions using templates and GPT4 to emulate human-like commands. For grasp operations, the VLM model is employed to identify affordance regions, which are then converted into the required grasp poses by integrating depth information. • We introduce an affordance-based grasping baseline, Af- fordanceNet, which bridges the gap between VLM-based affordance prediction and real-robot general graspi"}
{"id": "2507.23734v1_chunk12", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "ses by integrating depth information. • We introduce an affordance-based grasping baseline, Af- fordanceNet, which bridges the gap between VLM-based affordance prediction and real-robot general grasping. • We conduct extensive experiments, including zero-shot and out-of-domain affordance segmentation, real-robot grasping evaluation, which show great performance. 2. Related Work The investigation of affordances has a rich history in the computer vision and robotics communities [1, 12, 13, 25]. For embodied agents to successfully interact with the func- tional components within a scene, they need the capabil- ity to comprehend visual affordances. To advance research in this area, previous studies have dedicated significant re- sources to two main areas: benchmarks and algorithms. Benchmark. With the advances in deep learning, there has been a growing demand for the creation of large-scale af- fordance databases. UMD [38] stands out as a prior effort, which defines objects with effective "}
{"id": "2507.23734v1_chunk13", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "With the advances in deep learning, there has been a growing demand for the creation of large-scale af- fordance databases. UMD [38] stands out as a prior effort, which defines objects with effective affordances as those that an agent can grasp to produce an effect on another ob- ject. In addition, it offers a dataset of 10k RGB-D images, each accompanied by a pixel-level affordance segmentation mask. OPRA [11] proposes a different affordance learn- ing pattern, which employs demonstration videos to guide the prediction of interaction region on a target image. It includes 20,612 video clips, each corresponding to an im- age and an annotation of interaction heatmap and action la- bel. Despite the pioneering efforts in affordance learning,the existing datasets [7, 11, 35, 38, 40, 51] still face con- straints regarding affordance category diversity, image qual- ity, and scene complexity. Afterward, Luo et al. establish a large-scale affordance grounding dataset, AGD20k [36] , which contai"}
{"id": "2507.23734v1_chunk14", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "ace con- straints regarding affordance category diversity, image qual- ity, and scene complexity. Afterward, Luo et al. establish a large-scale affordance grounding dataset, AGD20k [36] , which contains 20k exocentric images from 36 affordance categories. Guo et al . build the HANDANL dataset [15] for real-world robot manipulation, which provides precise handle annotation for 17 hardware and kitchen tool cate- gories. Recently, there are a variety of datasets on affor- dance prediction that have been introduced with different output formats, such as key points [22, 31, 43, 49], bound- ing box [55, 56], pixel-wise mask [28, 29, 32]. However, most current research tends to focus on specific domains, such as robotics or first-person perspectives, which limits their applicability to other areas. In contrast, our research aims to investigate the potential for open-world generaliza- tion in affordance prediction by leveraging extensive data. Algorithm. In the deep learning era, the common me"}
{"id": "2507.23734v1_chunk15", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "s. In contrast, our research aims to investigate the potential for open-world generaliza- tion in affordance prediction by leveraging extensive data. Algorithm. In the deep learning era, the common meth- ods use supervised learning for affordance prediction [7, 9, 15, 35, 40]. However, these approaches struggle to generalize to domains outside of their training environ- ments. To tackle this issue, certain studies utilize transfer learning [27, 36, 37, 62] or self-supervised learning tech- niques [6] to enable affordance prediction from data orig- inating in different domains. With the significant progress of VLM, attempting these foundation models in affordance prediction has attracted much interest in the research com- munity [17, 28, 31, 45, 61, 64]. However, these methodsData source Domain Annotation Rea. Inst. Categories HANDAL [15] Wild ❶ 8.5k 17 Open-X [42] Robot ❸❹❺ - 124 GraspNet [10] Robot ❶❺ - 32 EgoObjects [68] Ego ❷❹❺ 17.4k 74 RLBench [20] Simulation ❺ - 10 Table 2. Detail"}
{"id": "2507.23734v1_chunk16", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "main Annotation Rea. Inst. Categories HANDAL [15] Wild ❶ 8.5k 17 Open-X [42] Robot ❸❹❺ - 124 GraspNet [10] Robot ❶❺ - 32 EgoObjects [68] Ego ❷❹❺ 17.4k 74 RLBench [20] Simulation ❺ - 10 Table 2. Details of training data annotation in RAGNet. The affordance mask annotation are in §3.2 and the generation details of reasoning instructions (Rea. Inst.) can be found in §3.3. usually learn from limited affordance demonstration data, which opens up opportunities for further research into how large-scale data can impact generalization in open-world scenarios and enhance knowledge reasoning. 3. Dataset In this section, we will present how to build the large-scale reasoning-based affordance segmentation benchmark RAG- Net. First, we gather extensive data from various sources, as detailed in §3.1. Then, we introduce five tools to annotate this data with grasping-oriented affordance masks accord- ing to the original dataset characteristics in §3.2. Mean- while, we offer detailed instructions for re"}
{"id": "2507.23734v1_chunk17", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "n, we introduce five tools to annotate this data with grasping-oriented affordance masks accord- ing to the original dataset characteristics in §3.2. Mean- while, we offer detailed instructions for reasoning-based af- fordance segmentation in §3.3. Lastly, we establish multi- ple validation benchmarks for testing grasping-oriented af- fordance segmentation in §3.4. The data annotation pipeline is illustrated in Fig. 2. 3.1. Data Source Given that robotics can function across various embodiment realms, including real-world settings and robotic hardware, we collect a broad spectrum of data. The dataset com- prises sources such as wild data ( i.e., HANDAL [15]), real robots ( i.e., Open-X [42], GraspNet [10]), and ego-centric data ( i.e., EgoObjects [68]). For a fair comparison environ- ment, we consider a collection of simulation data ( i.e., RL- bench [20]). We gather 273k images as shown in Table 2. 3.2. Affordance Map Annotation To facilitate affordance segmentation with reduced manua"}
{"id": "2507.23734v1_chunk18", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": " we consider a collection of simulation data ( i.e., RL- bench [20]). We gather 273k images as shown in Table 2. 3.2. Affordance Map Annotation To facilitate affordance segmentation with reduced manual input, we develop a suite of annotation tools. The annota- tion workflow is designed to be adaptable, taking into ac- count the functional and physical characteristics of various object categories. For instance, when dealing with an object like a soda can, the robotic system typically grasps the en- tire object, necessitating full object annotation. On the other hand, an object such as a wok requires precise annotation of its handle for effective grasping. This tool suite comprises five annotation tools, each with its own level of priority. ❶Original mask : Certain datasets specify affordance seg- mentation with precision (such as HANDAL dataset [15]), while others, like those featuring objects without handles (e.g., computer mouse, soda can, and pen), do not require User: Can you find a"}
{"id": "2507.23734v1_chunk19", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "e seg- mentation with precision (such as HANDAL dataset [15]), while others, like those featuring objects without handles (e.g., computer mouse, soda can, and pen), do not require User: Can you find a mug for tea? Assistant: The mug has a handle which you can hold to pour the coffee. [AFF]User: I need something to drink coffee. Assistant: The mug is suitable for this, its handle can be held to drink coffee safely. [AFF]Figure 3. Easy (left) v.s.Hard (right) Reasoning Instruction. The hard version has no category name itself. detailed affordance masks for grasping. In these cases, we utilize the original masks as our affordance annotations without further refinement. ❷SAM2 : For objects that lack handles, SAM2 [50] can still be utilized to generate a mask, when only the ground-truth bounding box is available (like EgoObjects [68]). ❸Florence2 + SAM2 : Due to the presence of language in- structions (like Open-X [42]), Florence2 [60] which gen- erates polygon boxes coupled with SAM2 can b"}
{"id": "2507.23734v1_chunk20", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "g box is available (like EgoObjects [68]). ❸Florence2 + SAM2 : Due to the presence of language in- structions (like Open-X [42]), Florence2 [60] which gen- erates polygon boxes coupled with SAM2 can be used to produce an affordance map for objects that are handle-free. ❹VLPart + SAM2 : VLPart [54] enables part-level recogni- tion ( i.e., knife handle and mug handle), hence we leverage it and SAM2 for segmenting object handles if the corre- sponding category has been trained within VLPart. ❺Human (+ SAM2) : If the above four tools fail to com- plete the affordance segmentation accurately, we will con- sider manual affordance annotation. The use of SAM2 is optional, particularly when dealing with video sequences. According to the original annotation information offered by data sources, we employ different compositions for af- fordance mask annotation. The composition details are listed in Table 2. We incorporate more detailed annotations, encompassing variations in tool arrangement acros"}
{"id": "2507.23734v1_chunk21", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "ploy different compositions for af- fordance mask annotation. The composition details are listed in Table 2. We incorporate more detailed annotations, encompassing variations in tool arrangement across subsets and categories, within the supplementary. Besides, we pro- vide several representative examples in Fig. 1 and more an- notation examples can also be found in the supplementary. 3.3. Reasoning Instruction Annotation As previously mentioned, the current VLM possesses com- pelling reasoning capabilities. To harness these capabilities for affordance reasoning, we construct a set of instructions. In this section, we introduce three types of instructions, in- cluding one template-based and two reasoning-based. The first kind of instruction is template-based . For in- stance, a template is “Please segment the affordance map of<category name> in this image”. This template canValidation Set Images Zero-shot Reason. Anno. HANDAL [15] 65k - N/A ❶ HANDAL†[15] 1k - N/A ❶ GraspNet seen [10] 1k"}
{"id": "2507.23734v1_chunk22", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "“Please segment the affordance map of<category name> in this image”. This template canValidation Set Images Zero-shot Reason. Anno. HANDAL [15] 65k - N/A ❶ HANDAL†[15] 1k - N/A ❶ GraspNet seen [10] 1k - N/A ❶❺ GraspNet novel [10] 1k ✓ N/A ❶❺ 3DOI [43] 1k ✓ N/A ❺ HANDAL†[15] 1k - easy ❶ HANDAL†[15] 1k - hard ❶ 3DOI [43] 1k ✓ easy ❺ Table 3. Details of validation set.†means a HANDAL subset. Gray means zero-shot setting. As highly replicated images, we randomly select 1k images from the source dataset for validation. be utilized in our entire dataset for affordance prediction. The second category consists of easy reasoning-based in- structions that are based on straightforward reasoning. A key aspect of these instructions is the explicit mention of the object being referred to. In contrast to template-based approaches and previous studies [17, 26, 45], the third cat- egory comprises hard reasoning-based instructions, which do not include the category name. For precise identifica- tion of "}
{"id": "2507.23734v1_chunk23", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": " template-based approaches and previous studies [17, 26, 45], the third cat- egory comprises hard reasoning-based instructions, which do not include the category name. For precise identifica- tion of the target object, the hard mode instructions utilize a functional description. Fig. 3 presents a typical example. Towards grasping a mug, the easy instruction might be “Can you find a mug for tea”, while the hard instruction is “I need something to drink coffee”. To produce these reasoning-based instructions with min- imal human labor and computational resources, we make full use of the capabilities of GPT-4 [2]. The specific prompt utilized is detailed in the supplementary material. Accord- ing to our data, we craft 8.5k hard instructions for the HAN- DAL dataset, 12.7k easy ones and 4.7k hard ones for the EgoObjects, totaling 26k reasoning-based instructions. 3.4. Evaluation Dataset To assess the open-world generalization of our affordance prediction model, we contribute two distinct ze"}
{"id": "2507.23734v1_chunk24", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "rd ones for the EgoObjects, totaling 26k reasoning-based instructions. 3.4. Evaluation Dataset To assess the open-world generalization of our affordance prediction model, we contribute two distinct zero-shot af- fordance evaluation scenarios. The first scenario evaluates the affordance model by extending its predictions to object categories not encountered during training. The second sce- nario evaluates the model generalization ability across dif- ferent data domains. For this purpose, we select the 3DOI dataset [43], which includes data from Articulation [44], EpicKitchen [8], and Taskonomy [65], ensuring that there is no overlap with the training data in the validation set. We present four different validation sets in Table 3. Due to high-similarity images, we construct a subset from the HANDAL, which is marked with ‘†’. For the same reason, other validation sets also utilize 1k images from the source dataset. HANDAL and GraspNet seen represent the ob- ject categories and image doma"}
{"id": "2507.23734v1_chunk25", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "e HANDAL, which is marked with ‘†’. For the same reason, other validation sets also utilize 1k images from the source dataset. HANDAL and GraspNet seen represent the ob- ject categories and image domains that the model has been trained on. GraspNet novel represents the unseen object category, while 3DOI refers to the unseen image domain.In addition, we provide three reasoning-based affordance segmentation validation sets, as illustrated in Table 3. The reasoning instruction version used in each subset is marked as “easy” or “hard”. We utilize the generalized Intersection over Union (gIoU) and complete Intersection over Union (cIoU) as our primary metrics. 4. AffordanceNet To achieve the goal of open-world grasping, we propose a comprehensive framework, named AffordanceNet. This model consists of two key components: AffordanceVLM for predicting affordance segmentation mask and pose gen- eration for transforming the mask into grasper position in 3D space. The overall framework is illustr"}
{"id": "2507.23734v1_chunk26", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "sts of two key components: AffordanceVLM for predicting affordance segmentation mask and pose gen- eration for transforming the mask into grasper position in 3D space. The overall framework is illustrated in Fig. 2. 4.1. AffordanceVLM Our AffordanceVLM is based on the vision-language seg- mentation model LISA [26] and incorporates two essential task-specific modifications to enhance affordance predic- tion: (1) developing a specialized system prompt, and (2) introducing a unique <AFF> token. Specifically, we first process the input image using an image encoder ( i.e., ViT-CLIP [46]), which is then pro- jected into the LLM’s embedding space via a linear layer projector. Meanwhile, the language prompt is tokenized by a text tokenizer, where each affordance instruction is en- hanced by “You are an embodied robot.” The resulting im- age and text features are concatenated and fed into the LLM (i.e., Vicuna-7B [66]). For general segmentation, the LLM generates a response that includes a spec"}
{"id": "2507.23734v1_chunk27", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "re an embodied robot.” The resulting im- age and text features are concatenated and fed into the LLM (i.e., Vicuna-7B [66]). For general segmentation, the LLM generates a response that includes a special token <SEG> . However, since <SEG> is a token within the LLM’s vocab- ulary, its representation is confined to a fixed feature space. This limitation restricts its representation capacity, thereby affecting the quality of the decoded mask. To address this issue, we introduce another special token <AFF> to enrich the original mask embedding. The underlying motivation is to explicitly direct the final mask embedding to focus more on affordance-specific language expressions. Finally, SAM [24] is used as a mask decoder to convert this mask embedding into a pixel-wise mask. Implementation Details. Beyond our reasoning-based af- fordance segmentation data, we also incorporate a variety of generic segmentation datasets into our training, which use<SEG> token. During inference, we first extrac"}
{"id": "2507.23734v1_chunk28", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "ls. Beyond our reasoning-based af- fordance segmentation data, we also incorporate a variety of generic segmentation datasets into our training, which use<SEG> token. During inference, we first extract the <AFF> token, followed by the <SEG> token. More imple- mentation details about data sampling and training settings can be found in the supplementary. 4.2. Pose Generator Once obtained AffordanceVLM that delivers precise affor- dance segmentation prediction, we start to apply the model to robotic grasping tasks, aiming to bridge the gap for theMethodHANDAL HANDAL†GraspNet seen GraspNet novel 3DOI gIoU cIoU gIoU cIoU gIoU cIoU gIoU cIoU gIoU cIoU Foundation Model without LLMs VLPart [54] + SAM2 [50] 40.9 28.9 40.7 27.6 - - - - - - Grounding DINO [34] + SAM2 [50] 34.7 26.8 34.9 26.9 - - - - - - Florence 2 [60] + SAM2 [50] 39.7 22.4 39.4 22.5 - - - - - - Generalist MLLMs LISA [26] 16.2 12.0 15.4 11.8 17.7 17.7 25.2 24.1 21.5 13.7 GLaMM [48] 24.9 17.2 25.1 17.0 21.6 10.5 19.2 8.6 19.7 14.1"}
{"id": "2507.23734v1_chunk29", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "- - Florence 2 [60] + SAM2 [50] 39.7 22.4 39.4 22.5 - - - - - - Generalist MLLMs LISA [26] 16.2 12.0 15.4 11.8 17.7 17.7 25.2 24.1 21.5 13.7 GLaMM [48] 24.9 17.2 25.1 17.0 21.6 10.5 19.2 8.6 19.7 14.1 AffordanceNet (Ours) 60.3 60.8 60.5 60.3 63.3 64.0 45.6 33.2 37.4 37.4 Table 4. Quantitative results on affordance segmentation. We use the fixed format of “ <categoty name> handle” in foundation models without LLMs, while utilize “affordance map of <categoty name> ” in generalist MLLMs. Gray means zero-shot benchmark. MethodHANDAL†Grasp. novel 3DOI gIoU cIoU gIoU cIoU gIoU cIoU LISA 16.2 12.0 25.2 24.1 21.5 13.7 + HANDAL 56.3 54.9 16.6 14.4 18.0 12.2 + Open-X 59.3 56.7 19.2 18.4 24.5 16.0 + EgoObejcts 61.8 61.6 8.0 6.9 35.5 34.6 + GraspNet 61.7 61.7 51.5 38.5 40.9 41.8 + Reasoning 56.5 55.4 43.0 33.8 36.8 40.2 + RLBench 56.7 55.0 42.8 33.2 36.5 39.9 Ours 60.5 60.3 45.6 33.2 37.4 37.4 Table 5. Ablation study of data on affordance segmentation. Each data is added one by one. Compared to “+"}
{"id": "2507.23734v1_chunk30", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "43.0 33.8 36.8 40.2 + RLBench 56.7 55.0 42.8 33.2 36.5 39.9 Ours 60.5 60.3 45.6 33.2 37.4 37.4 Table 5. Ablation study of data on affordance segmentation. Each data is added one by one. Compared to “+ RLBench”, our final model is enhanced by task-specific modifications, including specialized system prompt and unique <AFF> token (see §4.1). final steps in object grasping and manipulation. The im- pressive affordance segmentation results can be found in §5.1 and §5.2. In the following part, we will discuss more details about our grasp pose generator and the process of converting 2D affordance masks into the 3D pose estimates required for robotic arms. As illustrated in Fig. 2, we project the depth maps into 3D space for precise grasp pose generation. Let Pdenotes the set of points on the 2D image. Firstly, we filter the af- fordance region by applying the affordance mask MtoP through a binary multiplication ⊗, resulting in ˆP=P⊗M. Subsequently, for each 2D position (u, v)within the point"}
{"id": "2507.23734v1_chunk31", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "image. Firstly, we filter the af- fordance region by applying the affordance mask MtoP through a binary multiplication ⊗, resulting in ˆP=P⊗M. Subsequently, for each 2D position (u, v)within the point setˆP, we have  x y z 1 =T·K−1 u×d v×d d 1 , (1) where d is the along the axis orthogonal to the image plane, (x, y, z )are the corresponding world coordinates. K∈R4×4and T∈R4×4represent the camera intrinsic and extrinsic parameters, respectively. After getting the 3D position of object affordance, we can use various graspingMethodHANDAL (easy) HANDAL (hard) 3DOI gIoU cIoU gIoU cIoU gIoU cIoU G-DINO 3.6 3.0 3.4 3.1 4.1 3.9 LISA 15.5 11.9 12.3 8.1 12.3 8.1 GLaMM 4.7 3.5 5.0 3.5 4.4 2.9 Ours 58.3 58.1 58.2 57.8 38.1 39.4 Table 6. Quantitative results on reasoning-based affordance segmentation. ‘G-DINO’ is the short name of Grounding-DINO. They all use reasoning-based instructions as language prompts. models to generate grasper position. Finally, the grasper arrive the reques"}
{"id": "2507.23734v1_chunk32", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "e segmentation. ‘G-DINO’ is the short name of Grounding-DINO. They all use reasoning-based instructions as language prompts. models to generate grasper position. Finally, the grasper arrive the requested position and grasp the target object. 5. Experiments on Visual Affordance To minimize unnecessary source expenditure, we initially validate the quality of affordance segmentation (§4.1) prior to object grasping (§4.2). 5.1. Evaluation on Affordance Segmentation Implementation Details. To evaluate the task of affor- dance segmentation, we implement various advanced open- sourced approaches with potential affordance segmentation ability. In specific, we choose the foundation models with- out LLMs ( e.g., VLPart [54], Grounding DINO [34], Flo- rence2 [60]), and the generalist MLLMs ( e.g., LISA [26], GLaMM [48]). Since these foundation models only out- put bounding boxes or polygon boxes, we additionally em- ploy SAM2 [50] for mask refinement. We load their offi- cial checkpoints for dire"}
{"id": "2507.23734v1_chunk33", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "SA [26], GLaMM [48]). Since these foundation models only out- put bounding boxes or polygon boxes, we additionally em- ploy SAM2 [50] for mask refinement. We load their offi- cial checkpoints for direct evaluation. Given the ambiguous nature of the “affordance map of <category name> ” concept, which poses a challenge to the foundation mod- els without LLMs, we shift to a fixed format of instruction “<category name> handle” and evaluate them on only HANDAL dataset as each object have a handle. In contrast, we employ “affordance map of <category name> ” when testing the generalist MLLMs. Experiment Results. The main results on affordance seg- mentation are shown in Table 4. We can see our model out-hammer dragon drawer wok top drawerFigure 4. Affordance segmentation from our AffordanceNet . Even though they source from various data sources, such as wild, robot, ego-centric and simulation, our model can accurately capture their affordance region. More visualizations are included in supple"}
{"id": "2507.23734v1_chunk34", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "Even though they source from various data sources, such as wild, robot, ego-centric and simulation, our model can accurately capture their affordance region. More visualizations are included in supplementary. I need a fork for the dinner table. Could you grab a mug for me, please? I need a tool to drive nails into wood. Fetch something to drink hot tea. Can you find a knife for me? Figure 5. Reasoning-based affordance segmentation from our AffordanceNet . The left examples represent easy reasoning-based instructions with referent name, while the right are hard instructions that include object function or intention rather than the name itself. performs all other competitors across all datasets. Besides, the scores on the entire HANDAL dataset and its small sub- set are similar, guaranteeing the diversity and representative of the small subset. Several visualizations across various domains are shown in Fig. 4. Here, the wok handle is accu- rately segmented even if its scene has never bee"}
{"id": "2507.23734v1_chunk35", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": " the diversity and representative of the small subset. Several visualizations across various domains are shown in Fig. 4. Here, the wok handle is accu- rately segmented even if its scene has never been encoun- tered, indicating remarkable open-world generalization. Ablation Study. It is of interest to examine the impact of individual datasets on affordance segmentation tasks. In Ta- ble 5, we incrementally incorporate each dataset into the training process. We can see that the absence of HANDAL data significantly impairs the model performance on the HANDAL test set. Furthermore, the incorporation of rea- soning data leads to a slight decline in performance metrics. However, the introduction of task-specific modifications, such as a specialized system prompt and a unique <AFF> token, enhances model performance. Overall, our model shows powerful open-world affordance segmentation. 5.2. Evaluation on Reasoning Affordance We also test the foundation models and generalist MLLMs on reasoning"}
{"id": "2507.23734v1_chunk36", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "es model performance. Overall, our model shows powerful open-world affordance segmentation. 5.2. Evaluation on Reasoning Affordance We also test the foundation models and generalist MLLMs on reasoning-based affordance segmentation datasets. All the experiment settings are aligned with the above section. Experiment Results. Despite this challenging task, the re- sults in Table 6 show our model outperforms other meth- ods by a large margin. We provide several reasoning-based qualitative results in Fig. 5. The last two examples show that our model can predict precise hammer and mug han- dles even if there is no target mentioned in the instructions. These results confirm the reasoning ability of our model. Remark. The above two vision experiments demonstrate that only our model excels in both affordance perception and reasoning in zero-shot domain. This provides a solid foundation for the subsequent object grasping tasks.6. Experiments on Object Grasping 6.1. Evaluation on Real Robot Imple"}
{"id": "2507.23734v1_chunk37", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "affordance perception and reasoning in zero-shot domain. This provides a solid foundation for the subsequent object grasping tasks.6. Experiments on Object Grasping 6.1. Evaluation on Real Robot Implementation Details. To evaluate the effectiveness of our model for open-world generalization in real-world en- vironments, we introduce a series of manipulation experi- ments. Specifically, we deploy UR5 robot arm with a third- person RGB-D camera (Intel RealSense). We design 10 dis- tinct grasping tasks, including grasping the can, pen, screw- driver, hammer, wok, mouse, circle, toy, spatula, scissors. Half of them require accurately localizing the affordance region, like the screwdriver handle. Each task is performed 10 times, and we report the average success rate. Note that, we never provide any demonstration images or videos from this scene for our model training. The well-trained Affor- danceVLM is directly used for the zero-shot evaluation. As for the pose generation, we follow Grasp"}
{"id": "2507.23734v1_chunk38", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "e any demonstration images or videos from this scene for our model training. The well-trained Affor- danceVLM is directly used for the zero-shot evaluation. As for the pose generation, we follow GraspNet [10] condition 3D affordance point cloud to generate a 3D grasp proposal for grasper operation. We compare our model with a pop- ular grasping model, GraspNet [10]. Since GraspNet lacks the capability for language-conditioned grasping, we ensure that only the target object remains on the table. Experiment Results. Table 7 displays the performance comparison between ours and GraspNet. Clearly, our model AffordanceNet has superior success rates, even in challeng- ing and complicated environments. Fig. 6 highlights four sequences, which show accurate affordance perception and effective object grasping ( i.e., screwdriver, wok, circle, and mouse). More grasping examples are in our supplementary. Ablation Study. Table 8 shows ablations of AffordanceNet on affordance prediction models (repla"}
{"id": "2507.23734v1_chunk39", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": " grasping ( i.e., screwdriver, wok, circle, and mouse). More grasping examples are in our supplementary. Ablation Study. Table 8 shows ablations of AffordanceNet on affordance prediction models (replaced by VLPart and LISA) and instruction types (replaced by easy and hard reasoning-based instructions) using five tasks. As demon- strated, AffordanceNet delivers superior affordance predic-Figure 6. Object grasping results from our AffordanceNet on robot arm UR5 . The instructions are “ I need a screwdriver for repairing ”, “Can you hand me the wok, please? ”, “Give me the circle ”, “Please hand me a computer mouse ”, respectively. Method Can Pen Screwdriver Hammer Wok Mouse Circle Toy Spatula Scissors Average GraspNet [10] 40% 10% 10% 20% 60% 50% 10% 60% 30% 30% 32% AffordanceNet (Ours) 80% 60% 60% 80% 70% 80% 40% 90% 70% 70% 70% Table 7. Average success rates on robotic grasping. GraspNet cannot support language (see §6.1). Each task is conducted by 10 times. Method Can Pen Screw. Ham. "}
{"id": "2507.23734v1_chunk40", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "60% 60% 80% 70% 80% 40% 90% 70% 70% 70% Table 7. Average success rates on robotic grasping. GraspNet cannot support language (see §6.1). Each task is conducted by 10 times. Method Can Pen Screw. Ham. Wok Average VLPart [54] 70% 30% 40% 30% 0 34% LISA [26] 80% 40% 0 10% 0 26% Easy Reasoning 70% 50% 50% 80% 60% 62% Hard Reasoning 60% 40% 40% 60% 40% 48% AffordanceNet 80% 60% 60% 80% 70% 70% Table 8. Ablation studies on real-robot grasping in terms of different affordance prediction models ( i.e., VLPart and LISA) and different instructions ( i.e., easy reasoning and hard reasoning). tion while maintaining its reasoning capabilities. 6.2. Evaluation on Simulation Implementation Details. We conduct simulations based on RLBench sub-task [20] to validate our approach, including open drawer, close jar, and slide block to target. In practice, we divide each task into several keyframes, as follows: open drawer (3 keyframes), close jar (4 keyframes), slide block to target (5 keyframes). More det"}
{"id": "2507.23734v1_chunk41", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "e jar, and slide block to target. In practice, we divide each task into several keyframes, as follows: open drawer (3 keyframes), close jar (4 keyframes), slide block to target (5 keyframes). More details are in the supplementary. Experiment Results. We conduct 25 episodes for each task and calculate its average success rate as the primary evalu- ation metric. The quantitative results are in Table 9. Com- pared to another LLM-based method LLARV A [41] that is fine-tuned for a specific environment, our model focuses onTask LLARV A [41] AffordanceNet (Ours) Open drawer 60% 56% Slide block to target 100% 64% Close jar 28% 44% Average 62% 54.7% Table 9. Success rates of our model on RLBench simulation. Each task is conducted by 25 episodes. stronger generalization. Therefore, achieving comparable performance on RLBench is quite satisfying. 7. Conclusion In this work, we presented a new large-scale and diverse reasoning-based affordance segmentation dataset, named RAGNet, which aims to adva"}
{"id": "2507.23734v1_chunk42", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "ble performance on RLBench is quite satisfying. 7. Conclusion In this work, we presented a new large-scale and diverse reasoning-based affordance segmentation dataset, named RAGNet, which aims to advance the capabilities of general robotic grasping systems in varied open-world scenarios. Further, we proposed the model AffordanceNet, a compre- hensive framework including the AffordanceVLM and the grasping module, which uses our extensive affordance data to achieve open-world affordance capture and 3D grasper pose prediction, respectively. Through extensive experi- ments, we observed the superior performance and gener- alization ability of our AffordanceNet in zero-shot affor- dance segmentation, reasoning-based affordance segmenta- tion, real-robot grasping evaluation, and simulation tasks.References [1] Function-based generic recognition for multiple object cate- gories. CVGIP: Image Understanding , 1994. 3 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah- mad, Ilge Akkaya, Flo"}
{"id": "2507.23734v1_chunk43", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "References [1] Function-based generic recognition for multiple object cate- gories. CVGIP: Image Understanding , 1994. 3 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah- mad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. 5 [3] Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances from human videos as a versa- tile representation for robotics. In CVPR , 2023. 1 [4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakr- ishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817 , 2022. 2 [5] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco- stuff: Thing and stuff classes in context. In CVPR , 2018. 1 [6] Joya Chen, Difei Gao, Kevin Qinghong Lin, and Mike Zheng Shou. Affor"}
{"id": "2507.23734v1_chunk44", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": " 2 [5] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco- stuff: Thing and stuff classes in context. In CVPR , 2018. 1 [6] Joya Chen, Difei Gao, Kevin Qinghong Lin, and Mike Zheng Shou. Affordance grounding from demonstration video to target image. In CVPR , 2023. 3 [7] Ching-Yao Chuang, Jiaman Li, Antonio Torralba, and Sanja Fidler. Learning to act properly: Predicting and explaining affordances from images. In CVPR , 2018. 3 [8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision: Collection, pipeline and chal- lenges for epic-kitchens-100. IJCV , 2022. 2, 5 [9] Thanh-Toan Do, Anh Nguyen, and Ian Reid. Affordancenet: An end-to-end deep learning approach for object affordance detection. In ICRA , 2018. 3 [10] Hao-Shu Fang, Chenxi Wang, Minghao Gou, and Cewu Lu. Graspnet-1billion: A large-scale benchmark for general ob- ject grasping. I"}
{"id": "2507.23734v1_chunk45", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "arning approach for object affordance detection. In ICRA , 2018. 3 [10] Hao-Shu Fang, Chenxi Wang, Minghao Gou, and Cewu Lu. Graspnet-1billion: A large-scale benchmark for general ob- ject grasping. In CVPR , 2020. 4, 5, 7, 8, 1 [11] Kuan Fang, Te-Lin Wu, Daniel Yang, Silvio Savarese, and Joseph J Lim. Demo2vec: Reasoning object affordances from online videos. In CVPR , 2018. 3 [12] Vittorio Ferrari and Andrew Zisserman. Learning visual at- tributes. NeurIPS , 2007. 3 [13] James J Gibson. The ecological approach to visual percep- tion: classic edition . Psychology press, 2014. 3 [14] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR , 2022. 2 [15] Andrew Guo, Bowen Wen, Jianhe Yuan, Jonathan Tremblay, Stephen Tyree, Jeffrey Smith, and Stan Birchfield. HAN- DAL: A dataset of real-world manipulable object categories "}
{"id": "2507.23734v1_chunk46", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "video. In CVPR , 2022. 2 [15] Andrew Guo, Bowen Wen, Jianhe Yuan, Jonathan Tremblay, Stephen Tyree, Jeffrey Smith, and Stan Birchfield. HAN- DAL: A dataset of real-world manipulable object categories with pose annotations, affordances, and reconstructions. In IROS , 2023. 2, 3, 4, 5, 1 [16] Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, and Yang Gao. Copa: General robotic manipulation through spatial constraints of parts with foundation models. arXiv preprint arXiv:2403.08248 , 2024. 1[17] Siyuan Huang, Iaroslav Ponomarenko, Zhengkai Jiang, Xi- aoqi Li, Xiaobin Hu, Peng Gao, Hongsheng Li, and Hao Dong. Manipvqa: Injecting robotic affordance and physi- cally grounded information into multi-modal large language models. In IROS , 2024. 2, 3, 5 [18] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. V oxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973 , 2023. 1 [19] Wenlong Huang, Chen Wang, Yunzh"}
{"id": "2507.23734v1_chunk47", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": " Yunzhu Li, Jiajun Wu, and Li Fei-Fei. V oxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973 , 2023. 1 [19] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of rela- tional keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652 , 2024. 1 [20] Stephen James, Zicong Ma, David Rovick Arrojo, and An- drew J Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Let- ters, 2020. 4, 8 [21] Juntao Jian, Xiuping Liu, Manyi Li, Ruizhen Hu, and Jian Liu. Affordpose: A large-scale dataset of hand-object inter- actions with affordance-driven hand pose. In ICCV , 2023. 1 [22] Yuanchen Ju, Kaizhe Hu, Guowei Zhang, Gu Zhang, Min- grun Jiang, and Huazhe Xu. Robo-abc: Affordance gener- alization beyond categories via semantic correspondence for robot manipulation. In ECCV , 2024. 3 [23] Bumsoo Kim, Junhyun Lee, Jaewoo Kang, "}
{"id": "2507.23734v1_chunk48", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "in- grun Jiang, and Huazhe Xu. Robo-abc: Affordance gener- alization beyond categories via semantic correspondence for robot manipulation. In ECCV , 2024. 3 [23] Bumsoo Kim, Junhyun Lee, Jaewoo Kang, Eun-Sol Kim, and Hyunwoo J Kim. Hotr: End-to-end human-object in- teraction detection with transformers. In CVPR , 2021. 1 [24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White- head, Alexander C Berg, Wan-Yen Lo, et al. Segment any- thing. In ICCV , 2023. 5 [25] Hedvig Kjellstr ¨om, Javier Romero, and Danica Kragi ´c. Vi- sual object-action recognition: Inferring object affordances from human demonstration. Computer Vision and Image Un- derstanding , 2011. 3 [26] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmenta- tion via large language model. In CVPR , 2024. 2, 5, 6, 8, 1 [27] Gen Li, Varun Jampani, Deqing Sun, and Laura Sevilla-Lara. Locate: Localize and transfer o"}
{"id": "2507.23734v1_chunk49", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "u, and Jiaya Jia. Lisa: Reasoning segmenta- tion via large language model. In CVPR , 2024. 2, 5, 6, 8, 1 [27] Gen Li, Varun Jampani, Deqing Sun, and Laura Sevilla-Lara. Locate: Localize and transfer object parts for weakly super- vised affordance grounding. In CVPR , 2023. 3 [28] Gen Li, Deqing Sun, Laura Sevilla-Lara, and Varun Jampani. One-shot open affordance learning with foundation models. InCVPR , 2024. 2, 3 [29] Gen Li, Nikolaos Tsagkas, Jifei Song, Ruaridh Mon- Williams, Sethu Vijayakumar, Kun Shao, and Laura Sevilla- Lara. Learning precise affordances from egocentric videos for robotic manipulation. arXiv preprint arXiv:2408.10123 , 2024. 2, 3 [30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML , 2023. 2 [31] Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yux- ing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and HaoDong. Manipllm: Embodied multimodal large l"}
{"id": "2507.23734v1_chunk50", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "d large language models. In ICML , 2023. 2 [31] Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yux- ing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and HaoDong. Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. In CVPR , 2024. 2, 3 [32] Yicong Li, Na Zhao, Junbin Xiao, Chun Feng, Xiang Wang, and Tat-seng Chua. Laso: Language-guided affordance seg- mentation on 3d object. In CVPR , 2024. 3 [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS , 2024. 2, 1 [34] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499 , 2023. 6 [35] Timo Luddecke and Florentin Worgotter. Learning to seg- ment affordances. In ICCV Workshops , 2017. 3 [36] Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and Dacheng Tao. Learning affordance"}
{"id": "2507.23734v1_chunk51", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "23. 6 [35] Timo Luddecke and Florentin Worgotter. Learning to seg- ment affordances. In ICCV Workshops , 2017. 3 [36] Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and Dacheng Tao. Learning affordance grounding from exocen- tric images. In CVPR , 2022. 2, 3 [37] Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and Dacheng Tao. Leverage interactive affinity for affordance learning. In CVPR , 2023. 3 [38] Austin Myers, Ching L Teo, Cornelia Ferm ¨uller, and Yiannis Aloimonos. Affordance detection of tool parts from geomet- ric features. In ICRA , 2015. 2, 3 [39] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompt- ing elicits actionable knowledge for vlms. arXiv preprint arXiv:2402.07872 , 2024. 1 [40] Anh Nguyen, Dimitrios Kanoulas, Darwin G Caldwell, and Nikos G Tsagarakis. Object-based affordances detection with convolutional neural networks and dense conditional random fields"}
{"id": "2507.23734v1_chunk52", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "7872 , 2024. 1 [40] Anh Nguyen, Dimitrios Kanoulas, Darwin G Caldwell, and Nikos G Tsagarakis. Object-based affordances detection with convolutional neural networks and dense conditional random fields. In IROS , 2017. 3 [41] Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, and Roei Herzig. Llarva: Vision-action instruction tuning enhances robot learning. arXiv preprint arXiv:2406.11815 , 2024. 8 [42] Abby O’Neill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864 , 2023. 2, 4, 1 [43] Shengyi Qian and David F Fouhey. Understanding 3d object interaction from a single image. In ICCV , 2023. 2, 3, 5 [44] Shengyi Qian, Linyi Jin, Chris Rockwell, Siyi Chen, and David F Fouhey. Understanding 3d object articulation in in- ternet videos. In CVPR , 2022. 5 [45"}
{"id": "2507.23734v1_chunk53", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "om a single image. In ICCV , 2023. 2, 3, 5 [44] Shengyi Qian, Linyi Jin, Chris Rockwell, Siyi Chen, and David F Fouhey. Understanding 3d object articulation in in- ternet videos. In CVPR , 2022. 5 [45] Shengyi Qian, Weifeng Chen, Min Bai, Xiong Zhou, Zhuowen Tu, and Li Erran Li. Affordancellm: Grounding affordance from vision language models. In CVPR , 2024. 2, 3, 5 [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn- ing transferable visual models from natural language super- vision. In ICML , 2021. 5[47] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Mar- quez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In CVPR , 2023. 2, 1 [48] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdel- rahman Shaker, Salman Khan, Hisham Cholakkal, Rao M Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S K"}
{"id": "2507.23734v1_chunk54", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "es of common objects. In CVPR , 2023. 2, 1 [48] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdel- rahman Shaker, Salman Khan, Hisham Cholakkal, Rao M Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S Khan. Glamm: Pixel grounding large multimodal model. In CVPR , 2024. 2, 6 [49] Adam Rashid, Satvik Sharma, Chung Min Kim, Justin Kerr, Lawrence Yunliang Chen, Angjoo Kanazawa, and Ken Gold- berg. Language embedded radiance fields for zero-shot task- oriented grasping. In CoRL , 2023. 3 [50] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R¨adle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 , 2024. 4, 6, 1 [51] Johann Sawatzky, Abhilash Srikantha, and Juergen Gall. Weakly supervised affordance detection. In CVPR , 2017. 3 [52] Dandan Shan, Jiaqi Geng, Michelle Shu, and David F Fouhey. Understanding human hands in contact at internet scale. In CVPR , 2020. 1 [53] "}
{"id": "2507.23734v1_chunk55", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "eakly supervised affordance detection. In CVPR , 2017. 3 [52] Dandan Shan, Jiaqi Geng, Michelle Shu, and David F Fouhey. Understanding human hands in contact at internet scale. In CVPR , 2020. 1 [53] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In CoRL , 2022. 1 [54] Peize Sun, Shoufa Chen, Chenchen Zhu, Fanyi Xiao, Ping Luo, Saining Xie, and Zhicheng Yan. Going denser with open-vocabulary part segmentation. In ICCV , 2023. 2, 4, 6, 8, 1 [55] An Dinh Vuong, Minh Nhat Vu, Hieu Le, Baoru Huang, Binh Huynh, Thieu V o, Andreas Kugi, and Anh Nguyen. Grasp-anything: Large-scale grasp dataset from foundation models. arXiv preprint arXiv:2309.09818 , 2023. 3 [56] An Dinh Vuong, Minh Nhat Vu, Baoru Huang, Nghia Nguyen, Hieu Le, Thieu V o, and Anh Nguyen. Language- driven grasp detection. In CVPR , 2024. 3 [57] Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, An- dre Wang He, Vivek Myers,"}
{"id": "2507.23734v1_chunk56", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": ", and Anh Nguyen. Language- driven grasp detection. In CVPR , 2024. 3 [57] Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, An- dre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: A dataset for robot learning at scale. In CoRL , 2023. 2 [58] Xiaohan Wang, Yuehu Liu, Xinhang Song, Yuyi Liu, Six- ian Zhang, and Shuqiang Jiang. An interactive navigation method with effect-oriented affordance. In CVPR , 2024. 1 [59] Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. Move as you say interact as you can: Language-guided human motion generation with scene af- fordance. In CVPR , 2024. 1 [60] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing a unified representation for a variety of vision tasks. In CVPR , 2024. 4, 6, 1 [61] Ran Xu, Yan Shen, Xiaoqi Li, Ruihai Wu, and Hao Dong. Natura"}
{"id": "2507.23734v1_chunk57", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "chael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing a unified representation for a variety of vision tasks. In CVPR , 2024. 4, 6, 1 [61] Ran Xu, Yan Shen, Xiaoqi Li, Ruihai Wu, and Hao Dong. Naturalvlm: Leveraging fine-grained natural language for affordance-guided visual manipulation. arXiv preprint arXiv:2403.08355 , 2024. 2, 3[62] Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Jiebo Luo, and Zheng-Jun Zha. Grounding 3d object affordance from 2d interactions in images. In ICCV , 2023. 3 [63] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expres- sions. In ECCV , 2016. 1 [64] Qiaojun Yu, Siyuan Huang, Xibin Yuan, Zhengkai Jiang, Ce Hao, Xin Li, Haonan Chang, Junbo Wang, Liu Liu, Hong- sheng Li, et al. Uniaff: A unified representation of affor- dances for tool usage and articulation with vision-language models. arXiv preprint arXiv:2409.20551 , 2024. 2, 3 [65] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Ji"}
{"id": "2507.23734v1_chunk58", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "ntation of affor- dances for tool usage and articulation with vision-language models. arXiv preprint arXiv:2409.20551 , 2024. 2, 3 [65] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In CVPR , 2018. 5 [66] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. NeurIPS , 2023. 5 [67] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR , 2017. 1 [68] Chenchen Zhu, Fanyi Xiao, Andr ´es Alvarado, Yasmine Babaei, Jiabo Hu, Hichem El-Mohri, Sean Culatana, Roshan Sumbaly, and Zhicheng Yan. Egoobjects: A large-scale egocentric dataset for fine-grained object understanding. In ICCV , 2023. 4, 1RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General G"}
{"id": "2507.23734v1_chunk59", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "eng Yan. Egoobjects: A large-scale egocentric dataset for fine-grained object understanding. In ICCV , 2023. 4, 1RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping Supplementary Material 8. Details of Data Annotation As the original data sources, such as HANDAL [15], Open- X [42], EgoObjects [68], GraspNet [10], provide original annotation information ( e.g., ground-truth boxes or masks), we make full use of them for minimal human intervention. From these data, we emphasize grasping-oriented objects, encompassing both those with handles and those without. Therefore, as described before, we design five annotation tools:❶: Original mask, ❷: SAM2 [50], ❸: Florence2 [60] + SAM2, ❹: VLPart [54] + SAM2, ❺: Human (+ SAM2). The details of how to compose these tools for one specific dataset are shown in Table 11. In addition, Table 11 pro- vides the reasoning instruction annotation details. In sum- mary, our dataset RAGNet includes a broad range of da"}
{"id": "2507.23734v1_chunk60", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "these tools for one specific dataset are shown in Table 11. In addition, Table 11 pro- vides the reasoning instruction annotation details. In sum- mary, our dataset RAGNet includes a broad range of data domains, categories, and reasoning instructions, establish- ing a robust basis for open-world grasping applications. 9. Affordance Annotation Examples Since our benchmark RAGNet includes a significant num- ber of grasping-oriented objects from various domains (like robot, wild, and ego-centric domains), we highlight this as- pect by showcasing additional examples of affordance seg- mentation annotations in Fig. 8. For each affordance map annotation, the candidate objects are initially identified to determine if they possess a handle. Then, their affordance maps are carefully annotated according to our tool priority. For example, the banana from Open-X [42] is segmented using the combination of Florence2 [60] and SAM2 [50] ac- cording to its original grasping instruction. The knife han- "}
{"id": "2507.23734v1_chunk61", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "ding to our tool priority. For example, the banana from Open-X [42] is segmented using the combination of Florence2 [60] and SAM2 [50] ac- cording to its original grasping instruction. The knife han- dle from EgoObjects [68] can be accurately grounded using VLPart [54], and its output box can be further transformed into a pixel-wise mask using SAM2 [50]. Regarding the mi- crowave handle in the EgoObjects dataset [68], it has been annotated manually because there is no suitable tool avail- able for automated annotation. In conclusion, we collect a total of 273k diverse images along with their corresponding affordance annotations. 10. Reasoning-based Affordance Examples More reasoning-based affordance segmentation examples are shown in Fig. 10. It contains two types of instructions, easy instructions and hard instructions. As seen, the easy instructions include the target object name, while the hard ones only include functional descriptions without the ob- ject name. These instructions a"}
{"id": "2507.23734v1_chunk62", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "tructions and hard instructions. As seen, the easy instructions include the target object name, while the hard ones only include functional descriptions without the ob- ject name. These instructions are generated by GPT-4 and the corresponding prompts used in GPT-4 are listed in Ta-ble 12 and Table 13. The highlighted “words” are category names at most times. We sometimes provide additional key- words about potential grasping action for some categories, for aligning the instructions with the image content. For example, if the microwave is closed, we would assign the keywords “microwave, open the door”. 11. Implementation Details of AffordanceNet Beyond our reasoning-based affordance segmentation data, we also incorporate a variety of generic segmentation datasets into our training. This diverse generic set includes data for semantic segmentation ( e.g., ADE20k [67], COCO- Stuff [5], PACO [47]), referring segmentation ( e.g., Ref- COCO [63]), VQA ( e.g., LLaV A-150k [33]) and reasoning-"}
{"id": "2507.23734v1_chunk63", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "verse generic set includes data for semantic segmentation ( e.g., ADE20k [67], COCO- Stuff [5], PACO [47]), referring segmentation ( e.g., Ref- COCO [63]), VQA ( e.g., LLaV A-150k [33]) and reasoning- based segmentation ( e.g., ReasonSeg [26]). The data sam- pling ratios are presented in Table 10. We deploy eight NVIDIA A100 GPUs (80GB) to train our model, with a learning rate of 2e-5. The training loss follows [26], which uses binary focal loss for map prediction and cross-entropy loss for text output. We utilize a batch size of 40 without gradient accumulation. 12. More Results on Visual Affordance We provide more visualization results of affordance seg- mentation from our AffordanceVLM model in Fig. 10. The testing images are selected from multiple validation sets, such as GraspNet Novel , 3DOI, and HANDAL. We employ template-based, easy reasoning-based, and hard reasoning-based instructions for affordance map prediction, respectively. It is obvious that our AffordanceVLM can un- de"}
{"id": "2507.23734v1_chunk64", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "l , 3DOI, and HANDAL. We employ template-based, easy reasoning-based, and hard reasoning-based instructions for affordance map prediction, respectively. It is obvious that our AffordanceVLM can un- derstand these high-level human instructions, and transform them into precise affordance maps. Meanwhile, our model can deal with various challenging situations like unseen cat- egories or domains. Both suggest that our model possesses robust open-world reasoning capabilities, which will signif- icantly enhance subsequent object-grasping tasks. 13. More Results on Real Robot Beyond the evaluation tasks in our main manuscript, such as grasping can, pen, screwdriver, hammer, and wok, we also evaluate the open-world generalization capabilities of our model by utilizing a broader range of instructions en- compassing various unseen categories in real-robot envi- ronments, like panda, toy, circle and so on. The real- robot experiment videos are included within the same di- rectory. These results d"}
{"id": "2507.23734v1_chunk65", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "ns en- compassing various unseen categories in real-robot envi- ronments, like panda, toy, circle and so on. The real- robot experiment videos are included within the same di- rectory. These results demonstrate impressive open-worldFigure 7. Object grasping results from our AffordanceNet on RLBench . The instruction of the top video is “ Open the top drawer ”, and the bottom one refers to “ Close the green jar ”. Data Semantic Seg Referring Seg Reasoning-based Seg VQA Affordance Seg Reasoning-based Affordance Seg Ratio 3 1 1 1 9 3 Table 10. Data sampling ratios during training. object perception and grasping proficiency. 14. More Results on RLBench We present several visualization results from the simulation task RLBench in Figure 7. The top video demonstrates the task “open the top drawer,” while the bottom video illus- trates “close the green jar”. As shown, our model success- fully completes both tasks with high accuracy.adjustable wrench hammer ladle power drill strainer banana rxb"}
{"id": "2507.23734v1_chunk66", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": ",” while the bottom video illus- trates “close the green jar”. As shown, our model success- fully completes both tasks with high accuracy.adjustable wrench hammer ladle power drill strainer banana rxbar chocolate sponge middle drawer berry sushi fork pepsi can knife spatula blue chip bag carrot chocolate oven brush pen remote control soap drawer door fork frying pan microwave oven scissors knife mug screwdriver spoon wok banana black mouse camel pear panteneFigure 8. More affordance segmentation examples from our RAGNet. It covers various data sources, like robot, wild, and ego-centric domains.User: Can you heat up the food in the microwave oven? Assistant: Open the microwave oven, place the food inside. User: I require a hammer. Assistant: Grasp the hammer by the handle to use it effectively. User: I need a knife to cut vegetables. Assistant: Grasp the knife by its handle, be cautious with the sharp edge. User: Can you hand me a bottle, please? Assistant: The bottle can be grasped by "}
{"id": "2507.23734v1_chunk67", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "ely. User: I need a knife to cut vegetables. Assistant: Grasp the knife by its handle, be cautious with the sharp edge. User: Can you hand me a bottle, please? Assistant: The bottle can be grasped by its body for easy handling. User: Could you bring me a watch, please? Assistant: You can pick up the watch by its body. User: Could you heat up food quickly? Assistant: To open the microwave, pull its handle to open the door for placing the food inside. User: I need help driving in some nails. Assistant: Grab the hammer by its handle and use its head to drive the nails in. User: I need to fry some vegetables. Please hand me the equipment. Assistant: Grasp the frying pan and place it on the stove. User: Cut the vegetables, please. Assistant: Take the knife handle and slice the vegetables with the knife. User: I need to cut this paper into two. Assistant: Use the scissors to cut the paper, hold the handle with one hand. User: I want to flip something in a pan. Assistant: The spatula can help"}
{"id": "2507.23734v1_chunk68", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "he knife. User: I need to cut this paper into two. Assistant: Use the scissors to cut the paper, hold the handle with one hand. User: I want to flip something in a pan. Assistant: The spatula can help flip food in pans. Hold its handle to flip comfortably. User: Can you help me separate liquid from solid? Assistant: The strainer is here, you can hold its handle to strain the mixture. User: I need a tool to tighten screws. Assistant: Pick up a screwdriver and use its handle to turn and control screws. User: I want to measure ingredients accurately. Assistant: The measuring cup is here, use its handle to level ingredients for measurement. User: I want to drive nails into wood. Assistant: The hammer is here. Grab its handle to strike nails. Easy Instructions Hard InstructionsFigure 9. More reasoning-based affordance segmentation examples from our RAGNet. It includes two types of reasoning instructions: easy instructions and hard instructions.User: Please segment the affordance map of fork"}
{"id": "2507.23734v1_chunk69", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "asoning-based affordance segmentation examples from our RAGNet. It includes two types of reasoning instructions: easy instructions and hard instructions.User: Please segment the affordance map of fork in this image.User: Please segment the affordance map of jug in this image.User: Please segment the affordance map of left door of refrigeratorin this image.User: Please segment the affordance map of microwave oven in this image.User: Please segment the affordance map of wok in this image. User: Please hand me the phone.User: Can you hand me a spoon?User: Can you open the door for me?User: Can you find a fork for me?User: Can you hand me a spatula, please? User: We need to drive a nail into wood.User: I need a tool to turn nuts and bolts.User: I need something to drive nails into.User: I need a tool to tighten or loosen screws.User: I need to flip the pancake.User: Please segment the affordance map of lion in this image.User: Please segment the affordance map of phillips screwdriver in th"}
{"id": "2507.23734v1_chunk70", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": " a tool to tighten or loosen screws.User: I need to flip the pancake.User: Please segment the affordance map of lion in this image.User: Please segment the affordance map of phillips screwdriver in this image.User: Please segment the affordance map of large elephant in this image.User: Please segment the affordance map of giraﬂe in this image.User: Please segment the affordance map of white mouse in this image. User: Can you open the drawer for me? User: Please hand me the eggplant.User: I need a carrot for the salad.User: Can you fetch a banana?User: Can you set the alarm clock for me, please? User: Help me separate liquids from solids. User: Please mix ingredients for baking. User: I need something to cook my food in.User: I need something to pour my coffee into.User: Please get me a container to serve coffee in.Template-based Easy Reasoning Hard ReasoningFigure 10. More experiment results from our model. We use template-based, easy reasoning-based, and hard reasoning-based instruc- "}
{"id": "2507.23734v1_chunk71", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "tainer to serve coffee in.Template-based Easy Reasoning Hard ReasoningFigure 10. More experiment results from our model. We use template-based, easy reasoning-based, and hard reasoning-based instruc- tions, respectively.Dataset Subset Annotation Tool and Categories HANDAL -❶: strainer, fixed joint plier, hammer, ladle, whisk, measuring cup, locking plier, power drill, ad- justable wrencher, mug, ratchet, utensil, combinational wrench, pots pan, spatula, screwdriver, slip joint plier Open-X RT-1 ❸: redbull can, rxbar blueberry, green can, apple, orange can, 7up can, sponge, pepsi can, orange, paper bowl, green rice chip bag, banana, coke can, blue chip bag, water bottle, white bowl, rxbar chocolate, 7up can, brown chip bag, blue plastic bottle, green jalapeno chip bag, blue water bottle, ❺: right fridge door, bottom drawer, left fridge door, middle drawer, top drawer Bridge ❸: apple, apple slice, avocado, ball, banana, banana plush, baster, beet, beetroot, bell pepper, berry, blackberry"}
{"id": "2507.23734v1_chunk72", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": " right fridge door, bottom drawer, left fridge door, middle drawer, top drawer Bridge ❸: apple, apple slice, avocado, ball, banana, banana plush, baster, beet, beetroot, bell pepper, berry, blackberry, board, book, bot, bottle, bowel, bowl, bread, bread roll, broccoli, bunny, butter, cake, cake slice, can, cap, capsicum, carrot, cereal, cheese, cheese slice, cheese wedge, cherry, cake, cake slice, can, cap, capsicum, carrot, cauliflower, cereal, cheese slice, cherry, chicken drumstick, chicken leg, chicken piece, chili pepper, chocolate, croissant, cucumber, detergent, dishcloth, doll, dough, drumstick, egg, eggplant, eggroll, garlic, half bun, hot dog, hotdog, lime, lobster tail, mango, meat, mouse, plastic fish, plush animal, sausage, soap, stuffed animal, stuffed dog, stuffed mushroom, stuffed cheetah, stuffed duck, stuffed pig, strawberry, sushi, tube, turkey leg, yam ❹: knife ❺: brush, cutter, drawer of box, fork, gripper, hairbrush, ice cream scoop, kettle, laddle, microwave, mug"}
{"id": "2507.23734v1_chunk73", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "stuffed cheetah, stuffed duck, stuffed pig, strawberry, sushi, tube, turkey leg, yam ❹: knife ❺: brush, cutter, drawer of box, fork, gripper, hairbrush, ice cream scoop, kettle, laddle, microwave, mug, oven, pot, pan, saucepan, scissors, scrub brush, scrubber, spatula, spork, teapot, teal brush, wok EgoObjects -❷: alarm clock, balloon, blanket, book, bottle, bowl, box, computer mouse, doll, envelope, eraser, flowerpot, flying disc, football, game controller/pad, glasses, glove, goggles, lipstick, necklace, paper, paper towel, pen, pencil, pencil case, perfume, phone charger, picture frame, pillow, plate, post-it, poster, pottery, remote control, ring, shirt, shorts, skateboard, soap, sock, stapler, sun hat, sunglasses, tablet computer, teddy bear, tennis ball, toothpaste, towel, umbrella, vase, wallet, watch ❹: spoon, mug, screwdriver, knife, wrench ❺: microwave oven, washing machine, wok, oven, drawer, teapot, toothbrush, wardrobe, door, jug, refrigerator, tap, tennis racket, spatula,"}
{"id": "2507.23734v1_chunk74", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "vase, wallet, watch ❹: spoon, mug, screwdriver, knife, wrench ❺: microwave oven, washing machine, wok, oven, drawer, teapot, toothbrush, wardrobe, door, jug, refrigerator, tap, tennis racket, spatula, fork, frying pan, scissors, hammer GraspNet -❶: dish, cracker box, pear, camel, peach, tape, banana, head shoulders care, black mouse, tomato soup can, darlie toothpaste, rhinocero, baoke marker, hosjam, pantene, racquetball, cups, sum37 secret repair, gorilla, kispa cleanser, hippo, toy airplane, dabao wash soup, weiquan, strawberry, dabao facewash, head shoulders supreme, dabao sod, large elephant, darlie box, nzskincare mouth rinse, plum ❺: flat screwdriver, power drill, scissors, mug HANDAL Reasoning- Hard Instructions : strainer, fixed joint plier, hammer, ladle, whisk, measuring cup, locking plier, power drill, adjustable wrencher, mug, ratchet, utensil, combinational wrench, pots pan, spatula, screwdriver, slip joint plier EgoObjects Reasoning- Easy Instructions : alarm clock, ball"}
{"id": "2507.23734v1_chunk75", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "cking plier, power drill, adjustable wrencher, mug, ratchet, utensil, combinational wrench, pots pan, spatula, screwdriver, slip joint plier EgoObjects Reasoning- Easy Instructions : alarm clock, balloon, blanket, book, bottle, bowl, box, computer mouse, doll, envelope, eraser, flowerpot, flying disc, football, game controller/pad, glasses, glove, goggles, lip- stick, necklace, paper, paper towel, pen, pencil, pencil case, perfume, phone charger, picture frame, pillow, plate, post-it, poster, pottery, remote control, ring, shirt, shorts, skateboard, soap, sock, sta- pler, sun hat, sunglasses, tablet computer, teddy bear, tennis ball, toothpaste, towel, umbrella, vase, wallet, watch, spoon, mug, screwdriver, knife, wrench, microwave oven, washing machine, wok, oven, drawer, teapot, toothbrush, wardrobe, door, jug, refrigerator, tap, tennis racket, spatula, fork, frying pan, scissors, hammer Hard Instructions : spoon, mug, screwdriver, knife, wrench, microwave oven, washing machine, wok,"}
{"id": "2507.23734v1_chunk76", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "sh, wardrobe, door, jug, refrigerator, tap, tennis racket, spatula, fork, frying pan, scissors, hammer Hard Instructions : spoon, mug, screwdriver, knife, wrench, microwave oven, washing machine, wok, oven, drawer, teapot, toothbrush, wardrobe, jug, refrigerator, tap, tennis racket, spatula, fork, frying pan, scissors, hammer Table 11. Annotation details of RAGNet on tool composition for different subsets and categories.messages = [‘role’: ‘system’, ‘content’: ‘You are a helpful assistant. Based on several words where the first is category name, please design an instruction <1>and instruction <2>in embodied scenes. The instruction <1> must include object category name itself. The instruction <2>must include object category name itself. The instruction <2>must belongs to embodied manipulation and give action if instruction <1>provides. The instruction <2>does not exceed 50 words.’, ‘role’: ‘user’, ‘content’: ‘mug’, ‘role’: ‘assistant’, ‘content’: ‘ <1>I need a drink. Please find a mug t"}
{"id": "2507.23734v1_chunk77", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "on and give action if instruction <1>provides. The instruction <2>does not exceed 50 words.’, ‘role’: ‘user’, ‘content’: ‘mug’, ‘role’: ‘assistant’, ‘content’: ‘ <1>I need a drink. Please find a mug to fill water. <2>The mug has a handle as affordance map. So the robot can hold its handle.’ ‘role’: ‘user’, ‘content’: ‘knife’ ‘role’: ‘assistant’, ‘content’: ‘ <1>Please give me a knife to cut apple. <2>The knife has a handle, and you can use its handle to cut apple.’, ‘role’: ‘user’, ‘content’: ‘hammer’, ‘role’: ‘assistant’, ‘content’: ‘ <1>What is the proper way to hold the hammer? <2>The correct method is to hold the hammer by its handle.’, ‘role’: ‘user’, ‘content’: ‘fork’, ‘role’: ‘assistant’, ‘content’: <1>Kindly pick up the fork. <2>You will be holding the fork handle.’, ‘role’: ‘user’, ‘content’: ‘screwdriver’, ‘role’: ‘assistant’, ‘content’: ‘ <1>I need a tool to tighten or loosen screws. <2>The screwdriver is here, hold its handle to turn and control screws.’, ‘role’: ‘user’, ‘c"}
{"id": "2507.23734v1_chunk78", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": ", ‘content’: ‘screwdriver’, ‘role’: ‘assistant’, ‘content’: ‘ <1>I need a tool to tighten or loosen screws. <2>The screwdriver is here, hold its handle to turn and control screws.’, ‘role’: ‘user’, ‘content’: ‘ words ’ ] Table 12. Language prompt when generating easy reasoning-based instructions on HANDAL via GPT-4. messages = [‘role’: ‘system’, ‘content’: ‘You are a helpful assistant. Based on several words where the first is category name, please design an instruction <1>and instruction <2>in embodied scenes. The instruction <1> must not include object category name itself. The instruction <2>must include object category name itself. The instruction <2>must belongs to embodied manipulation and give action if instruction <1>provides. The instruction <2>does not exceed 50 words.’, ‘role’: ‘user’, ‘content’: ‘microwave, open’, ‘role’: ‘assistant’, ‘content’: ‘ <1>Heat up food quickly . <2>The microwave is closed, so it can be open to access the food inside.’ ‘role’: ‘user’, ‘content’: ‘"}
{"id": "2507.23734v1_chunk79", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": "‘user’, ‘content’: ‘microwave, open’, ‘role’: ‘assistant’, ‘content’: ‘ <1>Heat up food quickly . <2>The microwave is closed, so it can be open to access the food inside.’ ‘role’: ‘user’, ‘content’: ‘knife’ ‘role’: ‘assistant’, ‘content’: ‘ <1>I want to cut a bread. <2>The knife has a handle, you can use its handle to cut bread.’, ‘role’: ‘user’, ‘content’: ‘computer mouse’, ‘role’: ‘assistant’, ‘content’: ‘ <1>Give me a tool to control the cursor on the screen. <2>The computer mouse is here. It has not handle, so you can grasp its whole body.’, ‘role’: ‘user’, ‘content’: ‘fork’, ‘role’: ‘assistant’, ‘content’: <1>Use to pierce and lift food. <2>The fork is here, and its handle can be grasped.’, ‘role’: ‘user’, ‘content’: ‘screwdriver’, ‘role’: ‘assistant’, ‘content’: ‘ <1>I need a tool to tighten or loosen screws. <2>The screwdriver is here, hold its handle to turn and control screws.’, ‘role’: ‘user’, ‘content’: ‘ words ’ ] Table 13. Language prompt when generating hard reasoning-bas"}
{"id": "2507.23734v1_chunk80", "paper_id": "2507.23734v1", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "text": " tighten or loosen screws. <2>The screwdriver is here, hold its handle to turn and control screws.’, ‘role’: ‘user’, ‘content’: ‘ words ’ ] Table 13. Language prompt when generating hard reasoning-based instructions on HANDAL via GPT-4."}
{"id": "2507.23719v1_chunk0", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation 1Parker McDonnell,2Lingsheng Meng,1,2Hari Krishna Hariprasad, 1Alexander Hedrick,1Eduardo Miscles,1Samuel Gilinsky, 2Jean-Michel Mongeau,1Kaushik Jayaram∗ 1Department of Mechanical Engineering, University of Colorado Boulder, 1111 Engineering Drive, Boulder, CO 80301, USA 2Department of Mechanical Engineering, Penn State University ∗To whom correspondence should be addressed; E-mail: kaushik.jayaram@colorado.edu. August 1, 2025 Keywords: tactile sensor, capacitive sensing and robophysical antenna Abstract: The American cockroach ( Periplaneta americana ) uses its soft antennae to guide decision making by extracting rich tactile information from tens of thousands of distributed mechanosensors. Although tactile sensors enable robust, autonomous perception and navigation in natural systems, replicating these capabilities in insect-scale robots remains challenging due to stringent size, weight, "}
{"id": "2507.23719v1_chunk1", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "though tactile sensors enable robust, autonomous perception and navigation in natural systems, replicating these capabilities in insect-scale robots remains challenging due to stringent size, weight, and power constraints that limit existing sensor technologies. To overcome these limita- tions, we introduce CITRAS (Cockroach Inspired Tactile Robotic Antenna Sensor), a bioinspired, multi-segmented, compliant laminate sensor with embedded capaci- tive angle sensors. CITRAS is compact (73.7×15.6×2.1 mm), lightweight (491 mg), and low-power (32 mW), enabling seamless integration with miniature robotic plat- forms. The segmented compliant structure passively bends in response to environmen-tal stimuli, achieving accurate hinge angle measurements with maximum errors of just 0.79◦(quasistatic bending) and 3 .58◦(dy- namic bending). Experimental evaluations demonstrate CITRAS’ multifunctional tactile perception capabilities: predicting base-to-tip distances with 7 .75 % error, estimating envir"}
{"id": "2507.23719v1_chunk2", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "ding) and 3 .58◦(dy- namic bending). Experimental evaluations demonstrate CITRAS’ multifunctional tactile perception capabilities: predicting base-to-tip distances with 7 .75 % error, estimating environmental gap widths with 6.73 % error, and distinguishing surface tex- tures through differential sensor response. The future integration of this bioinspired tactile antenna in insect-scale robots ad- dresses critical sensing gaps, promising enhanced autonomous exploration, obstacle avoidance, and environmental mapping in complex, confined environments. 1 Move By Feel - Need for Tac- tile Perception Insect-scale robots ( <2 cm, <2 g; [1, 2, 3, 4, 5, 6]) have significantly advanced in their capabilities in recent years [7], now successfully navigating environ- ments whose complexity and confinement approach 1arXiv:2507.23719v1 [cs.RO] 31 Jul 20251 MOVE BY FEEL - NEED FOR TACTILE PERCEPTION 2 Figure 1: Robotic antenna prototype overview and concept of operation (A)Final robotic antenna pro- "}
{"id": "2507.23719v1_chunk3", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "ment approach 1arXiv:2507.23719v1 [cs.RO] 31 Jul 20251 MOVE BY FEEL - NEED FOR TACTILE PERCEPTION 2 Figure 1: Robotic antenna prototype overview and concept of operation (A)Final robotic antenna pro- totype assembly and scale reference. (B)Example of a robotic antenna integrated on mm-scale robot for future tactile exploration applications. those of their natural counterparts. For instance, drawing inspiration from the compliant exoskeletons of arthropods, recent miniature robots are now capa- ble of adaptive morphological changes, enabling un- precedented locomotion in confined spaces [8]. No- table examples include shape-morphing robots such as CLARI [9] and its miniature variant mCLARI [10], capable of lateral body compression to navigate narrow horizontal gaps. Such small-scale robots of- fer new opportunities for robotics, including environ- mental monitoring [11], high-value asset inspection [12], search-and-rescue operations [13], and targeted healthcare delivery [14]. Despite t"}
{"id": "2507.23719v1_chunk4", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "of- fer new opportunities for robotics, including environ- mental monitoring [11], high-value asset inspection [12], search-and-rescue operations [13], and targeted healthcare delivery [14]. Despite these advances, re- liable autonomous operation remains elusive due to severe size, weight, and power (SWAP) constraints, significantly limiting onboard sensing and perception capabilities. To achieve reliable autonomy in complex real-world environments, insect-scale robots require robust sens-ing capabilities explicitly tailored to their SWAP constraints. Vision-based systems have traditionally dominated robotic autonomy, proving highly effec- tive in larger humanoid [15], dog [16] and cat-scale quadrupedal platforms [17]. However, on the insect scale, vision-based sensing remains impractical due to substantial demands on power, weight, and on- board computational resources [18]. Moreover, visual sensors can quickly degrade in reliability under low- light, confined, or adverse environmenta"}
{"id": "2507.23719v1_chunk5", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "due to substantial demands on power, weight, and on- board computational resources [18]. Moreover, visual sensors can quickly degrade in reliability under low- light, confined, or adverse environmental conditions commonly encountered in real-world scenarios [19]. To address these limitations, larger robotic systems typically employ sensor fusion strategies that inte- grate multiple sensing modalities, such as LiDAR, optical depth sensors [20], and ground-contact sen- sors [21]. Unfortunately, such traditional sensors are inherently bulky, energy-intensive, or both, making them unsuitable for direct downscaling to insect-sized robotic platforms [13, 14]. Consequently, developing lightweight, low-power yet robust sensing solutions that can function effectively under these challenging constraints remains a critical research challenge. Despite these challenges, recent advances have produced successful examples of miniature sensors specifically tailored to insect-scale platforms, includ- in"}
{"id": "2507.23719v1_chunk6", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "ints remains a critical research challenge. Despite these challenges, recent advances have produced successful examples of miniature sensors specifically tailored to insect-scale platforms, includ- ing photodiodes for ambient light tracking [6, 22], miniature time-of-flight sensors for altitude estima- tion [23], inertial measurement units for orientation control [24], biohybrid sensors enabling olfactory per- ception [25], integrated actuator proprioception sys- tems [26, 27], and visual-inertial attitude control [18, 28]. Although these sensing approaches meet strict SWAP constraints, their functionality remains limited for tasks such as close-range obstacle detec- tion, tactile surface identification, and robust envi- ronmental mapping. For example, miniature single- axis LiDAR sensors, while capable of distance mea- surements up to ≈2 m, frequently suffer performance degradation under bright sunlight conditions because of low optical power. Similarly, microelectrome- chanical ultra"}
{"id": "2507.23719v1_chunk7", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " while capable of distance mea- surements up to ≈2 m, frequently suffer performance degradation under bright sunlight conditions because of low optical power. Similarly, microelectrome- chanical ultrasonic rangefinders, currently available in compact packages (3 .5×3.5×1.3 mm), often pro- duce inaccurate or false readings when encountered with soft or perforated surfaces due to acoustic ab- sorption effects. Thus, while non-contact sensing technologies continue to improve, they alone have2 INSECT-INSPIRED MINIATURE DISTRIBUTED TACTILE PROBE 3 yet to provide the reliable close-range environmen- tal perception necessary for insect-scale robots to au- tonomously navigate cluttered and confined spaces [13, 14]. Biological tactile sensors, such as vertebrate whiskers [29, 30, 31] and arthropod antennae [32, 33], provide animals with exceptionally rich spatiotempo- ral tactile information, allowing effective navigation and environmental perception. Inspired by these bio- logical systems, num"}
{"id": "2507.23719v1_chunk8", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "antennae [32, 33], provide animals with exceptionally rich spatiotempo- ral tactile information, allowing effective navigation and environmental perception. Inspired by these bio- logical systems, numerous robotic tactile sensors have been developed, predominantly based on whisker- like architectures due to their conceptual simplicity. For instance, Kent et al. developed a vision-based whisker array capable of detecting contact and airflow stimuli using optical tracking [34]. Despite promising functionality, the system’s reliance on consistent illu- mination and computationally intensive image pro- cessing renders it impractical for untethered insect- scale applications. Similarly, Sullivan et al. demon- strated an active whisker-inspired tactile sensor em- ploying a Hall-effect magnetic sensor integrated at the base of a motorized whisker, enabling effective texture and distance discrimination [35]. However, each sensor unit’s considerable mass ( ≈8 g) and high power demand ( ≈0.9 W) "}
{"id": "2507.23719v1_chunk9", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "ensor integrated at the base of a motorized whisker, enabling effective texture and distance discrimination [35]. However, each sensor unit’s considerable mass ( ≈8 g) and high power demand ( ≈0.9 W) significantly exceed insect- scale SWAP constraints. Zhu et al. introduced a self-powered triboelectric bionic antenna capable of obstacle avoidance and material identification with- out external power, yet it was limited by a single sensing element and slow response times (350 ms), restricting the robot’s speed and precision in tac- tile localization [36]. Recent work exploring adap- tive compliance in tactile sensing further emphasizes the critical role of mechanical tuning in sensor func- tionality [37, 38]. Demir et al. presented a multi- segmented bioinspired antenna with tunable compli- ance and Hall-effect joint-angle sensors [39, 40], suc- cessfully demonstrating tactile surface mapping and wall-following capabilities [41]. However, the bulky sensing elements and the rigid structur"}
{"id": "2507.23719v1_chunk10", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "ance and Hall-effect joint-angle sensors [39, 40], suc- cessfully demonstrating tactile surface mapping and wall-following capabilities [41]. However, the bulky sensing elements and the rigid structure impede effec- tive miniaturization. More recently, Shahmiri et al. developed a flexible capacitive sensor array (ShArc) capable of multipoint curvature estimation, lever- aging flex-circuit fabrication [42]. Although highly promising, ShArc’s performance was constrained by its low sampling rate (10 Hz) and structural complex-ity, limiting dynamic capabilities and further minia- turization. Thus, despite notable advances, existing bioinspired tactile sensors still fall short of the spe- cific requirements for insect-scale operation, particu- larly in terms of compactness, power efficiency, and bandwidth [13, 14]. To address the critical gap in insect-scale tac- tile sensing, we present CITRAS (Cockroach In- spired Tactile Robotic Antenna Sensor), a bioin- spired, multi-segmented, complian"}
{"id": "2507.23719v1_chunk11", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "d bandwidth [13, 14]. To address the critical gap in insect-scale tac- tile sensing, we present CITRAS (Cockroach In- spired Tactile Robotic Antenna Sensor), a bioin- spired, multi-segmented, compliant tactile antenna specifically aimed at increasing the situational aware- ness of insect-scale robotic platforms. Drawing direct inspiration from the antenna mechanics and morphol- ogy of the American cockroach ( Periplaneta ameri- cana), [32, 33, 43]), CITRAS features a linearly de- creasing stiffness profile, fabricated through a mono- lithic laminate manufacturing technique [44, 45]. CITRAS comprises eight compliant flexural hinges, each integrated with high-resolution axial capacitive strain sensors capable of detecting femtofarad-level (≈1 fF) capacitance changes induced by hinge deflec- tion. This highly sensitive instrumentation allows CITRAS to capture and quantify both static and dy- namic mechanical stimuli effectively, despite its com- pact thickness of only 2 .11 mm. Like its b"}
{"id": "2507.23719v1_chunk12", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "tion. This highly sensitive instrumentation allows CITRAS to capture and quantify both static and dy- namic mechanical stimuli effectively, despite its com- pact thickness of only 2 .11 mm. Like its biological counterpart, CITRAS passively adapts its shape to environmental contact, enabling robust tactile inter- actions including obstacle distance estimation, profile reconstruction, and surface texture discrimination. Crucially, CITRAS meets the stringent SWAP con- straints inherent to insect-scale robots, significantly enhancing their autonomous tactile perception capa- bilities and paving the way for robust navigation in complex, confined environments. 2 Insect-inspired Miniature Distributed Tactile Probe In this section, we present the design, theory of oper- ation and fabrication of CITRAS, a bioinspired tac- tile antenna specifically engineered as a distributed mechanosensor array with integrated routing for sig- nal transmission and measurement (Figure 2). Us- ing bioinspiration "}
{"id": "2507.23719v1_chunk13", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "TRAS, a bioinspired tac- tile antenna specifically engineered as a distributed mechanosensor array with integrated routing for sig- nal transmission and measurement (Figure 2). Us- ing bioinspiration from the cockroach antenna mor-2 INSECT-INSPIRED MINIATURE DISTRIBUTED TACTILE PROBE 4 phology, CITRAS aims to replicate key biological ad- vantages, including compliance, spatially distributed sensing, and multifunctional tactile perception. We specifically address the critical challenges associated with integrating robust high-resolution sensing into miniature laminate structures [44, 45], essential for scalability and practical deployment in insect-scale robotic platforms [2, 10]. Here, we describe our in- novative approach to overcome these challenges, de- tailing both design considerations and manufacturing methods to enable precise tactile sensing within strin- gent SWAP constraints. 2.1 Design and Sensor Modeling The core objective of the CITRAS design is to replicate the multifunct"}
{"id": "2507.23719v1_chunk14", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "s and manufacturing methods to enable precise tactile sensing within strin- gent SWAP constraints. 2.1 Design and Sensor Modeling The core objective of the CITRAS design is to replicate the multifunctional tactile capabilities of cockroach antennas within the stringent SWAP con- straints of insect-scale robots. To achieve these goals, we considered two critical aspects: (1) the antenna’s mechanical structure, including hinge segmentation and tapered stiffness, and (2) the choice and opti- mization of sensor transduction mechanisms. In the following, we outline our mechanical modeling ap- proach, followed by a justification for choosing ca- pacitive sensing and a detailed explanation of sensor theory and operation. 2.1.1 Mechanical Model of Tapered Antenna Stiffness The mechanical design of the segmented CITRAS antenna draws inspiration from the flexural stiffness gradient observed in the antenna of the American cockroach, where flexural stiffness decreases exponen- tially from base to "}
{"id": "2507.23719v1_chunk15", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " the segmented CITRAS antenna draws inspiration from the flexural stiffness gradient observed in the antenna of the American cockroach, where flexural stiffness decreases exponen- tially from base to tip. The flexural stiffness gradi- ent allows the cockroach antenna to passively con- form to environmental features, simplifying tactile perception through morphological computation and enhanced spatial resolution [32, 43]. To replicate this advantageous mechanical be- havior within practical fabrication constraints, we adopted a linearly decreasing stiffness profile in CIT- RAS, explicitly tailoring the beam geometry and hinge dimensions along its length. This linear stiff- ness gradient significantly simplifies tactile sensing:segments closer to the antenna base provide struc- tural support and efficiently transmit forces, while more compliant distal segments readily conform to environmental contours, thereby enhancing tactile sensitivity and spatial resolution near the antenna tip. Pre"}
{"id": "2507.23719v1_chunk16", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " and efficiently transmit forces, while more compliant distal segments readily conform to environmental contours, thereby enhancing tactile sensitivity and spatial resolution near the antenna tip. Previous analytical and robophysical studies have characterized the mechanical effects of antenna tapering and its direct impact on tactile sensing per- formance [32, 39]. Guided by these studies, CIT- RAS’s design parameters, hinge width linearly de- creasing from 8 .0 mm at the base to 3 .62 mm at the tip, uniform thickness of 25 µm, and hinge length of 150µm, were experimentally optimized to achieve the desired balance between stiffness, compliance, and tactile sensitivity. These design choices guided by insect antennae could enable robust and efficient tac- tile exploration in constrained and cluttered environ- ments typical of insect-scale robotic operations. 2.1.2 Capacitive sensing as preferred mechanosensor transduction method Tactile sensors, both biological and engineered, trans- du"}
{"id": "2507.23719v1_chunk17", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " cluttered environ- ments typical of insect-scale robotic operations. 2.1.2 Capacitive sensing as preferred mechanosensor transduction method Tactile sensors, both biological and engineered, trans- duce mechanical interactions into measurable sig- nals through various mechanisms, including capaci- tive displacement detection [42], piezoelectric charges [46], Hall-effect sensing [47], resistive strain gauges [48], and optical methods [49]. Piezoelectric sensors, while highly sensitive to dynamic stimuli, typically exhibit poor low-frequency response, limiting their applicability for static or slow tactile interactions cru- cial for insect-scale navigation. Hall-effect sensors of- fer reliable angle measurements, but often necessitate bulky permanent magnets or ferrous-loaded compos- ites, significantly exceeding the size and weight con- straints inherent to miniature robots. Resistive strain sensors, though straightforward to implement, typ- ically suffer from high thermal noise, low se"}
{"id": "2507.23719v1_chunk18", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "ignificantly exceeding the size and weight con- straints inherent to miniature robots. Resistive strain sensors, though straightforward to implement, typ- ically suffer from high thermal noise, low sensitiv- ity, and limited operational lifespans due to fatigue- induced degradation, compromising reliability in pro- longed field applications. Optical strain measure- ment systems, despite their high resolution, currently require bulky external components and sophisticated integration, making them impractical for miniatur- ized onboard deployment. In contrast, capacitive displacement sensing presents a highly advantageous2 INSECT-INSPIRED MINIATURE DISTRIBUTED TACTILE PROBE 5 solution for insect-scale applications due to its inher- ent characteristics of low power consumption, com- pact form factor, high linearity, high sensitivity, and absolute sensing capability, thus avoiding the need for continuous recalibration or complex incremental tracking [50]. These features make capacitive sens"}
{"id": "2507.23719v1_chunk19", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "factor, high linearity, high sensitivity, and absolute sensing capability, thus avoiding the need for continuous recalibration or complex incremental tracking [50]. These features make capacitive sens- ing the most suitable choice for integration within the strict SWAP constraints of CITRAS, providing robust and reliable tactile feedback necessary for au- tonomous navigation at small scales. 2.1.3 Sensor Theory of Operation CITRAS integrates eight identical capacitive mechanosensors, each positioned at flexural hinges along its segmented beam structure, to measure the joint angle at its location. To illustrate the sensor operation and theoretical basis, we consider a single hinge mechanosensor unit, which fundamen- tally functions as a parallel-plate capacitor. Each mechanosensor consists of two conductive layers: a fixed electrode (the flexure layer in tan) and a sliding electrode (the sensor layer in cyan), vertically separated by a defined spacing h(Figure 2A). As the hinge bends by"}
{"id": "2507.23719v1_chunk20", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "of two conductive layers: a fixed electrode (the flexure layer in tan) and a sliding electrode (the sensor layer in cyan), vertically separated by a defined spacing h(Figure 2A). As the hinge bends by an angle ∆ θ(starting from a neutral angle of 0), the sensor layer moves relative to the flexure layer, changing the overlap between these electrode plates (Figure 2B). This overlap variation directly alters the mechanosensor’s effective capac- itance, C, according to the parallel-plate capacitor relationship: C=ϵ0ϵrwl d(1) where ϵ0represents the vaccuum permittivity, ϵris the relative permittivity of the dielectric, wis the electrode plate width, lis the overlapping length and dis the plate spacing. The overlap length changes proportionally with the hinge bending angle, θ: l(θ) =l0+δ(θ) =l0+h∆θ (2) where, l0is the initial overlap length (at zero deflec- tion), and δis the change in overlap resulting from hinge bending. Substituting this overlap relationship (Equation 2) into the capacita"}
{"id": "2507.23719v1_chunk21", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "h∆θ (2) where, l0is the initial overlap length (at zero deflec- tion), and δis the change in overlap resulting from hinge bending. Substituting this overlap relationship (Equation 2) into the capacitance equation (Equation3) yields a relationship between capacitance ( C) and bending angle: C(θ) =C0+ϵ0ϵrw h∆θ d(3) where C0is the nominal capacitance at zero deflec- tion. By performing an initial calibration (see the experimental section, Section 3.1), C0is determined and typically subtracted off, producing a linear rela- tionship between capacitance change (∆ C=C−C0) and bending angle ( θ) allowing us to determine joint angle change by measuring capacitance change as: ∆θ=∆Cd ϵ0ϵrwh(4) By integrating eight of these mechanosensors along the segmented antenna, CITRAS achieves dis- tributed tactile sensing, enabling precise reconstruc- tion of antenna shape profiles and accurate estima- tion of contact angles and environmental distances. 2.2 Fabrication CITRAS is fabricated using a precision"}
{"id": "2507.23719v1_chunk22", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "ile sensing, enabling precise reconstruc- tion of antenna shape profiles and accurate estima- tion of contact angles and environmental distances. 2.2 Fabrication CITRAS is fabricated using a precision laminate manufacturing approach inspired by methods pre- viously developed for insect-scale robotic devices [44, 45]. This method enables the precise integra- tion of multiple functional materials into compact, lightweight structures suitable for insect-scale plat- forms. The resulting laminate structure mimics the physical characteristics of biological antennae, high- lighting their potential as near-scale robophysical models [51] capable of high-fidelity hypothesis test- ing without the need for dynamic scaling. 2.2.1 Laminate Functional Architecture Each CITRAS antenna consists of 14 individual ma- terial layers, vertically stacked and bonded into a cohesive laminate structure. The detailed laminate stackup for a representative hinge segment is depicted in Figure 2C, illustrating indiv"}
{"id": "2507.23719v1_chunk23", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "idual ma- terial layers, vertically stacked and bonded into a cohesive laminate structure. The detailed laminate stackup for a representative hinge segment is depicted in Figure 2C, illustrating individual material layers and their functional grouping. The laminate struc- ture is functionally organized into two primary cate- gories: (1) mechanical and electrical interconnection2 INSECT-INSPIRED MINIATURE DISTRIBUTED TACTILE PROBE 6 Figure 2: Overview of sensor theory of operation, material stackup and manufacturing methods. (A) Hinge and sensing layer flexure mechanics and notation. (B)Simplified sensor concept of operation during positive and negative deflection. (C)Antenna material stackup of single hinge. (D)Manufacturing processes involved in antenna fabrication. (E)Render of complete antenna assembly with sensor breakout board. (F) Demonstration of in-plane bending and environmental interaction of antenna. layers (layers 1–6) and (2) capacitive angular sens- ing layers (layers 7–1"}
{"id": "2507.23719v1_chunk24", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "antenna assembly with sensor breakout board. (F) Demonstration of in-plane bending and environmental interaction of antenna. layers (layers 1–6) and (2) capacitive angular sens- ing layers (layers 7–14). The mechanical layers form the compliant flexural hinges that serially link nine rigid antennal segments, constraining bending to pla- nar (two-degree-of-freedom) motion at eight specific locations along the antenna’s length. 2.2.2 Process and Assembly Key fabrication innovations in CITRAS include the reliable creation of vertical electrical interconnects (vias) for robust signal transmission from individual mechanosensor units to the antenna base, as well as precise electromechanical alignment across multilayer laminate structures to ensure optimal sensor perfor-mance and structural integrity. The major steps of the CITRAS fabrication pipeline are illustrated in Figure 2D and described in detail below. We first selected specific materials and designed in- dividual layers based on mech"}
{"id": "2507.23719v1_chunk25", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "rity. The major steps of the CITRAS fabrication pipeline are illustrated in Figure 2D and described in detail below. We first selected specific materials and designed in- dividual layers based on mechanical, electrical, and sensing requirements. For inter-layer bonding, we chose an acrylic adhesive film (Pyralux FR1500, DuPont, thickness: 12 .7µm) due to its low mass, high peel strength, and minimal adhesive flow during cur- ing. For rigid structural layers, we selected polyimide (Kapton, DuPont, thickness: 127 µm) due to its ade- quate mechanical strength and rapid laser processing time. Additionally, a thinner polyimide film (thick- ness: 25 µm) was specifically chosen for the compliant2 INSECT-INSPIRED MINIATURE DISTRIBUTED TACTILE PROBE 7 mechanical flexure layers, due to its favorable me- chanical properties and its ability to withstand re- peated mechanical bending cycles. For sensing and electrical interconnection layers, we employed com- mercially available gold-coated polyimid"}
{"id": "2507.23719v1_chunk26", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " me- chanical properties and its ability to withstand re- peated mechanical bending cycles. For sensing and electrical interconnection layers, we employed com- mercially available gold-coated polyimide substrates (Stanford Advanced Materials; polyimide thickness: 25µm, initial gold thickness: 25 nm). To enhance electrical conductivity, an additional 75 nm gold layer was sputtered onto these substrates, achieving a total conductor thickness of 100 nm (Figure 2D1). To create the vertical interconnect sensor sublam- inate, individual layers were selectively patterned (surface rastered or cut through, as appropriate), achieving precise geometric shapes and alignment fea- tures (Figure 2D2). Specifically, the conductive gold layer was raster etched from the polyimide substrates, forming electrode patterns, conductive traces, and vias. Adhesive and rigid structural layers were sim- ilarly laser-machined to obtain the required geome- tries. To ensure accurate stacking, identical align- ment p"}
{"id": "2507.23719v1_chunk27", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "de patterns, conductive traces, and vias. Adhesive and rigid structural layers were sim- ilarly laser-machined to obtain the required geome- tries. To ensure accurate stacking, identical align- ment pin holes were cut in all layers during laser processing (Figure 2D3). After fabricating individual layers, we performed a precise, layer-by-layer stack- ing procedure using a custom pin alignment jig (Fig- ure 2D3). The stacked layers were partially cured in a controlled heat press (Master Press; 12 min at 30 PSI and 300◦F) to establish sufficient interlayer adhesion, preventing conductive ink leakage between layers during subsequent via formation (Figure 2D4). Reliable vertical electrical interconnections (one via per mechanosensor unit) were established by dispens- ing semi-sintering silver conductive ink (ACI Ma- terials FS0142) using a pneumatic dispenser (SRA, APPLSRA205-220V). After via filling, the partially cured laminate underwent an additional curing step in a thermal oven (15 mi"}
{"id": "2507.23719v1_chunk28", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "conductive ink (ACI Ma- terials FS0142) using a pneumatic dispenser (SRA, APPLSRA205-220V). After via filling, the partially cured laminate underwent an additional curing step in a thermal oven (15 min at 150◦ ◦C) to fully solid- ify the conductive ink and establish robust vertical connections (Figure 2D5). This sequential stacking, partial curing, and via formation procedure was then repeated for the top mechanosensor electrode layers. Protective polyimide layers were subsequently added to provide mechanical protection and limit unwanted vertical electrode movement during hinge deflection. To prevent restriction of sliding electrode movement, custom cutouts in the heat press tooling were created above each mechanosensor location, reducing appliedpressure in these sensitive regions. The mechanical sublaminate was separately manu- factured following a similar laminate procedure, then bonded with the sensor sublaminate in a final heat press cycle (90 min at 30 PSI and 325◦F, Figure 2D6)."}
{"id": "2507.23719v1_chunk29", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "chanical sublaminate was separately manu- factured following a similar laminate procedure, then bonded with the sensor sublaminate in a final heat press cycle (90 min at 30 PSI and 325◦F, Figure 2D6). A final precision laser cut was performed to re- lease the integrated electromechanical structure from the surrounding material (Figure 2D7). Released an- tenna structures were press-fit onto custom-designed, 20-pin, 1 mm-pitch connectors mounted on break- out printed circuit boards (PCBs). Electrical con- nectivity between antenna mechanosensor units and PCBs was established by extruding conductive ink over each connector pin and curing, forming robust signal transmission pathways (Figure 2D8). The entire fabrication process, from initial mate- rial patterning to the completed electromechanical antenna, requires approximately six hours, yielding three fully operational CITRAS antennas per fabri- cation cycle. This systematic approach ensures ro- bust, consistent, and scalable production "}
{"id": "2507.23719v1_chunk30", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "cal antenna, requires approximately six hours, yielding three fully operational CITRAS antennas per fabri- cation cycle. This systematic approach ensures ro- bust, consistent, and scalable production of antennas specifically suited for insect-scale tactile sensing and robotic applications. 2.2.3 Final Prototype Based on the theoretical model presented in Sec- tion 2.1.3 and the iterative experimental validation, we optimized the physical parameters of CITRAS to achieve a desirable balance between mechanical stiffness, compliance, sensitivity and ease of fabri- cation. The finalized dimensions explicitly chosen for the antenna are: hinge width linearly taper- ing from 8 .00 mm at the antenna base to 3 .62 mm at the distal tip, uniform hinge thickness of 25 µm, and consistent hinge length of 150 µm. With these parameters, each mechanosensor unit is predicted to exhibit an angular sensitivity of approximately 7.14 fF/°. Considering the inherent noise floor of our capacitive measurement ci"}
{"id": "2507.23719v1_chunk31", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " 150 µm. With these parameters, each mechanosensor unit is predicted to exhibit an angular sensitivity of approximately 7.14 fF/°. Considering the inherent noise floor of our capacitive measurement circuitry ( ≈0.3 fF), each sensor can theoretically resolve angular deflections as small as 0 .04°/count. The final assembled CIT- RAS antenna, including integrated flex-PCB readout electronics, measures approximately 73 .7 mm in to- tal length, 15 .6 mm in width and 2 .11 mm in thick- ness. Its total mass is 491 mg, and its operational3 DISTRIBUTED TACTILE SENSING PERFORMANCE 8 Figure 3: Experimental setup overview (A) Eight channel capacitive to digital converter (CDC) PCB records sensor capacitance and transmits data over serial connection to host PC. (B)Schematic view of experimental setup configured to collect data for quasistatic and dynamic results. (C)View of antenna and tracking markers from speed camera perspective. DLTdv was used to extract marker X-Y location from video. (D)Detai"}
{"id": "2507.23719v1_chunk32", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "onfigured to collect data for quasistatic and dynamic results. (C)View of antenna and tracking markers from speed camera perspective. DLTdv was used to extract marker X-Y location from video. (D)Detailed block diagram of experimental setup subsystems and interconnections. power consumption is approximately 32 mW, ideal for SWAP-limited insect-scale platform integration. 3 Distributed Tactile Sensing Performance This section describes the experimental setup, methodologies, and procedures employed to evaluatethe tactile sensing performance of CITRAS (Section 3.1. We systematically characterize the mechanosen- sors’ performance under quasistatic (Section 3.2) and dynamic bending conditions (Section 3.3), establish- ing their accuracy, resolution, and dynamic response capabilities. Finally, we examine the angular sens- ing limitations arising from mechanosensor satura- tion during large deflections in Section .3 DISTRIBUTED TACTILE SENSING PERFORMANCE 9 3.1 Experimental Setup The experimen"}
{"id": "2507.23719v1_chunk33", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " examine the angular sens- ing limitations arising from mechanosensor satura- tion during large deflections in Section .3 DISTRIBUTED TACTILE SENSING PERFORMANCE 9 3.1 Experimental Setup The experimental setup used to characterize the mechanical and sensory characteristics of the CIT- RAS antenna is depicted in Figure 3. The setup includes custom mechanical fixtures for antenna mounting, specialized readout circuitry to measure individual mechanosensors, and a servo motor (Robo- tis, Dynamixel AX-12A) to apply controlled repeat- able deflections along the antenna. All compo- nents were rigidly mounted on an aluminum bread- board (MB1503F/M, Thorlabs) to minimize un- wanted movements. The setup was designed to be reconfigurable for different experiments, with specific modifications described in the relevant sections. The CITRAS antenna was connected through a 20-pin connector at its base to a custom breakout printed circuit board (PCB) that interfaced directly with measurement electroni"}
{"id": "2507.23719v1_chunk34", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "d in the relevant sections. The CITRAS antenna was connected through a 20-pin connector at its base to a custom breakout printed circuit board (PCB) that interfaced directly with measurement electronics (Figure 3A). This PCB was bolted onto custom-designed 3D-printed fix- tures, constraining the antenna’s motion to a two- dimensional plane (X/Y plane), as shown in Fig- ure 3B. Changes in capacitance of each mechanosen- sor unit, at the femtofarad level, were measured us- ing two four-channel capacitance-to-digital converter (CDC) chips (FDC2214, Texas Instruments). Capac- itive measurements were sampled using an nRF52840 microcontroller module (XIAO BLE Sense, Seeed Studio) via a 400 kHz I ²C communication bus and transmitted to a host computer through a high-speed USB serial interface, achieving an average sampling rate of approximately 80 Hz per mechanosensor. Controlled antenna deflections were achieved by programming the servo motor equipped with spheri- cal end-effector (diameter:"}
{"id": "2507.23719v1_chunk35", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "hieving an average sampling rate of approximately 80 Hz per mechanosensor. Controlled antenna deflections were achieved by programming the servo motor equipped with spheri- cal end-effector (diameter: 10 mm) to specific trajec- tories and thus making contact with the antenna at the desired location and speed. The servo was pow- ered by a 12 V DC supply, with serial communica- tion managed by a USB-to-serial converter (Robotis, U2D2). To obtain precise ground-truth hinge angles, a high-speed video camera mounted directly above the experimental setup captured movements of white markers painted at discrete locations along the an- tenna’s profile. Marker positions in recorded videos were tracked using the digitizing software DLTdv and processed using custom MATLAB scripts to recon-struct the antenna’s profile and accurately determine the hinge angles. Due to minor manufacturing variations, the true vertical spacing between the sensor electrodes may vary slightly across individual mechanose"}
{"id": "2507.23719v1_chunk36", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "ntenna’s profile and accurately determine the hinge angles. Due to minor manufacturing variations, the true vertical spacing between the sensor electrodes may vary slightly across individual mechanosen- sor units and different antenna samples. Thus, each mechanosensor requires an individual calibra- tion prior to testing, to ensure accurate angle deter- mination and optimal sensor performance. Figure 4: Quasistatic sensor characterization ex- perimental setup and results. (A) Quasistatic ex- perimental setup shown from antenna tracking cam- era perspective. (B)Servo motor angle versus time profile during characterization experiment. (C)Ca- pacitance change versus hinge angle data plotted for mechanosensor at the hinge closest to the base (H1).3 DISTRIBUTED TACTILE SENSING PERFORMANCE 10 3.2 Quasistatic Characterization We first characterized the quasistatic performance of the mechanosensor units to quantify their accu- racy and repeatability under controlled bending con- ditions. A rep"}
{"id": "2507.23719v1_chunk37", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "2 Quasistatic Characterization We first characterized the quasistatic performance of the mechanosensor units to quantify their accu- racy and repeatability under controlled bending con- ditions. A representative experiment is shown in Figure 4A, where a servo motor-driven end-effector applied precise deflections to the antenna, rigidly mounted at the base, at defined locations along its length. The servo rotation followed a periodic trian- gular profile with a peak deflection of approximately 25◦, as illustrated in Figure 4B. Quasistatic condi- tions were ensured by applying a low motor rotational speed of 0 .12 rad /s, corresponding to a linear speed of 4.6 mm /s at the contact point. Mechanosensors H7 and H8 were excluded from the subsequent anal- yses, as controlled bending at these distal segments was unreliable due to experimental limitations (e.g., end-effector contact near the tip causes excessive slip- ping). Figure 4C shows representative capacitance changes for hinge H1 as a "}
{"id": "2507.23719v1_chunk38", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "tal segments was unreliable due to experimental limitations (e.g., end-effector contact near the tip causes excessive slip- ping). Figure 4C shows representative capacitance changes for hinge H1 as a function of angular de- flection, yielding a linear sensitivity (over small an- gles) of approximately 26 .5 fF/°. This observed sensi- tivity significantly exceeds the theoretical prediction (7.14 fF/°, Equation 4), likely due to manufacturing imperfections and unintended variations in electrode spacing during bending. To systematically relate ca- pacitance changes to hinge angles, third-order poly- nomial fits provided the best relationships, with coef- ficients of determination ( R2) consistently exceeding 0.99 across all hinges in the operational range. Using these experimentally derived third-order transfer functions, hinge angles were predicted from raw mechanosensor capacitance data collected dur- ing small-angle deflections. Figure 5A compares predicted hinge angles (solid lines) w"}
{"id": "2507.23719v1_chunk39", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "hird-order transfer functions, hinge angles were predicted from raw mechanosensor capacitance data collected dur- ing small-angle deflections. Figure 5A compares predicted hinge angles (solid lines) with ground truth data (dashed lines) for mechanosensors H1–H6, demonstrating strong agreement. To quantify per- formance, Figure 5B presents hinge angle prediction errors as average and maximum error curves across hinges H1–H6. The average angle prediction error was minimal (0 .056±0.079◦; mean ±1 s.d.), and max- imum errors were similarly low (0 .25±0.28◦). These results confirm that mechanosensors accurately and reliably capture hinge angular states during qua-sistatic bending. Consequently, precise reconstruc- tion of the antenna’s shape profile is achievable, facil- itating accurate perception of environmental geome- try and enabling robust tactile exploration and intel- ligent decision making in future insect-scale robotic platforms. Finally, the above experiments reveal that calibra-"}
{"id": "2507.23719v1_chunk40", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "tion of environmental geome- try and enabling robust tactile exploration and intel- ligent decision making in future insect-scale robotic platforms. Finally, the above experiments reveal that calibra- tion procedures are critical to compensate for manu- Figure 5: Quasistatic sensor based angle prediction performance results. (A) Comparison of predicted and ground truth angles for actuated hinges (1-6). (B)Maximum and average error plots across actu- ated hinges (1-6). A maximum error of 0 .795degrees is measured on hinge 3.3 DISTRIBUTED TACTILE SENSING PERFORMANCE 11 Figure 6: Sensor response during dynamic bending (A) High speed optical image sequence depicting antenna motion during dynamic bending. (B) Antenna hinge numbering legend from base to tip. (C) Comparison of sensor based prediction and ground truth hinge angle (1 −8).(D) Angular error between actual and predicted hinge angles. Average error (red) and maximum error (gray) across all hinges. (E) Spatiotemporal ”Tactile Image”"}
{"id": "2507.23719v1_chunk41", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "tion and ground truth hinge angle (1 −8).(D) Angular error between actual and predicted hinge angles. Average error (red) and maximum error (gray) across all hinges. (E) Spatiotemporal ”Tactile Image” representation of antenna angular response over space and time. Unique tactile images are generated based on object type, contact location and speed. facturing variations and therefore, each mechanosen- sor unit was individually calibrated prior to experi- mental evaluation to ensure accurate angle determi- nation and optimize sensor performance.3.3 Dynamic Characterization Having established mechanosensor performance dur- ing quasistatic conditions, we next characterized both the mechanical dynamics of the antenna and the sen- sor response to rapid deflections, conditions represen- tative of realistic tactile navigation and interaction scenarios. The experimental setup for dynamic char-3 DISTRIBUTED TACTILE SENSING PERFORMANCE 12 acterization was identical to the quasistatic tests de- sc"}
{"id": "2507.23719v1_chunk42", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " realistic tactile navigation and interaction scenarios. The experimental setup for dynamic char-3 DISTRIBUTED TACTILE SENSING PERFORMANCE 12 acterization was identical to the quasistatic tests de- scribed above. In these experiments, the servo motor displaced the antenna tip by approximately 56◦, cor- responding to a horizontal displacement of 39 .2 mm at H8. Upon reaching this displacement, contact was released, allowing the antenna to freely oscillate until coming to rest. A representative time sequence of this dynamic antenna motion is illustrated in Figure 6A. Video tracking data indicated second-order exponentially decaying sinusoidal responses for each hinge angle across the antenna (Figure 6B-C). We found a strong agreement with sensor-predicted hinge angles and ground truth measurements with an average error of 0.10±0.27◦indicating effective dynamic performance quantification with our approach. Using standard system identification methods, we measured a natural frequency of 31"}
{"id": "2507.23719v1_chunk43", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "ements with an average error of 0.10±0.27◦indicating effective dynamic performance quantification with our approach. Using standard system identification methods, we measured a natural frequency of 31 .71±0.29 rad /s and a damping ratio of 0 .035±0.012, averaged across all mechanosensor hinges. Notably, the measured damping ratio is approximately an order of magni- tude lower than values reported for the biological an- tenna of the American cockroach ( ≈0.3; [32]). This difference arises primarily from the low viscoelastic- ity of the polyimide and low friction of hinge flexures compared to biological antenna tissues and struc- tures. Such low damping prolongs the antenna’s settling time after rapid deflection, potentially lim- iting the speed at which closed-loop tactile naviga- tion could reliably occur. However, this damping characteristic can be coarsely tuned during fabrica- tion: increasing bonding pressure applied in the heat press step enhances friction between sliding electrod"}
{"id": "2507.23719v1_chunk44", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "d reliably occur. However, this damping characteristic can be coarsely tuned during fabrica- tion: increasing bonding pressure applied in the heat press step enhances friction between sliding electrode layers, allowing practical adjustments to achieve de- sired damping levels. More effective future methods could be integrating highly viscoelastic elastomers [52], tunable smart materials [53, 54], fluid-filled structural elements [55] and actively controlled mech- anisms [56, 57] into the fabrication of bioinspired an- tennae. To visualize and exploit the spatial-temporal patterns captured by the antenna’s distributed mechanosensors during dynamic deflections, we in- voke the concept of a tactile image [58]. This repre- sentation plots the hinge deflection angles as a func- tion of both spatial position along the antenna (hingenumber, Y-axis) and time (X-axis), analogous to a waterfall plot. As seen in Figure 6E, the tactile image visually reveals unique and rich spatiotemporal pat- ter"}
{"id": "2507.23719v1_chunk45", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "tial position along the antenna (hingenumber, Y-axis) and time (X-axis), analogous to a waterfall plot. As seen in Figure 6E, the tactile image visually reveals unique and rich spatiotemporal pat- terns generated by interactions between the antenna and its environment, suggesting its potential utility for classification tasks using established computer vi- sion techniques [59, 60]. Such methods could enable robust tactile recognition, differentiating object ge- ometry, contact location, texture, or even identifying specific interaction patterns, which are essential ca- pabilities [34, 38, 61, 62] for insect-scale autonomous navigation. 3.4 Large angle deflection and sensing limits To identify the operational limits of mechanosensor units under large-angle deflections, we conducted ex- periments involving substantial tip deflection. Specif- ically, the servo motor displaced the antenna tip by approximately 88◦relative to the base hinge (H1), a deflection significantly beyond typical ope"}
{"id": "2507.23719v1_chunk46", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "ments involving substantial tip deflection. Specif- ically, the servo motor displaced the antenna tip by approximately 88◦relative to the base hinge (H1), a deflection significantly beyond typical operational ranges (Figure 7A). Due to the mechanical design, hinges H1 through H3 experienced the largest ini- tial angular deflections, measuring 24 .1◦, 26.4◦, and 23.7◦, respectively. Figure 7B illustrates a repre- sentative mechanosensor (H3) capacitance response curve as a function of hinge angle, showing measur- able responses for small deflections within approxi- mately ±10◦and saturation beyond this range. De- flections at hinges H4 and distal segments remained within the linear saturation limits. The impact of mechanosensor saturation was evi- dent when predicting hinge angles from mechanosen- sor capacitance during dynamic testing (Figure 7C). Prior to t= 1 s, a substantial discrepancy was observed between predicted (sensor-derived) and ground-truth hinge angles due to sensor satur"}
{"id": "2507.23719v1_chunk47", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "anosen- sor capacitance during dynamic testing (Figure 7C). Prior to t= 1 s, a substantial discrepancy was observed between predicted (sensor-derived) and ground-truth hinge angles due to sensor saturation. Quantitative analysis of prediction errors for hinges H1–H3 (Figure 7D) revealed a temporary loss of accu- rate angle tracking during large deflections. However, once deflections returned within approximately ±10◦, accurate angle predictions immediately resumed, in- dicating that saturation does not permanently de- grade sensor accuracy or calibration. Although mechanosensor saturation imposes prac-4 APPLICATIONS 13 Figure 7: Sensor angle limit characterization during large bending. (A)Large antenna deflection at mo- ment of release. (B)Sensor capacitive change versus angle response versus hinge angle curve. Estimated sensor linear limits indicated by A&B.(C)Sensor predicted vs. ground truth hinge angle. Marker 2 indicates estimated time when linear tracking is re- gained. (D) Absol"}
{"id": "2507.23719v1_chunk48", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "sus hinge angle curve. Estimated sensor linear limits indicated by A&B.(C)Sensor predicted vs. ground truth hinge angle. Marker 2 indicates estimated time when linear tracking is re- gained. (D) Absolute angular error versus time for hinges 1 −3.tical limitations on accurate sensing for objects with large curvature or extreme proximity to environmen- tal obstacles, these constraints can be mitigated in future designs. Specifically, mechanical modifications such as increased electrode overlap and extended ax- ial electrode travel could extend the mechanosensors’ operational dynamic range, improving linearity and overall performance. 4 Applications The previously detailed characterization experiments demonstrate that CITRAS provides accurate and re- liable tactile sensing capabilities. To further illustrate its potential for real-world robotic tasks, we evaluated CITRAS in several realistic tactile sensing scenar- ios relevant to confined space navigation with insect- scale robotic platf"}
{"id": "2507.23719v1_chunk49", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "rther illustrate its potential for real-world robotic tasks, we evaluated CITRAS in several realistic tactile sensing scenar- ios relevant to confined space navigation with insect- scale robotic platforms. First, we investigated the sensor’s ability to accurately predict body-to-wall dis- tances through tactile interactions with periodic sur- faces (Section 4.1). Next, we examined the sensor’s performance in estimating environmental gap widths, a capability inspired explicitly by biological antenna behaviors crucial for navigating confined spaces (Sec- tion 4.2). Finally, we demonstrate the potential of CITRAS to discriminate between surface textures, such as differentiating smooth from rough (Section 4.3). Together, these practical experiments validate CITRAS’s promising applicability to real-world tac- tile exploration tasks. 4.1 Predicting Body-to-wall Distance through Touch Accurate estimation of distance from a robot’s body to surrounding environmental features is critical for rob"}
{"id": "2507.23719v1_chunk50", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "real-world tac- tile exploration tasks. 4.1 Predicting Body-to-wall Distance through Touch Accurate estimation of distance from a robot’s body to surrounding environmental features is critical for robust tactile navigation tasks such as obstacle avoid- ance, exploration, and wall-following [63]. To demon- strate CITRAS’s suitability for such tasks, we evalu- ated the antenna’s ability to accurately predict body- to-wall distances (BTWD) when deflected by peri- odic profiles. In these experiments, a servo motor rotated a rigid gear-like disk with a periodic pro- file (sinusoidal, triangular, or square-wave) into con- tact with the antenna tip, while the antenna base4 APPLICATIONS 14 remained fixed (Figure 8A). Ground-truth BTWD values were calculated by tracking marker positions at the antenna tip and base using video analysis. Predicted BTWD values were derived by convert- ing measured mechanosensor capacitances into hinge angles using experimentally determined calibration functions (S"}
{"id": "2507.23719v1_chunk51", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "nna tip and base using video analysis. Predicted BTWD values were derived by convert- ing measured mechanosensor capacitances into hinge angles using experimentally determined calibration functions (Section 3.2) and subsequently performing forward kinematic calculations. Comparing predicted and ground-truth BTWD data revealed strong agreement for all tested pro- files. For the sinusoidal profile (Figure 8B), pre- dicted distances matched ground-truth values closely, with a maximum prediction error of approximately 7.75 % occurring at extreme limits where the antenna tip momentarily slips off before reengaging into the next gear tooth. For the square-wave and triangular profiles, even lower maximum errors of 4 .56 % and 6.34 %, respectively, were measured. These results Figure 8: Prediction of body to wall distance (BTWD) with sensorized antenna (A)Schematic di- agram of experimental setup with measurement defi- nitions. (B)Comparison of actual and sensor based estimate of body to wall "}
{"id": "2507.23719v1_chunk52", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " of body to wall distance (BTWD) with sensorized antenna (A)Schematic di- agram of experimental setup with measurement defi- nitions. (B)Comparison of actual and sensor based estimate of body to wall distance (top), and resulting percent error (bottom). For sinusoidal profile, pre- dicted BTWD closely matches tracking marker based ground truth results (top), with a maximum percent error of 7 .75% (bottom).confirm that CITRAS effectively functions as a re- liable tactile sensing probe, providing accurate and repeatable distance estimation during dynamic inter- actions and enable robots to effectively navigate con- fined environments through touch alone. 4.2 Estimating Environmental Gap Widths When navigating cluttered environments, insect-scale robots frequently encounter narrow gaps that require estimation to determine navigability. Biological sys- tems such as cockroaches employ tactile exploration strategies using their antennae to reliably assess gap dimensions prior to traversal at"}
{"id": "2507.23719v1_chunk53", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "equire estimation to determine navigability. Biological sys- tems such as cockroaches employ tactile exploration strategies using their antennae to reliably assess gap dimensions prior to traversal attempts [8]. To evalu- ate CITRAS’s capability for similar tactile gap mea- surements, we simulated environmental gaps using a 3D-printed slotted disk, equipped with rigid posts of known spacing (21 .25 mm, 48 mm, and 78 .23 mm). With the antenna centered between these simulated gap edges, the servo rotated to bring the gap edges sequentially into contact with the antenna tip (Fig- ure 9A). Using a threshold mechanosensor capaci- tance change (3 .5 fF, over six standard deviations above the noise floor) to detect gap edges, we pre- dicted gap width from these angular measurements. Predicted gap widths closely matched actual values: 22.68 mm (6 .73 % error), 50 .45 mm (5 .11 % error), and 78 .23 mm (4 .66 % error). These results clearly demonstrated accurate gap width predictions for all tes"}
{"id": "2507.23719v1_chunk54", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " widths closely matched actual values: 22.68 mm (6 .73 % error), 50 .45 mm (5 .11 % error), and 78 .23 mm (4 .66 % error). These results clearly demonstrated accurate gap width predictions for all tested cases (Figure 9B) and highlight CITRAS’s ef- fectiveness in tactile gap exploration. 4.3 Texture Discrimination Perception of environmental textures could be criti- cal for insect-scale robots deployed for inspection and maintenance tasks [12]. To investigate CITRAS’s po- tential for texture discrimination, we performed tac- tile experiments using two distinct surface textures: a smooth surface (3D-printed PLA) and a rough sur- face (Velcro hooks). Each surface was mounted on a rotating servo, contacting the antenna tip at a con- stant linear speed of approximately 2 .3 mm /s (Figure 10A). Sensor data represented as 3D tactile images (Figure 10B) show clear differences between the two5 DISCUSSION 15 Figure 9: Sizing up the environment: measuring unknown gap dimensions (A)Diagram of gap"}
{"id": "2507.23719v1_chunk55", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " Sensor data represented as 3D tactile images (Figure 10B) show clear differences between the two5 DISCUSSION 15 Figure 9: Sizing up the environment: measuring unknown gap dimensions (A)Diagram of gap detection experimental setup. Three gap widths were experimentally simulated, and later estimated by post processing the fixture angle and response of sensor one. (B)Gap width detection results. Left to right: for increasing gap widths, raw sensor data was plotted against gap position. A sensor threshold of 3 .5fF(6σSTD above noise floor) was applied to infer the contact locations of either side of the gap, and finally compared to the ground truth gap width. textures. The smooth PLA surface induced minimal deflections, producing small mechanosensor capaci- tance variations (6 .2 fF, peak-to-peak average across hinges), whereas the rough Velcro surface caused pro- nounced stick-slip motion with significantly higher Figure 10: Antenna response to variable surface roughness. (A) Simplified d"}
{"id": "2507.23719v1_chunk56", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "eak average across hinges), whereas the rough Velcro surface caused pro- nounced stick-slip motion with significantly higher Figure 10: Antenna response to variable surface roughness. (A) Simplified diagram of test fixtures used for texture discrimination experiments. (B)Re- sulting tactile images generated from raw sensor re- sponse. Both smooth and rough surfaces generate analogously textured tactile images.capacitance variations (30 .1 fF, peak-to-peak aver- age). Although more sophisticated measures of sur- face variations [27] can be utilized depending on task specifications in future studies, these experi- ments demonstrate CITRAS’s potential for tactile- based material classification and texture perception. 5 Discussion Accurate environmental tactile perception is crit- ical for enabling autonomous robotic navigation and exploration tasks in complex, confined spaces. While larger robotic platforms rely on sensor fu- sion approaches involving multiple bulky and power- intensive s"}
{"id": "2507.23719v1_chunk57", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "bling autonomous robotic navigation and exploration tasks in complex, confined spaces. While larger robotic platforms rely on sensor fu- sion approaches involving multiple bulky and power- intensive sensors, insect-scale robots require com- pact, lightweight, and power-efficient sensing so- lutions optimized explicitly for their SWAP con- straints. CITRAS addresses these stringent re- quirements by directly leveraging bioinspiration from cockroach antennae, integrating mechanical compli- ance, distributed mechanosensing, and multifunc- tional tactile perception into a single laminate struc- ture. Our comprehensive characterization of CITRAS demonstrated robust quasistatic and dynamic sensing performance. Quasistatic tests confirmed high accu- racy of mechanosensors with minimal angular pre-REFERENCES 16 diction errors (average: 0 .056◦; maximum: 0 .795◦), enabling precise curvature determination during con- trolled deflections. Dynamic characterization re- vealed accurate tracking of r"}
{"id": "2507.23719v1_chunk58", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "-REFERENCES 16 diction errors (average: 0 .056◦; maximum: 0 .795◦), enabling precise curvature determination during con- trolled deflections. Dynamic characterization re- vealed accurate tracking of rapidly changing angles (average: 0 .10◦), although low damping suggests fu- ture design improvements to enhance dynamic set- tling times through targeted material selection or structural modifications. Beyond basic mechani- cal validation, CITRAS demonstrated effective mul- tifunctional tactile capabilities, including accurate estimation of body-to-wall distances ( ≤8 % error), precise measurement of environmental gap dimen- sions ( ≤7 % error), and easy discrimination between smooth and rough surface textures using spatiotem- poral tactile images. Future work will explicitly focus on advancing fab- rication techniques to improve mechanical robust- ness. Incorporating highly viscoelastic elastomers [52], tunable smart materials [53, 54], fluid-filled structures [55], and actively controlle"}
{"id": "2507.23719v1_chunk59", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "g fab- rication techniques to improve mechanical robust- ness. Incorporating highly viscoelastic elastomers [52], tunable smart materials [53, 54], fluid-filled structures [55], and actively controlled damping el- ements [56, 57] could significantly improve sensor re- liability and dynamic response characteristics. These enhancements would enable CITRAS-inspired anten- nae to more closely replicate the sophisticated tac- tile sensing behaviors of biological antennae, facili- tating reliable autonomous navigation for insect-scale robotic platforms in real-world, cluttered environ- ments. Acknowledgments Special thanks to all the members of the Animal In- spired Movement and Robotics Lab at CU Boulder, and the Bio-Motion Systems Lab at Penn State for their support and guidance on this project. This work was supported by the Army Research Office (ARO) under grant number W911NF−23−1−0039 (to J.M.M. and K.J.) and National Science Foundation under CAREER grant number 2443869 (to K.J.). Confl"}
{"id": "2507.23719v1_chunk60", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "t. This work was supported by the Army Research Office (ARO) under grant number W911NF−23−1−0039 (to J.M.M. and K.J.) and National Science Foundation under CAREER grant number 2443869 (to K.J.). Conflict of Interest The authors declare no conflicts of interest in the completion of this work.Code Availability All code used to generate figures and process data is available on the paper GitHub repository: https://github.com/ Animal-Inspired-Motion-And-Robotics-Lab/ Paper-Robotic-Antenna References [1] Neel Doshi, Kaushik Jayaram, Samantha Castel- lanos, Scott Kuindersma, and Robert J Wood. Effective locomotion at multiple stride frequen- cies using proprioceptive feedback on a legged microrobot. Bioinspiration & biomimetics , 14(5):056001, 2019. Publisher: IOP Publishing. [2] Kaushik Jayaram, Jennifer Shum, Samantha Castellanos, E. Farrell Helbling, and Robert J. Wood. Scaling down an insect-size microrobot, HAMR-VI into HAMR-Jr. In 2020 IEEE Inter- national Conference on Robotics and Aut"}
{"id": "2507.23719v1_chunk61", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " Jennifer Shum, Samantha Castellanos, E. Farrell Helbling, and Robert J. Wood. Scaling down an insect-size microrobot, HAMR-VI into HAMR-Jr. In 2020 IEEE Inter- national Conference on Robotics and Automa- tion (ICRA) , pages 10305–10311, May 2020. ISSN: 2577-087X. [3] Shashwat Singh, Reed Truax, and Ryan St Pierre. Buffalo byte: A highly mobile and au- tonomous millirobot platform. IEEE Robotics and Automation Letters , 2024. [4] Sukjun Kim, Aaron M Johnson, and Sarah Berg- breiter. Picotaur: A 15 mg hexapedal robot with electrostatically driven, 3d-printed legs. Ad- vanced Intelligent Systems , 6(10):2400196, 2024. [5] Jiaming Liang, Yichuan Wu, Justin K. Yim, Huimin Chen, Zicong Miao, Hanxiao Liu, Ying Liu, Yixin Liu, Dongkai Wang, Wenying Qiu, Zhichun Shao, Min Zhang, Xiaohao Wang, Junwen Zhong, and Liwei Lin. Electrostatic footpads enable agile insect-scale soft robots with trajectory control. Science Robotics , 6(55):eabe7906, June 2021. Publisher: Ameri- can Association for the A"}
{"id": "2507.23719v1_chunk62", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "en Zhong, and Liwei Lin. Electrostatic footpads enable agile insect-scale soft robots with trajectory control. Science Robotics , 6(55):eabe7906, June 2021. Publisher: Ameri- can Association for the Advancement of Science. [6] Kyle Johnson, Zachary Englhardt, Vicente Ar- royos, Dennis Yin, Shwetak Patel, and VikramREFERENCES 17 Iyer. MilliMobile: An autonomous battery- free wireless microrobot. In Proceedings of the 29th Annual International Conference on Mobile Computing and Networking , pages 1–16. ACM, 2023. [7] G. C. H. E. de Croon, J. J. G. Dupeyroux, S. B. Fuller, and J. A. R. Marshall. Insect-inspired AI for autonomous robots. Science Robotics , 7(67):eabl6334, June 2022. Publisher: American Association for the Advancement of Science. [8] Kaushik Jayaram and Robert J. Full. Cock- roaches traverse crevices, crawl rapidly in con- fined spaces, and inspire a soft, legged robot. Proceedings of the National Academy of Sci- ences , 113(8):E950–E957, February 2016. Pub- lisher: Proceed"}
{"id": "2507.23719v1_chunk63", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "aches traverse crevices, crawl rapidly in con- fined spaces, and inspire a soft, legged robot. Proceedings of the National Academy of Sci- ences , 113(8):E950–E957, February 2016. Pub- lisher: Proceedings of the National Academy of Sciences. [9] Heiko Kabutz and Kaushik Jayaram. Design of CLARI: A Miniature Modular Origami Pas- sive Shape-Morphing Robot. Advanced Intelli- gent Systems , 5(12):2300181, 2023. [10] Heiko Kabutz, Alexander Hedrick, William P. McDonnell, and Kaushik Jayaram. mCLARI: A Shape-Morphing Insect-Scale Robot Capa- ble of Omnidirectional Terrain-Adaptive Loco- motion in Laterally Confined Spaces. In 2023 IEEE/RSJ International Conference on Intelli- gent Robots and Systems (IROS) , pages 8371– 8376, October 2023. ISSN: 2153-0866. [11] Emanuele Aucone, Steffen Kirchgeorg, Al- ice Valentini, Lo¨ ıc Pellissier, Kristy Deiner, and Stefano Mintchev. Drone-assisted collec- tion of environmental dna from tree branches for biodiversity monitoring. Science robotics , 8(74):"}
{"id": "2507.23719v1_chunk64", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "g, Al- ice Valentini, Lo¨ ıc Pellissier, Kristy Deiner, and Stefano Mintchev. Drone-assisted collec- tion of environmental dna from tree branches for biodiversity monitoring. Science robotics , 8(74):eadd5762, 2023. [12] S´ ebastien D. de Rivaz, Benjamin Goldberg, Neel Doshi, Kaushik Jayaram, Jack Zhou, and Robert J. Wood. Inverted and vertical climbing of a quadrupedal microrobot using electroadhe- sion. Science Robotics , 3(25):eaau3038, Decem- ber 2018. Publisher: American Association for the Advancement of Science.[13] Ryan St. Pierre and Sarah Bergbreiter. Toward autonomy in sub-gram terrestrial robots. Annual Review of Control, Robotics, and Autonomous Systems , 2(1):231–252, 2019. [14] Ugur Bozuyuk, Paul Wrede, Erdost Yildiz, and Metin Sitti. Roadmap for clinical translation of mobile microrobotics. Advanced Materials , 36(23):2311462, 2024. [15] Shuo Gao, Yanning Dai, and Arokia Nathan. Tactile and vision perception for intelligent humanoids. Advanced Intelligent Systems , 4(2)"}
{"id": "2507.23719v1_chunk65", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "le microrobotics. Advanced Materials , 36(23):2311462, 2024. [15] Shuo Gao, Yanning Dai, and Arokia Nathan. Tactile and vision perception for intelligent humanoids. Advanced Intelligent Systems , 4(2):2100074, 2022. [16] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning robust perceptive locomotion for quadrupedal robots in the wild. Science robotics , 7(62):eabk2822, 2022. [17] Gabriel B Margolis, Tao Chen, Kartik Paigwar, Xiang Fu, Donghyun Kim, Sang bae Kim, and Pulkit Agrawal. Learning to jump from pixels. InConference on Robot Learning , pages 1025– 1034. PMLR, 2022. [18] Sawyer Fuller, Zhitao Yu, and Yash P Talwekar. A gyroscope-free visual-inertial flight control and wind sensing system for 10-mg robots. Sci- ence Robotics , 7(72):eabq8184, 2022. [19] Yuxiao Zhang, Alexander Carballo, Hanting Yang, and Kazuya Takeda. Perception and sensing for autonomous vehicles under adverse weather conditions: A survey. ISPRS Journal of Ph"}
{"id": "2507.23719v1_chunk66", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "):eabq8184, 2022. [19] Yuxiao Zhang, Alexander Carballo, Hanting Yang, and Kazuya Takeda. Perception and sensing for autonomous vehicles under adverse weather conditions: A survey. ISPRS Journal of Photogrammetry and Remote Sensing , 196:146– 177, 2023. [20] Leopoldo de Simone, Emanuele Fanfarillo, Si- mona Maccherini, Tiberio Fiaschi, Giuseppe Alfonso, Franco Angelini, Manolo Garabini, and Claudia Angiolini. One small step for a robot, one giant leap for habitat monitor- ing: A structural survey of EU forest habi- tats with robotically-mounted mobile laser scan- ning (RMLS). Ecological Indicators , 160:111882, 2024.REFERENCES 18 [21] Giorgio Valsecchi, Ruben Grandia, and Marco Hutter. Quadrupedal locomotion on uneven ter- rain with sensorized feet. IEEE Robotics and Automation Letters , 5(2):1548–1555, 2020. [22] Sawyer B. Fuller, Alexander Sands, Andreas Haggerty, Michael Karpelson, and Robert J. Wood. Estimating attitude and wind veloc- ity using biomimetic sensors on a microrobotic"}
{"id": "2507.23719v1_chunk67", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "(2):1548–1555, 2020. [22] Sawyer B. Fuller, Alexander Sands, Andreas Haggerty, Michael Karpelson, and Robert J. Wood. Estimating attitude and wind veloc- ity using biomimetic sensors on a microrobotic bee. In 2013 IEEE International Conference on Robotics and Automation , pages 1374–1380, 2013. ISSN: 1050-4729. [23] E. Farrell Helbling, Sawyer B. Fuller, and Robert J. Wood. Altitude estimation and control of an insect-scale robot with an onboard prox- imity sensor. In Antonio Bicchi and Wolfram Burgard, editors, Robotics Research: Volume 1 , pages 57–69. Springer International Publishing, 2018. [24] Benjamin Goldberg, Raphael Zufferey, Neel Doshi, Elizabeth Farrell Helbling, Griffin Whit- tredge, Mirko Kovac, and Robert J. Wood. Power and control autonomy for high-speed loco- motion with an insect-scale legged robot. IEEE Robotics and Automation Letters , 3(2):987–993, 2018. [25] Melanie J Anderson, Joseph G Sullivan, Timo- thy K Horiuchi, Sawyer B Fuller, and Thomas L Daniel. A bio-hy"}
{"id": "2507.23719v1_chunk68", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "n insect-scale legged robot. IEEE Robotics and Automation Letters , 3(2):987–993, 2018. [25] Melanie J Anderson, Joseph G Sullivan, Timo- thy K Horiuchi, Sawyer B Fuller, and Thomas L Daniel. A bio-hybrid odor-guided autonomous palm-sized air vehicle. Bioinspiration & Biomimetics , 16(2):026002, 2020. [26] Kaushik Jayaram, Noah T Jafferis, Neel Doshi, Ben Goldberg, and Robert J Wood. Con- comitant sensing and actuation for piezoelectric microrobots. Smart Materials and Structures , 27(6):065028, 2018. [27] Heiko Kabutz and Kaushik Jayaram. Integrated proprioceptive piezoelectric actuators for minia- ture robots and devices. Smart Materials and Structures , 34(3):035004, 2025. Publisher: IOP Publishing. [28] Zhitao Yu, Joshua Tran, Claire Li, Aaron Weber, Yash P Talwekar, and Sawyer Fuller. Tinysense:A lighter weight and more power-efficient avion- ics system for flying insect-scale robots. arXiv preprint arXiv:2501.03416 , 2025. [29] Mitra J Hartmann. Active sensing capabilities of the"}
{"id": "2507.23719v1_chunk69", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " Tinysense:A lighter weight and more power-efficient avion- ics system for flying insect-scale robots. arXiv preprint arXiv:2501.03416 , 2025. [29] Mitra J Hartmann. Active sensing capabilities of the rat whisker system. Autonomous Robots , 11:249–254, 2001. [30] Robyn A Grant and Victor GA Goss. What can whiskers tell us about mammalian evolu- tion, behaviour, and ecology? Mammal Review , 52(1):148–163, 2022. [31] Taiki Adachi, Yasuhiko Naito, Patrick W Robin- son, Daniel P Costa, Luis A H¨ uckst¨ adt, Rachel R Holser, Wataru Iwasaki, and Akinori Takahashi. Whiskers as hydrodynamic prey sensors in forag- ing seals. Proceedings of the National Academy of Sciences , 119(25):e2119502119, 2022. [32] Jean-Michel Mongeau, Alican Demir, Chris J. Dallmann, Kaushik Jayaram, Noah J. Cowan, and Robert J. Full. Mechanical processing via passive dynamic properties of the cock- roach antenna can facilitate control during rapid running. Journal of Experimental Biology , 217(18):3333–3345, September "}
{"id": "2507.23719v1_chunk70", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "rt J. Full. Mechanical processing via passive dynamic properties of the cock- roach antenna can facilitate control during rapid running. Journal of Experimental Biology , 217(18):3333–3345, September 2014. [33] Jean-Michel Mongeau, Simon N. Sponberg, John P. Miller, and Robert J. Full. Sensory pro- cessing within antenna enables rapid implemen- tation of feedback control for high-speed running maneuvers. Journal of Experimental Biology , page jeb.118604, January 2015. [34] Teresa A Kent, Suhan Kim, Gabriel Kornilow- icz, Wenzhen Yuan, Mitra J. Z. Hartmann, and Sarah Bergbreiter. WhiskSight: A Reconfig- urable, Vision-Based, Optical Whisker Sensing Array for Simultaneous Contact, Airflow, and Inertia Stimulus Detection. IEEE Robotics and Automation Letters , 6(2):3357–3364, April 2021. Conference Name: IEEE Robotics and Automa- tion Letters. [35] Mathew H. Evans, Charles W. Fox, Nathan F. Lepora, Martin J. Pearson, J. Charles Sullivan, and Tony J. Prescott. The effect of whiskerREFERENC"}
{"id": "2507.23719v1_chunk71", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "erence Name: IEEE Robotics and Automa- tion Letters. [35] Mathew H. Evans, Charles W. Fox, Nathan F. Lepora, Martin J. Pearson, J. Charles Sullivan, and Tony J. Prescott. The effect of whiskerREFERENCES 19 movement on radial distance estimation: a case study in comparative robotics. Frontiers in Neu- rorobotics , 6:12, January 2013. [36] Dekuan Zhu, Jiangfeng Lu, Mingjie Zheng, Dongkai Wang, Jianyu Wang, Yixin Liu, Xiao- hao Wang, and Min Zhang. Self-powered bionic antenna based on triboelectric nanogenerator for micro-robotic tactile sensing. Nano Energy , 114:108644, September 2023. [37] Joseph H. Solomon and Mitra J. Hartmann. Robotic whiskers used to sense features. Na- ture, 443(7111):525–525, October 2006. Num- ber: 7111 Publisher: Nature Publishing Group. [38] Barry W. Mulvey and Thrishantha Nanayakkara. HAVEN: Haptic And Visual Environment Navigation by a Shape-Changing Mobile Robot with Multimodal Perception. Scientific Reports , 14(1):27018, November 2024. Publisher: Nature P"}
{"id": "2507.23719v1_chunk72", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "rishantha Nanayakkara. HAVEN: Haptic And Visual Environment Navigation by a Shape-Changing Mobile Robot with Multimodal Perception. Scientific Reports , 14(1):27018, November 2024. Publisher: Nature Publishing Group. [39] Alican Demir, Edward W Samson, and Noah J Cowan. A tunable physical model of arthropod antennae. In 2010 IEEE International Confer- ence on Robotics and Automation , pages 3793– 3798, Anchorage, AK, May 2010. IEEE. [40] A Demir, EG Samson, JM Mongeau, K Jayaram, RJ Full, and NJ Cowan. A tunable, multiseg- mented robotic antenna for identifying and test- ing biomechanical design principles. In INTE- GRATIVE AND COMPARATIVE BIOLOGY , volume 51, pages E182–E182. OXFORD UNIV PRESS INC JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA, 2011. [41] A.G. Lamperski, O.Y. Loh, B.L. Kutscher, and N.J. Cowan. Dynamical Wall Following for a Wheeled Robot Using a Passive Tactile Sensor. InProceedings of the 2005 IEEE International Conference on Robotics and Automation , pages 3838–3"}
{"id": "2507.23719v1_chunk73", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "Kutscher, and N.J. Cowan. Dynamical Wall Following for a Wheeled Robot Using a Passive Tactile Sensor. InProceedings of the 2005 IEEE International Conference on Robotics and Automation , pages 3838–3843, April 2005. ISSN: 1050-4729. [42] Fereshteh Shahmiri and Paul H. Dietz. ShArc: A Geometric Technique for Multi-Bend/ShapeSensing. In Proceedings of the 2020 CHI Confer- ence on Human Factors in Computing Systems , pages 1–12, Honolulu HI USA, April 2020. ACM. [43] Lingsheng Meng, Parker McDonnell, Kaushik Jayaram, and Jean-Michel Mongeau. Mechan- ical and morphological features of the cockroach antenna confer flexibility, reveal a kinematic chain system and predict strain information for proprioception. bioRxiv , pages 2025–04, 2025. [44] R. J. Wood, S. Avadhanula, R. Sahai, E. Steltz, and R. S. Fearing. Microrobot Design Using Fiber Reinforced Composites. Journal of Me- chanical Design , 130(5), March 2008. [45] J P Whitney, P S Sreetharan, K Y Ma, and R J Wood. Pop-up book MEMS. Jou"}
{"id": "2507.23719v1_chunk74", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "d R. S. Fearing. Microrobot Design Using Fiber Reinforced Composites. Journal of Me- chanical Design , 130(5), March 2008. [45] J P Whitney, P S Sreetharan, K Y Ma, and R J Wood. Pop-up book MEMS. Jour- nal of Micromechanics and Microengineering , 21(11):115021, 2011. [46] Zeyu Wang, Frank P.-W. Lo, Yunran Huang, Junhong Chen, James Calo, Wei Chen, and Benny Lo. Tactile perception: a biomimetic whisker-based method for clinical gastrointesti- nal diseases screening. npj Robotics , 1(1):1–15, October 2023. Number: 1 Publisher: Nature Publishing Group. [47] Hao Ren, Liu Yang, Hong-yuan Chang, Tieshan Zhang, Gen Li, Xiong Yang, Yifeng Tang, Wan- feng Shang, and Yajing Shen. A robust and omnidirectional-sensitive electronic antenna for tactile-induced perception. Nature Communica- tions, 16(1):3135, 2025. Publisher: Nature Pub- lishing Group. [48] Linpeng Liu, Shichao Niu, Junqiu Zhang, Zhengzhi Mu, Jing Li, Bo Li, Xiancun Meng, Changchao Zhang, Yueqiao Wang, Tao Hou, Zhiwu Han, Shu Yang, "}
{"id": "2507.23719v1_chunk75", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": ":3135, 2025. Publisher: Nature Pub- lishing Group. [48] Linpeng Liu, Shichao Niu, Junqiu Zhang, Zhengzhi Mu, Jing Li, Bo Li, Xiancun Meng, Changchao Zhang, Yueqiao Wang, Tao Hou, Zhiwu Han, Shu Yang, and Luquan Ren. Bioinspired, omnidirectional, and hyper- sensitive flexible strain sensors. Advanced Materials , 34(17):2200823, 2022. eprint: https://advanced.onlinelibrary.wiley.com/doi/pdf/10.1002/adma.202200823. [49] Yuxiang Li, Zhihe Guo, Xuyang Zhao, Sheng Liu, Zhenmin Chen, Wen-Fei Dong, ShixiangREFERENCES 20 Wang, Yun-Lu Sun, and Xiang Wu. An all- optical multidirectional mechano-sensor inspired by biologically mechano-sensitive hair sensilla. Nature Communications , 15(1):2906, 2024. Pub- lisher: Nature Publishing Group. [50] Andrew J. Fleming. A review of nanometer resolution position sensors: Operation and per- formance. Sensors and Actuators A: Physical , 190:106–126, 2013. [51] Jeffrey Aguilar, Tingnan Zhang, Feifei Qian, Mark Kingsbury, Benjamin McInroe, Nicole Ma- zouchova, "}
{"id": "2507.23719v1_chunk76", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "n sensors: Operation and per- formance. Sensors and Actuators A: Physical , 190:106–126, 2013. [51] Jeffrey Aguilar, Tingnan Zhang, Feifei Qian, Mark Kingsbury, Benjamin McInroe, Nicole Ma- zouchova, Chen Li, Ryan Maladen, Chaohui Gong, Matt Travers, et al. A review on locomo- tion robophysics: the study of movement at the intersection of robotics, soft matter and dynam- ical systems. Reports on Progress in Physics , 79(11):110001, 2016. [52] Roderic S Lakes. Viscoelastic materials . Cam- bridge university press, 2009. [53] Alexander Hedrick, Heiko Kabutz, Lawrence Smith, Robert MacCurdy, and Kaushik Ja- yaram. Femtosecond laser fabricated nitinol liv- ing hinges for millimeter-sized robots. IEEE Robotics and Automation Letters , 9(6):5449– 5455, 2024. [54] Hayley D McClintock, Neel Doshi, Agustin Iniguez-Rabago, James C Weaver, Noah T Jaf- feris, Kaushik Jayaram, Robert J Wood, and Jo- hannes TB Overvelde. A fabrication strategy for reconfigurable millimeter-scale metamaterials. Advan"}
{"id": "2507.23719v1_chunk77", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": " Agustin Iniguez-Rabago, James C Weaver, Noah T Jaf- feris, Kaushik Jayaram, Robert J Wood, and Jo- hannes TB Overvelde. A fabrication strategy for reconfigurable millimeter-scale metamaterials. Advanced Functional Materials , 31(46):2103428, 2021. [55] Nicholas Kellaris, Philipp Rothemund, Yi Zeng, Shane K Mitchell, Garrett M Smith, Kaushik Jayaram, and Christoph Keplinger. Spider- inspired electrohydraulic actuators for fast, soft-actuated joints. Advanced Science , 8(14):2100916, 2021. [56] Stuart Diller, Carmel Majidi, and Steven H Collins. A lightweight, low-power electroadhe- sive clutch and spring for exoskeleton actua- tion. In 2016 IEEE International Conference onRobotics and Automation (ICRA) , pages 682– 689. IEEE, 2016. [57] Ronan Hinchet and Herbert Shea. High force density textile electrostatic clutch. Advanced Materials Technologies , 5(4):1900895, 2020. [58] Paul Bach-y Rita, Carter C Collins, Frank A Saunders, Benjamin White, and Lawrence Scad- den. Vision substitution"}
{"id": "2507.23719v1_chunk78", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "xtile electrostatic clutch. Advanced Materials Technologies , 5(4):1900895, 2020. [58] Paul Bach-y Rita, Carter C Collins, Frank A Saunders, Benjamin White, and Lawrence Scad- den. Vision substitution by tactile image projec- tion. Nature , 221(5184):963–964, 1969. [59] Dengsheng Lu and Qihao Weng. A survey of image classification methods and techniques for improving classification performance. Interna- tional journal of Remote sensing , 28(5):823–870, 2007. [60] Waseem Rawat and Zenghui Wang. Deep convo- lutional neural networks for image classification: A comprehensive review. Neural computation , 29(9):2352–2449, 2017. [61] Martin J Pearson, Ben Mitchinson, J Charles Sullivan, Anthony G Pipe, and Tony J Prescott. Biomimetic vibrissal sensing for robots. Philo- sophical Transactions of the Royal Society B: Bi- ological Sciences , 366(1581):3085–3096, 2011. [62] Zhiqiang Yu, Yue Guo, Jiaji Su, Qiang Huang, Toshio Fukuda, Changyong Cao, and Qing Shi. Bioinspired, Multifunctional, Activ"}
{"id": "2507.23719v1_chunk79", "paper_id": "2507.23719v1", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "text": "he Royal Society B: Bi- ological Sciences , 366(1581):3085–3096, 2011. [62] Zhiqiang Yu, Yue Guo, Jiaji Su, Qiang Huang, Toshio Fukuda, Changyong Cao, and Qing Shi. Bioinspired, Multifunctional, Active Whisker Sensors for Tactile Sensing of Mobile Robots. IEEE Robotics and Automation Let- ters, 7(4):9565–9572, October 2022. Conference Name: IEEE Robotics and Automation Letters. [63] Jusuk Lee, S.N. Sponberg, O.Y. Loh, A.G. Lam- perski, R.J. Full, and N.J. Cowan. Templates and Anchors for Antenna-Based Wall Following in Cockroaches and Robots. IEEE Transactions on Robotics , 24(1):130–143, February 2008."}
{"id": "2507.23698v1_chunk0", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents Shaofei Cai*1, Zhancun Mu*1, Haiwen Xia1, Bowei Zhang1, Anji Liu2, Yitao Liang1 1Institute for Artificial Intelligence, Peking University 2School of Computing, National University of Singapore {caishaofei, muzhancun, 2300010813, zhangbowei }@stu.pku.edu.cn, anjiliu@nus.edu.sg, yitaol@pku.edu.cn Abstract While Reinforcement Learning (RL) has achieved remark- able success in language modeling, its triumph hasn’t yet fully translated to visuomotor agents. A primary challenge in RL models is their tendency to overfit specific tasks or envi- ronments, thereby hindering the acquisition of generalizable behaviors across diverse settings. This paper provides a pre- liminary answer to this challenge by demonstrating that RL- finetuned visuomotor agents in Minecraft can achieve zero- shot generalization to unseen worlds. Specifically, we explore RL’s potential to enhance generalizable spatial r"}
{"id": "2507.23698v1_chunk1", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "ge by demonstrating that RL- finetuned visuomotor agents in Minecraft can achieve zero- shot generalization to unseen worlds. Specifically, we explore RL’s potential to enhance generalizable spatial reasoning and interaction capabilities in 3D worlds. To address challenges in multi-task RL representation, we analyze and establish cross- view goal specification as a unified multi-task goal space for visuomotor policies. Furthermore, to overcome the signifi- cant bottleneck of manual task design, we propose automated task synthesis within the highly customizable Minecraft envi- ronment for large-scale multi-task RL training, and we con- struct an efficient distributed RL framework to support this. Experimental results show RL significantly boosts interac- tion success rates by 4×and enables zero-shot generaliza- tion of spatial reasoning across diverse environments, includ- ing real-world settings. Our findings underscore the immense potential of RL training in 3D simulated environments,"}
{"id": "2507.23698v1_chunk2", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "zero-shot generaliza- tion of spatial reasoning across diverse environments, includ- ing real-world settings. Our findings underscore the immense potential of RL training in 3D simulated environments, espe- cially those amenable to large-scale task generation, for sig- nificantly advancing visuomotor agents’ spatial reasoning. Code — https://github.com/CraftJarvis/ROCKET-3 1 Introduction Reinforcement Learning (RL) has shown immense potential in solving complex tasks, particularly in sequential decision- making (Mnih et al. 2015; Silver et al. 2016). Generally, applying RL to train multi-task policies typically relies on meticulously designing reward functions from scratch to guide agents in learning specific task knowledge. How- ever, this approach has been widely noted for problems like catastrophic forgetting (Vithayathil Varghese and Mahmoud 2020a) and multi-task interference (Taylor and Stone 2011), which severely hinder RL’s generalization capabilities in complex multi-task envir"}
{"id": "2507.23698v1_chunk3", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "catastrophic forgetting (Vithayathil Varghese and Mahmoud 2020a) and multi-task interference (Taylor and Stone 2011), which severely hinder RL’s generalization capabilities in complex multi-task environments. In recent years, the rapid advancement of Large Language Models (LLMs) (Achiam *These authors contributed equally.et al. 2023; DeepSeek-AI et al. 2025) has introduced a fun- damentally new paradigm for RL’s application. It demon- strates that RL is no longer merely a tool for learning a spe- cific task; instead, it can serve as a crucial technique dur- ing the post-training phase to enhance core LLM capabili- ties such as logical reasoning and instruction following. This paradigm shift in RL is largely attributable to two key fac- tors: first, the general knowledge acquired during large-scale pre-training, and second, how “next-token prediction” uni- fies the LLM’s task representation space, enabling the model to process diverse language tasks coherently. While RL has achieved rem"}
{"id": "2507.23698v1_chunk4", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "ge-scale pre-training, and second, how “next-token prediction” uni- fies the LLM’s task representation space, enabling the model to process diverse language tasks coherently. While RL has achieved remarkable success in language modeling, its triumph hasn’t yet fully translated to visuomo- tor agents. A primary challenge lies in RL models’ tendency to overfit specific tasks or environments, hindering the ac- quisition of generalizable behaviors and cross-environment generalization. This paper provides a preliminary answer to this challenge by demonstrating that RL-finetuned visuo- motor agents can achieve zero-shot generalization of their enhanced spatial reasoning capabilities to unseen environ- ments (including other 3D environments and the real world). To achieve this, we need to construct a unified and effi- cient multi-task goal space. We believe an ideal visuomotor agent’s goal space should possess the following key proper- ties: openness to accommodate an infinite variety of task"}
{"id": "2507.23698v1_chunk5", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "ct a unified and effi- cient multi-task goal space. We believe an ideal visuomotor agent’s goal space should possess the following key proper- ties: openness to accommodate an infinite variety of tasks; unambiguity to ensure the agent’s precise understanding of task intent; scalability to support large-scale task generation; andcurriculum property to enable the agent to progressively learn complex skills. After a thorough analysis of current mainstream task representation methods, we finally select cross-view goal specification (Cai et al. 2025) as our unified task space. This means that any task involving interaction with a specific object in an open world can be uniformly rep- resented by: selecting a novel camera view from which the target object is observable, and generating a precise segmen- tation mask of that target object. This representation inher- ently fuses visual information with task objectives, laying a solid foundation for subsequent RL training. To support large-scale "}
{"id": "2507.23698v1_chunk6", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "segmen- tation mask of that target object. This representation inher- ently fuses visual information with task objectives, laying a solid foundation for subsequent RL training. To support large-scale RL post-training, we face the chal- lenge of synthesizing training tasks at scale. We choose the highly customizable open-world environment Minecraft as the RL training platform for our policies. Minecraft’s flexi- bility allows us to synthesize a vast number of task instances, spanning various visual perspectives and exhibiting smooth 1arXiv:2507.23698v1 [cs.RO] 31 Jul 2025transitions in difficulty, by randomly sampling factors such as world seed, terrain, camera view, and target object. This automated task generation mechanism resolves the bottle- neck of manual task design, enabling us to conduct large- scale multi-task training unprecedented in scope. To address the engineering challenges posed by large-scale RL train- ing, we further implement an efficient distributed RL frame- work. "}
{"id": "2507.23698v1_chunk7", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "onduct large- scale multi-task training unprecedented in scope. To address the engineering challenges posed by large-scale RL train- ing, we further implement an efficient distributed RL frame- work. This framework effectively overcomes the bottlenecks of trajectory collection and data transmission prevalent in existing RL frameworks (Moritz et al. 2017) within com- plex environments (like Minecraft), while also supporting stable training of long-sequence Transformer-based policies, ensuring we can leverage the synthesized large-scale tasks. Extensive RL post-training within the complex Minecraft on100,000 tasks reveals a remarkable 4×increase in the agent’s success rate in performing interactions under signifi- cant variations in cross views. Notably, we further demon- strate the efficacy of this RL-enhanced agent by deploy- ing it zero-shot to DMLab (Beattie et al. 2016), Unreal En- gine (Zhong et al. 2024), and real-world settings, where we observe compelling evidence of its general"}
{"id": "2507.23698v1_chunk8", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " of this RL-enhanced agent by deploy- ing it zero-shot to DMLab (Beattie et al. 2016), Unreal En- gine (Zhong et al. 2024), and real-world settings, where we observe compelling evidence of its generalized cross-view spatial reasoning capabilities. These findings strongly vali- date that RL can serve as a potent post-training mechanism for substantially augmenting the core competencies of vi- suomotor policies, endowing them with exceptional domain generalization. Our contributions are as three-fold: 1. We propose an innovative method for large-scale, au- tomated synthesis, generating over 100,000 Minecraft training tasks to overcome the bottleneck of manual de- sign. This enables us to perform the first multi-task rein- forcement learning in the challenging Minecraft. 2. We develop an efficient distributed RL framework to ad- dress engineering challenges in complex environments, ensuring stable training of long-sequence policies. 3. We empirically demonstrate that RL can serve as a pow"}
{"id": "2507.23698v1_chunk9", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "cient distributed RL framework to ad- dress engineering challenges in complex environments, ensuring stable training of long-sequence policies. 3. We empirically demonstrate that RL can serve as a pow- erful post-training mechanism for visuomotor policies, showing a remarkable 4×increase in interaction success rates and compelling zero-shot generalization of cross- view spatial reasoning in diverse, unseen environments. 2 Related Works and Preliminaries Imitation Learning IL centers on enabling an agent to learn behavior policies by observing expert demonstrations. It transforms complex decision-making into a supervised learning task: given a state St, predict the action Atan expert would take. This is typically achieved by minimizing the be- havioral discrepancy between the policy πθand the expert policy πE, often using maximum likelihood estimation for discrete actions in behavior cloning (Pomerleau 1988): max θE(St,At)∼DE[logπθ(At|St)]. (1) Through large-scale expert data, IL empowe"}
{"id": "2507.23698v1_chunk10", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "he expert policy πE, often using maximum likelihood estimation for discrete actions in behavior cloning (Pomerleau 1988): max θE(St,At)∼DE[logπθ(At|St)]. (1) Through large-scale expert data, IL empowers agents to ac- quire rich world knowledge, generalized patterns, and an implicit understanding of task intentions, rapidly building foundational behavioral capabilities. For instance, large lan- guage models (LLMs) are fundamentally driven by large- scale imitation learning via next token prediction, internaliz- ing language structures and world knowledge from vast textcorpora (Radford et al. 2019; Brown et al. 2020). Similarly, in visuomotor control, many leading vision-language-action models (VLAs), like DeepMind’s RT-X series (Brohan et al. 2022, 2023), are pre-trained on massive robot demonstra- tion datasets (Padalkar et al. 2023) using IL, gaining an ini- tial grasp of object physics, operational causality, and task instructions. However, IL’s effectiveness is constrained by expert"}
{"id": "2507.23698v1_chunk11", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "monstra- tion datasets (Padalkar et al. 2023) using IL, gaining an ini- tial grasp of object physics, operational causality, and task instructions. However, IL’s effectiveness is constrained by expert data quality, preventing it from surpassing expert per- formance or enabling autonomous exploration and error cor- rection. Crucially, it’s prone to the covariate shift prob- lem (Ross, Gordon, and Bagnell 2011)—where the agent’s actions lead to states S′unseen in expert data, causing per- formance to degrade sharply. Reinforcement Learning With its capacity for explo- ration and learning from rewards, RL has achieved remark- able success in single-task, clearly defined domains, such as AlphaGo (Silver et al. 2016) for Go or MOBA games like Dota 2 (Ye et al. 2020). Unlike IL, RL inherently allows agents to explore beyond expert data, discover novel strate- gies, and self-correct through environmental feedback, thus overcoming the covariate shift problem and even surpassing expert performa"}
{"id": "2507.23698v1_chunk12", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "lows agents to explore beyond expert data, discover novel strate- gies, and self-correct through environmental feedback, thus overcoming the covariate shift problem and even surpassing expert performance. The core optimization objective in RL is to maximize the agent’s expected cumulative reward: max θEτ∼πθ\u0014XT t=0γtRt\u0015 . (2) However, attempts to apply this RL paradigm for train- ing general-purpose agents in multitask, open-world, or high-dimensional observation spaces have frequently failed (Vithayathil Varghese and Mahmoud 2020b). This is mainly because, in complex open-world scenarios, RL faces significant challenges, notably sample inefficiency and sparse reward signal (Fan et al. 2022; Baker et al. 2022; Cai et al. 2023a). It is incredibly difficult to construct a dense reward signal that universally incentivizes behavior across multiple tasks. This often leads to agents struggling to re- ceive effective feedback during exploration, and they can easily fall into the traps of catas"}
{"id": "2507.23698v1_chunk13", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "that universally incentivizes behavior across multiple tasks. This often leads to agents struggling to re- ceive effective feedback during exploration, and they can easily fall into the traps of catastrophic forgetting and neg- ative transfer, causing them to unlearn previously acquired skills or for different tasks to conflict. A deeper underly- ing reason is that pure RL lacks prior general world knowl- edge and common sense , forcing the agent to learn every- thing about the environment and tasks from scratch, which is highly inefficient in complex, open-ended settings. Foundation-to-Finesse Learning Given the complemen- tary strengths of IL (efficient knowledge acquisition) and RL (exploration and refinement), and acknowledging the limita- tions of pure IL in generalization and the sample inefficiency of training RL from scratch in multi-task scenarios, the pre- vailing paradigm for LLM training has evolved into an effec- tive combination of both (Ouyang et al. 2022; DeepSeek-AI et"}
{"id": "2507.23698v1_chunk14", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "ple inefficiency of training RL from scratch in multi-task scenarios, the pre- vailing paradigm for LLM training has evolved into an effec- tive combination of both (Ouyang et al. 2022; DeepSeek-AI et al. 2025). This approach features a clear, progressive train- ing flow designed to build powerful agents (Ze et al. 2023; Yuan et al. 2024). First, IL serves as the builder of foun- dational knowledge and implicit reasoning capabilities . By training on vast amounts of expert data, agents efficiently learn and internalize large-scale general world knowledge, 2Table 1: Key Properties of Effective Task Spaces for Embodied Agents. Openness Refers to the diversity and infinitude of the task space. It enables agents to continuously encounter novel visual configu- rations, object arrangements, or interaction scenarios, preventing rote memorization. This ensures agents develop robust and generalizable visuomotor policies capable of handling unseen real-world complexities. Unambiguity Ensures tha"}
{"id": "2507.23698v1_chunk15", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "nteraction scenarios, preventing rote memorization. This ensures agents develop robust and generalizable visuomotor policies capable of handling unseen real-world complexities. Unambiguity Ensures that each task instance has clear, well-defined metrics and verifiable success criteria. For visuomotor agents, this means the goal state or action execution must be precisely measurable. Such clarity is vital for expert demonstrations in imitation learning (IL) and for designing effective reward signals during reinforcement learning (RL) fine-tuning. Scalability Emphasizes that the task space must facilitate the automated and large-scale generation of both demonstration data for IL pre-training and expanded task sets for RL fine-tuning. Crucially, reward functions for these tasks must be easily and efficiently designable, or verifiable without extensive human intervention. Curriculum A task space with curriculum properties provides a smooth transition in difficulty, offering a progressive le"}
{"id": "2507.23698v1_chunk16", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "and efficiently designable, or verifiable without extensive human intervention. Curriculum A task space with curriculum properties provides a smooth transition in difficulty, offering a progressive learning path from simple to complex. It contains a spectrum where agents gradually master basic skills, with simpler tasks serving as necessary building blocks for more intricate ones, thus facilitating knowledge transfer. common sense, behavioral patterns, and an implicit under- standing of diverse tasks. This observation-acquired gen- eralization lays the groundwork for subsequent causal and spatial reasoning, enabling agents to comprehend various instructions and contexts and produce initial, expected re- sponses. Subsequently, RL takes on the crucial role of re- fining and applying explicit reasoning capabilities . Build- ing upon the solid foundation laid by IL, agents enter real or simulated environments to further optimize their policy through active trial-and-error and reward feedba"}
{"id": "2507.23698v1_chunk17", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " reasoning capabilities . Build- ing upon the solid foundation laid by IL, agents enter real or simulated environments to further optimize their policy through active trial-and-error and reward feedback. At this stage, RL is no longer blind exploration from scratch but rather fine-tuning based on a well-initialized policy. This progressive relationship allows agents to efficiently learn “how to do” from imitation, and then “how to do better” through RL, ultimately translating implicit knowledge into actionable, verifiable reasoning capabilities. 3 Task Space for Generalizable RL In traditional multi-task RL, a visuomotor agent learns to master a small set of k predefined tasks. The task represen- tation in this paradigm is often a simple identifier (e.g., a one-hot vector), which lacks the semantic structure required for meaningful knowledge transfer, thus hindering general- ization. Our objective is more ambitious: to enable a policy to generalize from ktraining tasks to n≫knovel task"}
{"id": "2507.23698v1_chunk18", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " semantic structure required for meaningful knowledge transfer, thus hindering general- ization. Our objective is more ambitious: to enable a policy to generalize from ktraining tasks to n≫knovel tasks, or even to entirely new 3D environments. Achieving this leap requires a unified task space that can seamlessly bridge train- ing and generalization. We argue that an ideal task space must inherently satisfy four properties, shown in Table 1. Next, we analyze the following common task spaces. Natural Language as a task space offers high openness due to its inherent expressiveness and compositionality, easily facilitating diverse task sets with varying curricula difficul- ties. However, it exhibits high ambiguity for fine-grained spatial relationships, complicating large-scale reward design and verification, thus limiting its scalability for precise lo- calization and manipulation tasks. When the target object is invisible, language no longer provides meaningful guidance for exploration. "}
{"id": "2507.23698v1_chunk19", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "nd verification, thus limiting its scalability for precise lo- calization and manipulation tasks. When the target object is invisible, language no longer provides meaningful guidance for exploration. Figure 3 aillustrates the failure of the lan- guage space in multi-task RL within complex Minecraft. Instance Image defines tasks by providing close-up photos of a target object, often requiring the object to dominate the frame (e.g. 70% coverage) (Krantz et al. 2023). Although se- mantically rich, this representation inherently deemphasizesspatial context, limiting its utility for complex spatial rea- soning tasks. Lacking an explicit instance cue, this method suffers from target ambiguity, especially in the presence of other small objects in the background. And, it struggles with openness andcurriculum due to a narrow range of possible visual contexts, and its focus on appearance matching rather than understanding spatial relationships. Cross-View Goal Specification (CVGS) offers a metho"}
{"id": "2507.23698v1_chunk20", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "dcurriculum due to a narrow range of possible visual contexts, and its focus on appearance matching rather than understanding spatial relationships. Cross-View Goal Specification (CVGS) offers a method to specify any goal object using a segmentation mask from a third-person view. This approach inherently overcomes the rigid “qualification” constraints of Instance Image and, more importantly, demands the agent to reason about spatial rela- tionships between its current view and the third-person goal view. Its flexibility allows precise control over task difficulty by adjusting view distance and overlap, making it strong in openness andcurriculum . Its clear definition of goals also ensures high unambiguity and efficient scalability for large- scale task generation and reward verification. An notable advantage is: even if the agent can’t directly see the target object, the landmark shared across the views can still of- fer crucial guiding information. We adopt CVGS as our goal space beca"}
{"id": "2507.23698v1_chunk21", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "n notable advantage is: even if the agent can’t directly see the target object, the landmark shared across the views can still of- fer crucial guiding information. We adopt CVGS as our goal space because it naturally facilitates cross-domain generalization. The core capabilities it requires, reason- ing about visual views and spatial information within the same domain, are inherently suited for this. 4 Pipeline Design Task Formulation We define a task instance Tusing a combination of pixel images and instance mask: T= ⟨O1, Og, Mg, E⟩, where O1is the initial agent view ob- tained by resetting the environment, Ogis the goal observa- tion, provided from a distinct, often human-centric or third- person viewpoint. Crucially, Ogincludes a precise segmen- tation mask Mgthat explicitly highlights the target object. E denotes the interaction event, e.g. break item ,use item ,pick upandplace . The agent’s policy, denoted as πθ(At|O1:t,T), is a network that maps the agent’s observations O1:tand t"}
{"id": "2507.23698v1_chunk22", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "the target object. E denotes the interaction event, e.g. break item ,use item ,pick upandplace . The agent’s policy, denoted as πθ(At|O1:t,T), is a network that maps the agent’s observations O1:tand the task instance Tto a distribution over actions At. The core challenge is to learn a cross-view alignment; that is, to un- derstand the spatial relationship between its own O1:tand the goal specified by MginOg. Pre-Training via Imitation Learning Our IL stage fol- lows (Cai et al. 2025), which pre-trains policies on large- 3Goal Ego-View Random DistanceRandom CameraRandom TargetRandom SeedRandom Biome Random Position SampleTaskseasy𝒯!medium𝒯\"hard𝒯#Open WorldTransformer𝒯!𝑽𝒕𝑷𝒕𝑨𝒕𝑪𝒕𝒉𝒕𝒉𝒕\"𝟏 No SupervisionSignal during RL𝑶𝒕Environment FeedbackOnly RGB Images Keyboard & Mouse ActionsRandomly InitializedCritic HeadKV cache TaskIncentivizing Cross-View Spatial Reasoning ApproachUseBreak𝑹𝒕Outcome Reward Aync. Env.Pre-trained via Imitation LearningGoal Ego-View Action Mappingvelocity=+100forwardveloc"}
{"id": "2507.23698v1_chunk23", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "zedCritic HeadKV cache TaskIncentivizing Cross-View Spatial Reasoning ApproachUseBreak𝑹𝒕Outcome Reward Aync. Env.Pre-trained via Imitation LearningGoal Ego-View Action Mappingvelocity=+100forwardvelocity=-100back……pick = 1attack a3=+1forwarda3=−1back……a0=4.75xyaw=x0.1 meters forw.forward0.1 meters back.back……wheels rotate xyaw=x UnrealEngineDMLabRealWorldLarge-Scale Task SynthesisAsynchronous Reinforcement LearningZero-Shot Domain Generalization Figure 1: The Post-Training Pipeline. We synthesize large-scale, mixed-difficulty cross-view interaction tasks in an open- world environment by randomly sampling terrain, distances, target objects, and camera views. The foundational policy is fine-tuned using our distributed RL framework and then deployed in unseen 3D worlds via a simple action space mapping. scale trajectories collected via backward trajectory relabel- ing. We formulate the dataset with Ntrajectories as: D={(O(i) 1:T, A(i) 1:T, M(i) 1:T, V(i) 1:T, P(i) 1:T, E(i))}N i=1, (3) wh"}
{"id": "2507.23698v1_chunk24", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "ce mapping. scale trajectories collected via backward trajectory relabel- ing. We formulate the dataset with Ntrajectories as: D={(O(i) 1:T, A(i) 1:T, M(i) 1:T, V(i) 1:T, P(i) 1:T, E(i))}N i=1, (3) where V(i) t,P(i) t, andM(i) trespectively denote the target ob- ject’s visibility, geometric centroid, instance mask in frame O(i) t. As the target object remains the same with each trajec- toryτi, we can sample any frame index g∈[1, T]to build a task instance T(i)=D O(i) 1, O(i) g, M(i) g, E(i)E . To enhance the model’s sensitivity towards target perception, we maxi- mize the log-likelihood of a joint distribution as objective: max θ1 NTNX i=1TX t=1logπθ(A(i) t, V(i) t, P(i) t|O(i) 1:t,T(i)).(4) Large-Scale Cross-View Task Synthesis Given any task T=⟨O1, Og, Mg, E⟩, cross-view spatial reasoning in- volves analyzing the relationship between history views O1:t and goal view Ogto implicitly plan an executive path. Therefore, the discrepancy between O1andOgnaturally characterizes the task’s di"}
{"id": "2507.23698v1_chunk25", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "g in- volves analyzing the relationship between history views O1:t and goal view Ogto implicitly plan an executive path. Therefore, the discrepancy between O1andOgnaturally characterizes the task’s difficulty, with difficulty changes ex- hibiting a smooth, continuous relationship. We observe that pre-trained agents show weak foundational spatial reason- ing, succeeding only when O1andOgare minimally differ- ent.We aim to explore if RL can enhance this spatial reason- ing ability and enable transfer to other 3D environments. To this end, we designed an automated task synthesis method based on the Minecraft environment. Specifically, we first randomly sample a spawn location p0in the world and generate interactive objects (e.g., blocks, mobs) in its vicinity. Subsequently, we sample a distance d(which di- rectly influences task difficulty), teleport the player to a lo- cation at that distance, and adjust the camera view to encom- pass at least one object, thus obtaining a novel goal view"}
{"id": "2507.23698v1_chunk26", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "ce d(which di- rectly influences task difficulty), teleport the player to a lo- cation at that distance, and adjust the camera view to encom- pass at least one object, thus obtaining a novel goal view Og. We access the voxel information around the player in the Minecraft simulator, then select one of these objects as the interaction target. The bottom-center coordinate of this object is G= (Gx, Gy, Gz). By combining this with the player’s eye center coordinate U= (Ux, Uy, Uz), and theplayer’s yaw angle θyandpitch angle θp, we can construct the corresponding rotation matrix RM=\"cos(θy) 0 sin( θy) −sin(θp) sin(θy) cos( θp) sin( θp) cos( θy) −cos(θp) sin(θy)−sin(θp) cos( θp) cos( θy)# ,(5) Therefore, the object in the camera coordinate system can be expressed as C=RM(G−U). Subsequently, based on the dimensions of the Ogscreen H·W, the vertical field of view angle fy, and the principles of perspective projection, we can calculate its values in normalized device coordinates (NDC) (nx, ny), "}
{"id": "2507.23698v1_chunk27", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " on the dimensions of the Ogscreen H·W, the vertical field of view angle fy, and the principles of perspective projection, we can calculate its values in normalized device coordinates (NDC) (nx, ny), which are then finally converted into the screen’s pixel coordinates (u, v) fx= 2·arctan (tan ( fy/2)·W/H ), (6) nx=Cx Cz·tan(fx/2), ny=Cy Cz·tan(fy/2), (7) u= (nx+ 1)/2·W, v = (1−ny)/2·H. (8) As individual voxels cannot precisely represent an object’s complete shape, we incorporate a Segment Anything Model (SAM) (Ravi et al. 2024). This model utilizes a series of sampled points from the voxel’s cube as prompts to extract the target object’s full mask Mgin pixel space. After gen- erating the cross view, we use the “ spreadplayers p0 distance ” command to generate starting position and O1. Thedistance parameter directly influences task difficulty and curriculum design. Rewards are then automatically gen- erated by detecting changes in the object’s voxels within the simulator, leading to an "}
{"id": "2507.23698v1_chunk28", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "istance parameter directly influences task difficulty and curriculum design. Rewards are then automatically gen- erated by detecting changes in the object’s voxels within the simulator, leading to an outcome reward. Post-Training via Reinforcement Learning We optimize the policy using a combination of the Proximal Policy Opti- mization (PPO) (Schulman et al. 2017) and a KL constraint L=LPPO+β· LKL. Minimizing the KL divergence could enhance PPO’s training stability by preserving knowledge from a reference policy πref, where πrefis the initial pre- trained policy: LKL=DKL(πθ(·|O1:t,T)∥πref(·|O1:t,T)). (9) The policy loss of standard PPO is formulated as follows: LPPO=−Eth min\u0010 rt(θ)ˆAt,clip(rt(θ),1−ϵ,1 +ϵ)ˆAt\u0011i ,(10) 4Table 2: Overview of Training and Testing Environments. Minecraft (Guss et al. 2019)Version: 1.16.5. Observations: 640×360 pixels, 70-degree FoV . Actions: Mouse and keyboard operations. Purpose: Primary training and testing platform; rich dataset for pre-training, high fr"}
{"id": "2507.23698v1_chunk29", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "t al. 2019)Version: 1.16.5. Observations: 640×360 pixels, 70-degree FoV . Actions: Mouse and keyboard operations. Purpose: Primary training and testing platform; rich dataset for pre-training, high freedom for large-scale task synthesis, crucial for studying cross-view spatial reasoning and open-world interaction. Unreal (Zhong et al. 2024)Observations: 640×480pixels, highly realistic textures, visually complex. Actions: Movement, view adjustment, jumping, interaction. Purpose: Dedicated testing platform for personnel search and rescue missions, assessing agent’s ability to locate and transport casualties using cross-view clues in a high-fidelity environment. DMLab (Beattie et al. 2016)Observations: 320×240visual images. Actions: Comparable to Minecraft (Movement, view adjust, shoot, ...). Purpose: Game-based assessment of embodied agents’ navigation and interaction skills within partially observable settings (e.g., fruit collection). Utilized for validating generalization capabilities"}
{"id": "2507.23698v1_chunk30", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " Purpose: Game-based assessment of embodied agents’ navigation and interaction skills within partially observable settings (e.g., fruit collection). Utilized for validating generalization capabilities. Real World (Ilon 1975)Physical Embodiment: Robot car with Mecanum wheels. Observations: 640×360pixels from a 110-degree camera. Purpose: To ascertain whether learned cross-view spatial reasoning capabilities generalize to real-world. where ˆAtis the generalized advantage function (Schulman et al. 2015), rt(θ) = πθ(·|O1:t,T)/πθold(·|O1:t,T)is the importance sampling ratio. During the RL process, we only optimized the action head, while omitting supervision for the auxiliary heads that predict object visibility Vtand the cen- troid point Pt. Interestingly, our experiments show that even without explicit supervision, the policy retains the function- ality of these two heads after RL post-training, suggesting that the spatial reasoning learned for action control implic- itly benefits these p"}
{"id": "2507.23698v1_chunk31", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "t explicit supervision, the policy retains the function- ality of these two heads after RL post-training, suggesting that the spatial reasoning learned for action control implic- itly benefits these perceptual tasks. Distributed RL Framework Design No off-the-shelf RL framework currently meets our specific needs, primarily due to the following considerations: high communication costs , simulator instability , and long-term dependency handling . To tackle these issues, our framework assumes a cluster composed of a shared Network Attached Storage (NAS) and multiple compute nodes, incorporating the following core mechanisms: Asynchronous Data Collection : Roll- out workers can be deployed on any compute node. Each worker comprises an inference model and N independent Minecraft instances. These instances asynchronously send requests to a queue, and the model performs batch inference when the queue reaches its specified batch size. Optimized Data Transfer : We use Ray (Moritz et al. 2017) t"}
{"id": "2507.23698v1_chunk32", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "e instances asynchronously send requests to a queue, and the model performs batch inference when the queue reaches its specified batch size. Optimized Data Transfer : We use Ray (Moritz et al. 2017) to organize different compute nodes into a cluster. However, the trajec- tories collected by rollout workers are not sent directly to the trainer. Instead, they are stored directly in a database on the shared NAS, with the trainer receiving only data indices. This strategy significantly alleviates the consumption of net- work bandwidth during training, addressing the shortcom- ings observed in modern frameworks like RLlib (Liang et al. 2017). Support for Long Sequence Training : To facilitate the training of our Transformer-based policy on long se- quences, we introduce a memory-efficient, fragment-based storage method. Unlike traditional transition-based storage, our approach stores the K-V cache state (about 10MB per step) only once per fragment (as shown in Figure 2), dras- tically reduc"}
{"id": "2507.23698v1_chunk33", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "ment-based storage method. Unlike traditional transition-based storage, our approach stores the K-V cache state (about 10MB per step) only once per fragment (as shown in Figure 2), dras- tically reducing memory overhead. This, coupled with trun- cated Backpropagation Through Time (tBPTT), allows the policy to leverage K-V cache from thousands of prior frames (O1:t−1), which is vital for capturing long-term dependen- cies in hard tasks. Our framework allows us to simultane- ously launch 72 Minecraft instances in 3 compute nodes, achieving a collection speed of about 1000 FPS. We will ℎ!,(𝑂!,𝐴!,𝑅!,…,𝑂ℓ#$,𝐴ℓ#$,𝑅ℓ#$,𝑂ℓ)ℎ!,(𝑂!ℓ,𝐴!ℓ,𝑅!ℓ,…,𝑂!#$ℓ%$,𝐴!#$ℓ%$,𝑅!#$ℓ%$,𝑂(!#$)ℓ)Our fragment-based storage: Only store one state (K-V cache) per fragment. ℓis fragment length. Suitable for Transformer-based policy. ℎ(,(𝑂(,𝐴(,𝑅(,𝑂$)ℎ!#$ℓ%$,(𝑂!#$ℓ%$,𝐴!#$ℓ%$,𝑅!#$ℓ%$,𝑂(!#$)ℓ)… 𝑘+1ℓ 𝑡𝑟𝑎𝑛𝑠𝑖𝑡𝑖𝑜𝑛𝑠Transition-based storage (e.g. RLlib): Store one state per step, which is memory consuming for large states. Suitable"}
{"id": "2507.23698v1_chunk34", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "policy. ℎ(,(𝑂(,𝐴(,𝑅(,𝑂$)ℎ!#$ℓ%$,(𝑂!#$ℓ%$,𝐴!#$ℓ%$,𝑅!#$ℓ%$,𝑂(!#$)ℓ)… 𝑘+1ℓ 𝑡𝑟𝑎𝑛𝑠𝑖𝑡𝑖𝑜𝑛𝑠Transition-based storage (e.g. RLlib): Store one state per step, which is memory consuming for large states. Suitable for RNN-based policy. …𝑘 𝑓𝑟𝑎𝑔𝑚𝑒𝑛𝑡𝑠Figure 2: Trajectory Storage Comparison . open-source our RL training framework to foster further RL research in complex environments. Details about the RL framework can be found in supplementary materials. 5 Experiments 5.1 Environments As shown in Table 2, we focus on environments where the observation space is purely pixel-based and the action space is abstractly defined to facilitate generalization. This action space encompasses omnidirectional movement control (akin to WASD key combinations), continuous camera view ad- justments (similar to mouse-controlled pitch and yaw), and a set of functional object interaction actions (e.g., picking up or dropping items). This unified design aims to enable the effective transfer of learned capabilities across di"}
{"id": "2507.23698v1_chunk35", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "led pitch and yaw), and a set of functional object interaction actions (e.g., picking up or dropping items). This unified design aims to enable the effective transfer of learned capabilities across diverse, complex environments. For all the 3D worlds, the visual in- puts are resized to 224×224before feeding into the agent. 5.2 RL Post-Training Discoveries in Minecraft We conduct post-training on about 100,000 sampled tasks within the Minecraft environment. These tasks encompass various interaction types, including Approach ,Break , and Interact , as well as Hunt (subdivided into Melee Hunt and Archery ). Examples are shown in Figure 3 b. To facilitate curriculum learning for RL training, we implement diffi- culty levels for Approach ,Break , and Interact tasks. In easy difficulty tasks, the Manhattan distance between the agent’s starting position and the target location was approximately 20blocks. Conversely, hard difficulty tasks extended this distance to roughly 60blocks. By analyzin"}
{"id": "2507.23698v1_chunk36", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "e Manhattan distance between the agent’s starting position and the target location was approximately 20blocks. Conversely, hard difficulty tasks extended this distance to roughly 60blocks. By analyzing the training curves in Figure 3, we observe the followings. 4×Performance Leap Under Complex Views Task per- formance across all interaction types significantly improved, with the average success rate increasing from 7%to28%. 5Bird Eye View of Curriculum Tasks4060 20hard mediumeasyvisible zoneplayer positiontarget atarget btarget cWith Curriculum:-Difficulty levels: 20/40/60 blocks away-Tasks are distributed evenly across three difficulty levels (1:1:1 ratio). Without Curriculum:-Only hard tasks (60 blocks away) will be sampled during the reinforcement learning. Experiment Setup eApproachBreakInteractArcheryMelee Huntf ac db g goal view after RL: success to find the victim in gardenbefore RL: fail to locate the victim goal viewbefore RL: get stuck in front of an apple after RL: success t"}
{"id": "2507.23698v1_chunk37", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "akInteractArcheryMelee Huntf ac db g goal view after RL: success to find the victim in gardenbefore RL: fail to locate the victim goal viewbefore RL: get stuck in front of an apple after RL: success to collect the apple goal viewbefore RL: wandering without direction after RL: success to find the correct ballFigure 3: RL Post-Training Boosts Generalizable Spatial Reasoning and Open-World Interaction Capabilities. (a) RL training curves for five skills in the Minecraft environment. This panel shows simultaneous performance gains across all skills. It also highlights the policy’s performance collapse in later training stages without a KL divergence constraint. (b)Sample target viewpoints for each skill during training, encompassing various camera view ranges (e.g., eye-level and top-down). “Archery” involves long-range interaction with mobs, while “Melee Hunt” requires close-quarters combat. (c)Comparison of curriculum-based training (mixed difficulties) with non-curriculum training (har"}
{"id": "2507.23698v1_chunk38", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "Archery” involves long-range interaction with mobs, while “Melee Hunt” requires close-quarters combat. (c)Comparison of curriculum-based training (mixed difficulties) with non-curriculum training (hard tasks only). The “Discounted Reward” plot on the left shows curriculum learning leads to higher training efficiency and faster reward accumulation, while the “Value Function Explained Variance” plot on the right demonstrates it also accelerates value function learning. (d)Results table for current SOTA goal-conditioned agents in Minecraft. Success rate is reported. Our agent is the first to achieve successful multi-task RL in challenging Minecraft environment. Several representative single-task RL agents are also listed for reference. (e)Point Prediction and Visibility Prediction loss comparison before and after RL training. Losses for these heads on the pre-training dataset remain largely unchanged despite not being optimized during RL, indicating that RL preserved the policy’s original"}
{"id": "2507.23698v1_chunk39", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "ison before and after RL training. Losses for these heads on the pre-training dataset remain largely unchanged despite not being optimized during RL, indicating that RL preserved the policy’s original representations. (f)This panel shows significant improvements in DMLab30 fruit collection, robot car approach, and Unreal rescue reward after RL training, demonstrating the model’s effective generalization to unseen 3D worlds. (g)Case studies of domain transfer. We analyze some successful and failure cases here. More details can be found in supplementary details. We performed 32 runs for each experiment. 6Notably, for Archery , the success rate surged from less than 1%after pre-training to 28% following RL post-training, indicating that RL can unleash rare capabilities from pre- training. The improved success rates on hard tasks further demonstrate the model is acquiring exploration abilities. Ensuring Stable RL Post-Training with KL Figure 3 a reveals KL divergence is key to RL post-trai"}
{"id": "2507.23698v1_chunk40", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " improved success rates on hard tasks further demonstrate the model is acquiring exploration abilities. Ensuring Stable RL Post-Training with KL Figure 3 a reveals KL divergence is key to RL post-training stability. Specifically, this KL divergence is computed with respect to the initial imitation learning pretrained policy. Models with KL divergence (w/ KL) show more stable learning and con- sistently higher performance, avoiding the fluctuations and collapse seen in models without it (w/o KL). We also find that policies without pre-training failed in multi-task RL, highlighting Minecraft’s complexity and tasks’ difficulty. Language-Based RL: STEVE-1’s Adaptation Bottleneck Figure 3 ashows language-based STEVE-1 (Lifshitz et al. 2023), which is pre-trained on the Minecraft contractor data via imitation learning and post-trained with our RL pipeline, consistently achieves near-zero performance during RL stage. This highlights a critical limitation: natural language inherently struggles"}
{"id": "2507.23698v1_chunk41", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " via imitation learning and post-trained with our RL pipeline, consistently achieves near-zero performance during RL stage. This highlights a critical limitation: natural language inherently struggles to support effective spatial context rea- soning for distant or occluded target objects . Conversely, in situations where objects are not visible, our method can leverage background and landmark objects from a third- view perspective to aid in spatial reasoning, thereby provid- ing effective exploration guidance for RL. Mixed-Difficulty Curriculum for Accelerated Learning Unlike traditional easy-to-hard progressions, our curricu- lum adopts a mixed-difficulty training strategy. As Figure 3 c shows, using the Break interaction, we define three distinct difficulty levels: Easy ,Medium , and Hard , which are char- acterized by Manhattan distances of 20,40, and 60blocks, respectively. In our curriculum setup, the model is trained simultaneously and uniformly across all three difficulty lev- e"}
{"id": "2507.23698v1_chunk42", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "d , which are char- acterized by Manhattan distances of 20,40, and 60blocks, respectively. In our curriculum setup, the model is trained simultaneously and uniformly across all three difficulty lev- els. In contrast, the non-curriculum baseline is trained exclu- sively on the hard task. Notably, even though hard tasks con- stitute only one-third of the sampling frequency in our cur- riculum setting, we observe a higher performance improve- ment. The explained variance curve further illustrates this: the curriculum-trained model (blue) converges faster and reaches higher explained variance than the non-curriculum baseline (red). This strongly demonstrates that a mixed- difficulty curriculum can substantially accelerate the learn- ing of complex skills in RL environments. Robustness of Intrinsic Spatial Reasoning Figure 3 e reveals a key discovery: auxiliary prediction heads (cen- troid and visibility, Equation 4) maintain strong performance post-RL, degrading only slightly despite no ex"}
{"id": "2507.23698v1_chunk43", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "nsic Spatial Reasoning Figure 3 e reveals a key discovery: auxiliary prediction heads (cen- troid and visibility, Equation 4) maintain strong performance post-RL, degrading only slightly despite no explicit training during this stage. This sustained performance demonstrates the robustness of the agent’s intrinsic spatial reasoning. It in- dicates that the fundamental spatial understanding, fostered by the cross-view goal alignment task space, persists largely unchanged, preventing overfitting to downstream objectives. 5.3 Baselines Comparison in Minecraft To benchmark our model in complex Minecraft interac- tions, we compare it against mainstream end-to-end base-lines: STEVE-1, ROCKET-1 (Cai et al. 2024), ROCKET- 2 (Cai et al. 2025), GROOT (Cai et al. 2023b), PTGM (Yuan et al. 2024), RL-GPT (Liu et al. 2024) and LS-Imagine (Li et al. 2025). Given the significant variations among these baselines in terms of single/multi-task focus, task space, and training methods, we construct three pr"}
{"id": "2507.23698v1_chunk44", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "T (Liu et al. 2024) and LS-Imagine (Li et al. 2025). Given the significant variations among these baselines in terms of single/multi-task focus, task space, and training methods, we construct three progressively challeng- ing task groups: semantic understanding ,visible instance interaction , and invisible instance interaction . The seman- ticgroup includes tasks like “chop tree” and “hunt sheep with arrow”, completed upon semantic match. The visible instance group requires interaction with a specific object visible to the agent. The invisible instance group utilizes a third-view to specify the target, as it’s otherwise not visible from the agent’s current perspective. All three task groups necessitate multi-task capabilities, rendering many existing RL-based baselines (e.g. PTGM, RL-GPT) unsuitable due to their single-task nature. Figure 3 dillustrates that most baselines achieve success rates only in the first two task groups, whereas our proposed method uniquely attains a 48%success"}
{"id": "2507.23698v1_chunk45", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "unsuitable due to their single-task nature. Figure 3 dillustrates that most baselines achieve success rates only in the first two task groups, whereas our proposed method uniquely attains a 48%success rate in the third, most challenging group. This clearly demonstrates our approach’s significant superiority over existing baselines in handling complex, target-invisible Minecraft interaction tasks. 5.4 Generalizing RL Results Beyond Minecraft To validate our method’s generality, we investigate RL- enhanced capabilities transferring to unseen 3D worlds. We experiment in DMLab, Unreal virtual environments, and with a real-world Mecanum-wheeled robot. These share a pixel observation space and an action space abstractable to omnidirectional movement, camera adjustment, and func- tional presses, enabling efficient policy adaptation via sim- ple mapping. We present detailed adaptation, tasks, and view selection in the supplementary materials. Figures 3 fandgpresent quantitative results and cas"}
{"id": "2507.23698v1_chunk46", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "enabling efficient policy adaptation via sim- ple mapping. We present detailed adaptation, tasks, and view selection in the supplementary materials. Figures 3 fandgpresent quantitative results and case stud- ies. We observe the pre-trained policy shows weak gener- alization: success rates are low even with minor O1–Og differences (e.g., Ogeye-level, target visible in both). This baseline generalization stems from the DINO pre-trained ViT backbone seeing diverse 3D textures. However, the RL-enhanced policy significantly improves generalization: it succeeds even when Ogpresents a bird’s-eye view and the target is invisible in O1. Notably, in the real-world ball- finding task, RL boosts success by up to 41%, highlighting its substantial practical potential. Nevertheless, we observe failures, such as the robot car sometimes hitting obstacles and frequently failing on medium-to-long-range approach tasks in the real world. This indicates the policy’s perfor- mance is still impacted by the vi"}
{"id": "2507.23698v1_chunk47", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "such as the robot car sometimes hitting obstacles and frequently failing on medium-to-long-range approach tasks in the real world. This indicates the policy’s perfor- mance is still impacted by the visual texture gap, underscor- ing the need for scaling up training worlds. 6 Conclusion This work validates that reinforcement learning significantly boosts visuomotor agents’ cross-view reasoning and inter- action skills. We show these enhanced abilities generalize across diverse 3D environments, including the real world. We’ve also gained valuable insights from the RL post- training process. Future work will explore unified RL train- ing for 3D worlds with varied action spaces. 7References Achiam, O. J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; et al. 2023. GPT-4 Technical Report. Baker, B.; Akkaya, I.; Zhokhov, P.; Huizinga, J.; Tang, J.; Ecoffet, A.; Houghton, B.; Sampedro, R.; and Clune, J. 2022. Video PreTraining (VPT): L"}
{"id": "2507.23698v1_chunk48", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "J.; Altman, S.; et al. 2023. GPT-4 Technical Report. Baker, B.; Akkaya, I.; Zhokhov, P.; Huizinga, J.; Tang, J.; Ecoffet, A.; Houghton, B.; Sampedro, R.; and Clune, J. 2022. Video PreTraining (VPT): Learning to Act by Watch- ing Unlabeled Online Videos. ArXiv , abs/2206.11795. Beattie, C.; Leibo, J. Z.; Teplyashin, D.; Kaufmann, T. K.; Siddharth, N.; Clark, A.; Phillps, L.; Hughes, E.; Lamb, A.; Kelly, A.; Rowland, D. J.; Merel, J.; Wayne, G.; Porcel, N.; Noury, S.; Clark, S.; Babuschkin, I.; and Botvinick, M. 2016. DeepMind Lab. arXiv preprint arXiv:1612.03801 . Brohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y .; Chen, X.; Choromanski, K.; Ding, T.; Driess, D.; Dubey, A.; Finn, C.; et al. 2023. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818 . Brohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y .; Dabis, J.; Finn, C.; Gopalakrishnan, K.; Hausman, K.; Herzog, A.; Hsu, J.; Ibarz, J.; Ichter, B.; Irpan, A.; Jackson, T.; Jesmonth"}
{"id": "2507.23698v1_chunk49", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "rXiv:2307.15818 . Brohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y .; Dabis, J.; Finn, C.; Gopalakrishnan, K.; Hausman, K.; Herzog, A.; Hsu, J.; Ibarz, J.; Ichter, B.; Irpan, A.; Jackson, T.; Jesmonth, S.; Joshi, N. J.; Julian, R. C.; Kalashnikov, D.; Kuang, Y .; Leal, I.; Lee, K.-H.; Levine, S.; Lu, Y .; Malla, U.; Manju- nath, D.; Mordatch, I.; Nachum, O.; Parada, C.; Peralta, J.; Perez, E.; Pertsch, K.; Quiambao, J.; Rao, K.; Ryoo, M. S.; Salazar, G.; Sanketi, P. R.; Sayed, K.; Singh, J.; Sontakke, S. A.; Stone, A.; Tan, C.; Tran, H.; Vanhoucke, V .; Vega, S.; Vuong, Q. H.; Xia, F.; Xiao, T.; Xu, P.; Xu, S.; Yu, T.; and Zitkovich, B. 2022. RT-1: Robotics Transformer for Real- World Control at Scale. ArXiv , abs/2212.06817. Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan, T. J.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, "}
{"id": "2507.23698v1_chunk50", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "iwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan, T. J.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. ArXiv , abs/2005.14165. Cai, S.; Mu, Z.; Liu, A.; and Liang, Y . 2025. ROCKET- 2: Steering Visuomotor Policy via Cross-View Goal Align- ment. arXiv preprint arXiv:2503.02505 . Cai, S.; Wang, Z.; Lian, K.; Mu, Z.; Ma, X.; Liu, A.; and Liang, Y . 2024. ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting. arXiv preprint arXiv:2410.17856 . Cai, S.; Wang, Z.; Ma, X.; Liu, A.; and Liang, Y . 2023a. Open-World Multi-Task Control Through Goal-Aware Rep- resentation Learning and Adaptive Horizon Prediction. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 13734–13744. Cai"}
{"id": "2507.23698v1_chunk51", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "pen-World Multi-Task Control Through Goal-Aware Rep- resentation Learning and Adaptive Horizon Prediction. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 13734–13744. Cai, S.; Zhang, B.; Wang, Z.; Ma, X.; Liu, A.; and Liang, Y . 2023b. GROOT: Learning to Follow Instructions by Watch- ing Gameplay Videos. In The Twelfth International Confer- ence on Learning Representations . Caron, M.; Touvron, H.; Misra, I.; J ´egou, H.; Mairal, J.; Bo- janowski, P.; and Joulin, A. 2021. Emerging Properties inSelf-Supervised Vision Transformers. In Proceedings of the International Conference on Computer Vision (ICCV) . DeepSeek-AI; Guo, D.; Yang, D.; Zhang, H.; Song, J.-M.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; Zhang, X.; Yu, X.; Wu, Y .; Wu, Z. F.; Gou, Z.; Shao, Z.; Li, Z.; Gao, Z.; Liu, A.; Xue, B.; Wang, B.-L.; Wu, B.; Feng, B.; Lu, C.; Zhao, C.; Deng, C.; Zhang, C.; Ruan, C.; Dai, D.; Chen, D.; Ji, D.-L.; Li, E.; Lin, F.; Dai, F.; Luo, F.; Hao, G.; Ch"}
{"id": "2507.23698v1_chunk52", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "hao, Z.; Li, Z.; Gao, Z.; Liu, A.; Xue, B.; Wang, B.-L.; Wu, B.; Feng, B.; Lu, C.; Zhao, C.; Deng, C.; Zhang, C.; Ruan, C.; Dai, D.; Chen, D.; Ji, D.-L.; Li, E.; Lin, F.; Dai, F.; Luo, F.; Hao, G.; Chen, G.; Li, G.; Zhang, H.; Bao, H.; Xu, H.; Wang, H.; Ding, H.; Xin, H.; Gao, H.; Qu, H.; Li, H.; Guo, J.; Li, J.; Wang, J.; Chen, J.; Yuan, J.; Qiu, J.; Li, J.; Cai, J.; Ni, J.; Liang, J.; Chen, J.; Dong, K.; Hu, K.; Gao, K.; Guan, K.; Huang, K.; Yu, K.; Wang, L.; Zhang, L.; Zhao, L.; Wang, L.; Zhang, L.; Xu, L.; Xia, L.; Zhang, M.; Zhang, M.; Tang, M.; Li, M.; Wang, M.; Li, M.; Tian, N.; Huang, P.; Zhang, P.; Wang, Q.; Chen, Q.; Du, Q.; Ge, R.; Zhang, R.; Pan, R.; Wang, R.; Chen, R. J.; Jin, R.; Chen, R.; Lu, S.; Zhou, S.; Chen, S.; Ye, S.; Wang, S.; Yu, S.; Zhou, S.; Pan, S.; Li, S. S.; Zhou, S.; Wu, S.- K.; Yun, T.; Pei, T.; Sun, T.; Wang, T.; Zeng, W.; Zhao, W.; Liu, W.; Liang, W.; Gao, W.; Yu, W.-X.; Zhang, W.; Xiao, W.; An, W.; Liu, X.; Wang, X.; aokang Chen, X.; Nie, X.; Cheng, X.;"}
{"id": "2507.23698v1_chunk53", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "u, S.; Wu, S.- K.; Yun, T.; Pei, T.; Sun, T.; Wang, T.; Zeng, W.; Zhao, W.; Liu, W.; Liang, W.; Gao, W.; Yu, W.-X.; Zhang, W.; Xiao, W.; An, W.; Liu, X.; Wang, X.; aokang Chen, X.; Nie, X.; Cheng, X.; Liu, X.; Xie, X.; Liu, X.; Yang, X.; Li, X.; Su, X.; Lin, X.; Li, X. Q.; Jin, X.; Shen, X.-C.; Chen, X.; Sun, X.; Wang, X.; Song, X.; Zhou, X.; Wang, X.; Shan, X.; Li, Y . K.; Wang, Y . Q.; Wei, Y . X.; Zhang, Y .; Xu, Y .; Li, Y .; Zhao, Y .; Sun, Y .; Wang, Y .; Yu, Y .; Zhang, Y .; Shi, Y .; Xiong, Y .; He, Y .; Piao, Y .; Wang, Y .; Tan, Y .; Ma, Y .; Liu, Y .; Guo, Y .; Ou, Y .; Wang, Y .; Gong, Y .; Zou, Y .-J.; He, Y .; Xiong, Y .; Luo, Y .-W.; mei You, Y .; Liu, Y .; Zhou, Y .; Zhu, Y . X.; Huang, Y .; Li, Y .; Zheng, Y .; Zhu, Y .; Ma, Y .; Tang, Y .; Zha, Y .; Yan, Y .; Ren, Z.; Ren, Z.; Sha, Z.; Fu, Z.; Xu, Z.; Xie, Z.; guo Zhang, Z.; Hao, Z.; Ma, Z.; Yan, Z.; Wu, Z.; Gu, Z.; Zhu, Z.; Liu, Z.; Li, Z.-A.; Xie, Z.; Song, Z.; Pan, Z.; Huang, Z.; Xu, Z.; Zhang, Z.; and Zhang, Z. 20"}
{"id": "2507.23698v1_chunk54", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": ", Z.; Sha, Z.; Fu, Z.; Xu, Z.; Xie, Z.; guo Zhang, Z.; Hao, Z.; Ma, Z.; Yan, Z.; Wu, Z.; Gu, Z.; Zhu, Z.; Liu, Z.; Li, Z.-A.; Xie, Z.; Song, Z.; Pan, Z.; Huang, Z.; Xu, Z.; Zhang, Z.; and Zhang, Z. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. ArXiv , abs/2501.12948. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ArXiv , abs/2010.11929. Fan, L. J.; Wang, G.; Jiang, Y .; Mandlekar, A.; Yang, Y .; Zhu, H.; Tang, A.; Huang, D.-A.; Zhu, Y .; and Anand- kumar, A. 2022. MineDojo: Building Open-Ended Em- bodied Agents with Internet-Scale Knowledge. ArXiv , abs/2206.08853. Guss, W. H.; Houghton, B.; Topin, N.; Wang, P.; Codel, C.; Veloso, M. M.; and Salakhutdinov, R. 2019. MineRL: A Large-Scale Dataset of Minecraft Demonstrations. In Inter- nationa"}
{"id": "2507.23698v1_chunk55", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "Xiv , abs/2206.08853. Guss, W. H.; Houghton, B.; Topin, N.; Wang, P.; Codel, C.; Veloso, M. M.; and Salakhutdinov, R. 2019. MineRL: A Large-Scale Dataset of Minecraft Demonstrations. In Inter- national Joint Conference on Artificial Intelligence . Ilon, B. E. 1975. Wheeled vehicle. Krantz, J.; Gervet, T.; Yadav, K.; Wang, A.; Paxton, C.; Mottaghi, R.; Batra, D.; Malik, J.; Lee, S.; and Chaplot, D. S. 2023. Navigating to objects specified by images. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 10916–10925. Li, J.; Wang, Q.; Wang, Y .; Jin, X.; Li, Y .; Zeng, W.; and 8Yang, X. 2025. Open-World Reinforcement Learning over Long Short-Term Imagination. In ICLR . Liang, E.; Liaw, R.; Nishihara, R.; Moritz, P.; Fox, R.; Gold- berg, K.; Gonzalez, J. E.; Jordan, M. I.; and Stoica, I. 2017. RLlib: Abstractions for Distributed Reinforcement Learn- ing. In International Conference on Machine Learning . Lifshitz, S.; Paster, K.; Chan, H.; Ba, J.; and McIlraith, S. A."}
{"id": "2507.23698v1_chunk56", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": ".; and Stoica, I. 2017. RLlib: Abstractions for Distributed Reinforcement Learn- ing. In International Conference on Machine Learning . Lifshitz, S.; Paster, K.; Chan, H.; Ba, J.; and McIlraith, S. A. 2023. STEVE-1: A Generative Model for Text-to-Behavior in Minecraft. ArXiv , abs/2306.00937. Lin, H.; Wang, Z.; Ma, J.; and Liang, Y . 2023. Mcu: A task-centric framework for open-ended agent evaluation in minecraft. arXiv preprint arXiv:2310.08367 . Liu, S.; Yuan, H.; Hu, M.; Li, Y .; Chen, Y .; Liu, S.; Lu, Z.; and Jia, J. 2024. RL-GPT: Integrating Reinforcement Learn- ing and Code-as-policy. arXiv preprint arXiv:2402.19299 . Mnih, V .; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve- ness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M. A.; Fidjeland, A. K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.; Antonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg, S.; and Hassabis, D. 2015. Human-level control through deep reinforcement learning. Nature , 518: 529–533. Moritz, P.; Ni"}
{"id": "2507.23698v1_chunk57", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "tie, C.; Sadik, A.; Antonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg, S.; and Hassabis, D. 2015. Human-level control through deep reinforcement learning. Nature , 518: 529–533. Moritz, P.; Nishihara, R.; Wang, S.; Tumanov, A.; Liaw, R.; Liang, E.; Paul, W.; Jordan, M. I.; and Stoica, I. 2017. Ray: A Distributed Framework for Emerging AI Applications. ArXiv , abs/1712.05889. Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information pro- cessing systems , 35: 27730–27744. Padalkar, A.; Pooley, A.; Jain, A.; Bewley, A.; Herzog, A.; Irpan, A.; Khazatsky, A.; Rai, A.; Singh, A.; Brohan, A.; et al. 2023. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864 . Pomerleau, D. A. 1988. Alvinn: An autonomous land ve- hicle in a neural network. Advances in neural information proce"}
{"id": "2507.23698v1_chunk58", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "ment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864 . Pomerleau, D. A. 1988. Alvinn: An autonomous land ve- hicle in a neural network. Advances in neural information processing systems , 1. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. 2019. Language models are unsupervised multitask learners. OpenAI blog , 1(8): 9. Ravi, N.; Gabeur, V .; Hu, Y .-T.; Hu, R.; Ryali, C.; Ma, T.; Khedr, H.; R ¨adle, R.; Rolland, C.; Gustafson, L.; Mintun, E.; Pan, J.; Alwala, K. V .; Carion, N.; Wu, C.-Y .; Gir- shick, R.; Doll ´ar, P.; and Feichtenhofer, C. 2024. SAM 2: Segment Anything in Images and Videos. arXiv preprint arXiv:2408.00714 . Ross, S.; Gordon, G.; and Bagnell, D. 2011. A reduction of imitation learning and structured prediction to no-regret on- line learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics , 627–635. JMLR Workshop and Conference Proceedings. Schulman, J.; Moritz,"}
{"id": "2507.23698v1_chunk59", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "regret on- line learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics , 627–635. JMLR Workshop and Conference Proceedings. Schulman, J.; Moritz, P.; Levine, S.; Jordan, M.; and Abbeel, P. 2015. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438 .Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal Policy Optimization Algorithms. ArXiv , abs/1707.06347. Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; van den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V .; Lanctot, M.; Dieleman, S.; Grewe, D.; Nham, J.; Kalchbrenner, N.; Sutskever, I.; Lillicrap, T. P.; Leach, M.; Kavukcuoglu, K.; Graepel, T.; and Hassabis, D. 2016. Mastering the game of Go with deep neural networks and tree search. Nature , 529: 484–489. Taylor, M. E.; and Stone, P. 2011. An introduction to inter- task transfer for reinforcement learning. Ai Ma"}
{"id": "2507.23698v1_chunk60", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " Mastering the game of Go with deep neural networks and tree search. Nature , 529: 484–489. Taylor, M. E.; and Stone, P. 2011. An introduction to inter- task transfer for reinforcement learning. Ai Magazine , 32(1): 15–15. Vithayathil Varghese, N.; and Mahmoud, Q. H. 2020a. A survey of multi-task deep reinforcement learning. Electron- ics, 9(9): 1363. Vithayathil Varghese, N.; and Mahmoud, Q. H. 2020b. A Survey of Multi-Task Deep Reinforcement Learning. Elec- tronics , 9(9). Ye, D.; Liu, Z.; Sun, M.; Shi, B.; Zhao, P.; Wu, H.; Yu, H.; Yang, S.; Wu, X.; Guo, Q.; et al. 2020. Mastering complex control in moba games with deep reinforcement learning. InProceedings of the AAAI conference on artificial intelli- gence , volume 34, 6672–6679. Yuan, H.; Mu, Z.; Xie, F.; and Lu, Z. 2024. Pre-training goal-based models for sample-efficient reinforcement learn- ing. In The Twelfth International Conference on Learning Representations . Ze, Y .; Hansen, N.; Chen, Y .; Jain, M.; and Wang, X. 2023. Vi"}
{"id": "2507.23698v1_chunk61", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "g goal-based models for sample-efficient reinforcement learn- ing. In The Twelfth International Conference on Learning Representations . Ze, Y .; Hansen, N.; Chen, Y .; Jain, M.; and Wang, X. 2023. Visual reinforcement learning with self-supervised 3d rep- resentations. IEEE Robotics and Automation Letters , 8(5): 2890–2897. Zhong, F.; Wu, K.; Wang, C.; Chen, H.; Ci, H.; Li, Z.; and Wang, Y . 2024. Unrealzoo: Enriching photo-realistic virtual worlds for embodied ai. arXiv preprint arXiv:2412.20977 . 9Supplementary Materials Implementation Details and Extended Results A Cross-View Task Synthesis Details To generate a task, defined as ⟨O1, Og, Mg, E⟩, we follow a structured procedure: First, a world seed is sampled, and the player is randomly teleported to an available location ( p0) within a randomly se- lected biome. Subsequently, an interaction type ( E) is cho- sen from a predefined set: Approach ,Break ,Interact , and Hunt . Corresponding entities (e.g., blocks and mobs) rele- vant "}
{"id": "2507.23698v1_chunk62", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "a randomly se- lected biome. Subsequently, an interaction type ( E) is cho- sen from a predefined set: Approach ,Break ,Interact , and Hunt . Corresponding entities (e.g., blocks and mobs) rele- vant to the chosen interaction type are then generated within a predefined entity range around p0. Next, a random view range is determined by ∆x,∆y,∆z coordinates, along with pitch and yaw angles. The player is then teleported to the resulting cross-view position ( pg) to obtain the cross-view observation ( Og). Leveraging voxel information from the Minecraft simulator, a target object is selected from the generated entities, provided it is visible from pg. The centroid point and bounding box of this target object within Ogare extracted. These serve as prompts for SAM2 (using its largest checkpoint for optimal results) to generate the target’s mask ( Mg). The initial observation ( O1) is generated using the com- mand /spreadplayers around p0within a selected dis- tance. The distance is randomly"}
{"id": "2507.23698v1_chunk63", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " for optimal results) to generate the target’s mask ( Mg). The initial observation ( O1) is generated using the com- mand /spreadplayers around p0within a selected dis- tance. The distance is randomly selected from {20,40}, with each value representing a different level of task diffi- culty. To enhance task diversity, an alternative entity generation method is occasionally employed. Instead of generating en- tities at p0, entities are generated directly at pgby randomly sampling an unoccluded voxel. This approach is particu- larly beneficial for long-horizon tasks and certain edge cases within Interact tasks. The agent’s inventory and armor are randomly generated, while ensuring that all pre-requirements for interacting with specific entities are met. For example, an Archery task pro- vides a bow and 64arrows, while a Melee Hunt task equips the player with a random sword. Our reward design is in- tentionally simple, providing a binary reward for each task based on the return informatio"}
{"id": "2507.23698v1_chunk64", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "s a bow and 64arrows, while a Melee Hunt task equips the player with a random sword. Our reward design is in- tentionally simple, providing a binary reward for each task based on the return information supplied by the simulator. B Reinforcement Learning Design B.1 Training Details Model Choice For the Cross-View Goal Alignment task space, we utilize the 0.3B pre-trained ROCKET-2 check- point. For the language task space, the 0.6B STEVE-1 checkpoint is employed. During RL training, both the vision backbone of ROCKET-2 and the text encoder of STEVE- 1 remain fixed. Prompts for STEVE-1 are selected from its established prompt lists. Notably, the Approach task is not trained for STEVE-1, as it was not pre-trained for this spe- cific objective. Hyperparameter Settings We present the hyperparame- ter settings in Table 3. For the original PPO without KL, ρ is set to 0, while the other parameters remain unchanged. Toensure training stability, we apply clipping to both the gra- dients and the l"}
{"id": "2507.23698v1_chunk65", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "- ter settings in Table 3. For the original PPO without KL, ρ is set to 0, while the other parameters remain unchanged. Toensure training stability, we apply clipping to both the gra- dients and the log ratio. B.2 RL Framework Pipeline Our distributed Reinforcement Learning framework is en- gineered to tackle large-scale, long-horizon training tasks within Minecraft. It operates on a compute cluster with a shared Network-Attached Storage (NAS) and leverages Ray for resource coordination and fault tolerance. The core de- sign ensures scalability by decoupling data collection from training and optimizing inter-process communication. The logic is split between two primary components: Roll- out Workers for data collection and Trainers for model opti- mization. To minimize network overhead, workers write tra- jectory data (fragments) directly to the shared NAS. Syn- chronization is achieved using a lightweight index file con- taining only metadata, which trainers poll to discover new data. "}
{"id": "2507.23698v1_chunk66", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "kers write tra- jectory data (fragments) directly to the shared NAS. Syn- chronization is achieved using a lightweight index file con- taining only metadata, which trainers poll to discover new data. A key aspect of our design is a fragment-based storage strategy that optimizes for storage efficiency (Figure 4). Un- like frameworks like RLlib (Liang et al. 2017) that store model-dependent latent states (K-V caches for Transformer- XL based models, which are disk space consuming) with each transition, our Rollout Workers only store the initial la- tent state at the beginning of each contiguous fragment of experience . This approach dramatically reduces the storage footprint, as a single latent state is stored for hundreds of transitions. The subsequent latent states within the fragment are then recomputed on-the-fly segment by segment during the Truncated Backpropagation Through Time (tBPTT) pro- cess, trading a small amount of computation for a massive reduction in disk space usage. Th"}
{"id": "2507.23698v1_chunk67", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "then recomputed on-the-fly segment by segment during the Truncated Backpropagation Through Time (tBPTT) pro- cess, trading a small amount of computation for a massive reduction in disk space usage. The trainer is specifically designed to support long- sequence policy training for stateful models. It samples long, overlapping sequences from storage and employs tBPTT. As detailed in the Trainer procedure, the long sequence is pro- cessed in smaller segments (e.g., 128 steps, corresponding to the model’s context length). The final hidden state from one segment is then passed to the next ( hk←hk−1.detach ()), allowing the model to build a memory that spans thou- sands of timesteps while keeping gradient computation man- agable. The complete workflow is detailed in Algorithm 1. Our experimental hardware consisted of a dedicated train- ing node with eight NVIDIA A800 GPUs (one per trainer worker) and three data collection nodes with two NVIDIA 3090 GPUs each (one GPU per rollout worker). We "}
{"id": "2507.23698v1_chunk68", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " hardware consisted of a dedicated train- ing node with eight NVIDIA A800 GPUs (one per trainer worker) and three data collection nodes with two NVIDIA 3090 GPUs each (one GPU per rollout worker). We lever- aged automatic mixed precision (AMP) to accelerate train- ing. This distributed setup sustained a throughput of ap- proximately 500 environment frames per second (FPS), with each experiment requiring about three days to run. 10Table 3: Key hyperparameters for PPO training. Hyperparameter Value Hyperparameter Value PPO Algorithm Learning Rate 2×10−5Weight Decay 0.04 Discount Factor ( γ) 0.999 Max Gradient Norm 0.5 GAE Lambda ( λ) 0.95 Log Ratio Range 1.03 PPO Clip Ratio 0.2 KL Divergence Coeff. ( ρ) 0.2 Value Function Coeff. 0.5 KL Coeff. Decay 0.9995 Training Configuration Context Length 128 Training Iterations 4000 Effective Batch Size 10 Fragment Length 256 Epochs per Iteration 1 Automatic Mixed Precision True Replay Buffer Max Chunks 4800 Fragments per Chunk 1 Max Reuse 1 Max Sta"}
{"id": "2507.23698v1_chunk69", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "h 128 Training Iterations 4000 Effective Batch Size 10 Fragment Length 256 Epochs per Iteration 1 Automatic Mixed Precision True Replay Buffer Max Chunks 4800 Fragments per Chunk 1 Max Reuse 1 Max Staleness 1 Figure 4: Our fragment-based storage strategy. Our rollout workers only save the initial latent states (K-V caches) at the beginning of each contiguous fragment. Latent states within the fragments are computed on the fly during tBPTT. C Evaluation Protocols C.1 Minecraft Evaluation Benchmark Choices To evaluate our model and its base- lines, we define three task groups of progressively increasing difficulty: semantic understanding ,visible instance interac- tion, and invisible instance interaction . For a rigorous eval- uation, both our model and the baselines are subjected to the identical conditions specified within each task group. The first group, semantic understanding, is adapted from the Mine tasks in MCU (Lin et al. 2023). These tasks only require the agent to correctly in"}
{"id": "2507.23698v1_chunk70", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "ntical conditions specified within each task group. The first group, semantic understanding, is adapted from the Mine tasks in MCU (Lin et al. 2023). These tasks only require the agent to correctly interpret and follow language- based instructions. The second group, visible instance interaction, is based on the Minecraft Interaction Benchmark (Cai et al. 2024). Here, the agent must not only understand the instruction but also successfully locate and interact with the correct object instance (e.g., “the sheep on the right”). The third and most challenging group, invisible instance interaction, is generated by our novel task synthesis pipeline.These tasks introduce several distinct difficulties: •Exploration under pressure: The target instance is of- ten not visible from the agent’s spawn point, demanding that the agent explore the environment using visual cues. A tight time limit of 600 steps (approximately 30 sec- onds) makes efficient exploration critical, as a wrong turn can lead to "}
{"id": "2507.23698v1_chunk71", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "int, demanding that the agent explore the environment using visual cues. A tight time limit of 600 steps (approximately 30 sec- onds) makes efficient exploration critical, as a wrong turn can lead to failure. •Complex, game-like scenarios: The generated environ- ments are designed to mimic authentic gameplay. Agents must contend with emergent challenges such as switch- ing between tools, handling nearby hostile mobs, and navigating complex terrains and biomes. •Challenging skill requirements: The tasks may require skills, like archery, that pre-trained models often fail to demonstrate, despite the presence of these skills in the training data. 11Algorithm 1: Core Logic of the Distributed RL Framework 1:procedure ROLLOUT WORKER 2: Initialize :Nparallel environments, local model, buffer B 3: loop 4: Asynchronously collect observations O={o1, . . . , o m}from environments 5: ifinference queue is full then 6: A←model.inference (Obatch) ▷Batched inference for GPU efficiency 7: Dispatch acti"}
{"id": "2507.23698v1_chunk72", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " loop 4: Asynchronously collect observations O={o1, . . . , o m}from environments 5: ifinference queue is full then 6: A←model.inference (Obatch) ▷Batched inference for GPU efficiency 7: Dispatch actions Ato corresponding environments 8: end if 9: Store fragmemts {hi,(si+ℓ, ai+ℓ, ri+ℓ, si+ℓ+1)}in local buffer B ▷ h iis the hidden state, ℓis the fragment length. 10: ifBreaches threshold then 11: Write fragment data from Bto NAS 12: Append metadata of Bto index file on NAS 13: Clear B 14: end if 15: end loop 16:end procedure 17:procedure TRAINER 18: Initialize : Policy model πθ, optimizer 19: loop 20: Poll index file on NAS to find new trajectory indices 21: Sample batch of long sequences Sfrom NAS using indices 22: Initialize hidden state h0 23: foreach truncated segment SkinSdo 24: L(θ)←calculate loss(Sk, hk−1) 25: Calculate ∇θL(θ) ▷Perform tBPTT 26: hk←hk−1.detach () ▷Propagate hidden state for next segment 27: end for 28: Update model weights θusing aggregated gradients 29: Periodica"}
{"id": "2507.23698v1_chunk73", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "lculate loss(Sk, hk−1) 25: Calculate ∇θL(θ) ▷Perform tBPTT 26: hk←hk−1.detach () ▷Propagate hidden state for next segment 27: end for 28: Update model weights θusing aggregated gradients 29: Periodically save model checkpoint θto NAS 30: end loop 31:end procedure Table 4: Bridging the Minecraft Action Space and Other 3D Games. “/” denotes the masked action. Minecraft DeepMind Lab Robot Car Unreal forward = 1 a[3] = 1 0.1 meters forward velocity = +100 back= 1 a[3] = −1 0.1 meters backward / left= 1 a[2] = −1 0.1 meters left / right= 1 a[2] = 1 0.1 meters right velocity =−100 use= 1 / / / attack = 1 a[4] = 1 trigger beeper pick = 1 yaw=x a [0] = 4 .75x yaw by wheels =xangular =x pitch =x a [1] = 2 .78x pitch for camera =xviewport =x jump = 1 / / / sneak = 1 a[6] = 1 / / composite actions the same time sequential the same time C.2 Unseen Environments Evaluation Action space mapping We facilitate the agent’s applica- tion in novel environments by constructing a rule-based ac- tion mapping"}
{"id": "2507.23698v1_chunk74", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "e same time sequential the same time C.2 Unseen Environments Evaluation Action space mapping We facilitate the agent’s applica- tion in novel environments by constructing a rule-based ac- tion mapping (Table 4). Critically, this method obviates the need for environment-specific fine-tuning , as our trials demonstrated that such this approach is quite insensitive to the choice of action mapping. Unreal Zoo Rescue Task For this task, we adapt the Level 3 environment from the ATEC Challenge in Unreal (Zhong et al. 2024). In this scenario, the agent must identify in-jured individuals by interpreting surrounding visual cues, pick them up, and transport them to designated stretchers—a process that demands strong spatial reasoning abilities. Im- ages of the injured person serve as prompts for our model. Furthermore, this Unreal Engine environment provides ob- servations at a 640×480resolution, a notable deviation from the 640x360 resolution of the Minecraft training data. This discrepancy ser"}
{"id": "2507.23698v1_chunk75", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " model. Furthermore, this Unreal Engine environment provides ob- servations at a 640×480resolution, a notable deviation from the 640x360 resolution of the Minecraft training data. This discrepancy serves as a key test of the agent’s robustness and its ability to generalize across different visual domains. The agent is rewarded in two stages: 0.5 for retrieving an injured person and 0.5 for the successful transfer. DMLab30 Fruit Collection This task is set in the explore object locations small environment from DMLab30 (Beattie et al. 2016). The agent must collect fruits within 300 steps, following human-generated prompts curated from live gameplay. Real World Experiments Environment Protocols Our real world experiments are conducted indoors, using a remote inference server (one NVIDIA 4090 GPU) that synchronously transmits com- puted actions to the robot car. The car remains blocked while awaiting results. Once received—whether a single command 12Figure 5: The zero shot setting for real"}
{"id": "2507.23698v1_chunk76", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "GPU) that synchronously transmits com- puted actions to the robot car. The car remains blocked while awaiting results. Once received—whether a single command 12Figure 5: The zero shot setting for real world environ- ments. The goal would be blocked by the paper box if the car naively rotates towards the direction. Figure 6: The easy and hard variant of cross-view ap- proach setting. or a set of actions (e.g., yaw, pitch, forward)—the actions are executed sequentially. After completion, an onboard cam- era image of 640×360resolution is sent back to the server for the next inference step. To align with the motion con- trol scheme used in Minecraft, the vehicle’s wheel motors control both translational and yaw movements, while a ded- icated camera motor adjusts pitch. We did not intention- ally choose the forward distance. Due to the latency in mechanical execution and stabilization, the vehicle oper- ates at a control frequency of 2Hz, significantly slower than Minecraft’s 15+Hz frame ra"}
{"id": "2507.23698v1_chunk77", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "ly choose the forward distance. Due to the latency in mechanical execution and stabilization, the vehicle oper- ates at a control frequency of 2Hz, significantly slower than Minecraft’s 15+Hz frame rate. Primary Cross-View Goal We propose a cross-view ap- proach setting shown in Figure 5, including the initial im- age observed by the agent, global environment layout, and the goal image. The goal image is captured from a top-down perspective by holding the car in the air. In the easy vari- ant shown in the left of Figure 6, a simple rightward yaw suffices to bring the yellow ball into view; both ROCKET-2 and our method succeed reliably, with exhibiting a slightly higher short-range success rate. In the hard variant, the ball is occluded by a paper box, forcing the car to detour around the obstacle and then reorient its viewpoint. ROCKET-2 fre- quently stalls: rotating in place without progress and suc- ceeds in only 3 of 12 trials. In contrast, our method shows clear recovery behavior a"}
{"id": "2507.23698v1_chunk78", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "bstacle and then reorient its viewpoint. ROCKET-2 fre- quently stalls: rotating in place without progress and suc- ceeds in only 3 of 12 trials. In contrast, our method shows clear recovery behavior and active re-planning: it completes the detour from both the left and right sides in 8 of 12 tri- als. Three trajectories begin with substantial deviations (e.g., navigating outside the goal frame), but subsequently realign toward the target and succeed, demonstrating that early er- rors do not preclude eventual task completion. We masked the pitch action for simplicity and observed negligible dif- ference in performance. Figure 7: A long distance approach task. The agent fails in the marble hallway due to Out Of Distribution challenges and perform better in the indoor case. Figure 8: Different goal captures. Goals from phone cam- eras does not deteriorate the performance of our method. Additional Variants We also evaluated several other set- tings, including an alternative goal image(Figu"}
{"id": "2507.23698v1_chunk79", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "t goal captures. Goals from phone cam- eras does not deteriorate the performance of our method. Additional Variants We also evaluated several other set- tings, including an alternative goal image(Figure 8) and a long distance approach task(Figure 7) in different environ- ment layouts. The goal captured from the phone with dif- ferent lighting and camera parameters does not deteriorate the performance. Further breakdowns of successes and fail- ure modes under these conditions are provided in the failure analysis section. D Model Architecture We use ROCKET-2 (Cai et al. 2025) as the pre-trained model. ROCKET-2 is designed to align goals across differ- ent views. It processes training trajectories, each containing a global condition ( cg), a sequence of visual observations (ot), and their corresponding segmentation masks ( mt) over timet. A specific time step gwith a valid mask is selected as the cross-view reference. For consistency, all visual inputs (ot) and masks ( mt) are resized to "}
{"id": "2507.23698v1_chunk80", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "responding segmentation masks ( mt) over timet. A specific time step gwith a valid mask is selected as the cross-view reference. For consistency, all visual inputs (ot) and masks ( mt) are resized to 224×224pixels. First, ROCKET-2 extracts features from the visual data: Each visual observation otis processed by a frozen DINO- pretrained ViT-B/16 (Dosovitskiy et al. 2020; Caron et al. 2021) (Vision Transformer, Base architecture, 16x16 pixel patches). This encoder outputs 196 visual tokens, denoted as{ˆoi t}196 i=1. For computational efficiency, this ViT-B/16 en- 13coder remains frozen during the entire training process. Sep- arately, each segmentation mask mtis encoded using a train- able ViT-tiny/16, which also produces 196 tokens, {ˆmi t}196 i=1. Next, the model integrates information from the cross- view reference ( og, mg) to ensure spatial alignment. It com- bines the encoded visual tokens and mask tokens by concate- nating their channels, and then processes them by a Feed- Forwar"}
{"id": "2507.23698v1_chunk81", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "om the cross- view reference ( og, mg) to ensure spatial alignment. It com- bines the encoded visual tokens and mask tokens by concate- nating their channels, and then processes them by a Feed- Forward Network (FFN) to create a fused spatial represen- tation hi g. hi gis then used in a non-causal Transformer encoder, which takes the current visual tokens and this fused cross- view condition as input. By concatenating these into a se- quence of 392 tokens, this ‘SpatialFusion’ step combines spatial details from the current view with the cross-view ref- erence, producing a detailed frame-level representation xt. After obtaining the frame-level representation xt, a causal TransformerXL architecture captures temporal relationships between frames, resulting in a rich temporal representation ft. Finally, ftis fed into a lightweight network responsible to predict the action ( ˆat), centroid ( ˆpt), and visibility ( ˆvt) at the current time step. The model’s training is guided by a negative lo"}
{"id": "2507.23698v1_chunk82", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " Finally, ftis fed into a lightweight network responsible to predict the action ( ˆat), centroid ( ˆpt), and visibility ( ˆvt) at the current time step. The model’s training is guided by a negative log-likelihood loss function, which is summed over all time steps for each episode n, effectively acting as a cross-entropy-like loss to minimize the discrepancy between predicted and ground truth values: L(n) =L(n)X t=1−an tlog ˆan t−pn tlog ˆpn t−vn tlog ˆvn t. E Analyzing Failure Cases We conduct a detailed analysis of failure cases in both Minecraft and unseen environments. Minecraft Three primary reasons lead to these failures: •Occasional Segmentation Issues: This issue stems from several factors, including the fact that SAM (Segment Anything Model) is not specifically trained for Minecraft environments, and the presence of occlusions from ele- ments like the message bar or the agent’s hands, which obstruct objects. However, as vision-language models continue to improve, these challeng"}
{"id": "2507.23698v1_chunk83", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "t environments, and the presence of occlusions from ele- ments like the message bar or the agent’s hands, which obstruct objects. However, as vision-language models continue to improve, these challenges are expected to be effectively resolved. •Insufficient Visual Cues: Certain cross-view scenar- ios fail to provide adequate visual cues necessary for task completion. This necessitates extensive exploration, leading to high failure rates within limited timeframes. •Lack of Incentive for Latent Skills: Although cer- tain latent skills—such as pillar jumping, shield defense against hostile mobs, or parkouring—may exist in the pre-trained models, they are not incentivized or rein- forced during the RL process. Consequently, these abili- ties remain latent and are rarely exhibited by the agents when required. Unreal Zoo Rescue Task The failure in the Unreal Zoo Rescue Task can be attributed to several factors. First, in highly complex environments, agents often struggle with ac- curate spat"}
{"id": "2507.23698v1_chunk84", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": " when required. Unreal Zoo Rescue Task The failure in the Unreal Zoo Rescue Task can be attributed to several factors. First, in highly complex environments, agents often struggle with ac- curate spatial reasoning, making it difficult to navigate andcomplete objectives. Second, certain necessary skills—such as opening doors—are not present in Minecraft and thus are absent from the agent’s repertoire; addressing these gaps may require test-time training or fine-tuning. Finally, issues such as unintended collisions or “clipping” through objects also contribute to unsuccessful task completion. DMLab30 Fruit Collections The failure in the DMLab30 Fruit Collections task stems from several key issues. First, the low distinctiveness of DMLab30’s environments makes it difficult for the agent to distinguish between different ob- servations, leading to confusion during navigation. Addi- tionally, agents sometimes get stuck in dead ends, likely due to discrepancies between the environment dynamic"}
{"id": "2507.23698v1_chunk85", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "stinguish between different ob- servations, leading to confusion during navigation. Addi- tionally, agents sometimes get stuck in dead ends, likely due to discrepancies between the environment dynamics of DMLab30 and those of Minecraft. Interestingly, for the pre-trained ROCKET-2 agent, the primary cause of failure is its difficulty in accurately shooting the fruit, suggesting that ROCKET-2 lacks robustness to subtle skill differences, which hinders effective transfer. Real World Experiments Our method suffers from se- vere OOD challenges in the real world. First, the discrep- ancy in camera viewpoints. In Minecraft, the agent perceives the world from an elevated, human-like perspective, whereas in the real-world robotic platform, the onboard camera is mounted at a much lower height. This results in severe per- spective distortions and fundamentally different visual dis- tributions. For example, a large portion of the frame is often occupied by the floor or monotonous white walls, whic"}
{"id": "2507.23698v1_chunk86", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "results in severe per- spective distortions and fundamentally different visual dis- tributions. For example, a large portion of the frame is often occupied by the floor or monotonous white walls, which af- fects depth perception and spatial reasoning. Second, the dy- namics of the real world are subtly different from Minecraft especially near objects, such as the forward distance, colli- sion and higher chances to get stuck when turning. These two factors deteriorates the model performance. When translated back into Minecraft units, the cross-view setting corresponds to easy difficulty level: optimal control sequences require only about 30 steps to reach the goal. However, our real-world policies exhibit lower success rates, reduced stability, and less efficient trajectories compared to their Minecraft counterparts. Though the recovery capabil- ity of our method differentiates it from ROCKET-2, subop- timal exploratory behaviors occur more frequently , sug- gesting a higher likelihood "}
{"id": "2507.23698v1_chunk87", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "eir Minecraft counterparts. Though the recovery capabil- ity of our method differentiates it from ROCKET-2, subop- timal exploratory behaviors occur more frequently , sug- gesting a higher likelihood of deviation from the shortest or most direct paths. Other failure cases in longer approach tasks stem from the observation mismatch. The longer approach task (Fig- ure 7) places the goal basketball directly in front of the car, but with more than 10 meters away; the goal image is shot 0.5 meters before the ball, while the goal is only 30 pix- els wide in the initial 640×480observation. Despite the absence of obstacles, the agent repeatedly engaged in inef- ficient behaviors, alternating between brief forward move- ments and 360 degree spinning in place. This was particu- larly evident in the hallway, with white marble floors, white walls and bright lighting, which we hypothesize caused the image input to fall too far outside the distribution encoun- tered during training. Notably, When th"}
{"id": "2507.23698v1_chunk88", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "hallway, with white marble floors, white walls and bright lighting, which we hypothesize caused the image input to fall too far outside the distribution encoun- tered during training. Notably, When the same experiment is conducted in an office corridor with textured gray carpet, the car’s exploration remains more focused and directed even with more sideways. In all long approach tasks however, 14the agent rarely takes straight trajectories. These find- ings also revealed that our current model, although effective in short-range navigation and fine-grained corrections, per- forms poorly in sparse and visually homogeneous settings such as long hallways, and the reaction-based policy does not guarantee efficiency. We hypothesize that explicit spa- tial planning might relieve these issues. F Demonstration Showcases We provide visualizations of our demonstrations in Figure 9b and Figure 10. Comparisons between Hard and Easy tasks are also illustrated in Figure 9b, where, in Hard tasks, rele"}
{"id": "2507.23698v1_chunk89", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": "emonstration Showcases We provide visualizations of our demonstrations in Figure 9b and Figure 10. Comparisons between Hard and Easy tasks are also illustrated in Figure 9b, where, in Hard tasks, relevant instances are often not directly visible. We also present a gallery showcasing both successful examples and challenging corner cases from our task synthesis results. Oc- casional issues such as poor segmentation or insufficient vi- sual cues can increase the difficulty of training and eval- uation. However, as vision-language models continue to advance, we expect these challenges to be effectively ad- dressed. Some cases further highlight the necessity of using SAM, as relying solely on bounding boxes for masks can result in occlusion and ambiguity. 15(a)Gallery of task synthesis results , illustrating both successful cases and challenging corner cases. (b)Minecraft Demonstrations. We present demonstrations on the Approach ,Break ,Interact ,Melee Hunt , and Archery tasks. Additionally"}
{"id": "2507.23698v1_chunk90", "paper_id": "2507.23698v1", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "text": ", illustrating both successful cases and challenging corner cases. (b)Minecraft Demonstrations. We present demonstrations on the Approach ,Break ,Interact ,Melee Hunt , and Archery tasks. Additionally, we compare performance on Hard versus Easy tasks, noting that Hard tasks typically require exploration guided by visual cues. Figure 9: (a) Gallery of task synthesis results. (b) Minecraft Demonstrations.16Figure 10: Zero-Shot Environment Showcases. We evaluated both the pre-trained ROCKET-2 and our agent in Unreal (Zhong et al. 2024), DMLab30 (Beattie et al. 2016), and real-world environments. Experimental results demonstrate that this reinforce- ment learning approach can significantly improve performance even in unseen settings. 17"}
{"id": "2507.23682v1_chunk0", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "villa-X : Enhancing Latent Action Modeling in Vision-Language-Action Models Xiaoyu Chen2*†, Hangxing Wei3*†, Pushi Zhang1*, Chuheng Zhang1*, Kaixin Wang1*, Yanjiang Guo2, Rushuai Yang4†, Yucen Wang5†, Xinquan Xiao2†, Li Zhao1*‡, Jianyu Chen2, and Jiang Bian1 1Microsoft Research,2Tsinghua University,3Wuhan University,4Hong Kong University of Science and Technology,5Nanjing University Visual-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent work has begun to explore the incorporation of latent actions , an abstract representation of visual change between two frames, into VLA pre-training. In this paper, we introduce villa-X , a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorpora"}
{"id": "2507.23682v1_chunk1", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "tion (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training . Together, these contributions enable villa-X to achieve superior performance across simulated environments including SIM- PLER and LIBERO, as well as on two real-world robot setups including gripper and dexterous hand manipulation. We believe the ViLLA paradigm holds significant promise, and that our villa-X provides a strong foundation for future research. Keywords: Latent Action, Vision-Language-Action Model code: https://github.com/microsoft/villa-x site: https://aka.ms/villa-x 1 Introduction Latent action learning has emerged as a promising approach for the pretraining of vision-language-action (VLA) models [ 5,11,13,30,31,35,52,68], enabling learning from both robot data and human video data [ 1,11,52,67]. At the core of these methods is a Latent Action Mod"}
{"id": "2507.23682v1_chunk2", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "raining of vision-language-action (VLA) models [ 5,11,13,30,31,35,52,68], enabling learning from both robot data and human video data [ 1,11,52,67]. At the core of these methods is a Latent Action Model (LAM), which compresses visual changes between successive frames into latent tokens, referred to as latent actions. These latent actions are expected to capture motion semantics. These tokens function as pseudo-action labels for imitation learning and enriching robot policy training with abundant, action-free data. Yet two questions remain: how to learn latent actions and how to incorporate latent actions into VLA pre- training ? In this paper, we introduce villa-X , a novel Visual-Language-Latent-Action (ViLLA) frame- work that advances both key aspects of latent action modeling. For the latent action learning component, a key limitation is the absence of direct alignment between visual transitions and underlying robot states and actions. Although robot data with low-level states and a"}
{"id": "2507.23682v1_chunk3", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "atent action learning component, a key limitation is the absence of direct alignment between visual transitions and underlying robot states and actions. Although robot data with low-level states and actions are often available alongside action-free videos during pretraining, current methods choose to ignore these signals and focus solely on the visual part. As a result, latent actions can remain ungrounded in the robot’s physical dynamics, resulting in a weak correspondence between latent action and robot behavior. To address this issue, we incorporate a proprio Forward Dynamics Model (FDM) module into the LAM as an auxiliary decoder. This module predicts future robot proprioceptive states and actions, encouraging the learned latent actions to better reflect the agent’s behavior—not just the observed visual changes. Moreover, this design improves the interpretability of the learned latent actions and enables them to be more easily translated into executable robot actions. As a result, "}
{"id": "2507.23682v1_chunk4", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "he observed visual changes. Moreover, this design improves the interpretability of the learned latent actions and enables them to be more easily translated into executable robot actions. As a result, latent actions become a robust intermediary between vision-based representations and low-level controls. To exploit this structure, we propose to model the latent action and robot action distribution jointly through a joint diffusion process, and conditioning robot action generation on latent actions generation through attention. Compared to existing methods on latent action pre-training, our approach allows for more effective and structured information transfer from latent actions to robot actions. We conduct comprehensive evaluations of villa-X on two simulated environments, SIMPLER and LIBERO, as well as two real-world setups, including various robot platforms with gripper and dexterous *Equal contribution.†Interns at Microsoft Research.‡Project lead. 1arXiv:2507.23682v1 [cs.RO] 31 Jul "}
{"id": "2507.23682v1_chunk5", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "RO, as well as two real-world setups, including various robot platforms with gripper and dexterous *Equal contribution.†Interns at Microsoft Research.‡Project lead. 1arXiv:2507.23682v1 [cs.RO] 31 Jul 2025villa-X : A Vision-Language-Latent-Action Model Figure 1: (a) High-level architecture of our villa-X method. (b) Detailed structure of the Latent Action Model (LAM) component within villa-X . hand manipulation. Our results demonstrate that villa-X achieves superior performance, laying a robust foundation for future research in this domain. Our contributions are listed as follows: •We improve latent action learning by introducing an extra proprio FDM, which aligns latent tokens with underlying robot states and actions and grounds them in physical dynamics. •We propose to jointly learn a latent action expert and a robot action expert through joint diffusion in the policy model, conditioning robot action prediction on latent actions to fully exploit their potential. •Our method demonstrat"}
{"id": "2507.23682v1_chunk6", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "ent action expert and a robot action expert through joint diffusion in the policy model, conditioning robot action prediction on latent actions to fully exploit their potential. •Our method demonstrates superior performance on simulated environments as well as on real- world robotic tasks. The latent action expert can effectively plan into future with both visual and proprio state planning. 2 Related Work Visual-Language-Action Models Vision-Language-Action (VLA) models [ 5,11,13,30,31,35,52,68] leverage pre-trained vision-language models (VLMs) to generate robot actions using visual and language cues. They either directly repurpose VLMs for action prediction [ 11,30,35,67] or use action experts to map VLM outputs to robot actions [ 5,31,52]. While training on large-scale datasets like Open X- Embodiment [ 13] enhance the generalization ability of VLAs, cross-embodiment generalization remains challenging due to diverse robot setups and configurations. Utilizing unlabeled trajectory dat"}
{"id": "2507.23682v1_chunk7", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "n X- Embodiment [ 13] enhance the generalization ability of VLAs, cross-embodiment generalization remains challenging due to diverse robot setups and configurations. Utilizing unlabeled trajectory data with pseudo-labels such as latent actions [ 11,67], language sub-goals [ 49], or visual sub-goals [ 68] supports overcoming these challenges. Our method adopts the latent action approach and improves both the modeling of latent actions and their integration into VLA pretraining. Modeling Latent Actions for VLA Pretraining Recent exploration into latent actions began with LAPO [ 58] and Genie [ 7], which primarily focused on the video game domain. Dynamo [ 15] adopted a similar architecture, using inverse and forward dynamics models to shape state representations. LAOM [ 51] propose to use supervision to learn better latent actions in the presense of distractors on Mujoco environments. For robotic learning, methods have started to incorporate latent actions into VLA pretraining [ 1,8,10,1"}
{"id": "2507.23682v1_chunk8", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "pervision to learn better latent actions in the presense of distractors on Mujoco environments. For robotic learning, methods have started to incorporate latent actions into VLA pretraining [ 1,8,10,11, 38,52,66,67]. LAPA [ 67] proposes to learn from videos, and trains its latent actions and Vision-Language Model (VLM) using either human or robot video data. Concurrently, IGOR [ 10] learns latent actions from a mixture of human and robot videos, marking the first demonstration of successful latent action transfer between humans and robots in a unified action space for embodied AI. Moto-GPT [ 11] co-fine- tunes both latent and robot action labels. GR00T [ 52] treats latent actions as a distinct embodiment, while Go-1 [ 1] generates robot actions conditioned on discrete latent tokens. UniVLA [ 8] proposes a two-stage training pipeline to learn task-centric latent actions. More recent works like [ 38,66] explore the continuous latent actions. By contrast, our approach jointly models laten"}
{"id": "2507.23682v1_chunk9", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": " [ 8] proposes a two-stage training pipeline to learn task-centric latent actions. More recent works like [ 38,66] explore the continuous latent actions. By contrast, our approach jointly models latent and robot actions through a joint diffusion process, conditioning robot action generation on latent actions for more effective and 2villa-X : A Vision-Language-Latent-Action Model Grasp Things Pick Things Up Move Left Move Right Figure 2: Visualization of image pairs with similar latent actions. structured information transfer. Our method offers better integration compared to LAPA [ 67] and GR00T [ 52], incorporates immediate visual context unlike Moto-GPT [ 11], and avoids inconsistencies caused by teacher forcing seen in Go-1 [1], achieving robust test-time reasoning. 3 Method At a high level, our villa-X consists of two main components, as illustrated in Figure 1a: (i) a LAM(Latent Action Model) module that infers latent actions from a pair of observations; (ii)aACT(Actor) module that"}
{"id": "2507.23682v1_chunk10", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "vel, our villa-X consists of two main components, as illustrated in Figure 1a: (i) a LAM(Latent Action Model) module that infers latent actions from a pair of observations; (ii)aACT(Actor) module that jointly models latent actions and robot actions given an initial visual observation and a textual task instruction. We describe each component in detail in the following subsections. Learning in villa-X is carried out in three stages: (1) pretraining the LAMmodule, (2) pretraining the ACTmodule, and (3) finetuning ACTon target-embodiment data. 3.1 Latent Action Model Modeling latent actions enables the use of abundant action-free video demonstration data and also holds the promise of improving generalization between robot and human motions [ 11,67]. Building on these prior works, our LAM(depicted in Figure 1b) learns an inverse dynamics model (IDM) to extract latent actions ztfrom two adjacent video frames, otand ot+K. The interval Kis a configurable window size, chosen to balance percept"}
{"id": "2507.23682v1_chunk11", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "cted in Figure 1b) learns an inverse dynamics model (IDM) to extract latent actions ztfrom two adjacent video frames, otand ot+K. The interval Kis a configurable window size, chosen to balance perceptibility and generalizability: large enough to capture meaningful motion but small enough to maintain transferability. Previous works [ 11,67] train IDM by jointly optimizing a forward dynamics model (FDM) to predict future frames. Given the current frame otand a latent action zt, the model minimizes the discrepancy between the predicted frame ˆot+Kand the ground truth ot+K. Although robot data with low-level states and actions are often available alongside action-free video data, these approaches completely discards such information, focusing solely on visual changes. Since our ultimate objective is to use latent actions to facilitate real robots policy learning, bridging vision-language prompts and low-level controls would be beneficial. To this end, we propose augmenting LAMpretraining w"}
{"id": "2507.23682v1_chunk12", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "ive is to use latent actions to facilitate real robots policy learning, bridging vision-language prompts and low-level controls would be beneficial. To this end, we propose augmenting LAMpretraining with proprioceptive supervision. Specifically, since ztrepresents a higher-level action abstraction, we introduce an additional proprio FDM module that predicts future robot states and actions Ksteps ahead given the current robot state qt and the latent action zt. This alignment of high-level abstractions and physical robot dynamics enhances the suitability of latent action for robot policy learning. However, because robot trajectories are often embodiment specific, conditioning solely on qtand zt may embed platform-dependent features in the latent action, hindering cross-embodiment transfer. To alleviate this issue, we provide the proprio FDM with an additional embodiment context input (e.g., dataset ID, control frequency), isolating transferable action representations from embodiment part"}
{"id": "2507.23682v1_chunk13", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "o alleviate this issue, we provide the proprio FDM with an additional embodiment context input (e.g., dataset ID, control frequency), isolating transferable action representations from embodiment particulars. 3villa-X : A Vision-Language-Latent-Action Model Figure 3: Structure of the ACTmodule. Implementation In practice, instead of predicting a single latent action between two frames (as Figure 1b depicts), LAM receives a sequence of TLAM observation frames as input and outputs TLAM−1 latent actions between consecutive frames. The inverse dynamics model first encodes the frame sequence with a Spatial–Temporal Transformer [ 65], then applies value quantization to produce TLAM−1 latent actions z∈RNz×Dd. The FDM is implemented using a Vision Transformer (ViT), while the proprio FDM is a two-layer MLP with dual output heads. The embodiment context input is embedded using learnable embeddings and concatenated with the robot state before being fed into the proprio FDM. For further implement"}
{"id": "2507.23682v1_chunk14", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "layer MLP with dual output heads. The embodiment context input is embedded using learnable embeddings and concatenated with the robot state before being fed into the proprio FDM. For further implementation details and configuration specifics, please refer to the appendix. \u0003Training and Inference We train LAMon diverse data sources, including robot trajectories and human egocentric videos. For data lacking low-level state and action labels, we omit the proprio FDM branch and optimize only the FDM reconstruction loss. By leveraging this diverse data, our goal is to learn a generalizable intermediate representation for downstream policy learning. Once trained, only the IDM is required to extract latent actions from a pair of frames at inference, the FDM and proprioceptive FDM serve primarily as diagnostic visualization tools. We use a mixture of datasets containing both human video and robot action data including OpenX [ 13], Ego4D [ 22], etc. For more details, please refer to Appendix A."}
{"id": "2507.23682v1_chunk15", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "as diagnostic visualization tools. We use a mixture of datasets containing both human video and robot action data including OpenX [ 13], Ego4D [ 22], etc. For more details, please refer to Appendix A.1. 3.2 Actor Module In the same vein as common VLA models [ 5,30,31,52,67], our ACTmodule builds upon a pre-trained vision-language model (VLM) and is trained to model a generalizable robot control policy that outputs actions given a textual task instruction and an initial observation. With the pre-trained LAM, we are able to incorporate latent actions as an additional intermediate representation in our framework, rather than directly mapping vision and language inputs to low-level robot actions. The latent actions serve as a bridging language that connects task prompts to robot action planning. Figure 3 provides an illustration of our ACT module, which consists of three components: VLM, ACT-latent , and ACT-robot . Given a textual task prompt land a visual observation ot, the VLM first ex"}
{"id": "2507.23682v1_chunk16", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "g. Figure 3 provides an illustration of our ACT module, which consists of three components: VLM, ACT-latent , and ACT-robot . Given a textual task prompt land a visual observation ot, the VLM first extracts visual and language features. Then, the ACT-latent component, trained with a diffusion objective, models a sequence of nlatent actions zt,zt+K,. . .,zt+(n−1)K, conditioned on the features from the VLM. Subsequently, the ACT-robot component, also implemented as a diffusion model, predicts a sequence of mlow-level robot actions, conditioned on the same visual and language features as well as the latent actions. Conditioning across components is achieved using uni-directional attention over inter- mediate features, as indicated by the dashed line in Figure 3. Additionally, ACT-robot can incorporate wrist camera input when available. Unlike prior work [ 5], which feeds wrist camera data into the VLM from the start, our design is more computationally efficient. Another consideration is t"}
{"id": "2507.23682v1_chunk17", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "corporate wrist camera input when available. Unlike prior work [ 5], which feeds wrist camera data into the VLM from the start, our design is more computationally efficient. Another consideration is that we view the wrist camera input as more embodiment-specific and less generalizable, which is why we incorporate it into ACT-robot rather than the VLM. The first two components, VLM and ACT-latent , are designed to handle the more generalizable aspects of the policy. One might ask whether the length of the robot action sequence must match that of the latent action sequence, i.e., whether m=nK. In our design, this is not required. We allow different lengths for latent and robot action sequences, as long as they share the same starting time step t. For example, we may model 10 latent actions (corresponding to 20 robot actions if K=2) but choose to predict only 4 robot 4villa-X : A Vision-Language-Latent-Action Model actions (effectively relying on the first 2 latent actions). This flexibil"}
{"id": "2507.23682v1_chunk18", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "corresponding to 20 robot actions if K=2) but choose to predict only 4 robot 4villa-X : A Vision-Language-Latent-Action Model actions (effectively relying on the first 2 latent actions). This flexibility enables us to maximize the benefits of modeling latent actions. We highlight three key elements of our design for incorporating latent actions: •Latent actions as a mid-level bridge. Previous works, including LAPA [ 67] and GR00T [ 52], treat latent actions similarly to robot actions and do not explicitly model the hierarchical structure between them. For example, GR00T simply treats latent actions as an additional action type and trains them alongside robot actions within a shared diffusion transformer. In contrast, our model treats latent actions as a distinct mid-level representation that bridges high-level vision and language prompts with low-level robot actions. •Explicit transfer from latent to robot actions. Unlike LAPA [ 67] and GR00T [ 52], where the transfer from latent actio"}
{"id": "2507.23682v1_chunk19", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "at bridges high-level vision and language prompts with low-level robot actions. •Explicit transfer from latent to robot actions. Unlike LAPA [ 67] and GR00T [ 52], where the transfer from latent actions to robot actions occurs only implicitly through pre-trained weight initialization, our model enables an explicit connection. The robot action diffusion process is directly conditioned on the latent action diffusion process, allowing more effective and structured information transfer from latent actions to robot actions. •Modeling latent actions sequences. While prior works (LAPA [ 67] and GO-1 [ 1] ) incorporate latent actions, they only model one-step predictions rather than planning over longer horizons at the latent action level. In contrast, our approach models a sequence of future latent actions, enabling structured planning at both the latent and robot action levels. Implementation We use a pre-trained PaliGemma model [ 3] as our VLM. Both ACT-latent and ACT-robot adopt a transfor"}
{"id": "2507.23682v1_chunk20", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "nt actions, enabling structured planning at both the latent and robot action levels. Implementation We use a pre-trained PaliGemma model [ 3] as our VLM. Both ACT-latent and ACT-robot adopt a transformer architecture following [ 69], and are trained using flow matching [ 39,42]. Wrist camera images are tokenized following the approach used in HPT [ 63]. Similar to the proprio FDM in LAM, we incorporate embodiment context embeddings in ACT-robot to account for embodiment- specific information. Inspired by Moto [ 11] and RDT [ 43], we apply attention masking during training: with 50% probability, we completely disable attention from latent actions to robot actions; otherwise, we randomly mask out attention from 50% of the latent actions. Further implementation details are provided in the appendix. \u0003Training, Finetuning and Inference During training, we update all components of the model, including both the pre-trained and randomly initialized parts. The model is trained jointly using los"}
{"id": "2507.23682v1_chunk21", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "endix. \u0003Training, Finetuning and Inference During training, we update all components of the model, including both the pre-trained and randomly initialized parts. The model is trained jointly using losses on both latent actions and robot actions, leveraging a mixture of datasets containing both human video and robot action data including OpenX [ 13], Ego4D [ 22], etc. Finetuning follows the same procedure as training, with the exception that, for unseen embodiments, we initialize a new context embedding. For robot action inference, we perform denoising steps simultaneously in both ACT-Latent andACT-Robot . For more details, please refer to Appendix A.2. 4 Experiments In this section, we aim to answer the following questions through experiments: • Does our improved LAMlearn higher-quality latent actions? • Can ACT-latent successfully plan future motions? • Can the actor module effectively leverage the pre-trained latent actions? •How does villa-X compare to existing VLA baselines in both"}
{"id": "2507.23682v1_chunk22", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "tent actions? • Can ACT-latent successfully plan future motions? • Can the actor module effectively leverage the pre-trained latent actions? •How does villa-X compare to existing VLA baselines in both simulated benchmarks and real-world robot tasks? 4.1 Does our improved LAMlearn higher-quality latent actions? In this subsection, we evaluate whether our improved latent action modeling enhances the quality of the learned latent actions. The key component of our LAMis the incorporation of the proprio FDM module. To assess its impact, we compare our model (denoted w/pp ) to a variant without the proprio FDM module (denoted wo/pp ). Probing First, a core expectation for latent actions is that they should carry information useful for predicting low-level robot actions. To test this, we conduct a probing experiment. Specifically, after training the latent action models, we freeze them and train a simple 3-layer MLP to predict the corresponding robot actions for each latent action. Probing is"}
{"id": "2507.23682v1_chunk23", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "a probing experiment. Specifically, after training the latent action models, we freeze them and train a simple 3-layer MLP to predict the corresponding robot actions for each latent action. Probing is conducted on the LIBERO dataset [ 40], 5villa-X : A Vision-Language-Latent-Action Model Table 1: Evaluation results on SIMPLER for different variants of our villa-X (top group) and alternative approaches for incorporating latent actions (bottom group). ”Ours” refers to the w/pp described in the main text. MethodGoogle robot WidowX robot Pick Move Drawer Place Avg. Carrot Eggplant Spoon Cube Avg. Ours 81.7 55.4 38.4 4.2 44.9 24.2 71.7 48.3 19.2 40.8 wo/pp 77.0 52.7 42.6 2.8 43.8 22.5 57.5 43.3 5.9 32.3 wo/LAM 42.1 24.6 38.4 0.0 26.3 25.8 60.8 36.7 9.2 33.1 LAPA-style 64.7 28.8 38.0 5.6 34.2 0.8 0.0 2.5 0.8 1.0 Go-1-style 29.0 38.0 31.3 4.6 25.7 5.8 50.8 1.7 1.0 14.8 which is not used for training latent action models. We train the MLP on the training split of LIBERO and evaluate it using t"}
{"id": "2507.23682v1_chunk24", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "8 0.0 2.5 0.8 1.0 Go-1-style 29.0 38.0 31.3 4.6 25.7 5.8 50.8 1.7 1.0 14.8 which is not used for training latent action models. We train the MLP on the training split of LIBERO and evaluate it using the L1 loss on the validation split. Figure 4: Probing experiment results.We are interested in how closely the predicted actions match the ground truth. In LIBERO, the robot action space has 8 di- mensions: 3 for position, 4 for rotation, and 1 for the gripper. Rather than averaging the error across dimensions, we focus on the maximum L1 error across all action dimensions, as we want to avoid large deviations in any single aspect of the action. For each model variant ( w/pp and wo/pp ), we com- pute the number of validation samples whose maximum L1 error falls below a threshold. By sweeping this threshold, we count how many samples fall within each error bin. A better model should yield more samples with low errors. For each error bin, we compute the difference in the number of samples betw"}
{"id": "2507.23682v1_chunk25", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "s threshold, we count how many samples fall within each error bin. A better model should yield more samples with low errors. For each error bin, we compute the difference in the number of samples between the w/pp andwo/pp variants and present the results in Figure 4. The w/pp variant produces more samples with smaller errors, while the wo/pp variant has more samples in the high-error bins. This demonstrates the effectiveness of the proprio FDM module in capturing information from the robot actions. Policy Pre-training Next, we compare how the latent actions generated by different variants of LAM (w/pp andwo/pp ) influence policy pre-training. Unlike the main experiments, we pretrain models in this section on a mixture of 10% Fractal [ 6] data, 10% Bridge V2 [ 18] data, and 100% Something-Something V2 [20] data, to reduce computation cost while remaining a setting where limited robot data are available for training the VLA model. The resulting policies are evaluated in the SIMPLER envir"}
{"id": "2507.23682v1_chunk26", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "omething V2 [20] data, to reduce computation cost while remaining a setting where limited robot data are available for training the VLA model. The resulting policies are evaluated in the SIMPLER environment [ 32], a simulation benchmark explicitly designed to mitigate the gap between simulated and real-world robotic environments. It comprises two platforms, namely Google robot and WidowX robot, each with four manipulation tasks. We evaluate our method on all the 8 tasks on the visual matching setting. The results are summarized in Table 1. We observe that w/pp clearly outperforms wo/pp , demonstrating the effectiveness of incorporating the proprio FDM module. Additionally, we include a baseline that does not use latent actions (denoted wo/LAM ) and is trained solely to predict robot actions. The performance of wo/LAM is significantly worse, indicating that pre-training with latent actions is essential. LAM visualization Figure 2 visualizes image pairs sharing the same latent action, de"}
{"id": "2507.23682v1_chunk27", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "s. The performance of wo/LAM is significantly worse, indicating that pre-training with latent actions is essential. LAM visualization Figure 2 visualizes image pairs sharing the same latent action, demonstrating that these pairs correspond to similar underlying robot behaviors. To further demonstrate the transfer ability of our LAM, we extract latent actions from arbitrary video sequences, map them to robot actions using the proprio FDM, and execute the resulting robot action in SIMPLER simulator. The simulated motions closely reproduce the original demonstrations, indicating that latent actions are both aligned with and grounded in the robot’s actions. For more examples, please check Appendix C.1. LAM transfer consistency We provide two visualizations to further evaluate the transfer consistency of the learned latent actions. Specifically, we first use the trained LAM to extract a sequence of latent actions from a source video. These latent actions are then applied to different initia"}
{"id": "2507.23682v1_chunk28", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "istency of the learned latent actions. Specifically, we first use the trained LAM to extract a sequence of latent actions from a source video. These latent actions are then applied to different initial images, allowing a 6villa-X : A Vision-Language-Latent-Action Model Figure 5: Transfer of human demonstrations to new scenes via latent actions and a world model. (a) moving right. (b) moving downwards. Please visit our website for more demos. Figure 6: Transfer of video demonstrations into robot action executions via latent actions and a proprio FDM. Please visit our website for more demos. world model to generate subsequent frames conditioned on these actions. As shown in Figure 5, the transfer rollouts (highlighted in blue) successfully identify the movable parts and demonstrate behaviors consistent with the original video. We also evaluate the alignment between latent and robot actions. To do this, we use the proprio FDM to decode latent actions into executable robot actions. These a"}
{"id": "2507.23682v1_chunk29", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": " consistent with the original video. We also evaluate the alignment between latent and robot actions. To do this, we use the proprio FDM to decode latent actions into executable robot actions. These actions are then executed in the SIMPLER simulator, with the resulting rollouts visualized in Figure 6. In each example, the top row shows the original video demonstration from which latent actions were extracted, while the bottom row displays the corresponding simulated execution. The simulated motions closely match the original demonstrations, confirming that the latent actions learned by villa-X are well-aligned with and grounded in the robot actions. 4.2 Can ACT-latent successfully plan future motions? In this experiment, we demostrate the motion planning capabilities of ACT-latent by visualizing its planned actions with a world model. For a given initial image and language instruction, ACT-latent generates a sequence of latent actions, and the world model takes the inital frame and lat"}
{"id": "2507.23682v1_chunk30", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "lizing its planned actions with a world model. For a given initial image and language instruction, ACT-latent generates a sequence of latent actions, and the world model takes the inital frame and latent actions as inputs to render the latent actions into planned future videos. Figure 7 shows results for both in- 7villa-X : A Vision-Language-Latent-Action Model Pick the green fruit into the potMove the silver pot to the left burner Initial Frame Open top drawer Initial Frame Pick the green fruit into the potMove the silver pot to the left burner Initial Frame Open bottom drawer Pick the green fruit into the potMove the silver pot to the left burner Initial Frame Pick up the spoon Initial Frame Touch the leafTouch the corn Initial Frame Pick up the cupIn-distribution: pick up objects In-distribution: open drawers Out -of-distribution: touch the emoji on real robot Out -of-distribution: pick up objects on real robo t Figure 7: Transfer of video demonstrations into robot action executions"}
{"id": "2507.23682v1_chunk31", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "tribution: open drawers Out -of-distribution: touch the emoji on real robot Out -of-distribution: pick up objects on real robo t Figure 7: Transfer of video demonstrations into robot action executions via latent actions and a proprio FDM. Please visit our website for more demos. distribution and out-of-distribution samples, where the in-distribution samples are randomly sampled from the validation set but from the same datasets as used in training, and the out-of-distribution samples are from new real-world scenarios. The results demonstrate that ACT-latent successfully follows the language instructions to solve the tasks, while accurately identifying target objects and generating latent actions that successfully follow the instructions. ACT-latent also successfully identifies the concepts in the emoji, which rarely appears in robot datasets, suggesting that villa-X keeps the general vision-language capabilities in the initial VLM model after pre-training. 4.3 Can the actor module effe"}
{"id": "2507.23682v1_chunk32", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": " in the emoji, which rarely appears in robot datasets, suggesting that villa-X keeps the general vision-language capabilities in the initial VLM model after pre-training. 4.3 Can the actor module effectively leverage the pre-trained latent actions? Given high-quality latent actions produced by the pre-trained LAM, we investigate whether our design can effectively leverage them to pre-train robot control policies. We compare our approach against two recent methods that also utilize latent actions, albeit in different ways: LAPA [67] and GO-1 [1]. To isolate the effect of how latent actions are incorporated, we implement LAPA-style and GO-1-style models based on our architecture for a fair comparison. For the LAPA-style model, we follow a two-stage pre-training protocol: we first train the VLM to predict latent actions, then replace the latent action prediction head with a robot action prediction head and continue training on data with robot action labels. For the GO-1-style model, we im"}
{"id": "2507.23682v1_chunk33", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "LM to predict latent actions, then replace the latent action prediction head with a robot action prediction head and continue training on data with robot action labels. For the GO-1-style model, we implement a separate latent planner that autoregressively predicts latent actions. The robot action prediction component remains largely unchanged as in our main design. Following the experiment setup in the previous subsection, we train all models on the same dataset mixture and then evaluate the resulting policies in the SIMPLER environment [ 32]. The results are shown in Table 1. Compared to other two approaches, our method achieves significantly higher performance, validating the effectiveness of our design for incorporating latent actions into VLA pre-training. 4.4 Evaluating villa-X in Simulation 4.4.1 SIMPLER Benchmark Baselines and Experimental Setup We use the SIMPLER benchmark as described above. In this section, we compare against two categories of prior work: •Vision-Language-Act"}
{"id": "2507.23682v1_chunk34", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "ulation 4.4.1 SIMPLER Benchmark Baselines and Experimental Setup We use the SIMPLER benchmark as described above. In this section, we compare against two categories of prior work: •Vision-Language-Action (VLA) models: RT-1-X [ 13], Octo-base [ 54], OpenVLA [ 30], RoboVLMs [ 34], which learn policies solely from mixed robot datasets. •Latent-Action based methods: GR00T [ 53], MoTo [ 11] and LAPA [ 67], which additionally exploit unlabelled videos by inferring latent actions. 8villa-X : A Vision-Language-Latent-Action Model Table 2: Comparison on SIMPLER of villa-X and existing methods. Methods marked with ∗are evalu- ated directly after pretraining, whereas other methods are evaluated after post-training on corresponding dataset. MethodGoogle Robot WidowX Robot Pick Move Drawer Place Avg. Carrot Eggplant Spoon Cube Avg.. RT-1-X ∗ 56.7 31.7 59.7 21.3 42.4 4.2 0.0 0.0 0.0 1.1 Octo-base ∗ 17.0 4.2 22.7 0.0 11.0 8.3 43.1 12.5 0.0 16.0 OpenVLA ∗ 16.3 46.2 35.6 0.0 24.5 0.0 4.1 0.0 0.0 1.0 Ro"}
{"id": "2507.23682v1_chunk35", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "arrot Eggplant Spoon Cube Avg.. RT-1-X ∗ 56.7 31.7 59.7 21.3 42.4 4.2 0.0 0.0 0.0 1.1 Octo-base ∗ 17.0 4.2 22.7 0.0 11.0 8.3 43.1 12.5 0.0 16.0 OpenVLA ∗ 16.3 46.2 35.6 0.0 24.5 0.0 4.1 0.0 0.0 1.0 RoboVLMs ∗ 72.7 66.3 26.8 36.1 50.5 25.0 0.0 20.8 8.3 13.5 RoboVLMs 77.3 61.7 43.5 24.1 51.7 20.8 79.2 45.8 4.2 37.5 GR00T 0.7 1.9 2.9 0.0 1.4 0.0 13.9 1.4 0.0 3.8 MoTo 74.0 60.4 43.1 N/A N/A N/A N/A N/A N/A N/A LAPA N/A N/A N/A N/A N/A 45.8 58.3 70.8 54.2 57.3 Ours w/o latent 56.3 25.8 27.3 13.9 30.8 31.3 74.6 61.7 28.3 49.0 Ours 98.7 75.0 59.3 5.6 59.6 46.3 64.6 77.9 61.3 62.5 Except where noted ( ∗), all models follow a two-stage pretraining–finetuning protocol, including a general pretraining phase on large-scale mixed data, followed by finetuning on a dataset of specific embodiment. We also include an ablation ( villa-X w/o latent) that removes our latent-action expert while keeping all other components unchanged. Reported baseline scores are taken directly from the original publication"}
{"id": "2507.23682v1_chunk36", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": " include an ablation ( villa-X w/o latent) that removes our latent-action expert while keeping all other components unchanged. Reported baseline scores are taken directly from the original publications, while missing entries are marked as N/A. Experimental Results Table 2 summarizes the success rates on both platforms. Our full model achieves the highest score on average success rate on the Google robot (59.6%) and the WidowX robot (62.5%). This improvement over VLA methods, which cannot exploit unlabelled video, demonstrates the benefit of our incorporating human videos into policy learning. Moreover, our approach outperforms other latent-action methods, indicating that our specific mechanism for leveraging video data is more effective. Finally, the gap between our full model and the “ villa-X w/o latent” ablation confirms that the latent- action expert is essential for achieving these gains. 4.4.2 LIBERO Benchmark The LIBERO benchmark [ 40] evaluates knowledge transfer in multitask a"}
{"id": "2507.23682v1_chunk37", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "lla-X w/o latent” ablation confirms that the latent- action expert is essential for achieving these gains. 4.4.2 LIBERO Benchmark The LIBERO benchmark [ 40] evaluates knowledge transfer in multitask and lifelong robot learning problems for robotic manipulation, consisting of four task suites: LIBERO-Spatial evaluates the model’s performance under novel layouts with the same task and object types, LIBERO-Goal evaluates the model’s performance under novel tasks with the same object types and layouts, LIBERO-Object evalu- ates the model’s performance under novel object types with the same tasks and layouts, LIBERO-Long evaluates the model’s performance under diverse set of objects, layouts and backgrounds. Each task suite contains 10 tasks with 50 human demonstrations per task for fine-tuning. Baselines and Experimental Setup We compare with the following existing models: Diffusion Pol- icy [ 12] trained from scratch, Octo [ 54], and OpenVLA [ 30]. All models follow a two-stage pretrainin"}
{"id": "2507.23682v1_chunk38", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": ". Baselines and Experimental Setup We compare with the following existing models: Diffusion Pol- icy [ 12] trained from scratch, Octo [ 54], and OpenVLA [ 30]. All models follow a two-stage pretraining- finetuning protocol. We finetune villa-X andvilla-X w/o latent on the demonstration data of the each task suite separately, and test on the LIBERO simulator for 10 tasks and 20 trials per task on each task suite. Experimental Results Table 3 summarizes the success rates on each task suite of LIBERO. Our model achieves better performance than existing methods in all the four task suites. Also, our model with latent action achieves higher performance on 3 of the 4 task suites and average performance, confirming that the proposed latent action expert improves the manipulation performance. 9villa-X : A Vision-Language-Latent-Action Model Table 3: Evaluation on 4 LIBERO task suites of villa-X and existing methods. Method Spatial Object Goal Long Average Diffusion Policy [12] 78.3 92.5 68.3 5"}
{"id": "2507.23682v1_chunk39", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "la-X : A Vision-Language-Latent-Action Model Table 3: Evaluation on 4 LIBERO task suites of villa-X and existing methods. Method Spatial Object Goal Long Average Diffusion Policy [12] 78.3 92.5 68.3 50.5 72.4 Octo-base [54] 78.9 85.7 84.6 51.1 75.1 OpenVLA [30] 84.7 88.4 79.2 53.7 76.5 Ours w/o latent 86.0 86.5 85.0 70.0 81.9 Ours 97.5 97.0 91.5 74.5 90.1 Figure 8: Real-world robot evaluation platforms: (top) Realman robot arm platform with a gripper and (bottom) Xarm robot arm with Xhand dexterous hand. Platform setups are shown on the left, with corresponding evaluation tasks on the right. Please visit our website for more videos. 4.5 Evaluating villa-X on Real-world Robots To further assess generalization, we evaluate our approach on two real -world robotic platforms: a Realman robot arm with a gripper, and an Xarm manipulator equipped with a 12 -DOF Xhand dexterous hand. See Figure 8 for details. Realman robot arm with gripper On the robot-arm manipulation platform, we use a 6-DoF "}
{"id": "2507.23682v1_chunk40", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "m with a gripper, and an Xarm manipulator equipped with a 12 -DOF Xhand dexterous hand. See Figure 8 for details. Realman robot arm with gripper On the robot-arm manipulation platform, we use a 6-DoF Realman RM 75 robot arm and a 1-DoF Inspire gripper. We fine-tune and evaluate our policy on “Pick-in” (pick the block into a bowl), “Pick-out” (pick the block out of a bowl), “Stack” (stack the block onto another block), “Unstack” (unstack the block from another block), and “Push” (push the block to a given location) tasks. For fine-tuning, we collect a dataset of 375 trajectories through teleoperation (with 75 trajectories for each task), where the object layout and table setup are fixed and only the object locations are dynamic. We conduct two sets of evaluation: In task evaluation, we remain the table setup the same as data collection; in generalization evaluation, we change the color of the block and table cover. We evaluate the policy with 10 trials per task with each trial conducted"}
{"id": "2507.23682v1_chunk41", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "in the table setup the same as data collection; in generalization evaluation, we change the color of the block and table cover. We evaluate the policy with 10 trials per task with each trial conducted with different object location, where all the experiment settings are the same for the evaluation of different policies (including the object location and lightening condition). The results are shown in Table 5, demonstrating that villa-X outperforms existing baselines in both task and generalization evaluation sets. Xarm robot arm with Xhand dexterous hand On the dexterous -hand platform, we use the Xhand, a 12-DoF dexterous hand with five flexible fingers, mounted on a 7-DoF Xarm robot arm. Fine -tuning is performed on the Xhand Dataset [ 26], which comprises 4,000 trajectories spanning 13 task categories. Since no dexterous-hand data were used during pretraining, this evaluation can test embodiment transfer ability. We select five representative tasks—pick -and-place, cube stacking, cu"}
{"id": "2507.23682v1_chunk42", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "task categories. Since no dexterous-hand data were used during pretraining, this evaluation can test embodiment transfer ability. We select five representative tasks—pick -and-place, cube stacking, cup upright placement, water pouring and ball flicking. The results are summarized in Table 4 for (i) seen tasks, where objects are randomly replaced or additional distractors are added, and (ii) unseen tasks, which use unseen objects or backgrounds. The performances are evaluated under 50 runs for pick and place, 20 runs for stack cube and 10 runs for others. Table 4 demonstrates that our method outperforms existing baselines. 10villa-X : A Vision-Language-Latent-Action Model Table 4: Evaluation on Xarm robot arm of villa-X and existing methods. MethodPick & Place Stack Cube Place Cup Upright Pour Water Flick Ball seen unseen seen unseen seen unseen seen unseen seen unseen GR-1 56 40 15 5 0 0 0 0 40 10 GR00T 44 28 20 0 20 0 0 0 30 0 Ours w/o latent 72 60 70 40 40 30 40 10 50 30 Ours 84 68 7"}
{"id": "2507.23682v1_chunk43", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "our Water Flick Ball seen unseen seen unseen seen unseen seen unseen seen unseen GR-1 56 40 15 5 0 0 0 0 40 10 GR00T 44 28 20 0 20 0 0 0 30 0 Ours w/o latent 72 60 70 40 40 30 40 10 50 30 Ours 84 68 75 50 60 30 60 30 50 40 Table 5: Evaluation on Realman robot arm of villa-X and existing methods. Method Pick in Pick out Push Stack Unstack Change block color Change table cover GR00T 30 70 10 10 60 50 30 Ours w/o latent 40 80 30 60 70 40 30 Ours 30 100 50 50 100 60 60 5 Conclusion, Limitations, and Future Works In this paper, we presented villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that improves both the learning of latent actions and their incorporation into VLA pre-training. Our experi- ments demonstrate that our enhanced Latent Action Model learns higher-quality latent actions, and our improved policy model more effectively leverages these learned actions. Overall, our method exhibits superior performance in both simulated environments and real-world robotic tasks."}
{"id": "2507.23682v1_chunk44", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": " actions, and our improved policy model more effectively leverages these learned actions. Overall, our method exhibits superior performance in both simulated environments and real-world robotic tasks. One limitation is that the proposed latent expert, although effective at future planning through both visual and proprioceptive state planning, is not fully explored in this work. For example, future research could learn a critic with prior knowledge from foundation vision-language models, allowing multiple samples from the latent expert and rejecting planned trajectories that do not follow the language instruction. We leave this aspect as future work to further improve the capability of the ViLLA framework. References [1]AgiBot-World-Contributors, Bu, Q., Cai, J., Chen, L., Cui, X., Ding, Y., Feng, S., Gao, S., He, X., Huang, X., Jiang, S., Jiang, Y., Jing, C., Li, H., Li, J., Liu, C., Liu, Y., Lu, Y., Luo, J., Luo, P ., Mu, Y., Niu, Y., Pan, Y., Pang, J., Qiao, Y., Ren, G., Ruan, C., Sh"}
{"id": "2507.23682v1_chunk45", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": ", Feng, S., Gao, S., He, X., Huang, X., Jiang, S., Jiang, Y., Jing, C., Li, H., Li, J., Liu, C., Liu, Y., Lu, Y., Luo, J., Luo, P ., Mu, Y., Niu, Y., Pan, Y., Pang, J., Qiao, Y., Ren, G., Ruan, C., Shan, J., Shen, Y., Shi, C., Shi, M., Shi, M., Sima, C., Song, J., Wang, H., Wang, W., Wei, D., Xie, C., Xu, G., Yan, J., Yang, C., Yang, L., Yang, S., Yao, M., Zeng, J., Zhang, C., Zhang, Q., Zhao, B., Zhao, C., Zhao, J., and Zhu, J. Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv: 2503.06669 , 2025. [2]Belkhale, S., Cui, Y., and Sadigh, D. Hydra: Hybrid robot actions for imitation learning. arxiv , 2023. [3]Beyer, L., Steiner, A., Pinto, A. S., Kolesnikov, A., Wang, X., Salz, D., Neumann, M., Alabdulmohsin, I., Tschannen, M., Bugliarello, E., Unterthiner, T., Keysers, D., Koppula, S., Liu, F., Grycner, A., Gritsenko, A., Houlsby, N., Kumar, M., Rong, K., Eisenschlos, J., Kabra, R., Bauer, M., Bo ˇsnjak, M., Chen"}
{"id": "2507.23682v1_chunk46", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "annen, M., Bugliarello, E., Unterthiner, T., Keysers, D., Koppula, S., Liu, F., Grycner, A., Gritsenko, A., Houlsby, N., Kumar, M., Rong, K., Eisenschlos, J., Kabra, R., Bauer, M., Bo ˇsnjak, M., Chen, X., Minderer, M., Voigtlaender, P ., Bica, I., Balazevic, I., Puigcerver, J., Papalampidi, P ., Henaff, O., Xiong, X., Soricut, R., Harmsen, J., and Zhai, X. Paligemma: A versatile 3b vlm for transfer. arXiv preprint arXiv: 2407.07726 , 2024. [4]Beyer, L., Steiner, A., Pinto, A. S., Kolesnikov, A., Wang, X., Salz, D., Neumann, M., Alabdulmohsin, I., Tschannen, M., Bugliarello, E., Unterthiner, T., Keysers, D., Koppula, S., Liu, F., Grycner, A., Gritsenko, A., Houlsby, N., Kumar, M., Rong, K., Eisenschlos, J., Kabra, R., Bauer, M., Bo ˇsnjak, M., Chen, X., Minderer, M., Voigtlaender, P ., Bica, I., Balazevic, I., Puigcerver, J., Papalampidi, P ., Henaff, O., Xiong, X., Soricut, R., Harmsen, J., and Zhai, X. Paligemma: A versatile 3b vlm for transfer, 2024. URL https://arxiv.org/abs/2407.0"}
{"id": "2507.23682v1_chunk47", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "a, I., Balazevic, I., Puigcerver, J., Papalampidi, P ., Henaff, O., Xiong, X., Soricut, R., Harmsen, J., and Zhai, X. Paligemma: A versatile 3b vlm for transfer, 2024. URL https://arxiv.org/abs/2407.07726 . [5]Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Groom, L., Hausman, K., Ichter, B., Jakubczak, S., Jones, T., Ke, L., Levine, S., Li-Bell, A., Mothukuri, M., Nair, S., Pertsch, K., 11villa-X : A Vision-Language-Latent-Action Model Shi, L. X., Tanner, J., Vuong, Q., Walling, A., Wang, H., and Zhilinsky, U. π0: A vision-language- action flow model for general robot control. arXiv preprint arXiv: 2410.24164 , 2024. [6]Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jackson, T., Jesmonth, S., Joshi, N. J., Julian, R. C., Kalashnikov, D., Kuang, Y., Leal, I., Lee, K.-H., Levine, S., Lu, Y., Malla, U., Manjunath, D., Mordatch, I., Nachum, O., Para"}
{"id": "2507.23682v1_chunk48", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "r, B., Irpan, A., Jackson, T., Jesmonth, S., Joshi, N. J., Julian, R. C., Kalashnikov, D., Kuang, Y., Leal, I., Lee, K.-H., Levine, S., Lu, Y., Malla, U., Manjunath, D., Mordatch, I., Nachum, O., Parada, C., Peralta, J., Perez, E., Pertsch, K., Quiambao, J., Rao, K., Ryoo, M., Salazar, G., Sanketi, P . R., Sayed, K., Singh, J., Sontakke, S., Stone, A., Tan, C., Tran, H., Vanhoucke, V ., Vega, S., Vuong, Q., Xia, F., Xiao, T., Xu, P ., Xu, S., Yu, T., and Zitkovich, B. Rt-1: Robotics transformer for real-world control at scale. Robotics: Science and Systems , 2022. doi: 10.48550/arXiv.2212.06817. [7]Bruce, J., Dennis, M. D., Edwards, A., Parker-Holder, J., Shi, Y., Hughes, E., Lai, M., Mavalankar, A., Steigerwald, R., Apps, C., et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning , 2024. [8]Bu, Q., Yang, Y., Cai, J., Gao, S., Ren, G., Yao, M., Luo, P ., and Li, H. Univla: Learning to act anywhere with task-centric latent actions,"}
{"id": "2507.23682v1_chunk49", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "st International Conference on Machine Learning , 2024. [8]Bu, Q., Yang, Y., Cai, J., Gao, S., Ren, G., Yao, M., Luo, P ., and Li, H. Univla: Learning to act anywhere with task-centric latent actions, 2025. URL https://arxiv.org/abs/2505.06111 . [9]Chen, L. Y., Adebola, S., and Goldberg, K. Berkeley UR5 demonstration dataset. https://sites. google.com/view/berkeley-ur5/home . [10] Chen, X., Guo, J., He, T., Zhang, C., Zhang, P ., Yang, D. C., Zhao, L., and Bian, J. Igor: Image-goal representations are the atomic control units for foundation models in embodied ai. arXiv preprint arXiv:2411.00785 , 2024. [11] Chen, Y., Ge, Y., Li, Y., Ge, Y., Ding, M., Shan, Y., and Liu, X. Moto: Latent motion token as the bridging language for robot manipulation. arXiv preprint arXiv: 2412.04445 , 2024. [12] Chi, C., Xu, Z., Feng, S., Cousineau, E., Du, Y., Burchfiel, B., Tedrake, R., and Song, S. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Re"}
{"id": "2507.23682v1_chunk50", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "] Chi, C., Xu, Z., Feng, S., Cousineau, E., Du, Y., Burchfiel, B., Tedrake, R., and Song, S. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research , pp. 02783649241273668, 2023. [13] Collaboration, O. X.-E., O’Neill, A., Rehman, A., Maddukuri, A., Gupta, A., Padalkar, A., Lee, A., Pooley, A., Gupta, A., Mandlekar, A., Jain, A., Tung, A., Bewley, A., Herzog, A., Irpan, A., Khazatsky, A., Rai, A., Gupta, A., Wang, A., Kolobov, A., Singh, A., Garg, A., Kembhavi, A., Xie, A., Brohan, A., Raffin, A., Sharma, A., Yavary, A., Jain, A., Balakrishna, A., Wahid, A., Burgess-Limerick, B., Kim, B., Sch ¨olkopf, B., Wulfe, B., Ichter, B., Lu, C., Xu, C., Le, C., Finn, C., Wang, C., Xu, C., Chi, C., Huang, C., Chan, C., Agia, C., Pan, C., Fu, C., Devin, C., Xu, D., Morton, D., Driess, D., Chen, D., Pathak, D., Shah, D., B ¨uchler, D., Jayaraman, D., Kalashnikov, D., Sadigh, D., Johns, E., Foster, E., Liu, F., Ceola, F., Xia, F., Zhao, F., F"}
{"id": "2507.23682v1_chunk51", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "., Devin, C., Xu, D., Morton, D., Driess, D., Chen, D., Pathak, D., Shah, D., B ¨uchler, D., Jayaraman, D., Kalashnikov, D., Sadigh, D., Johns, E., Foster, E., Liu, F., Ceola, F., Xia, F., Zhao, F., Frujeri, F. V ., Stulp, F., Zhou, G., Sukhatme, G. S., Salhotra, G., Yan, G., Feng, G., Schiavi, G., Berseth, G., Kahn, G., Yang, G., Wang, G., Su, H., Fang, H.-S., Shi, H., Bao, H., Amor, H. B., Christensen, H. I., Furuta, H., Walke, H., Fang, H., Ha, H., Mordatch, I., Radosavovic, I., Leal, I., Liang, J., Abou-Chakra, J., Kim, J., Drake, J., Peters, J., Schneider, J., Hsu, J., Bohg, J., Bingham, J., Wu, J., Gao, J., Hu, J., Wu, J., Wu, J., Sun, J., Luo, J., Gu, J., Tan, J., Oh, J., Wu, J., Lu, J., Yang, J., Malik, J., Silv ´erio, J., Hejna, J., Booher, J., Tompson, J., Yang, J., Salvador, J., Lim, J. J., Han, J., Wang, K., Rao, K., Pertsch, K., Hausman, K., Go, K., Gopalakrishnan, K., Goldberg, K., Byrne, K., Oslund, K., Kawaharazuka, K., Black, K., Lin, K., Zhang, K., Ehsani, K., Lekkala"}
{"id": "2507.23682v1_chunk52", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": " Lim, J. J., Han, J., Wang, K., Rao, K., Pertsch, K., Hausman, K., Go, K., Gopalakrishnan, K., Goldberg, K., Byrne, K., Oslund, K., Kawaharazuka, K., Black, K., Lin, K., Zhang, K., Ehsani, K., Lekkala, K., Ellis, K., Rana, K., Srinivasan, K., Fang, K., Singh, K. P ., Zeng, K.-H., Hatch, K., Hsu, K., Itti, L., Chen, L. Y., Pinto, L., Fei-Fei, L., Tan, L., Fan, L. J., Ott, L., Lee, L., Weihs, L., Chen, M., Lepert, M., Memmel, M., Tomizuka, M., Itkina, M., Castro, M. G., Spero, M., Du, M., Ahn, M., Yip, M. C., Zhang, M., Ding, M., Heo, M., Srirama, M. K., Sharma, M., Kim, M. J., Kanazawa, N., Hansen, N., Heess, N., Joshi, N. J., Suenderhauf, N., Liu, N., Palo, N. D., Shafiullah, N. M. M., Mees, O., Kroemer, O., Bastani, O., Sanketi, P . R., Miller, P . T., Yin, P ., Wohlhart, P ., Xu, P ., Fagan, P . D., Mitrano, P ., Sermanet, P ., Abbeel, P ., Sundaresan, P ., Chen, Q., Vuong, Q., Rafailov, R., Tian, R., Doshi, R., Mart’in-Mart’in, R., Baijal, R., Scalise, R., Hendrix, R., Lin, R., Qian"}
{"id": "2507.23682v1_chunk53", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "gan, P . D., Mitrano, P ., Sermanet, P ., Abbeel, P ., Sundaresan, P ., Chen, Q., Vuong, Q., Rafailov, R., Tian, R., Doshi, R., Mart’in-Mart’in, R., Baijal, R., Scalise, R., Hendrix, R., Lin, R., Qian, R., Zhang, R., Mendonca, R., Shah, R., Hoque, R., Julian, R., Bustamante, S., Kirmani, S., Levine, S., Lin, S., Moore, S., Bahl, S., Dass, S., Sonawani, S., Song, S., Xu, S., Haldar, S., Karamcheti, S., Adebola, S., Guist, S., Nasiriany, S., Schaal, S., Welker, S., Tian, S., Ramamoorthy, S., Dasari, S., Belkhale, S., Park, S., Nair, S., Mirchandani, S., Osa, T., Gupta, T., Harada, T., Matsushima, T., Xiao, T., Kollar, T., Yu, T., Ding, T., Davchev, T., Zhao, T. Z., Armstrong, T., Darrell, T., Chung, T., Jain, V ., Vanhoucke, V ., Zhan, W., Zhou, W., Burgard, W., Chen, X., Chen, X., Wang, X., Zhu, X., Geng, X., Liu, X., Liangwei, X., Li, X., Pang, Y., Lu, Y., Ma, Y. J., Kim, 12villa-X : A Vision-Language-Latent-Action Model Y., Chebotar, Y., Zhou, Y., Zhu, Y., Wu, Y., Xu, Y., Wang, Y., Bi"}
{"id": "2507.23682v1_chunk54", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": " Zhu, X., Geng, X., Liu, X., Liangwei, X., Li, X., Pang, Y., Lu, Y., Ma, Y. J., Kim, 12villa-X : A Vision-Language-Latent-Action Model Y., Chebotar, Y., Zhou, Y., Zhu, Y., Wu, Y., Xu, Y., Wang, Y., Bisk, Y., Dou, Y., Cho, Y., Lee, Y., Cui, Y., Cao, Y., Wu, Y.-H., Tang, Y., Zhu, Y., Zhang, Y., Jiang, Y., Li, Y., Li, Y., Iwasawa, Y., Matsuo, Y., Ma, Z., Xu, Z., Cui, Z. J., Zhang, Z., Fu, Z., and Lin, Z. Open X-Embodiment: Robotic learning datasets and RT-X models. https://arxiv.org/abs/2310.08864 , 2023. [14] Cui, Z. J., Wang, Y., Shafiullah, N. M. M., and Pinto, L. From play to policy: Conditional behavior generation from uncurated robot data. arXiv preprint arXiv:2210.10047 , 2022. [15] Cui, Z. J., Pan, H., Iyer, A., Haldar, S., and Pinto, L. Dynamo: In-domain dynamics pretraining for visuo-motor control. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tom- czak, J. M., and Zhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Ne"}
{"id": "2507.23682v1_chunk55", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "o-motor control. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tom- czak, J. M., and Zhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024 , 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 3b8db54b629e00537b59cbc6612026d7-Abstract-Conference.html . [16] Damen, D., Doughty, H., Farinella, G. M., Fidler, S., Furnari, A., Kazakos, E., Moltisanti, D., Munro, J., Perrett, T., Price, W., et al. The epic-kitchens dataset: Collection, challenges and baselines. IEEE Transactions on Pattern Analysis and Machine Intelligence , 43(11):4125–4141, 2020. [17] Dass, S., Yapeter, J., Zhang, J., Zhang, J., Pertsch, K., Nikolaidis, S., and Lim, J. J. CLVR jaco play dataset, 2023. URL https://github.com/clvrai/clvr_jaco_play_dataset . [18] Ebert, F., Yang, Y., Schmeckpeper, K., Bucher, B., Georgakis, G., Daniilidis, K., Finn, C., an"}
{"id": "2507.23682v1_chunk56", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": " and Lim, J. J. CLVR jaco play dataset, 2023. URL https://github.com/clvrai/clvr_jaco_play_dataset . [18] Ebert, F., Yang, Y., Schmeckpeper, K., Bucher, B., Georgakis, G., Daniilidis, K., Finn, C., and Levine, S. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. arXiv preprint arXiv:2109.13396 , 2021. [19] Fang, H.-S., Fang, H., Tang, Z., Liu, J., Wang, J., Zhu, H., and Lu, C. Rh20t: A robotic dataset for learning diverse skills in one-shot. In RSS 2023 Workshop on Learning for Task and Motion Planning , 2023. [20] Goyal, R., Ebrahimi Kahou, S., Michalski, V ., Materzynska, J., Westphal, S., Kim, H., Haenel, V ., Fruend, I., Yianilos, P ., Mueller-Freitag, M., Hoppe, F., Thurau, C., Bax, I., and Memisevic, R. The ”something something” video database for learning and evaluating visual common sense. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) , Oct 2017. [21] Goyal, R., Kahou, S. E., Michalski, V ., Materzy ´nska, J., Wes"}
{"id": "2507.23682v1_chunk57", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "rning and evaluating visual common sense. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) , Oct 2017. [21] Goyal, R., Kahou, S. E., Michalski, V ., Materzy ´nska, J., Westphal, S., Kim, H., Haenel, V ., Fruend, I., Yianilos, P ., Mueller-Freitag, M., Hoppe, F., Thurau, C., Bax, I., and Memisevic, R. The ”something something” video database for learning and evaluating visual common sense, 2017. URL https: //arxiv.org/abs/1706.04261 . [22] Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., Martin, M., Nagarajan, T., Radosavovic, I., Ramakrishnan, S. K., Ryan, F., Sharma, J., Wray, M., Xu, M., Xu, E. Z., Zhao, C., Bansal, S., Batra, D., Cartillier, V ., Crane, S., Do, T., Doulaty, M., Erapalli, A., Feichtenhofer, C., Fragomeni, A., Fu, Q., Fuegen, C., Gebreselasie, A., Gonzalez, C., Hillis, J., Huang, X., Huang, Y., Jia, W., Khoo, W., Kolar, J., Kottur, S., Kumar, A., Landini, F., Li, C., Li"}
{"id": "2507.23682v1_chunk58", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "A., Feichtenhofer, C., Fragomeni, A., Fu, Q., Fuegen, C., Gebreselasie, A., Gonzalez, C., Hillis, J., Huang, X., Huang, Y., Jia, W., Khoo, W., Kolar, J., Kottur, S., Kumar, A., Landini, F., Li, C., Li, Y., Li, Z., Mangalam, K., Modhugu, R., Munro, J., Murrell, T., Nishiyasu, T., Price, W., Puentes, P . R., Ramazanova, M., Sari, L., Somasundaram, K., Southerland, A., Sugano, Y., Tao, R., Vo, M., Wang, Y., Wu, X., Yagi, T., Zhu, Y., Arbelaez, P ., Crandall, D., Damen, D., Farinella, G. M., Ghanem, B., Ithapu, V . K., Jawahar, C. V ., Joo, H., Kitani, K., Li, H., Newcombe, R., Oliva, A., Park, H. S., Rehg, J. M., Sato, Y., Shi, J., Shou, M. Z., Torralba, A., Torresani, L., Yan, M., and Malik, J. Ego4d: Around the World in 3,000 Hours of Egocentric Video. In IEEE/CVF Computer Vision and Pattern Recognition (CVPR) , 2022. [23] Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et al. Ego4d: Around the world in 3,000 hours "}
{"id": "2507.23682v1_chunk59", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "n Recognition (CVPR) , 2022. [23] Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 18995–19012, 2022. [24] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition, 2015. URL https://arxiv.org/abs/1512.03385 . [25] Heo, M., Lee, Y., Lee, D., and Lim, J. J. Furniturebench: Reproducible real-world benchmark for long-horizon complex manipulation. In Robotics: Science and Systems , 2023. 13villa-X : A Vision-Language-Latent-Action Model [26] Hu, Y., Guo, Y., Wang, P ., Chen, X., Wang, Y.-J., Zhang, J., Sreenath, K., Lu, C., and Chen, J. Video prediction policy: A generalist robot policy with predictive visual representations. arXiv preprint arXiv:2412.14803 , 2024. [27] Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C."}
{"id": "2507.23682v1_chunk60", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "o prediction policy: A generalist robot policy with predictive visual representations. arXiv preprint arXiv:2412.14803 , 2024. [27] Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., and Finn, C. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning , pp. 991–1002. PMLR, 2022. [28] Kalashnikov, D., Irpan, A., Pastor, P ., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrish- nan, M., Vanhoucke, V ., et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. In CoRL , pp. 651–673, 2018. [29] Khazatsky, A., Pertsch, K., Nair, S., Balakrishna, A., Dasari, S., Karamcheti, S., Nasiriany, S., Srirama, M. K., Chen, L. Y., Ellis, K., Fagan, P . D., Hejna, J., Itkina, M., Lepert, M., Ma, Y. J., Miller, P . T., Wu, J., Belkhale, S., Dass, S., Ha, H., Jain, A., Lee, A., Lee, Y., Memmel, M., Park, S., Radosavovic, I., Wang, K., Zhan, A., Black, K., Chi, C., Hatch, K. B., Lin,"}
{"id": "2507.23682v1_chunk61", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "rt, M., Ma, Y. J., Miller, P . T., Wu, J., Belkhale, S., Dass, S., Ha, H., Jain, A., Lee, A., Lee, Y., Memmel, M., Park, S., Radosavovic, I., Wang, K., Zhan, A., Black, K., Chi, C., Hatch, K. B., Lin, S., Lu, J., Mercat, J., Rehman, A., Sanketi, P . R., Sharma, A., Simpson, C., Vuong, Q., Walke, H. R., Wulfe, B., Xiao, T., Yang, J. H., Yavary, A., Zhao, T. Z., Agia, C., Baijal, R., Castro, M. G., Chen, D., Chen, Q., Chung, T., Drake, J., Foster, E. P ., Gao, J., Herrera, D. A., Heo, M., Hsu, K., Hu, J., Jackson, D., Le, C., Li, Y., Lin, K., Lin, R., Ma, Z., Maddukuri, A., Mirchandani, S., Morton, D., Nguyen, T., O’Neill, A., Scalise, R., Seale, D., Son, V ., Tian, S., Tran, E., Wang, A. E., Wu, Y., Xie, A., Yang, J., Yin, P ., Zhang, Y., Bastani, O., Berseth, G., Bohg, J., Goldberg, K., Gupta, A., Gupta, A., Jayaraman, D., Lim, J. J., Malik, J., Mart ´ın-Mart ´ın, R., Ramamoorthy, S., Sadigh, D., Song, S., Wu, J., Yip, M. C., Zhu, Y., Kollar, T., Levine, S., and Finn, C. Droid: A large"}
{"id": "2507.23682v1_chunk62", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "pta, A., Gupta, A., Jayaraman, D., Lim, J. J., Malik, J., Mart ´ın-Mart ´ın, R., Ramamoorthy, S., Sadigh, D., Song, S., Wu, J., Yip, M. C., Zhu, Y., Kollar, T., Levine, S., and Finn, C. Droid: A large-scale in-the-wild robot manipulation dataset. 2024. [30] Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E., Lam, G., Sanketi, P ., et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246 , 2024. [31] Li, Q., Liang, Y., Wang, Z., Luo, L., Chen, X., Liao, M., Wei, F., Deng, Y., Xu, S., Zhang, Y., et al. Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650 , 2024. [32] Li, X., Hsu, K., Gu, J., Mees, O., Pertsch, K., Walke, H. R., Fu, C., Lunawat, I., Sieh, I., Kirmani, S., Levine, S., Wu, J., Finn, C., Su, H., Vuong, Q., and Xiao, T. Evaluating real-world robot manipulation policies in simulation. In Agrawal, P "}
{"id": "2507.23682v1_chunk63", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": " Walke, H. R., Fu, C., Lunawat, I., Sieh, I., Kirmani, S., Levine, S., Wu, J., Finn, C., Su, H., Vuong, Q., and Xiao, T. Evaluating real-world robot manipulation policies in simulation. In Agrawal, P ., Kroemer, O., and Burgard, W. (eds.), Conference on Robot Learning, 6-9 November 2024, Munich, Germany , volume 270 of Proceedings of Machine Learning Research , pp. 3705–3728. PMLR, 2024. URL https://proceedings.mlr.press/v270/li25c.html . [33] Li, X., Hsu, K., Gu, J., Pertsch, K., Mees, O., Walke, H. R., Fu, C., Lunawat, I., Sieh, I., Kirmani, S., Levine, S., Wu, J., Finn, C., Su, H., Vuong, Q., and Xiao, T. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941 , 2024. [34] Li, X., Li, P ., Liu, M., Wang, D., Liu, J., Kang, B., Ma, X., Kong, T., Zhang, H., and Liu, H. Towards generalist robot policies: What matters in building vision-language-action models. arXiv preprint arXiv:2412.14058 , 2024. [35] Li, X., Liu, M., Zhang, H., Yu, C., Xu, J."}
{"id": "2507.23682v1_chunk64", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "hang, H., and Liu, H. Towards generalist robot policies: What matters in building vision-language-action models. arXiv preprint arXiv:2412.14058 , 2024. [35] Li, X., Liu, M., Zhang, H., Yu, C., Xu, J., Wu, H., Cheang, C., Jing, Y., Zhang, W., Liu, H., Li, H., and Kong, T. Vision-language foundation models as effective robot imitators. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. URL https://openreview.net/forum?id=lFYj0oibGR . [36] Li, Y., Liu, M., and Rehg, J. M. In the eye of beholder: Joint learning of gaze and actions in first person video. In Proceedings of the European conference on computer vision (ECCV) , pp. 619–635, 2018. [37] Li, Y., Cao, Z., Liang, A., Liang, B., Chen, L., Zhao, H., and Feng, C. Egocentric prediction of action target in 3d. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2022. [38] Liang, A., Czempin, P ., Hong, M., Z"}
{"id": "2507.23682v1_chunk65", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "eng, C. Egocentric prediction of action target in 3d. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2022. [38] Liang, A., Czempin, P ., Hong, M., Zhou, Y., Biyik, E., and Tu, S. Clam: Continuous latent action models for robot learning from unlabeled demonstrations. arXiv preprint arXiv:2505.04999 , 2025. 14villa-X : A Vision-Language-Latent-Action Model [39] Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747 , 2022. [40] Liu, B., Zhu, Y., Gao, C., Feng, Y., Liu, Q., Zhu, Y., and Stone, P . Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310 , 2023. [41] Liu, H., Nasiriany, S., Zhang, L., Bao, Z., and Zhu, Y. Robot learning on the job: Human-in-the-loop autonomy and learning during deployment. In Robotics: Science and Systems (RSS) , 2023. [42] Liu, Q. Rectified flow: A marginal preserving approach to opti"}
{"id": "2507.23682v1_chunk66", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "Robot learning on the job: Human-in-the-loop autonomy and learning during deployment. In Robotics: Science and Systems (RSS) , 2023. [42] Liu, Q. Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577 , 2022. [43] Liu, S., Wu, L., Li, B., Tan, H., Chen, H., Wang, Z., Xu, K., Su, H., and Zhu, J. Rdt-1b: a diffusion foundation model for bimanual manipulation. arXiv preprint arXiv: 2410.07864 , 2024. [44] Liu, Y., Liu, Y., Jiang, C., Lyu, K., Wan, W., Shen, H., Liang, B., Fu, Z., Wang, H., and Yi, L. Hoi4d: A 4d egocentric dataset for category-level human-object interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 21013–21022, June 2022. [45] Luo, J., Xu, C., Liu, F., Tan, L., Lin, Z., Wu, J., Abbeel, P ., and Levine, S. Fmb: a functional manipulation benchmark for generalizable robotic learning. arXiv preprint arXiv:2401.08553 , 2024. [46] Lynch, C., Wahid, A., Tompson, J., Ding, T., B"}
{"id": "2507.23682v1_chunk67", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "J., Abbeel, P ., and Levine, S. Fmb: a functional manipulation benchmark for generalizable robotic learning. arXiv preprint arXiv:2401.08553 , 2024. [46] Lynch, C., Wahid, A., Tompson, J., Ding, T., Betker, J., Baruch, R., Armstrong, T., and Florence, P . Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters , 2023. [47] Mees, O., Borja-Diaz, J., and Burgard, W. Grounding language with visual affordances over unstructured data. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA) , London, UK, 2023. [48] Mendonca, R., Bahl, S., and Pathak, D. Structured world models from human videos. CoRL , 2023. [49] Mu, Y., Zhang, Q., Hu, M., Wang, W., Ding, M., Jin, J., Wang, B., Dai, J., Qiao, Y., and Luo, P . Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems , 36:25081–25094, 2023. [50] Nasiriany, S., Gao, T., Mandlekar, A., and Zhu, Y. Learning and retrie"}
{"id": "2507.23682v1_chunk68", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "nguage pre-training via embodied chain of thought. Advances in Neural Information Processing Systems , 36:25081–25094, 2023. [50] Nasiriany, S., Gao, T., Mandlekar, A., and Zhu, Y. Learning and retrieval from prior data for skill-based imitation learning. In Conference on Robot Learning (CoRL) , 2022. [51] Nikulin, A., Zisman, I., Tarasov, D., Lyubaykin, N., Polubarov, A., Kiselev, I., and Kurenkov, V . Latent action learning requires supervision in the presence of distractors, 2025. URL https: //arxiv.org/abs/2502.00379 . [52] NVIDIA, :, Bjorck, J., Casta ˜neda, F., Cherniadev, N., Da, X., Ding, R., Fan, L. J., Fang, Y., Fox, D., Hu, F., Huang, S., Jang, J., Jiang, Z., Kautz, J., Kundalia, K., Lao, L., Li, Z., Lin, Z., Lin, K., Liu, G., Llontop, E., Magne, L., Mandlekar, A., Narayan, A., Nasiriany, S., Reed, S., Tan, Y. L., Wang, G., Wang, Z., Wang, J., Wang, Q., Xiang, J., Xie, Y., Xu, Y., Xu, Z., Ye, S., Yu, Z., Zhang, A., Zhang, H., Zhao, Y., Zheng, R., and Zhu, Y. Gr00t n1: An ope"}
{"id": "2507.23682v1_chunk69", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "Nasiriany, S., Reed, S., Tan, Y. L., Wang, G., Wang, Z., Wang, J., Wang, Q., Xiang, J., Xie, Y., Xu, Y., Xu, Z., Ye, S., Yu, Z., Zhang, A., Zhang, H., Zhao, Y., Zheng, R., and Zhu, Y. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv: 2503.14734 , 2025. [53] NVIDIA, :, Bjorck, J., Casta ˜neda, F., Cherniadev, N., Da, X., Ding, R., Fan, L. J., Fang, Y., Fox, D., Hu, F., Huang, S., Jang, J., Jiang, Z., Kautz, J., Kundalia, K., Lao, L., Li, Z., Lin, Z., Lin, K., Liu, G., Llontop, E., Magne, L., Mandlekar, A., Narayan, A., Nasiriany, S., Reed, S., Tan, Y. L., Wang, G., Wang, Z., Wang, J., Wang, Q., Xiang, J., Xie, Y., Xu, Y., Xu, Z., Ye, S., Yu, Z., Zhang, A., Zhang, H., Zhao, Y., Zheng, R., and Zhu, Y. Gr00t n1: An open foundation model for generalist humanoid robots, 2025. URL https://arxiv.org/abs/2503.14734 . [54] Octo Model Team, Ghosh, D., Walke, H., Pertsch, K., Black, K., Mees, O., Dasari, S., Hejna, J., Xu, C., Luo, J., Kreiman, T., Tan, Y., C"}
{"id": "2507.23682v1_chunk70", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "anoid robots, 2025. URL https://arxiv.org/abs/2503.14734 . [54] Octo Model Team, Ghosh, D., Walke, H., Pertsch, K., Black, K., Mees, O., Dasari, S., Hejna, J., Xu, C., Luo, J., Kreiman, T., Tan, Y., Chen, L. Y., Sanketi, P ., Vuong, Q., Xiao, T., Sadigh, D., Finn, C., and Levine, S. Octo: An open-source generalist robot policy. In Proceedings of Robotics: Science and Systems , Delft, Netherlands, 2024. [55] Pei, B., Huang, Y., Xu, J., Chen, G., He, Y., Yang, L., Wang, Y., Xie, W., Qiao, Y., Wu, F., and Wang, L. Modeling fine-grained hand-object dynamics for egocentric video representation learning, 2025. URL https://arxiv.org/abs/2503.00986 . 15villa-X : A Vision-Language-Latent-Action Model [56] Quere, G., Hagengruber, A., Iskandar, M., Bustamante, S., Leidner, D., Stulp, F., and Vogel, J. Shared Control Templates for Assistive Robotics. In 2020 IEEE International Conference on Robotics and Automation (ICRA) , pp. 7, Paris, France, 2020. [57] Rosete-Beas, E., Mees, O., Kalweit, G., Bo"}
{"id": "2507.23682v1_chunk71", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": ". Shared Control Templates for Assistive Robotics. In 2020 IEEE International Conference on Robotics and Automation (ICRA) , pp. 7, Paris, France, 2020. [57] Rosete-Beas, E., Mees, O., Kalweit, G., Boedecker, J., and Burgard, W. Latent plans for task agnostic offline reinforcement learning. In Proceedings of the 6th Conference on Robot Learning (CoRL) , 2022. [58] Schmidt, D. and Jiang, M. Learning to act without actions. arXiv preprint arXiv:2312.10812 , 2023. [59] Shafiullah, N. M. M., Rai, A., Etukuru, H., Liu, Y., Misra, I., Chintala, S., and Pinto, L. On bringing robots home, 2023. [60] Walke, H., Black, K., Lee, A., Kim, M. J., Du, M., Zheng, C., Zhao, T., Hansen-Estruch, P ., Vuong, Q., He, A., Myers, V ., Fang, K., Finn, C., and Levine, S. Bridgedata v2: A dataset for robot learning at scale. In Conference on Robot Learning (CoRL) , 2023. [61] Wang, J., Zhang, Q., Chao, Y.-W., Wen, B., Guo, X., and Xiang, Y. Ho-cap: A capture system and dataset for 3d reconstruction and pose tr"}
{"id": "2507.23682v1_chunk72", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "g at scale. In Conference on Robot Learning (CoRL) , 2023. [61] Wang, J., Zhang, Q., Chao, Y.-W., Wen, B., Guo, X., and Xiang, Y. Ho-cap: A capture system and dataset for 3d reconstruction and pose tracking of hand-object interaction, 2024. URL https: //arxiv.org/abs/2406.06843 . [62] Wang, L., Chen, X., Zhao, J., and He, K. Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers. In Globerson, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J., and Zhang, C. (eds.), Advances in Neural Information Processing Systems , volume 37, pp. 124420– 124450. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/ paper/2024/file/e0f393e7980a24fd12fa6f15adfa25fb-Paper-Conference.pdf . [63] Wang, L., Chen, X., Zhao, J., and He, K. Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers. Neural Information Processing Systems , 2024. doi: 10.48550/arXiv.2409. 20537. [64] Wang, X., Kwon, T., Rad, M., Pan, B., Cha"}
{"id": "2507.23682v1_chunk73", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "roprioceptive-visual learning with heterogeneous pre-trained transformers. Neural Information Processing Systems , 2024. doi: 10.48550/arXiv.2409. 20537. [64] Wang, X., Kwon, T., Rad, M., Pan, B., Chakraborty, I., Andrist, S., Bohus, D., Feniello, A., Tekin, B., Frujeri, F. V ., Joshi, N., and Pollefeys, M. Holoassist: an egocentric human interaction dataset for interactive ai assistants in the real world. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 20270–20281, October 2023. [65] Xu, M., Dai, W., Liu, C., Gao, X., Lin, W., Qi, G.-J., and Xiong, H. Spatial-temporal transformer networks for traffic flow forecasting. arXiv preprint arXiv: 2001.02908 , 2020. [66] Yang, J., Shi, Y., Zhu, H., Liu, M., Ma, K., Wang, Y., Wu, G., He, T., and Wang, L. Como: Learning continuous latent motion from internet videos for scalable robot learning, 2025. URL https: //arxiv.org/abs/2505.17006 . [67] Ye, S., Jang, J., Jeon, B., Joo, S., Yang, J., Peng, B., Mandl"}
{"id": "2507.23682v1_chunk74", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "o: Learning continuous latent motion from internet videos for scalable robot learning, 2025. URL https: //arxiv.org/abs/2505.17006 . [67] Ye, S., Jang, J., Jeon, B., Joo, S., Yang, J., Peng, B., Mandlekar, A., Tan, R., Chao, Y.-W., Lin, B. Y., Liden, L., Lee, K., Gao, J., Zettlemoyer, L., Fox, D., and Seo, M. Latent action pretraining from videos. arXiv preprint arXiv: 2410.11758 , 2024. [68] Zhao, Q., Lu, Y., Kim, M. J., Fu, Z., Zhang, Z., Wu, Y., Li, Z., Ma, Q., Han, S., Finn, C., Handa, A., Liu, M.-Y., Xiang, D., Wetzstein, G., and Lin, T.-Y. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. arXiv preprint arXiv: 2503.22020 , 2025. [69] Zhou, C., Yu, L., Babu, A., Tirumala, K., Yasunaga, M., Shamis, L., Kahn, J., Ma, X., Zettlemoyer, L. S., and Levy, O. Transfusion: Predict the next token and diffuse images with one multi-modal model. International Conference on Learning Representations , 2025. 16villa-X : A Vision-Language-Latent-Action Model A Model Des"}
{"id": "2507.23682v1_chunk75", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "fusion: Predict the next token and diffuse images with one multi-modal model. International Conference on Learning Representations , 2025. 16villa-X : A Vision-Language-Latent-Action Model A Model Design A.1 Latent Action Model There are four modules in our latent action model. A ST-Transformer based inverse dynamic model (IDM), a vector quantization module, an image reconstruction forward dynamic model (FDM) and a robot proprio FDM. The IDM takes an 8 ×224×224 video clip as input, following a Patch Embedding with a patch size of 14 and 12 ST-blocks, where each block has a hidden dimension of 768 and 32 attention heads. The codebook size of vector quantization is set to 32. Image reconstruction FDM is a 12-layer ViT-base network, and robot proprio FDM is a 2-layer MLP with a hidden size of 768. Our model is trained with a learning rate of 1.5e-4, a batch size of 512, and a 2000-step linear warmup. We use the same loss weight for the image FDM and proprio FDM. The pretraining takes 4 da"}
{"id": "2507.23682v1_chunk76", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": " 768. Our model is trained with a learning rate of 1.5e-4, a batch size of 512, and a 2000-step linear warmup. We use the same loss weight for the image FDM and proprio FDM. The pretraining takes 4 days on 128 NVIDIA A100 GPUs. A.2 Actor Module Our VLA model comprises three components. First, the vision–language encoder is based on PaliGemma [4], a 3B-parameter VLM pretrained with 224 ×224 images and 128-token text inputs. Second and third, the latent-action expert and the robot-action expert are each implemented as 18-layer Transformer networks, mirroring PaliGemma’s design, with a hidden dimension of 1,024 and 8 attention heads. For the latent action sequence, we select a sequence length of N=6, and for the robot actions, we select a sequence length of M=4. We extend our policy head with a variant of HPT [ 62], assigning each embodiment its own pair of state- and action-projection layers while sharing all other parameters. Visual features from the wrist camera are extracted by a pret"}
{"id": "2507.23682v1_chunk77", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": " a variant of HPT [ 62], assigning each embodiment its own pair of state- and action-projection layers while sharing all other parameters. Visual features from the wrist camera are extracted by a pretrained ResNet-18 [ 24] and fused into the main model via a shared cross-attention head that maps the ResNet features into 16 tokens. During training, wrist-view inputs are randomly masked 50% of the time. We also observed that the latent-action representation can be overly exploited by the robot-action expert, so we regularize this with two complementary dropout schemes. First, we add a 50% attention-weight dropout on the latent-action stream. For the remaining tokens, we randomly mask 50% latent action tokens. This combined masking strategy encourages the model to learn robust, generalizable policy that will balance the predicted latent actions as well as the input image and instruction. Each expert contains approximately 300 M parameters and is trained from scratch. We train all componen"}
{"id": "2507.23682v1_chunk78", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "policy that will balance the predicted latent actions as well as the input image and instruction. Each expert contains approximately 300 M parameters and is trained from scratch. We train all components jointly using a learning rate of 5 e−5 with a 200-step linear warmup. We clip gradients to a maximum norm of 1.0 to ensure stable optimization. The pretraining takes 4 days on 64 NVIDIA A100 GPUs. B Dataset B.1 Data Mixture We curated a data mixture by combining both robot data and action-free human videos for our pretraining phase. For robot data, we draw primarily from OpenX [ 13] mixture and AgiBot [ 1]. For OpenX dataset, our base data mixture is created primarily based on [ 30,54]. In total, we use 223.5M trajectories with 1.6M frames of robot data. For human videos, we use a mixture of Ego4D [ 22], EgoPAT3D [ 37], EGTEA Gaze+ [ 36], EPIC-KITCHENS [ 16], HO-Cap [ 61], HOI4D [ 44], HoloAssist [ 64], RH20T [ 19], Something Something V2 [ 21]. Altogether, this yields 3.6M clips of hum"}
{"id": "2507.23682v1_chunk79", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "f Ego4D [ 22], EgoPAT3D [ 37], EGTEA Gaze+ [ 36], EPIC-KITCHENS [ 16], HO-Cap [ 61], HOI4D [ 44], HoloAssist [ 64], RH20T [ 19], Something Something V2 [ 21]. Altogether, this yields 3.6M clips of human videos. During LAM pretraining, we exclusively utilize the primary third-person camera view. For policy pretraining, we optionally incorporate the wrist-mounted view (when available), applying a 50% dropout. A full breakdown of our data mixture is listed in Table 6. B.2 Data Preprocessing For data cleaning, we adopt EgoHOD [ 55], a curated subset of Ego4D [ 22], and further filter the videos based on visual quality to ensure high-quality inputs for training. For both robot data and human videos, we apply random adjustments to brightness, contrast, saturation, and hue as data augmentation. In the case of robot data, we represent both proprioceptive states and actions using euler angles. 17villa-X : A Vision-Language-Latent-Action Model Dataset Mix Ratio (%) RT-1 Robot Action [6] 9.70 Agi"}
{"id": "2507.23682v1_chunk80", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "n the case of robot data, we represent both proprioceptive states and actions using euler angles. 17villa-X : A Vision-Language-Latent-Action Model Dataset Mix Ratio (%) RT-1 Robot Action [6] 9.70 AgiBot World Beta [1] 20.0 Kuka [28] 1.97 Bridge [18, 60] 5.47 Taco Play [47, 57] 0.76 Jaco Play [17] 0.12 Berkely Autolab UR5 [9] 0.31 Language Table [46] 0.11 Stanford Hydra Dataset [2] 1.61 NYU Franka Play Dataset [14] 0.22 Furniture Bench Dataset [25] 0.63 Austin Sailor Dataset [50] 0.57 Austin Sirius Dataset [41] 0.45 BC-Z [27] 3.47 DLR EDAN Shared Control [56] 0.01 CMU Stretch [48] 0.04 FMB Dataset [45] 0.73 DobbE [59] 0.37 DROID [29] 3.46 Ego4D [23, 55] 21.46 EgoPAT3D [37] 0.94 EGTEA Gaze+ [36] 0.89 EPIC-KITCHENS [16] 6.95 HO-Cap [61] 0.63 HOI4D [44] 1.99 HoloAssist [64] 4.77 RH20T [19] 5.56 Something-Something V2 [20] 6.82 Table 6: Our training data mixture used during the pretraining phase. C LAM visualization results C.1 Image Pairs with Similar Latent Actions Figure 9 visualizes ad"}
{"id": "2507.23682v1_chunk81", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "56 Something-Something V2 [20] 6.82 Table 6: Our training data mixture used during the pretraining phase. C LAM visualization results C.1 Image Pairs with Similar Latent Actions Figure 9 visualizes additional image pairs sharing the same latent action, demonstrating that these pairs correspond to similar underlying robot behaviors. The results demonstrate that similar latent actions represent the similar robot behaviors and low-level actions, in regardless of which embodiment (including human and different robots) is executing such action. This results support that villa-X learns cross-embodiment prior knowledge for manipulations with latent actions. D Simulation Evaluation Details D.1 SIMPLER Benchmark We evaluate on all eight SIMPLER [ 33] tasks in the visual matching setting, which include two robot platforms: Google Robot and WidowX. For Google Robot, the tasks are: (1) pick coke can (including horizontal, vertical and standing can configurations); (2) move an object near a target "}
{"id": "2507.23682v1_chunk82", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "ude two robot platforms: Google Robot and WidowX. For Google Robot, the tasks are: (1) pick coke can (including horizontal, vertical and standing can configurations); (2) move an object near a target object; (3) open / close top, middle or bottom drawer; and (4) place apple in a closed drawer, which includes two subtasks: first open top drawer, and then place the apple into the top drawer. On the widowX setup, the tasks consist of: (1) put a carrot on the plate; (2) put an eggplant on the basket; (3) put a spoon on the towel; (4) stack a green cube on a yellow one. 18villa-X : A Vision-Language-Latent-Action Model Grasp Things Pick Things Up Move Left Move Right Figure 9: Visualization of image pairs with similar latent actions. We follow the standard evaluation protocol to test by randomizing both configurations of the environ- ments. For the Google Robot tasks, we execute 300 trials of “Pick Coke Can”, 240 of “Move Near”, 216 of “Open/Close Drawer”, and 108 of “Place Apple in Closed "}
{"id": "2507.23682v1_chunk83", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "zing both configurations of the environ- ments. For the Google Robot tasks, we execute 300 trials of “Pick Coke Can”, 240 of “Move Near”, 216 of “Open/Close Drawer”, and 108 of “Place Apple in Closed Drawer”. For each WidowX task, we use 24 unique configurations. To ensure statistical significance, we test each configuration 10 times, yielding 240 rollouts per task. Reported results (Table 2) are the average success rates across these trials. Please refer to SIMPLER [33] for more details. For a fair comparison, we adopt the published performance metrics for RT-1-X [ 13], Octo-base [ 54], OpenVLA [ 30], RoboVLMs [ 34], MoTo [ 11], and LAPA [ 67] directly from their respective papers. In the case of GR00T [ 53], we use the official pretrained checkpoint and performe fine-tuning on the RT-1/Bridge dataset following the authors’ published guidelines accordingly. D.2 LIBERO The LIBERO benchmark [ 40] evaluates knowledge transfer in multitask and lifelong robot learning problems for robotic "}
{"id": "2507.23682v1_chunk84", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "ridge dataset following the authors’ published guidelines accordingly. D.2 LIBERO The LIBERO benchmark [ 40] evaluates knowledge transfer in multitask and lifelong robot learning problems for robotic manipulation, consisting of four task suites: LIBERO-Spatial ,LIBERO-Goal , LIBERO-Object ,LIBERO-Long . For fine-tuning of our models, we reinitialize the linear state encoder, action encoder, and action decoder, and tune the full parameters (except for the vision encoder). We fine-tune all the models for 50k gradient steps on the fine-tuning dataset on each task suite of LIBERO. We follow the standard evaluation protocol to test by randomizing the initial states of the environments. Reported results (Table 3) are the average success rates across on a total of 200 trials and a total of 20 trials per task on each task suite. Please refer to Section 4.2 and Appendix C of LIBERO [ 40] for more details. E Real-world Robot Platforms Evaluation Details E.1 Realman robot arm The Realman robot ar"}
{"id": "2507.23682v1_chunk85", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "als per task on each task suite. Please refer to Section 4.2 and Appendix C of LIBERO [ 40] for more details. E Real-world Robot Platforms Evaluation Details E.1 Realman robot arm The Realman robot arm setup is shown in Figure 8 (upper). We mount the gripper for Inspire Robot to the Realman RM75 robot arm. We use two camera views, including a primary view camera with the same view point as the images (used to demonstrate different tasks) shown in Figure 8 (upper) and a wrist camera. For fine-tuning of our models, we reinitialize the linear state encoder, action encoder, and action decoder, and tune the full parameters (except for the vision encoder). We fine-tune all the models for 60k gradient steps. We collect data on the following five tasks with their task instructions: • Put-in: “Pick the green block from the table into the blue bowl” • Put-out: “Pick the green block from the blue bowl onto the table” 19villa-X : A Vision-Language-Latent-Action Model •Push: “Push the green block t"}
{"id": "2507.23682v1_chunk86", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": " the green block from the table into the blue bowl” • Put-out: “Pick the green block from the blue bowl onto the table” 19villa-X : A Vision-Language-Latent-Action Model •Push: “Push the green block to position X” where “X” indicates the nine positions written on the table. • Stack: “Stack the wooden block onto the green block” • Unstack: “Unstack the wooden block from the green block” We collect 375 trajectories (75 trajectories for each task) for fine-tuning. The trajectories are collected at 10Hz. We post-process these trajectories to remove static frames with zero action, resulting in 120 steps on average in one trajectory. We evaluate the fine-tuned model on seven groups with 10 trials for each group. The first five groups contain the tasks the same as data collection. The last two groups are designed to evaluate the general- ization ability of the models. For the “change block color” group, we repeat the previous five tasks but change the green block into blue and red ones. For t"}
{"id": "2507.23682v1_chunk87", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "oups are designed to evaluate the general- ization ability of the models. For the “change block color” group, we repeat the previous five tasks but change the green block into blue and red ones. For the “change table cover” group, we change the table cover from red to brown and blue ones. The visualization example of each task for our model can be found in Figure 10. E.2 XHand dexterous hand The Xhand setup is shown in Figure 8 (lower). The 12-dof Xhand is mounted on a 7-dof XArm robot arm. There are two camera views, including a main 3-rd view camera, and a wrist camera. During fine-tuning, we reinitialize linear encoder and decoder modules for both state and action to accommodate the hand’s higher dimensionality. We use the dataset collected in [ 26] as our finetuning dataset, which comprises roughly 4,000 trajectories spanning 13 task categories and over 50 unique objects. For evaluation, we focus on five representative XHand tasks as depicted in Figure 8, namely pick-and-place, cub"}
{"id": "2507.23682v1_chunk88", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "rises roughly 4,000 trajectories spanning 13 task categories and over 50 unique objects. For evaluation, we focus on five representative XHand tasks as depicted in Figure 8, namely pick-and-place, cube stacking, upright cup placement, water pouring, and ball flicking. Each task is assessed under “seen” and “unseen” conditions: in the seen setting, the same objects and backgrounds encountered during training are used, albeit with randomized tabletop positions and optional distractors; in the unseen setting, either the target objects or the scene background (or both) were never encountered during finetuning, totaling more than 20 novel objects. During evaluation, we conducted 50 evaluation runs for the pick-and-place task, 20 runs for cube stacking, and 10 runs for each of the remaining tasks. The visualization example of each task can be found in Figure 11 and Figure 12. 20villa-X : A Vision-Language-Latent-Action Model Push the green block to position X Pick the green block from the ta"}
{"id": "2507.23682v1_chunk89", "paper_id": "2507.23682v1", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "text": "s. The visualization example of each task can be found in Figure 11 and Figure 12. 20villa-X : A Vision-Language-Latent-Action Model Push the green block to position X Pick the green block from the table into the blue bowl Pick the green block from the blue bowl onto the table Stack the wooden block onto the green block Unstack the wooden block from the green block Figure 10: Realman evaluation trajectory examples. 21villa-X : A Vision-Language-Latent-Action Model Figure 11: Xhand evaluation trajectory examples (part I). 22villa-X : A Vision-Language-Latent-Action Model Figure 12: Xhand evaluation trajectory examples (part II). 23"}
{"id": "2507.23677v1_chunk0", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes Xiaohan Li1*, Ziren Gong2*, Fabio Tosi2, Matteo Poggi2, Stefano Mattoccia2, Dong Liu1, Jun Wu3 1University of Science and Technology of China, 2University of Bologna, 3Fudan University, SplaTAM (Keetha et al. 2024) BGS-SLAM (LiDAR) BGS-SLAM (Ours) Figure 1: Comparison of Rendering and Depth Estimation. The top row shows RGB renderings generated by SplaTAM (Keetha et al. 2024), our BGS-SLAM method trained with LiDAR depth points, and our approach using only stereo RGB pairs with depth maps from deep stereo networks for supervision. The bottom row presents the corresponding depth renderings. Abstract 3D Gaussian Splatting (3DGS) has recently gained popular- ity in SLAM applications due to its fast rendering and high- fidelity representation. However, existing 3DGS-SLAM sys- tems have predominantly focused on indoor environments and relied on active depth sensors, leaving a gap for large- scale outdoor applications. We present BGS"}
{"id": "2507.23677v1_chunk1", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "ntation. However, existing 3DGS-SLAM sys- tems have predominantly focused on indoor environments and relied on active depth sensors, leaving a gap for large- scale outdoor applications. We present BGS-SLAM, the first binocular 3D Gaussian Splatting SLAM system designed for outdoor scenarios1. Our approach uses only RGB stereo pairs without requiring LiDAR or active sensors. BGS-SLAM leverages depth estimates from pre-trained deep stereo net- works to guide 3D Gaussian optimization with a multi-loss strategy enhancing both geometric consistency and visual quality. Experiments on multiple datasets demonstrate that BGS-SLAM achieves superior tracking accuracy and map- ping performance compared to other 3DGS-based solutions in complex outdoor environments. Introduction Simultaneous Localization and Mapping (SLAM), a core re- search area in computer vision, has been widely applied in autonomous driving, metaverse, and robotics. It primarily utilizes sensor data to estimate the state of a ro"}
{"id": "2507.23677v1_chunk2", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "n and Mapping (SLAM), a core re- search area in computer vision, has been widely applied in autonomous driving, metaverse, and robotics. It primarily utilizes sensor data to estimate the state of a robot while si- multaneously constructing an accurate scene representation. Traditional methods (Campos et al. 2021; Wang, Schworer, and Cremers 2017; Li, Liu, and Wu 2024) typically formu- late this as a maximum a posteriori (MAP) estimation prob- lem, where both robot ego-motion and scene modeling are described as factors in a graph for joint optimization. 1The code will be released in case of acceptance.In recent years, neural rendering-based methods have made significant advancements. The emergence of Neural Radiance Fields (NeRF) (Mildenhall et al. 2021) has pro- foundly influenced the community by revolutionizing novel view synthesis and scene representation, shifting the focus towards data-driven and differentiable rendering methods. Lately, 3D Gaussian Splatting (3DGS) (Kerbl et al. "}
{"id": "2507.23677v1_chunk3", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "unity by revolutionizing novel view synthesis and scene representation, shifting the focus towards data-driven and differentiable rendering methods. Lately, 3D Gaussian Splatting (3DGS) (Kerbl et al. 2023) has emerged as a promising alternative. By representing scenes as a collection of 3D Gaussians and leveraging an ef- ficient rasterization strategy, 3DGS achieves fast rendering while providing high-quality scene representation. This nat- urally aligns with SLAM’s requirements for real-time pro- cessing and accurate scene reconstruction, making 3DGS- SLAM (Keetha et al. 2024; Matsuki et al. 2024a) a rapidly growing research focus in recent years. However, existing 3DGS-SLAM methods mainly rely on dense and accurate depth maps from RGB-D sensors as training supervision for geometric reconstruction, while also being limited to small- scale indoor scenes. These methods achieve high-fidelity representations in controlled indoor environments but en- counter severe challenges in large, com"}
{"id": "2507.23677v1_chunk4", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "struction, while also being limited to small- scale indoor scenes. These methods achieve high-fidelity representations in controlled indoor environments but en- counter severe challenges in large, complex outdoor settings. First, active depth sensors like LiDAR and RGB-D cam- eras have inherent limitations outdoors. LiDAR systems are expensive, bulky, and power-intensive, making them imprac- tical for many applications. Consumer RGB-D sensors like Microsoft Kinect or Intel RealSense, while more affordable and compact, face even greater limitations outdoors. These devices have significantly shorter effective ranges (typically under 5 meters), and their infrared-based depth sensing be- comes unreliable in direct sunlight due to interference with their projected patterns. Second, outdoor scenes typicallyarXiv:2507.23677v1 [cs.RO] 31 Jul 2025span much larger scales. For example, in the KITTI dataset (Geiger, Lenz, and Urtasun 2012a), trajectories often ex- tend over kilometers, causing sub"}
{"id": "2507.23677v1_chunk5", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "es typicallyarXiv:2507.23677v1 [cs.RO] 31 Jul 2025span much larger scales. For example, in the KITTI dataset (Geiger, Lenz, and Urtasun 2012a), trajectories often ex- tend over kilometers, causing substantial memory consump- tion during scene reconstruction and making real-time, effi- cient large-scale mapping particularly challenging. Finally, outdoor environments frequently involve drastic viewpoint changes and limited frame overlap, resulting in insufficient optimization constraints. This leads to convergence difficul- ties and visual artifacts that destabilize training. To address these challenges, we propose a novel 3DGS- based architecture specifically designed for large-scale out- door environments such as autonomous driving scenarios. Our approach leverages passive RGB stereo cameras only, which are affordable and lightweight compared to expen- sive and cumbersome active sensors, and leverages the use of pre-trained deep stereo networks to generate dense depth maps that guide t"}
{"id": "2507.23677v1_chunk6", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "as only, which are affordable and lightweight compared to expen- sive and cumbersome active sensors, and leverages the use of pre-trained deep stereo networks to generate dense depth maps that guide the training of 3D Gaussians, effectively overcoming the lack of reliable depth information in outdoor environments. Additionally, we employ an external tracker based on ORB-SLAM2 (Mur-Artal, Montiel, and Tardos 2015), which significantly optimizes the entire pipeline and improves the overall system performance. To the best of our knowledge, we are the first to in- tegrate deep binocular stereo networks with 3DGS-SLAM specifically tailored for outdoor scenarios. Compared to the sparse LiDAR point clouds, stereo also provides more com- plete scene coverage, while demonstrating strong general- ization and robustness under challenging lighting (Tosi, Bar- tolomei, and Poggi 2025). Furthermore, conversely to ill- posed, single-view depth estimation approaches (Yang et al. 2024b,a; Ke et al. 202"}
{"id": "2507.23677v1_chunk7", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "ation and robustness under challenging lighting (Tosi, Bar- tolomei, and Poggi 2025). Furthermore, conversely to ill- posed, single-view depth estimation approaches (Yang et al. 2024b,a; Ke et al. 2024), stereo still provides proper metric estimates, grounded in epipolar geometry. Our experiments confirm that even approximate depth estimations from these networks significantly enhance the optimization process by guiding the positioning of 3D Gaussians and preventing ar- tifacts that typically occur when splats become trapped in incorrect geometric configurations. In summary, our contributions are the following: • We propose BGS-SLAM, the first 3D Gaussian Splatting SLAM system for outdoor environments using passive RGB stereo pairs only. • We integrate pre-trained deep stereo networks for dense depth supervision in 3D Gaussian optimization, showing that passive stereo can effectively replace expensive ac- tive sensors for outdoor scene reconstruction. • We introduce a combination of no"}
{"id": "2507.23677v1_chunk8", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "dense depth supervision in 3D Gaussian optimization, showing that passive stereo can effectively replace expensive ac- tive sensors for outdoor scene reconstruction. • We introduce a combination of normal-based and smoothness losses alongside depth-from-stereo supervi- sion to enhance geometric consistency, reduce artifacts, and improve overall mapping quality. • We present experiments on multiple large-scale outdoor datasets, including KITTI and KITTI-360, demonstrating that our approach significantly surpasses existing 3DGS- SLAM methods in outdoor scenarios, achieving superior tracking, mapping accuracy, and visual quality. Related Work Our work builds upon neural radiance field-based SLAM (Tosi et al. 2024), particularly focusing on RGB-only meth-ods and 3DGS for outdoor environments. Neural Implicit Representations for SLAM. Neural im- plicit representations have revolutionized SLAM research. iMAP (Sucar et al. 2021) pioneered this integration by em- ploying an MLP to map 3D coord"}
{"id": "2507.23677v1_chunk9", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "Neural Implicit Representations for SLAM. Neural im- plicit representations have revolutionized SLAM research. iMAP (Sucar et al. 2021) pioneered this integration by em- ploying an MLP to map 3D coordinates to color and density. NICE-SLAM (Zhu et al. 2022) addressed scalability through hierarchical representation using multiple pre-trained MLPs. V ox-Fusion (Yang et al. 2022) combined traditional volu- metric techniques with neural implicit representations, while Co-SLAM (Wang, Wang, and Agapito 2023) developed hy- brid encodings for robust camera tracking. For large-scale environments, GO-SLAM (Zhang et al. 2023b) implemented global optimization techniques includ- ing loop closure and bundle adjustment, whereas Point- SLAM (Sandstr ¨om et al. 2023) introduced a dynamic neural point cloud representation that adapts point density based on scene complexity. Most of these methods, however, rely on RGB-D sensors, limiting outdoor applications. RGB-only SLAM with External Supervision. RGB- "}
{"id": "2507.23677v1_chunk10", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "representation that adapts point density based on scene complexity. Most of these methods, however, rely on RGB-D sensors, limiting outdoor applications. RGB-only SLAM with External Supervision. RGB- only SLAM methods overcome depth ambiguity through various external supervision signals. DIM-SLAM (Li et al. 2023) employed neural implicit map representation with multi-resolution volume encoding and photometric warping loss. NICER-SLAM (Zhu et al. 2024b) incorporated monoc- ular depth and normal supervision alongside RGB render- ing losses. iMODE (Matsuki et al. 2023) utilized ORB- SLAM2 for camera pose estimation while enhancing re- construction through depth-rendered geometry supervision. NeRF-VO (Naumann et al. 2024) combined DPVO track- ing with DPT for depth estimation. Hi-SLAM (Zhang et al. 2023a) leveraged DROID-SLAM-based dense correspon- dence and monocular depth priors to address low-texture en- vironments. Recent approaches include MoD-SLAM (Zhou et al. 2024), which enhanced d"}
{"id": "2507.23677v1_chunk11", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "t al. 2023a) leveraged DROID-SLAM-based dense correspon- dence and monocular depth priors to address low-texture en- vironments. Recent approaches include MoD-SLAM (Zhou et al. 2024), which enhanced depth estimation through DPT and ZoeDepth, and MGS-SLAM (Zhu et al. 2024a), which unified sparse visual odometry with 3DGS through MVS- derived depth supervision. 3D Gaussian Splatting for SLAM. 3D Gaussian Splat- ting (Kerbl et al. 2023) offers faster rendering capabili- ties and improved representation of complex scenes com- pared to NeRF-based approaches. MonoGS (Matsuki et al. 2024b) pioneered this paradigm shift by leveraging 3D Gaussians with splatting rendering techniques for a sin- gle moving camera. Concurrently, Photo-SLAM (Huang et al. 2023) integrated explicit geometric features with im- plicit texture representations within a hyper primitives map. SplaTAM (Keetha et al. 2024) represented scenes as col- lections of simplified 3D Gaussians for high-quality color and depth image r"}
{"id": "2507.23677v1_chunk12", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "h im- plicit texture representations within a hyper primitives map. SplaTAM (Keetha et al. 2024) represented scenes as col- lections of simplified 3D Gaussians for high-quality color and depth image rendering, while GS-SLAM (Yan et al. 2024) introduced an adaptive expansion strategy and coarse- to-fine tracking technique. Recent advances include HF- GS SLAM (Sun et al. 2024b), which proposed rendering- guided densification strategies, and CG-SLAM (Hu et al. 2024), which implemented an uncertainty-aware 3D Gaus- sian field. MM3DGS-SLAM (Sun et al. 2024a) expanded to multi-modal inputs including inertial measurements, while RTG-SLAM (Peng et al. 2024) addressed large-scale en- vironments by enforcing binary opacity classifications. For monocular setups, MonoGS++ (Li et al. 2024) exploitedLeft Image InputRight ImageStereo Network Sky Segmentation(Optional) &ORB Feature DetectionInter-Frame TrackStereo MatchingTracking Thread Sampled Depth Map KeyFraming𝑂𝑅𝐵 𝐼𝑂𝑈𝑆𝐸(3)𝑇𝑖𝑚𝑒MappingWeighted Dept"}
{"id": "2507.23677v1_chunk13", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "t Image InputRight ImageStereo Network Sky Segmentation(Optional) &ORB Feature DetectionInter-Frame TrackStereo MatchingTracking Thread Sampled Depth Map KeyFraming𝑂𝑅𝐵 𝐼𝑂𝑈𝑆𝐸(3)𝑇𝑖𝑚𝑒MappingWeighted Depth LossRGB LossNormal LossSmoothness LossRasterization Rendering ResultLocal Map TrackDepthMap Sampling𝑇!\" Figure 2: Framework Overview. BGS-SLAM uses stereo images to reconstruct outdoor environments using 3D Gaussians. A pre-trained stereo network extracts dense depth maps from the stereo pairs, with optional sky masking to improve reconstruc- tion. The tracking thread estimates camera poses through feature matching and local bundle adjustment, while the keyframing thread maintains a buffer of key observations. In the mapping thread, a combination of depth, normal, and smoothness losses supervises the 3D Gaussian optimization, enhancing geometric consistency and visual quality of the reconstructed scenes. DPVO (Teed, Lipson, and Deng 2022) as an external tracker to estimate initial camera"}
{"id": "2507.23677v1_chunk14", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "vises the 3D Gaussian optimization, enhancing geometric consistency and visual quality of the reconstructed scenes. DPVO (Teed, Lipson, and Deng 2022) as an external tracker to estimate initial camera poses. However, in outdoor envi- ronments, LIV-GaussMap (Hong and et al. 2024) and MM- Gaussian (Wu et al. 2024) still relied on LiDAR sensors for accurate depth measurements. Our work bridges this gap by utilizing stereo RGB images and recent deep stereo match- ing networks (Tosi, Bartolomei, and Poggi 2025) to predict depth maps that supervise 3D Gaussian Splatting optimiza- tion, enabling high-quality SLAM in outdoor environments without relying on active depth sensors. Methods We detail our binocular BGS-SLAM approach for outdoor environments in this section, with an overview in Fig. 2. 3D Gaussian Splatting (3DGS) BGS-SLAM models the scene as a set of 3D Gaussians, de- noted as G={g1, g2, . . . , g N}, where Nis the number of Gaussians. Each 3D Gaussian giis parameterized by both app"}
{"id": "2507.23677v1_chunk15", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "D Gaussian Splatting (3DGS) BGS-SLAM models the scene as a set of 3D Gaussians, de- noted as G={g1, g2, . . . , g N}, where Nis the number of Gaussians. Each 3D Gaussian giis parameterized by both appearance attributes (color cirepresented by spherical har- monics and opacity oi∈[0,1]), geometric properties (center position µi∈R3and covariance matrix Σi∈R3×3) param- eters. The spatial influence of each Gaussian is defined as: gi(x) =e−1 2(x−µi)⊤Σ−1 i(x−µi)(1) where the covariance matrix Σi=RSS⊤R⊤withS∈R3 representing the spatial scale and R∈R3×3the rotation, parameterized by a quaternion. In the rendering process, 3D Gaussians are first projected onto the 2D camera plane as: µ′=JWµ,Σ′=JWΣWTJT(2)where Wis the rotational component of the viewing trans- formation TcwandJis the Jacobian matrix which performs linear approximation of the projective transformation. With alpha blending, the color and depth at each pixel are generated through: Cp=nX i=1ciαii−1Y j=1(1−αj), D p=nX i=1diαii−1Y j=1"}
{"id": "2507.23677v1_chunk16", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": " which performs linear approximation of the projective transformation. With alpha blending, the color and depth at each pixel are generated through: Cp=nX i=1ciαii−1Y j=1(1−αj), D p=nX i=1diαii−1Y j=1(1−αj) (3) where the opacity αiis: αi=oiexp\u0012 −1 2(x′−µ′ i)⊤Σ′−1 i(x′−µ′ i)\u0013 (4) During optimization, the parameters of all observed 3D Gaussians are iteratively refined through backpropagation using our mapping losses. For more details, please refer to (Kerbl et al. 2023; Chen and Wang 2024; Tosi et al. 2024). Deep Stereo Depth Estimation Given a pair of synchronized stereo images, IleftandIright, we employ a pre-trained deep stereo network, fθ, to estimate a dense disparity map dfor outdoor environments: d=fθ(Ileft, Iright). (5) Modern stereo matching networks have evolved into ad- vanced architectures that can be broadly classified into three main categories: Convolutional Neural Network (CNN)- based cost volume aggregation (Mayer et al. 2016; Kendall et al. 2017), transformer-based mode"}
{"id": "2507.23677v1_chunk17", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "hitectures that can be broadly classified into three main categories: Convolutional Neural Network (CNN)- based cost volume aggregation (Mayer et al. 2016; Kendall et al. 2017), transformer-based models (Li et al. 2021), and iterative optimization approaches (Lipson, Teed, and Deng2021). These networks typically construct a cost volume C(d)by establishing pixel-wise correspondences between the stereo image pair. Depending on the architecture, this correspondence computation can be performed using corre- lation layers, absolute or relative feature differences, or di- rect feature concatenation across the disparity range: C(d) = Ψ\u0000 Φ(Ileft), Td\u0000 Φ(Iright)\u0001\u0001 , d∈[dmin, dmax], (6) where Φ(·)denotes a feature extraction function, Td(·)rep- resents a disparity-dependent shift operation, and Ψ(·)de- fines the cost computation mechanism, which varies based on the network architecture (e.g., feature concatenation, sub- traction, or correlation). In our framework, we use pre-trained stereo netwo"}
{"id": "2507.23677v1_chunk18", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "de- fines the cost computation mechanism, which varies based on the network architecture (e.g., feature concatenation, sub- traction, or correlation). In our framework, we use pre-trained stereo networks that exhibit strong generalization across diverse environments, e.g., recent foundation models trained on extensive datasets (Wen et al. 2025; Cheng et al. 2025; Bartolomei et al. 2025; Jiang et al. 2025), allowing us to obtain reliable depth esti- mates without requiring additional domain-specific training. Disparity maps are converted to metric depth ˜Dvia the stan- dard stereo triangulation formula: ˜D=f·b d, (7) where fis the focal length and bthe stereo baseline. These dense depth maps provide rich geometric supervision for our 3DGS optimization process, offering significant advantages over LiDAR-based depth estimation methods due to their higher spatial density and more complete scene coverage. Sky Segmentation 3DGS often generates ambiguous floaters when rendering sky, leading t"}
{"id": "2507.23677v1_chunk19", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "s over LiDAR-based depth estimation methods due to their higher spatial density and more complete scene coverage. Sky Segmentation 3DGS often generates ambiguous floaters when rendering sky, leading to a significant number of unnecessary Gaus- sians and violating multi-view consistency. However, the sky typically occupies only a small portion of outdoor scenes, and the depth map in these regions tends to be inaccurate. To address this, we integrate a sky segmentation network (Xie et al. 2021) into our pipeline, which unifies transform- ers with a lightweight MLP. Notably, the sky segmentator is optional and designed to enhance system stability. Tracking While 3D Gaussian Splatting offers remarkable render- ing capabilities, it faces challenges in large-scale outdoor environments due to computational demands and conver- gence issues. To cope with these issues, we adopt an exter- nal tracker based on ORB-SLAM2 (Mur-Artal and Tard ´os 2017) for several key reasons: (1) feature-based metho"}
{"id": "2507.23677v1_chunk20", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "putational demands and conver- gence issues. To cope with these issues, we adopt an exter- nal tracker based on ORB-SLAM2 (Mur-Artal and Tard ´os 2017) for several key reasons: (1) feature-based methods like ORB-SLAM2 provide robust real-time tracking even in challenging outdoor conditions with varying illumination and viewpoints, (2) the sparse feature matching approach is computationally efficient compared to the dense optimiza- tion required for direct 3D Gaussian optimization, and (3) decoupling the tracking from the mapping thread allows us to maintain stable pose estimation while the 3D Gaussian representation is still being optimized.Specifically, at time i, the left image Ileftand correspond- ing right image Iright are processed into our tracking thread. Following ORB-SLAM2 (Mur-Artal and Tard ´os 2017), the ORB features are extracted from both the left and right im- ages which ensures the real-time performance compared to other feature points. Given the camera-inherent K, the "}
{"id": "2507.23677v1_chunk21", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "tal and Tard ´os 2017), the ORB features are extracted from both the left and right im- ages which ensures the real-time performance compared to other feature points. Given the camera-inherent K, the ORB features from the left image are searched for matches against those from the right image, yielding a set of stereo corre- spondences. For the following frames, the incremental cam- era pose is first initialized under a constant velocity motion model. Then, the matched stereo features are incorporated into a frame-level bundle adjustment (BA). The loss for the frame-level BA can be expressed as: min TiX i∈Oρ\u0010\u0002 zi−πs(Ti,Xi)\u0003TΩi\u0002 zi−πs(Ti,Xi)\u0003\u0011 , (8) where Tiis the camera pose of current frame i,ziis the observed stereo measurement, Xiis the corresponding 3D mappoint, πs(·)is the stereo projection function, Ωiis the inverse covariance matrix, and ρ(·)is a robust kernel cost function. Once the incremental ego-motion is estimated, a sliding window of selected keyframes is activated for loca"}
{"id": "2507.23677v1_chunk22", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "ction function, Ωiis the inverse covariance matrix, and ρ(·)is a robust kernel cost function. Once the incremental ego-motion is estimated, a sliding window of selected keyframes is activated for local bundle adjustment optimization. By constructing a local BA cost function over these keyframes, the tracking thread fur- ther reduce the reprojection error and improve the accuracy of ego-motion estimation. Formally, the local BA is: min {Tk},{Xj}X k∈KX j∈Okρ\u0010 eT kjΩkjekj\u0011 , (9) ekj=zkj−π\u0000 Tk,Xj\u0001 . (10) where {Tk}are the keyframe poses within the sliding win- dow,{Xj}are the 3D mappoints visible across keyframes, andzkjis the observation of point jin keyframe k. This lo- cal BA jointly optimizes camera poses and 3D structure, pro- viding accurate ego-motion estimates crucial for our map- ping thread. Precise pose estimation ensures proper align- ment between 3D Gaussians and stereo depth maps, prevent- ing the uncontrolled expansion of Gaussians that would oth- erwise lead to artifacts an"}
{"id": "2507.23677v1_chunk23", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": " thread. Precise pose estimation ensures proper align- ment between 3D Gaussians and stereo depth maps, prevent- ing the uncontrolled expansion of Gaussians that would oth- erwise lead to artifacts and excessive memory consumption in large outdoor scenes. KeyFraming In BGS-SLAM, we integrate a keyframing module to en- sure the robustness of our system. Most of the recent 3DGS- SLAM systems determine the keyframes according to time intervals. This approach is able to uniformly add keyframes, but the ego-motion in outdoor scenes rarely exhibits linear changes over time and contains intense camera movements. Thus, relying solely on time intervals often leads to inade- quate scene overlaps and catastrophic forgetting in outdoor scenes. The keyframing module first assesses the covisibil- ity from the intersection over union (IoU) of the observed ORB keypoints extracted with inter-frames. If the IoU falls below the threshold, the current frame Iiis registered as a new keyframe. To address sy"}
{"id": "2507.23677v1_chunk24", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "om the intersection over union (IoU) of the observed ORB keypoints extracted with inter-frames. If the IoU falls below the threshold, the current frame Iiis registered as a new keyframe. To address system instability caused by in- tense camera movements, the inter-frame transformation is evaluated to determine whether a significant movement oc- curs. If a large motion change appears, a new keyframe isinserted into the keyframe buffer T. In our paper, the in- tense camera movement is adaptively defined as 1.5 times the previous motion change: Tt WC(Tt−1 WC)−1>1.5×Tt−1 WC(Tt−2 WC)−1→ T . (11) To maintain computational efficiency, the system retains a limited number of keyframes within the sliding window to optimize both the ego-motion and 3D Gaussian representa- tions. Moreover, a keyframe will be marginalized from the sliding window if the keypoint IoU with the most recent keyframe drops below a specified threshold. Mapping The mapping thread receives camera poses from the track- ing th"}
{"id": "2507.23677v1_chunk25", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "ill be marginalized from the sliding window if the keypoint IoU with the most recent keyframe drops below a specified threshold. Mapping The mapping thread receives camera poses from the track- ing thread and stereo-estimated depth maps. It represents the scene as a collection of 3D Gaussians optimized through several complementary loss functions: RGB Loss. We supervise the color reconstruction using a combination of L1 and structural similarity (SSIM) losses: Lcolor =λrgb∥I(G, TCW)−I∥1 +λssimLssim(I(G, TCW), I), (12) where I(G, TCW)andIare the rendered and real RGB im- ages, respectively. Weighted Geometric Loss. While RGB loss is essen- tial, it provides insufficient supervision for outdoor scenes with large textureless regions. Therefore, we introduce a weighted depth loss that incorporates RGB gradient infor- mation: Lgeo=grgb1 nX log\u0010 1 +M Dsi−ˆDsi 1\u0011 (13) where grgb= exp\u0000 −∇I\u0001 ,∇Iis the RGB image gradient, n is the number of pixels, Mis a mask for valid depth values, Dsiis the st"}
{"id": "2507.23677v1_chunk26", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "ates RGB gradient infor- mation: Lgeo=grgb1 nX log\u0010 1 +M Dsi−ˆDsi 1\u0011 (13) where grgb= exp\u0000 −∇I\u0001 ,∇Iis the RGB image gradient, n is the number of pixels, Mis a mask for valid depth values, Dsiis the stereo network-estimated depth map, and ˆDsiis the rendered depth map. Based on empirical observations in our experimental vali- dation, we discovered that uniform sampling of stereo depth supervision significantly improves reconstruction quality compared to using the full depth map. This sampled depth loss is formally defined as: Lsampled geo =grgb+1 |S|X (i,j)∈Slog\u0010 1 +M Dsi,j−ˆDsi,j 1\u0011 (14) where S⊂ {1, . . . , H } × { 1, . . . , W }represents a uni- form subset of pixel coordinates. For our implementation, we sample approximately 25% of the total pixels using a reg- ular grid pattern. This approach offers several advantages: it reduces the influence of locally correlated errors in stereo depth estimation and promotes smoother optimization by ef- fectively regularizing the supervision sig"}
{"id": "2507.23677v1_chunk27", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "pproach offers several advantages: it reduces the influence of locally correlated errors in stereo depth estimation and promotes smoother optimization by ef- fectively regularizing the supervision signal. Normal Consistency Loss. To enhance geometric super- vision, we compute normal vectors from both the stereo- estimated and rendered depth maps, and enforce consistency between them: Ln=1 |H|X N−ˆN 1, (15)Methods PSNR ↑ SSIM ↑ LPIPS ↓ Depth L1 ↓ w/o ss 24.19 0.92 0.11 141.71 w/o wd 23.86 0.91 0.12 284.36 w/o nl 24.25 0.92 0.11 193.20 w/o sl 23.82 0.91 0.12 186.46 with ds 21.17 0.84 0.22 263.68 Ours 24.82 0.93 0.10 136.10 Table 1: Ablation Study on the KITTI dataset. We analyze the effectiveness of sky segmentator (ss), weighted depth loss (wd), normal loss (nl), smoothness loss (sl) and dense depth map supervision (ds) in our proposed SLAM system. Backbones PSNR ↑ SSIM ↑ LPIPS ↓ Depth L1 ↓ IGEV 23.34 0.90 0.14 278.20 IGEV++ 23.19 0.89 0.15 293.57 TCSM 23.16 0.89 0.15 300.29 MonSter-K 2"}
{"id": "2507.23677v1_chunk28", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "nse depth map supervision (ds) in our proposed SLAM system. Backbones PSNR ↑ SSIM ↑ LPIPS ↓ Depth L1 ↓ IGEV 23.34 0.90 0.14 278.20 IGEV++ 23.19 0.89 0.15 293.57 TCSM 23.16 0.89 0.15 300.29 MonSter-K 23.33 0.90 0.15 186.32 Mocha 24.17 0.91 0.12 462.00 FoundationStereo 24.57 0.92 0.11 132.65 MonSter-M 24.82 0.93 0.10 136.10 Table 2: Ablation Study on Stereo Network Selection. Evaluation of our method’s performance using different stereo networks. Depth L1 is in [cm], backbones in the up- per part are fine-tuned on KITTI datasets, while backbones in the bottom part are trained on a mix of datasets. where ˆNis the rendered normal map, Nis the normal map derived from stereo depth estimation, and Hdenotes the number of patches with valid normal values. Smoothness Loss. To ensure geometric consistency, we introduce smoothness loss that penalized abrupt changes in the normal map: Ls=1 |n|X rX i,j\u0010 ˆNi+r,j−ˆNi,j + ˆNi,j+r−ˆNi,j \u0011 . (16) where nis the number of valid depth values in the rendered"}
{"id": "2507.23677v1_chunk29", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "tency, we introduce smoothness loss that penalized abrupt changes in the normal map: Ls=1 |n|X rX i,j\u0010 ˆNi+r,j−ˆNi,j + ˆNi,j+r−ˆNi,j \u0011 . (16) where nis the number of valid depth values in the rendered depth map. Final Loss. The final mapping loss combines these com- ponents with appropiate weights: Lmapping =λrgbLrgb+λssimLssim+λgeoLsampled geo +λnLn+λsLs(17) where we set λrgb= 0.8,λssim = 0.2,λgeo= 0.1,λn= 0.1, andλs= 0.5in our experiments. Experiments Experimental Setup Datasets. We evaluate BGS-SLAM on the KITTI (Geiger, Lenz, and Urtasun 2012b) and the KITTI-360 datasets (Liao, Xie, and Geiger 2022). Both datasets provide rich sensor data from a vehicle platform with stereo cameras, Velo- dyne LiDAR, GPS, and IMU, covering diverse driving sce- narios including urban areas, residential streets, and high- ways under varying illumination conditions. We focus onMethods Metrics03 05 06 07 09 10Average(km/frames) (0.56/801) (2.2/2761) (1.2/1101) (0.69/1101) (1.7/1591) (0.92/1201) Point-S"}
{"id": "2507.23677v1_chunk30", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "streets, and high- ways under varying illumination conditions. We focus onMethods Metrics03 05 06 07 09 10Average(km/frames) (0.56/801) (2.2/2761) (1.2/1101) (0.69/1101) (1.7/1591) (0.92/1201) Point-SLAM† ATE ↓ 81.51 104.61 170.73 79.00 138.50 102.81 112.86 PSNR ↑ 9.09 12.58 4.33 11.89 11.69 8.26 9.64 SSIM ↑ 0.30 0.48 0.24 0.47 0.37 0.38 0.37 LPIPS ↓ 0.74 0.66 0.89 0.64 0.71 0.69 0.72 Depth-L1 ↓ 227.89 428.29 405.80 211.97 248.12 306.25 304.72 SplaTAM† ATE ↓ 10.20 37.13 53.78 32.82 70.23 33.96 39.69 PSNR ↑ 14.26 14.78 16.40 16.05 15.91 14.18 15.26 SSIM ↑ 0.47 0.48 0.55 0.63 0.54 0.45 0.52 LPIPS ↓ 0.56 0.53 0.46 0.43 0.52 0.56 0.51 Depth-L1 ↓ 277.91 319.99 474.61 355.67 673.33 277.86 396.89 MonoGS† ATE ↓ 57.87 51.77 92.81 51.23 81.23 61.96 66.14 PSNR ↑ 10.40 12.20 11.15 10.94 12.65 12.71 11.67 SSIM ↑ 0.25 0.37 0.28 0.38 0.42 0.38 0.35 LPIPS ↓ 0.71 0.65 0.76 0.67 0.71 0.68 0.70 Depth-L1 ↓ 681.49 403.81 575.07 568.34 666.94 674.65 595.05 BGS-SLAM (Lidar) ATE ↓ 1.77 1.86 0.90 0.60 4.76 3.7"}
{"id": "2507.23677v1_chunk31", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": ".67 SSIM ↑ 0.25 0.37 0.28 0.38 0.42 0.38 0.35 LPIPS ↓ 0.71 0.65 0.76 0.67 0.71 0.68 0.70 Depth-L1 ↓ 681.49 403.81 575.07 568.34 666.94 674.65 595.05 BGS-SLAM (Lidar) ATE ↓ 1.77 1.86 0.90 0.60 4.76 3.70 2.26 PSNR ↑ 11.87 6.92 10.47 8.04 8.08 10.40 9.30 SSIM ↑ 0.56 0.37 0.47 0.39 0.31 0.45 0.42 LPIPS ↓ 0.63 0.70 0.66 0.67 0.71 0.64 0.67 Depth-L1 ↓ 234.77 286.10 303.22 273.60 381.24 256.28 289.20 BGS-SLAM (Ours) ATE ↓ 1.77 1.86 0.90 0.60 4.76 3.70 2.26 PSNR ↑ 24.82 19.16 23.57 20.14 18.99 22.56 21.54 SSIM ↑ 0.93 0.72 0.87 0.78 0.73 0.85 0.81 LPIPS ↓ 0.10 0.29 0.15 0.22 0.30 0.18 0.21 Depth-L1 ↓ 136.10 253.03 226.89 192.99 371.00 161.70 223.62 Table 3: Quantitative Evaluation on the KITTI dataset. Our BGS-SLAM is evaluated on the whole image recorded on the sequences. Methods indicated with †fail to process the entire recorded image and therefore, their performance is reported on the first 300 frames of all sequences. MonoGS is reported in RGB-D mode. ATE RMSE [m] ↓, Depth L1 [cm] ↓and bol"}
{"id": "2507.23677v1_chunk32", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "ail to process the entire recorded image and therefore, their performance is reported on the first 300 frames of all sequences. MonoGS is reported in RGB-D mode. ATE RMSE [m] ↓, Depth L1 [cm] ↓and bold numbers indicate the best result. the KITTI Odometry split, which contains 22 sequences. Among them, we randomly select 6 sequences with ground truth poses, with trajectory lengths ranging from 0.56 km to 2.2 km. We additionally evaluate on KITTI-360, which of- fers expanded coverage and complexity, selecting multiple sequences with different scene scales and dynamics to val- idate BGS-SLAM’s robustness and scalability under more complex outdoor conditions. Evaluation Metrics. We evaluate BGS-SLAM on track- ing and mapping. For tracking, we report the RMSE of Ab- solute Trajectory Error (ATE). For rendering quality, we fol- low radiance-field-based SLAM methods and report PSNR, SSIM (Wang et al. 2004), and LPIPS (Zhang et al. 2018). Geometric accuracy is measured via Depth L1 error betwe"}
{"id": "2507.23677v1_chunk33", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "). For rendering quality, we fol- low radiance-field-based SLAM methods and report PSNR, SSIM (Wang et al. 2004), and LPIPS (Zhang et al. 2018). Geometric accuracy is measured via Depth L1 error between rendered depth maps and LiDAR ground truth. Implementation Details. All experiments, including BGS-SLAM and baselines, are run on a desktop with In- tel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz and a NVIDIA A40 GPU with 48Gb memory. We adopt ORB-SLAM2 (Mur-Artal and Tard ´os 2017) as the external tracker for robust pose estimation. For stereo depth, we use the publicly available MonSter (Cheng et al. 2025) network with pre- trained weights. For learning rate in 3DGS mapping, color is set to 2.5×10−3, rotation and scale to 10−3, opacity to 5×10−2, and the opacity removal threshold to 5×10−3. All results are averaged over three runs.Ablation Study Component Analysis. In Table 1, we present ablation ex- periments on a KITTI sequence to validate each compo- nent of our approach. Our full system a"}
{"id": "2507.23677v1_chunk34", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "esults are averaged over three runs.Ablation Study Component Analysis. In Table 1, we present ablation ex- periments on a KITTI sequence to validate each compo- nent of our approach. Our full system achieves the best overall performance across all metrics. Removing the sky segmentation module (“w/o ss”) leads to decreased visual quality metrics (PSNR -0.63dB) by introducing inaccurate supervision from sky regions. Without the weighted depth loss (“w/o wd”), depth accuracy deteriorates substantially (Depth L1 increases by 148.26 cm), while maintaining rea- sonable visual quality, highlighting its importance for geo- metric reconstruction. The absence of normal loss (“w/o nl”) or smoothness loss (“w/o sl”) results in increased depth er- rors and slightly reduced rendering quality, confirming their role in enhancing structural details. Using dense depth maps without our selective supervision strategy (“with ds”) per- forms worse, demonstrating that balancing supervision sig- nals is cruci"}
{"id": "2507.23677v1_chunk35", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": " their role in enhancing structural details. Using dense depth maps without our selective supervision strategy (“with ds”) per- forms worse, demonstrating that balancing supervision sig- nals is crucial. Stereo Network Analysis. Table 2 reports the perfor- mance of BGS-SLAM integrated with various state-of-the- art stereo matching networks. The upper section of the table includes models fine-tuned on the KITTI dataset (IGEV (Xu et al. 2023), IGEV++(Xu et al. 2024), TCSM(Zeng et al. 2024), and MonSter-K (Cheng et al. 2025), while the lower section includes models trained on a broader mix of datasets (Mocha (Chen et al. 2024), FoundationStereo (Wen et al. 2025), and MonSter-M (Cheng et al. 2025)). Notably, com- pared with KITTI-only models, networks trained on multi- ple datasets exhibit overall superior performance across bothMethods Metrics0002 0004 0005 0007 0008 0009Average(km/frames) (11.5/14k) (9.97/11.6k) (4.69/6.7k) (4.89/3.4k) (7.13/8.8k) (10.58/14k) Point-SLAM† ATE ↓ 99.56 161."}
{"id": "2507.23677v1_chunk36", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "rall superior performance across bothMethods Metrics0002 0004 0005 0007 0008 0009Average(km/frames) (11.5/14k) (9.97/11.6k) (4.69/6.7k) (4.89/3.4k) (7.13/8.8k) (10.58/14k) Point-SLAM† ATE ↓ 99.56 161.56 56.07 247.38 99.53 159.80 137.32 PSNR ↑ 12.65 7.90 12.90 12.21 7.16 5.83 9.77 SSIM ↑ 0.47 0.38 0.43 0.41 0.18 0.26 0.35 LPIPS ↓ 0.69 0.70 0.67 0.73 0.92 0.89 0.77 Depth-L1 ↓ 371.20 485.35 720.09 746.07 494.01 676.43 582.19 SplaTAM† ATE ↓ 56.19 67.55 23.96 138.98 58.12 57.13 66.99 PSNR ↑ 12.53 12.39 12.24 13.02 13.33 11.94 12.57 SSIM ↑ 0.29 0.34 0.31 0.34 0.36 0.40 0.34 LPIPS ↓ 0.60 0.57 0.61 0.58 0.58 0.58 0.59 Depth-L1 ↓ 492.51 586.81 684.68 727.28 501.30 724.98 619.59 MonoGS† ATE ↓ 43.91 79.70 31.11 177.12 52.08 103.45 81.23 PSNR ↑ 11.23 12.10 11.00 11.43 10.63 11.12 11.25 SSIM ↑ 0.32 0.38 0.33 0.32 0.30 0.47 0.35 LPIPS ↓ 0.73 0.69 0.70 0.68 0.69 0.67 0.69 Depth-L1 ↓ 616.64 681.48 818.60 797.51 629.82 880.14 737.36 BGS-SLAM (Lidar) ATE ↓ 3.43 3.25 2.81 2.77 5.55 6.28 4.01 PSNR ↑ 10.54"}
{"id": "2507.23677v1_chunk37", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "8 0.33 0.32 0.30 0.47 0.35 LPIPS ↓ 0.73 0.69 0.70 0.68 0.69 0.67 0.69 Depth-L1 ↓ 616.64 681.48 818.60 797.51 629.82 880.14 737.36 BGS-SLAM (Lidar) ATE ↓ 3.43 3.25 2.81 2.77 5.55 6.28 4.01 PSNR ↑ 10.54 10.27 10.99 10.21 10.07 9.04 10.19 SSIM ↑ 0.40 0.40 0.38 0.39 0.36 0.32 0.38 LPIPS ↓ 0.68 0.64 0.68 0.66 0.69 0.71 0.68 Depth-L1 ↓ 291.01 312.82 407.84 387.22 385.43 445.84 371.69 BGS-SLAM (Ours) ATE ↓ 3.43 3.25 2.81 2.77 5.55 6.28 4.01 PSNR ↑ 23.24 24.68 24.93 24.29 24.36 20.47 23.66 SSIM ↑ 0.87 0.90 0.91 0.88 0.89 0.80 0.87 LPIPS ↓ 0.18 0.14 0.14 0.17 0.15 0.26 0.17 Depth-L1 ↓ 314.80 215.93 285.88 457.50 306.47 428.89 334.91 Table 4: Quantitative Evaluation on the KITTI-360 dataset. Our BGS-SLAM is evaluated on the whole image recorded on the sequences. Methods indicated with †fail to process the entire image and is reported on the first 300 frames of all sequences. MonoGS is reported in RGB-D mode. Note that in the ”(km/frames)” row, ”k” is used as a shorthand for 1,000 frames. Methods"}
{"id": "2507.23677v1_chunk38", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "ss the entire image and is reported on the first 300 frames of all sequences. MonoGS is reported in RGB-D mode. Note that in the ”(km/frames)” row, ”k” is used as a shorthand for 1,000 frames. Methods 03 05 06 07 09 10 SplaTAM ✗ ✗ ✗ ✗ ✗ ✗ BGS-SLAM (Ours) 8.08 45.12 16.77 14.81 20.36 20.62 Table 5: Memory Consumption Analysis (GB). ✗indicates that the method fails to process full sequences, running out of memory after few hundreds frames. rendering quality metrics and geometric accuracy. In par- ticular, MonSter-M achieves the best PSNR (24.82), while maintaining a low depth error (136.10 cm), significantly out- performing models such as IGEV (278.20 cm) and TCSM (300.29 cm). Furthermore, these multi-dataset models ex- hibit stronger zero-shot generalization capability, which is critical for long-term SLAM deployment in unseen environ- ments. We therefore adopt MonSter-M as our default stereo network for optimal accuracy-generalization trade-off. Comparison with State-of-The-Art SLAM Tr"}
{"id": "2507.23677v1_chunk39", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "r long-term SLAM deployment in unseen environ- ments. We therefore adopt MonSter-M as our default stereo network for optimal accuracy-generalization trade-off. Comparison with State-of-The-Art SLAM Tracking and Mapping Performance. We compare BGS- SLAM against state-of-the-art radiance field-based SLAM methods on the KITTI and KITTI-360 datasets in terms of tracking accuracy and mapping quality. Quantitative re- sults are reported in Table 3 and Table 4. Due to memory constraints, methods like SplaTAM (Keetha et al. 2024), MonoGS (Matsuki et al. 2024a), and Point-SLAM (Sand- str¨om et al. 2023) were evaluated only on the first 300 frames per sequence. However, their tracking threads showed large pose estimation errors in outdoor environments, limiting their applicability in real-world large-scale scenes. In con-trast, our tracking, grounded in classical SLAM pose esti- mation, provides robust and accurate performance even in complex, large-scale scenarios. For mapping and view synthesi"}
{"id": "2507.23677v1_chunk40", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "e-scale scenes. In con-trast, our tracking, grounded in classical SLAM pose esti- mation, provides robust and accurate performance even in complex, large-scale scenarios. For mapping and view synthesis, BGS-SLAM substan- tially outperforms all baselines across all visual metrics, achieving an average PSNR improvement of over 6 dB in KITTI dataset. In KITTI-360 dataset, BGS-SLAM achieves more than a 10 dB improvement in PSNR and the best depth reconstruction accuracy with the lowest Depth L1 error. Fig. 3 illustrates the rendering performance of BGS- SLAM vs. baselines. A LiDAR-supervised variant is also shown, highlighting its advantages over active sensors in outdoor settings. Compared to SplaTAM, MonoGS, and Point-SLAM, our method achieves the highest fidelity and continuity in large-scale outdoor scenes. Memory Efficiency. Table. 5 shows the memory con- sumption analysis for all methods. SplaTAM fails to process complete KITTI sequences even on an NVIDIA A40 GPU with 48GB of memory,"}
{"id": "2507.23677v1_chunk41", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "le outdoor scenes. Memory Efficiency. Table. 5 shows the memory con- sumption analysis for all methods. SplaTAM fails to process complete KITTI sequences even on an NVIDIA A40 GPU with 48GB of memory, whereas BGS-SLAM succeeds at it. Conclusion In this paper, we present BGS-SLAM, the first 3DGS- SLAM system for outdoor scenarios using only stereo RGB input. Our novel contributions include leveraging pre- trained deep stereo networks for depth supervision and in- troducing a multi-loss optimization strategy that combines RGB, depth, normal, and smoothness losses to enhance ge- ometric consistency and novel view synthesis quality. Ex- periments on KITTI and KITTI-360 demonstrate that BGS- SLAM achieves superior tracking and mapping quality com-SplaTAM MonoGS Point-SLAM Ours (LiDAR) Ours GT Figure 3: Visualization of rendering quality on KITTI. pared to existing radiance-field SLAM approaches without requiring expensive LiDAR sensors. Limitations. BGS-SLAM does not yet operate in real- ti"}
{"id": "2507.23677v1_chunk42", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "igure 3: Visualization of rendering quality on KITTI. pared to existing radiance-field SLAM approaches without requiring expensive LiDAR sensors. Limitations. BGS-SLAM does not yet operate in real- time, with average tracking and mapping times of 0.24 s and 1.37 s per frame, respectively—posing a limitation for practical SLAM applications. The computational overhead is further increased by the inference time of the deep stereo network, in addition to the iterative optimization of 3D Gaussians for each frame. References Bartolomei, L.; Tosi, F.; Poggi, M.; and Mattoccia, S. 2025. Stereo anywhere: Robust zero-shot deep stereo matching even where either stereo or mono fail. In Proceedings of the Computer Vision and Pattern Recognition Conference , 1013–1027. Campos, C.; Elvira, R.; Rodr ´ıguez, J. J. G.; Montiel, J. M.; and Tard ´os, J. D. 2021. Orb-slam3: An accurate open-source library for visual, visual–inertial, and multimap slam. IEEE Transactions on Robotics , 37(6): 1874–1890. Chen"}
{"id": "2507.23677v1_chunk43", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "z, J. J. G.; Montiel, J. M.; and Tard ´os, J. D. 2021. Orb-slam3: An accurate open-source library for visual, visual–inertial, and multimap slam. IEEE Transactions on Robotics , 37(6): 1874–1890. Chen, G.; and Wang, W. 2024. A Survey on 3D Gaussian Splatting. arXiv preprint arXiv:2401.03890 . Chen, Z.; Long, W.; Yao, H.; Zhang, Y .; Wang, B.; Qin, Y .; and Wu, J. 2024. Mocha-stereo: Motif channel attention net- work for stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 27768–27777. Cheng, J.; Liu, L.; Xu, G.; Wang, X.; Zhang, Z.; Deng, Y .; Zang, J.; Chen, Y .; Cai, Z.; and Yang, X. 2025. Monster: Marry monodepth to stereo unleashes power. In Proceedings of the Computer Vision and Pattern Recognition Conference , 6273–6282. Geiger, A.; Lenz, P.; and Urtasun, R. 2012a. Are we ready for autonomous driving? the kitti vision benchmark suite.In2012 IEEE conference on computer vision and pattern recognition , 3354–3361. IEEE. Geiger, A.; "}
{"id": "2507.23677v1_chunk44", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "Lenz, P.; and Urtasun, R. 2012a. Are we ready for autonomous driving? the kitti vision benchmark suite.In2012 IEEE conference on computer vision and pattern recognition , 3354–3361. IEEE. Geiger, A.; Lenz, P.; and Urtasun, R. 2012b. Are we ready for Autonomous Driving? The KITTI Vision Bench- mark Suite. In Conference on Computer Vision and Pattern Recognition (CVPR) . Hong, S.; and et al. 2024. LIV-GaussMap: LiDAR-Inertial- Visual Fusion for Real-time 3D Radiance Field Map Ren- dering. IEEE Robotics and Automation Letters . Hu, J.; Chen, X.; Feng, B.; Li, G.; Yang, L.; Bao, H.; Zhang, G.; and Cui, Z. 2024. CG-SLAM: Efficient Dense RGB- D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field. In European Conference on Computer Vision (ECCV) . Huang, H.; Li, L.; Cheng, H.; and Yeung, S.-K. 2023. Photo- SLAM: Real-time Simultaneous Localization and Photore- alistic Mapping for Monocular, Stereo, and RGB-D Cam- eras. arXiv preprint arXiv:2311.16728 . Jiang, H.; Lou, Z.; Ding, L.; Xu, R"}
{"id": "2507.23677v1_chunk45", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "K. 2023. Photo- SLAM: Real-time Simultaneous Localization and Photore- alistic Mapping for Monocular, Stereo, and RGB-D Cam- eras. arXiv preprint arXiv:2311.16728 . Jiang, H.; Lou, Z.; Ding, L.; Xu, R.; Tan, M.; Jiang, W.; and Huang, R. 2025. Defom-stereo: Depth foundation model based stereo matching. In Proceedings of the Computer Vi- sion and Pattern Recognition Conference , 21857–21867. Ke, B.; Obukhov, A.; Huang, S.; Metzger, N.; Daudt, R. C.; and Schindler, K. 2024. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition , 9492–9502. Keetha, N.; Karhade, J.; Jatavallabhula, K. M.; Yang, G.; Scherer, S.; Ramanan, D.; and Luiten, J. 2024. Splatam: Splat track & map 3d gaussians for dense rgb-d slam. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition , 21357–21366. Kendall, A.; Martirosyan, H.; Dasgupta, S.; Henry, P.; Kennedy, R.; Bachrach, "}
{"id": "2507.23677v1_chunk46", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "r dense rgb-d slam. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition , 21357–21366. Kendall, A.; Martirosyan, H.; Dasgupta, S.; Henry, P.; Kennedy, R.; Bachrach, A.; and Bry, A. 2017. End-to-end learning of geometry and context for deep stereo regres- sion. In Proceedings of the IEEE international conference on computer vision , 66–75. Kerbl, B.; Kopanas, G.; Leimk ¨uhler, T.; and Drettakis, G. 2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions on Graphics , 42(4). Li, H.; Gu, X.; Yuan, W.; Yang, L.; Dong, Z.; and Tan, P. 2023. Dense RGB SLAM With Neural Implicit Maps. In Proceedings of the International Conference on Learning Representations . Li, R.-W.; Ke, W.; Li, D.; Tian, L.; and Barsoum, E. 2024. MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM. In British Conference on Machine Vision (BMVC) . Li, X.; Liu, D.; and Wu, J. 2024. CTO-SLAM: contour tracking for object-level robust 4D SLAM. In Proceed- ings"}
{"id": "2507.23677v1_chunk47", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "nd Accurate Monocular RGB Gaussian SLAM. In British Conference on Machine Vision (BMVC) . Li, X.; Liu, D.; and Wu, J. 2024. CTO-SLAM: contour tracking for object-level robust 4D SLAM. In Proceed- ings of the AAAI Conference on Artificial Intelligence , vol- ume 38, 10323–10331. Li, Z.; Liu, X.; Drenkow, N.; Ding, A.; Creighton, F. X.; Taylor, R. H.; and Unberath, M. 2021. Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF inter- national conference on computer vision , 6197–6206.Liao, Y .; Xie, J.; and Geiger, A. 2022. Kitti-360: A novel dataset and benchmarks for urban scene understanding in 2d and 3d. IEEE Transactions on Pattern Analysis and Machine Intelligence , 45(3): 3292–3310. Lipson, L.; Teed, Z.; and Deng, J. 2021. Raft-stereo: Mul- tilevel recurrent field transforms for stereo matching. In 2021 International Conference on 3D Vision (3DV) , 218– 227. IEEE. Matsuki, H.; Murai, R.; Kelly, P. H.; and Davis"}
{"id": "2507.23677v1_chunk48", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "J. 2021. Raft-stereo: Mul- tilevel recurrent field transforms for stereo matching. In 2021 International Conference on 3D Vision (3DV) , 218– 227. IEEE. Matsuki, H.; Murai, R.; Kelly, P. H.; and Davison, A. J. 2024a. Gaussian splatting slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 18039–18048. Matsuki, H.; Murai, R.; Kelly, P. H. J.; and Davison, A. J. 2024b. Gaussian Splatting SLAM. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . Matsuki, H.; Sucar, E.; Laidow, T.; Wada, K.; Scona, R.; and Davison, A. J. 2023. iMODE: Real-Time Incremen- tal Monocular Dense Mapping Using Neural Field. In 2023 IEEE International Conference on Robotics and Automation (ICRA) , 4171–4177. IEEE. Mayer, N.; Ilg, E.; Hausser, P.; Fischer, P.; Cremers, D.; Dosovitskiy, A.; and Brox, T. 2016. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE confe"}
{"id": "2507.23677v1_chunk49", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "; Fischer, P.; Cremers, D.; Dosovitskiy, A.; and Brox, T. 2016. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition , 4040–4048. Mildenhall, B.; Srinivasan, P. P.; Tancik, M.; Barron, J. T.; Ramamoorthi, R.; and Ng, R. 2021. Nerf: Representing scenes as neural radiance fields for view synthesis. Com- munications of the ACM , 65(1): 99–106. Mur-Artal, R.; Montiel, J. M. M.; and Tardos, J. D. 2015. ORB-SLAM: a versatile and accurate monocular SLAM system. IEEE transactions on robotics , 31(5): 1147–1163. Mur-Artal, R.; and Tard ´os, J. D. 2017. Orb-slam2: An open- source slam system for monocular, stereo, and rgb-d cam- eras. IEEE transactions on robotics , 33(5): 1255–1262. Naumann, J.; Xu, B.; Leutenegger, S.; and Zuo, X. 2024. NeRF-VO: Real-Time Sparse Visual Odometry With Neural Radiance Fields. IEEE Robotics and Automation Letters . Peng, Z.; Shao, T.;"}
{"id": "2507.23677v1_chunk50", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "5): 1255–1262. Naumann, J.; Xu, B.; Leutenegger, S.; and Zuo, X. 2024. NeRF-VO: Real-Time Sparse Visual Odometry With Neural Radiance Fields. IEEE Robotics and Automation Letters . Peng, Z.; Shao, T.; Liu, Y .; Zhou, J.; Yang, Y .; Wang, J.; and Zhou, K. 2024. Rtg-slam: Real-time 3d reconstruction at scale using gaussian splatting. In ACM SIGGRAPH 2024 Conference Papers , 1–11. Sandstr ¨om, E.; Li, Y .; Van Gool, L.; and R. Oswald, M. 2023. Point-SLAM: Dense Neural Point Cloud-based SLAM. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision (ICCV) . Sucar, E.; Liu, S.; Ortiz, J.; and Davison, A. J. 2021. iMAP: Implicit mapping and positioning in real-time. In Proceed- ings of the IEEE/CVF International Conference on Com- puter Vision , 6229–6238. Sun, L. C.; Bhatt, N. P.; Liu, J. C.; Fan, Z.; Wang, Z.; Humphreys, T. E.; and Topcu, U. 2024a. MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vi- sion, Depth, and Inertial Measurements. In IEEE/RSJ In- t"}
{"id": "2507.23677v1_chunk51", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "N. P.; Liu, J. C.; Fan, Z.; Wang, Z.; Humphreys, T. E.; and Topcu, U. 2024a. MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vi- sion, Depth, and Inertial Measurements. In IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS) .Sun, S.; Mielle, M.; Lilienthal, A. J.; and Magnusson, M. 2024b. High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification and Regularized Optimiza- tion. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) . IEEE. Teed, Z.; Lipson, L.; and Deng, J. 2022. Deep patch visual odometry. arXiv preprint arXiv:2208.04726 . Tosi, F.; Bartolomei, L.; and Poggi, M. 2025. A Survey on Deep Stereo Matching in the Twenties. International Jour- nal of Computer Vision . Tosi, F.; Zhang, Y .; Gong, Z.; Sandstr ¨om, E.; Mattoccia, S.; Oswald, M. R.; and Poggi, M. 2024. How nerfs and 3d gaus- sian splatting are reshaping slam: a survey. arXiv preprint arXiv:2402.13255 , 4: 1. Wang, H.; Wang, J.; an"}
{"id": "2507.23677v1_chunk52", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": ".; Sandstr ¨om, E.; Mattoccia, S.; Oswald, M. R.; and Poggi, M. 2024. How nerfs and 3d gaus- sian splatting are reshaping slam: a survey. arXiv preprint arXiv:2402.13255 , 4: 1. Wang, H.; Wang, J.; and Agapito, L. 2023. Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neu- ral Real-Time SLAM. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 13293–13302. Wang, R.; Schworer, M.; and Cremers, D. 2017. Stereo DSO: Large-scale direct sparse visual odometry with stereo cameras. In Proceedings of the IEEE international confer- ence on computer vision , 3903–3911. Wang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P. 2004. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image process- ing, 13(4): 600–612. Wen, B.; Trepte, M.; Aribido, J.; Kautz, J.; Gallo, O.; and Birchfield, S. 2025. Foundationstereo: Zero-shot stereo matching. In Proceedings of the Computer Vision and Pat- tern Rec"}
{"id": "2507.23677v1_chunk53", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "g, 13(4): 600–612. Wen, B.; Trepte, M.; Aribido, J.; Kautz, J.; Gallo, O.; and Birchfield, S. 2025. Foundationstereo: Zero-shot stereo matching. In Proceedings of the Computer Vision and Pat- tern Recognition Conference , 5249–5260. Wu, C.; Duan, Y .; Zhang, X.; Sheng, Y .; Ji, J.; and Zhang, Y . 2024. MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes. In RSJ International Conference on Intelligent Robots and Systems (IROS) . Xie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.; and Luo, P. 2021. SegFormer: Simple and efficient design for semantic segmentation with transformers. Advances in neural information processing systems , 34: 12077–12090. Xu, G.; Wang, X.; Ding, X.; and Yang, X. 2023. Iterative ge- ometry encoding volume for stereo matching. In Proceed- ings of the IEEE/CVF conference on computer vision and pattern recognition , 21919–21928. Xu, G.; Wang, X.; Zhang, Z.; Cheng, J.; Liao, C.; and Yang, X. 2024. IGEV++: "}
{"id": "2507.23677v1_chunk54", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "for stereo matching. In Proceed- ings of the IEEE/CVF conference on computer vision and pattern recognition , 21919–21928. Xu, G.; Wang, X.; Zhang, Z.; Cheng, J.; Liao, C.; and Yang, X. 2024. IGEV++: iterative multi-range geome- try encoding volumes for stereo matching. arXiv preprint arXiv:2409.00638 . Yan, C.; Qu, D.; Xu, D.; Zhao, B.; Wang, Z.; Wang, D.; and Li, X. 2024. GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . Yang, L.; Kang, B.; Huang, Z.; Xu, X.; Feng, J.; and Zhao, H. 2024a. Depth anything: Unleashing the power of large- scale unlabeled data. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition , 10371–10381. Yang, L.; Kang, B.; Huang, Z.; Zhao, Z.; Xu, X.; Feng, J.; and Zhao, H. 2024b. Depth anything v2. Advances in Neural Information Processing Systems , 37: 21875–21911. Yang, X.; Li, H.; Zhai, H.; Ming, Y .; Liu, Y .; and Zhang, G. 2022. V"}
{"id": "2507.23677v1_chunk55", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "Z.; Xu, X.; Feng, J.; and Zhao, H. 2024b. Depth anything v2. Advances in Neural Information Processing Systems , 37: 21875–21911. Yang, X.; Li, H.; Zhai, H.; Ming, Y .; Liu, Y .; and Zhang, G. 2022. V ox-Fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , 499–507. IEEE. Zeng, J.; Yao, C.; Wu, Y .; and Jia, Y . 2024. Temporally con- sistent stereo matching. In European Conference on Com- puter Vision , 341–359. Springer. Zhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang, O. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition , 586–595. Zhang, W.; Sun, T.; Wang, S.; Cheng, Q.; and Haala, N. 2023a. Hi-slam: Monocular real-time dense mapping with hybrid implicit fields. IEEE Robotics and Automation Let- ters. Zhang, Y .; Tosi, F.; Mattoccia, S.; and Poggi, M. 2023b. "}
{"id": "2507.23677v1_chunk56", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "eng, Q.; and Haala, N. 2023a. Hi-slam: Monocular real-time dense mapping with hybrid implicit fields. IEEE Robotics and Automation Let- ters. Zhang, Y .; Tosi, F.; Mattoccia, S.; and Poggi, M. 2023b. Go- slam: Global optimization for consistent 3d instant recon- struction. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 3727–3737. Zhou, H.; Guo, Z.; Ren, Y .; Liu, S.; Zhang, L.; Zhang, K.; and Li, M. 2024. MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction. Zhu, P.; Zhuang, Y .; Chen, B.; Li, L.; Wu, C.; and Liu, Z. 2024a. MGS-SLAM: Monocular Sparse Tracking and Gaus- sian Mapping with Depth Smooth Regularization. IEEE Robotics and Automation Letters . Zhu, Z.; Peng, S.; Larsson, V .; Cui, Z.; Oswald, M. R.; Geiger, A.; and Pollefeys, M. 2024b. NICER-SLAM: Neu- ral Implicit Scene Encoding for RGB SLAM. In Interna- tional Conference on 3D Vision (3DV) . Zhu, Z.; Peng, S.; Larsson, V .; Xu, W.; Bao, H.; Cui, Z.; Oswald, M. R.; and Pollefe"}
{"id": "2507.23677v1_chunk57", "paper_id": "2507.23677v1", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "text": "24b. NICER-SLAM: Neu- ral Implicit Scene Encoding for RGB SLAM. In Interna- tional Conference on 3D Vision (3DV) . Zhu, Z.; Peng, S.; Larsson, V .; Xu, W.; Bao, H.; Cui, Z.; Oswald, M. R.; and Pollefeys, M. 2022. Nice-slam: Neu- ral implicit scalable encoding for slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 12786–12796."}
{"id": "2507.23660v1_chunk0", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios Haoxuan Jiang, Peicong Qian, Yusen Xie, Xiaocong Li, Ming Liu, and Jun Ma, Senior Member, IEEE Abstract — LiDAR-based localization serves as a critical com- ponent in autonomous systems, yet existing approaches face persistent challenges in balancing repeatability, accuracy, and environmental adaptability. Traditional point cloud registration methods relying solely on offline maps often exhibit limited robustness against long-term environmental changes, leading to localization drift and reliability degradation in dynamic real-world scenarios. To address these challenges, this paper proposes DuLoc, a robust and accurate localization method that tightly couples LiDAR-inertial odometry with offline map-based localization, incorporating a constant-velocity motion model to mitigate outlier noise in real-world scenarios. Specifically, we develop a LiDAR-based localization framework that seamlessly integrates"}
{"id": "2507.23660v1_chunk1", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "localization, incorporating a constant-velocity motion model to mitigate outlier noise in real-world scenarios. Specifically, we develop a LiDAR-based localization framework that seamlessly integrates a prior global map with dynamic real-time local maps, enabling robust localization in unbounded and chang- ing environments. Extensive real-world experiments in ultra unbounded port that involve 2,856 hours of operational data across 32 Intelligent Guided Vehicles (IGVs) are conducted and reported in this study. The results attained demonstrate that our system outperforms other state-of-the-art LiDAR localization systems in large-scale changing outdoor environments. I. INTRODUCTION High-precision life-long localization in large-scale en- vironments faces fundamental challenges across various autonomous systems [1], [2], [3], [4]. While Real-Time Kinematic (RTK) localization has been widely adopted, its reliability is inherently limited by satellite signal availability and near metallic st"}
{"id": "2507.23660v1_chunk2", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "autonomous systems [1], [2], [3], [4]. While Real-Time Kinematic (RTK) localization has been widely adopted, its reliability is inherently limited by satellite signal availability and near metallic structures, with typical accuracy degra- dation from centimeters to meters in such scenarios. Map- based localization [5], [6] often encounters challenges due to environmental changes, leading to unstable and incorrect performance. These limitations drive the development of sensor-based localization approaches. Initial efforts focused on single-sensor solutions (e.g., LiDAR [7] or cameras [8]), but their susceptibility to environmental changes and percep- tual degradation in feature-deprived areas proved inadequate for large-scale deployments [9], [10], [11]. The evolution to This work was supported by the Guangdong provincial project under Grant 2023QN10Z006. (Corresponding author: Jun Ma.) Haoxuan Jiang and Yusen Xie are with Robotics and Autonomous Systems Thrust, The Hong Kong University"}
{"id": "2507.23660v1_chunk3", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "rted by the Guangdong provincial project under Grant 2023QN10Z006. (Corresponding author: Jun Ma.) Haoxuan Jiang and Yusen Xie are with Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511453, China (e-mail: hjiangax@connect.hkust- gz.edu.cn; yxie827@connect.hk-gz.edu.cn). Peicong Qian and Ming Liu are with Shenzhen Unity Drive Inno- vation Technology Co., Ltd., Shenzhen 518063, China (e-mail: epsilon- john9527@gmail.com; liu.ming.prc@gmail.com). Xiaocong Li is with the College of Information Science and Technology, Eastern Institute of Technology, Ningbo, Ningbo 315200, China (e-mail: xiaocongli@eitech.edu.cn). Jun Ma is with the Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511453, China, and also with the Division of Emerging Interdisciplinary Areas, The Hong Kong University of Science and Technology, Hong Kong SAR, China (e-mail: jun.ma@ust.hk).multi-"}
{"id": "2507.23660v1_chunk4", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "gzhou), Guangzhou 511453, China, and also with the Division of Emerging Interdisciplinary Areas, The Hong Kong University of Science and Technology, Hong Kong SAR, China (e-mail: jun.ma@ust.hk).multi-sensor fusion systems [12], [13], [14] address some shortcomings by combining complementary sensing modali- ties. However, these approaches still struggle with long-term consistency due to cumulative errors in odometry estimation and the absence of global constraints. Even state-of-the-art fusion methods exhibit gradual drift in kilometer-scale oper- ations [9], [10], [11], particularly in dynamic environments where moving objects distort perception observations. This fundamental limitation underscores the need for persistent global references in large-scale localization. To address these issues, in this paper, we propose a novel tightly-coupled framework that incorporates LiDAR- IMU odometry, integrates global offline map constraints, and leverages the constant velocity (CV) model, specif"}
{"id": "2507.23660v1_chunk5", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "es, in this paper, we propose a novel tightly-coupled framework that incorporates LiDAR- IMU odometry, integrates global offline map constraints, and leverages the constant velocity (CV) model, specifically designed to achieve accurate and robust localization in large- scale industrial environments. Firstly, we propose a tightly- coupled LiDAR-Inertial-CV odometry backbone based on an iterated Error-State Kalman Filter (iESKF) that ensures robust short-term tracking. Then, we design a dual-map architecture combining a static prior map with a dynamic local map, enabling simultaneous global consistency and local adaptability. Finally, we validate our approach in one of the most demanding real-world environments: automated port operations. Essentially, modern container ports present extreme localization challenges [15], including ultra large- scale unbounded scenes with persistent GPS-denial areas and severe multipath effects caused by dense metallic obstruc- tions. The frequent cargo han"}
{"id": "2507.23660v1_chunk6", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "ocalization challenges [15], including ultra large- scale unbounded scenes with persistent GPS-denial areas and severe multipath effects caused by dense metallic obstruc- tions. The frequent cargo handling, vehicle movement, and machinery operations further intensify the highly dynamic nature of these settings, rendering precise localization ex- ceptionally difficult. In addition, the challenges of mapping in such large areas are compounded by high mapping costs, difficulty in maintenance, high environmental repetitiveness, and the stringent accuracy requirements for localization. In our experiments, 2,856 hours of operational data across 32 Intelligent Guided Vehicles (IGVs) are involved. Our system demonstrates centimeter-level accuracy (mean: 8.3 cm, σ: 4.1 cm) while maintaining 99.98 %availability, outperform- ing existing LiDAR-SLAM baselines [13], [14] that failed catastrophically within 30 minutes of operation due to mis- alignment of the map or odometry drift. The main contribu"}
{"id": "2507.23660v1_chunk7", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "availability, outperform- ing existing LiDAR-SLAM baselines [13], [14] that failed catastrophically within 30 minutes of operation due to mis- alignment of the map or odometry drift. The main contributions of our work are summarized as follows: •We present a highly efficient dual-map LiDAR localiza- tion framework integrated into a multi-sensor tightly- coupled localization system, specifically designed for real-world large-scale, dynamic, and rapidly changing environments.arXiv:2507.23660v1 [cs.RO] 31 Jul 2025•We import an optimized Kalman gain computation method to improve computational efficiency and en- hance the accuracy of point cloud feature alignment during the filter measurement update process. •We propose a constant velocity (CV) model operating at up to 100 Hz, ensuring stable and continuous pose prediction even in case of sensor failures. •Our algorithm is deployed and validated in a port cover- ing an area of approximately one million square meters, providing accurate and "}
{"id": "2507.23660v1_chunk8", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "nd continuous pose prediction even in case of sensor failures. •Our algorithm is deployed and validated in a port cover- ing an area of approximately one million square meters, providing accurate and stable localization support for 32 IGVs; and this underscores the effectiveness of the proposed method in real-world settings. II. RELATED WORK A. LiDAR-Based SLAM LiDAR-based SLAM methods are widely used for map construction and self-localization. LeGO-LOAM [16] and F- LOAM [14] both utilize ground planes for feature extrac- tion and matching, with F-LOAM introducing a two-stage distortion compensation method. However, these methods struggle with handling dynamic environments and feature redundancy. SROM [13] uses a two-layer approach to esti- mate rotation and translation, refining the results with point- to-plane ICP [17], [18], though it can be computationally expensive. PFilter [19], an extension of F-LOAM, filters invalid features to improve accuracy but may still suffer from high co"}
{"id": "2507.23660v1_chunk9", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "ts with point- to-plane ICP [17], [18], though it can be computationally expensive. PFilter [19], an extension of F-LOAM, filters invalid features to improve accuracy but may still suffer from high computational overhead in large-scale scenar- ios. FEVO-LOAM [20] optimizes feature extraction with enhanced ground segmentation and curvature definitions, yet it may face limitations in complex urban environments with varied terrain. RF-LOAM [21] uses FA-RANSAC to remove dynamic objects, but its effectiveness diminishes with extreme environmental changes. WiCRF2 [22] enhances motion observability and minimizes redundancy, but it may struggle in highly dynamic or unstructured environments. The method in [23] employs angle-based feature extraction and voxel-based feature matching but may be sensitive to LiDAR viewpoint variations. CDP-LOAM [24] introduces clustering-directed points for attitude estimation, yet it is computationally heavy. Light-LOAM [25] offers a two-stage correspondence stra"}
{"id": "2507.23660v1_chunk10", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "tive to LiDAR viewpoint variations. CDP-LOAM [24] introduces clustering-directed points for attitude estimation, yet it is computationally heavy. Light-LOAM [25] offers a two-stage correspondence strategy for reliable registration but may not scale well in large-scale dynamic environments. B. Map-Based LiDAR Localization Recent map-based LiDAR localization methods focus on balancing accuracy, efficiency, and adaptability. LOL [26] reduces cumulative drift via geometric place recognition but struggles with partially similar scenes, suggesting dynamic map updates. DLL [5] optimizes point-to-map distances without feature extraction, yet its computational intensity de- mands hierarchical optimization for scalability. For dynamic environments, ROLL [6] activates temporary mapping during global matching failures, though latency issues necessitate adaptive triggering, while DMLL [27] mitigates alignment errors through differential constraints but requires proba- bilistic error modeling to add"}
{"id": "2507.23660v1_chunk11", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "matching failures, though latency issues necessitate adaptive triggering, while DMLL [27] mitigates alignment errors through differential constraints but requires proba- bilistic error modeling to address complex uncertainties. Efficiency-driven approaches (Thakur [28] and Block Map Fig. 1: The overall architecture of our proposed system, based on the iESKF filter, tightly integrates LiDAR, IMU data and constant velocity model for high-precision localization. The blue arrows indicate data input, the green arrows represent the forward process, theorange arrows depict the constraint and covariance propagation, and the purple arrows signify the output of the localization results. [29]) leverage lightweight maps and block switching for large-scale applications, yet risk accuracy loss or transition misalignments, calling for multi-resolution maps and bound- ary refinement. C. Multi-Sensor Fused Localization Multi-sensor fusion methods combine LiDAR with inertial measurements to address moti"}
{"id": "2507.23660v1_chunk12", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "tion misalignments, calling for multi-resolution maps and bound- ary refinement. C. Multi-Sensor Fused Localization Multi-sensor fusion methods combine LiDAR with inertial measurements to address motion distortion and environmen- tal dynamics. Tightly-coupled frameworks dominate recent advancements, where LINS [30] and LIO-EKF [31] utilize iterated/extended Kalman filters with adaptive error compen- sation, though their computational load escalates in large- scale environments. Meanwhile, LIO-SAM [32] enhances real-time performance through factor graphs and IMU pre- integration, while the FAST-LIO series [12], [33] innovates with state-dependent Kalman gain and raw point-to-map registration (FAST-LIO2 [33]), significantly reducing feature extraction overhead at the cost of degraded accuracy during aggressive maneuvers. For high-dynamic scenarios, Point- LIO [34] introduces stochastic kinematic modeling to handle IMU saturation, yet its efficacy remains constrained by inertial sensor qu"}
{"id": "2507.23660v1_chunk13", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "y during aggressive maneuvers. For high-dynamic scenarios, Point- LIO [34] introduces stochastic kinematic modeling to handle IMU saturation, yet its efficacy remains constrained by inertial sensor quality. III. METHODOLOGY As illustrated in Fig.1, our framework comprises several key modules: the system model in Sec. III-A, constant velocity prediction model in Sec. III-B, dual-map update strategy in Sec. III-C, IMU correction model in Sec. III- D, and additional implementation details including post- processing and LiDAR motion compensation in Sec. III-E. A. System Description Firstly, we define the state vector xof our system as x≜\u0002WRIWpIWvIωa b ωbaWgIRLIpL\u0003 (1) whereWRI,WpI, andWvIdenote the IMU rotation, position, and velocity in the world frame W.bωandbaare the IMU biases of angular velocity ωand linear acceleration a.Wgis the gravity vector in the world frame W.IRLand IpLare the extrinsic parameters between IMU frame and LiDAR frame. Inertial Measurement Unit (IMU) provides motio"}
{"id": "2507.23660v1_chunk14", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "elocity ωand linear acceleration a.Wgis the gravity vector in the world frame W.IRLand IpLare the extrinsic parameters between IMU frame and LiDAR frame. Inertial Measurement Unit (IMU) provides motion in- formation over short periods. When combined with inputsfrom other sensors and advanced algorithms, it achieves impressive accuracy within a tightly-coupled multi-sensor fusion framework. The IMU noise wis defined as: w≜\u0002nωnanbωnba\u0003 (2) where ωandaare the IMU angular velocity and linear acceleration. nωandnadenote the measurement noise of ωanda.nbωandnbaare random walk process noises. In our system, the state transition model at the sampling period ∆tis defined as xi+1=xi⊞(∆tf(xi,wi)) (3) where the symbol ⊞, as introduced in [12], represents the state transition of the system in the Lie algebra space. The function fin forward process can be derived as f(x,w)≜ ω−bω WvI+1 2\u0000WRI(a−ba) +Wg\u0001 ∆t WRI(a−ba) +Wg nω na nbω nba 03×1 03×1 03×1 (3) B. Constant Veloc"}
{"id": "2507.23660v1_chunk15", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "lgebra space. The function fin forward process can be derived as f(x,w)≜ ω−bω WvI+1 2\u0000WRI(a−ba) +Wg\u0001 ∆t WRI(a−ba) +Wg nω na nbω nba 03×1 03×1 03×1 (3) B. Constant Velocity Model Prediction The CV model is simple, robust, and reliable in environ- ments with IMU data interruptions or sensor failures. Unlike the constant acceleration model (which risks instability), or machine-learning models (which depend heavily on data quality and training), the CV model avoids complexity and ensures stability. In the case of sensor failures, such as data interruptions from the IMU, we introduce a 100 Hz timer and switch to a CV model for prediction. This method ensures that even in the absence of IMU data, the localization system does not get stuck waiting for data synchronization or immediately diverge. The system assumes that the linear and angular velocity states remain constant between two consecutive LiDAR frames, allowing the entire positioning pipeline to degrade"}
{"id": "2507.23660v1_chunk16", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "ization or immediately diverge. The system assumes that the linear and angular velocity states remain constant between two consecutive LiDAR frames, allowing the entire positioning pipeline to degrade into a pure LiDAR-based localization method using the CV model. The prediction model, as applied during the filter prediction phase, can be expressed as: xi+1=xi⊞(∆tf(xi,0)) (4) where the covariance propagation follows a process similar to that of the iESKF algorithm. C. LiDAR Dual-Map Updates Compared to directly preforming point cloud registration with a prior map, we transform the feature alignment process into the filter measurement update process within the iESKF framework based on point-to-plane constraints in the map M: MRj\u0000 xi,Lipj,Linj\u0001 =MujT\u0000MTIiIiTLi\u0000Lipj+Linj\u0001 −Mqj\u0001 (6)whereLipjandLinjare the LiDAR point jin current i- th frame of point cloud and its noise, and we assume that this noise is affected by zero-mean Gaussian white noise. Mujis the normal vector of the associated pl"}
{"id": "2507.23660v1_chunk17", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "dLinjare the LiDAR point jin current i- th frame of point cloud and its noise, and we assume that this noise is affected by zero-mean Gaussian white noise. Mujis the normal vector of the associated plane fitted using neighboring points ofLipjin the map M.MTIiis the transformation between the map frame Mand the IMU frame Ii.IiTLiis the transformation between the IMU frame Iiand the LiDAR frame Li.Mqjis another point on the associated fitted plane in the map M. Then, a dual-map based localization scheme tailored for complex, expansive and dynamic environments is introduced. Unlike methods that solely rely on prior maps for pose estimation and correction, this approach also employs a tightly coupled LiDAR-inertial odometry integrated with a local online dynamic map. Meanwhile, instead of running a separate odometry system, we achieve tightly coupled feature-to-map matching by associating point cloud features with both the static prior map Mglobal and the real-time local dynamic map Mlocal"}
{"id": "2507.23660v1_chunk18", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "ing a separate odometry system, we achieve tightly coupled feature-to-map matching by associating point cloud features with both the static prior map Mglobal and the real-time local dynamic map Mlocal within a single frame processing. 0=MglobalRj\u0000 xi,Lipj,Linj\u0001 +MlocalRj\u0000 xi,Lipj,Linj\u0001 (7) This integration enables more precise, reliable, and robust localization. Besides, to further accelerate computation, the new mathe- matically equivalent formula for calculating the Kalman gain is used, as proposed by FAST-LIO [12]. This reduces the computational complexity from the observation dimension to the state dimension, ensuring that the measurement up- date process remains computationally efficient and effective despite the large size of LiDAR point cloud observation dimension. D. IMU Linear Acceleration and Angular Velocity Updates In addition to performing the observation update using LiDAR data, the idea from Point-LIO [34] is incorporated, which treats IMU data as a measurement for filte"}
{"id": "2507.23660v1_chunk19", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "eleration and Angular Velocity Updates In addition to performing the observation update using LiDAR data, the idea from Point-LIO [34] is incorporated, which treats IMU data as a measurement for filter correction. Specifically, assuming that the IMU measurement is affected by zero-mean Gaussian white noise, and considering that IMU’s linear acceleration, angular velocity and their biases are included in the system state estimation, a measurement update can be performed upon receiving IMU data: ω=Iω−nω−bω−nbω (8) a=Ia−na−ba−nba (9) whereIωandIaare the measurements of IMU angular velocity and linear acceleration. Then, for the i-th frame of IMU measurement data, we can denote the IMU constraints as: R\u0000 xi,Iωi,nω\u0001 =Iωi−nω−bω−nbω−ωi (10) R\u0000 xi,Iai,na\u0001 =Iai−na−ba−nba−ai (11) These IMU constraints can be integrated in the following form for the observation update: 0=R\u0000 xi,ωI i,nω\u0001 +R\u0000 xi,aI i,na\u0001 (12)E. Implement Details LiDAR Motion Compensation . When the LiDAR is in motion, the points are"}
{"id": "2507.23660v1_chunk20", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "ts can be integrated in the following form for the observation update: 0=R\u0000 xi,ωI i,nω\u0001 +R\u0000 xi,aI i,na\u0001 (12)E. Implement Details LiDAR Motion Compensation . When the LiDAR is in motion, the points are sampled at different poses, which can introduce distortions in the data. To mitigate this, we adopt a motion compensation strategy similar to that used in Fast- LIO [12], leveraging filter state information. This approach involves maintaining a history of filter states over a specified period. For each point in the LiDAR frame, we interpolate this historical data to estimate the exact state at the moment the point was captured. By obtaining this state estimate, we can accurately project each point to a common reference frame, specifically the end of the point cloud scan. This process effectively compensates for distortions caused by the sensor’s motion, ensuring that the resulting point cloud is coherent and provides an accurate representation of the environment. Prior Map Construction an"}
{"id": "2507.23660v1_chunk21", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "ely compensates for distortions caused by the sensor’s motion, ensuring that the resulting point cloud is coherent and provides an accurate representation of the environment. Prior Map Construction and Local Map Updates. Constructing a LiDAR point cloud map is essential for high- precision localization systems. Initially, point cloud data often contains noise and redundancy. To address this, we use a simple yet effective interval sampling technique for denoising and downsampling, which improves both data quality and processing efficiency. For the global prior map, we combine the Fast-LIO2 algorithm [33] with high-precision inertial navigation system (INS) data to accurately align point clouds from various poses, ensuring the creation of a consistent global map. In large-scale, dynamic environments, the presence of nu- merous moving objects and structures can cause ghosting and unreliable information. To deal with this issue, we apply post-processing to the global map, removing dynamic "}
{"id": "2507.23660v1_chunk22", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "ronments, the presence of nu- merous moving objects and structures can cause ghosting and unreliable information. To deal with this issue, we apply post-processing to the global map, removing dynamic elements and only retaining static point cloud data to prevent localization drift. In the local dynamic map, we integrate the current frame’s point cloud data using corrected LiDAR poses. Continuous updates from the iESKF filter help miti- gate cumulative localization errors, maintaining the accuracy of the map. Outdated and distant data is also removed to op- timize resource usage and ensure efficient output during local point cloud map registration in LiDAR-inertial odometry. To enhance matching efficiency with both the local dy- namic map and the global prior map, we employ the incre- mental k-d tree (ikd-Tree) data structure [33] for incremental map management. The ikd-Tree efficiently handles point in- sertion, deletion, dynamic rebalancing, and nearest neighbor searches, significantl"}
{"id": "2507.23660v1_chunk23", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "l k-d tree (ikd-Tree) data structure [33] for incremental map management. The ikd-Tree efficiently handles point in- sertion, deletion, dynamic rebalancing, and nearest neighbor searches, significantly improving the speed and accuracy of point cloud registration. IV. EXPERIMENTS To validate the effectiveness of the proposed method, we conduct comprehensive experiments on our private datasets. The experimental setup is detailed in Sec. IV-A, while the localization results are presented in Sec. IV-B. A. Experiment Setup Baselines and Metrics. Our baselines include Light- LOAM [25], DLL [5], FAST-LIO2 [33], and Point-LIO [34], which represent state-of-the-art approaches in LiDAR (a) (b) Fig. 2: Experimental Setup. (a) The IGV , with a size of 15 m ×3 m ×1.7 m. (b) The fused point clouds from two diagonally positioned 16-line LiDARs, where the blue rectangle represents the vehicle body. The green point cloud corresponds to the LiDAR located in the upper-left corner, while the red point clo"}
{"id": "2507.23660v1_chunk24", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "two diagonally positioned 16-line LiDARs, where the blue rectangle represents the vehicle body. The green point cloud corresponds to the LiDAR located in the upper-left corner, while the red point cloud originates from the LiDAR in the lower-right corner. (a) (b) (c) Fig. 3: Point cloud map. (a) Example of the point cloud map without dynamic obstacle removal. (b) Example of the point cloud map with dynamic obstacle removal. (c) The resulting point cloud output with a dimension of 1538 m ×596 m. SLAM, localization, and odometry. We perform comparative experiments on these methods to assess the effectiveness of our proposed approach in various scenarios. To ensure a fair comparison, the results of these systems are obtained using the source code provided by their respective authors, with only minor adjustments made to the input data interfaces to adapt to our dataset. To assess the accuracy of SLAM, we use the Absolute Trajectory Error (ATE) of the pose trajectory as the evaluation metri"}
{"id": "2507.23660v1_chunk25", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "y minor adjustments made to the input data interfaces to adapt to our dataset. To assess the accuracy of SLAM, we use the Absolute Trajectory Error (ATE) of the pose trajectory as the evaluation metric and used the evotoolkit to compare and analyze the localization trajectories of different algorithms. Datasets and Sensor Equipments . To evaluate the perfor- mance of our method in large-scale, unbounded, and dynamic environments, we conduct a detailed comparison with other state-of-the-art algorithms. We select a port environment as the testing scenario and manually created a private compre- hensive dataset. The test area spans approximately 1 million square meters, with the dataset covering a total testing path of 6 km. Data was collected by driving an IGV throughout the port to capture diverse scenarios and conditions. The dataset includes high-quality 16-line LiDAR and 6-axis IMU(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n) (o) Fig. 4: Comparison results of the overall tra"}
{"id": "2507.23660v1_chunk26", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "iverse scenarios and conditions. The dataset includes high-quality 16-line LiDAR and 6-axis IMU(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n) (o) Fig. 4: Comparison results of the overall trajectories, incorporating both translational and rotational components, across various testing datasets. Each column represents the comparison results of different algorithms on each test dataset. (a), (d), (g), (j), and (m) represent the overall trajectories of each algorithm, (b), (e), (h), (k), and (n) represent the translation components of each algorithm, and (c), (f), (i), (l), and (o) represent the rotation components of each algorithm. Due to the excessive errors in DLL [5] and Light-LOAM [25], both methods failed in these scenarios, so we do not present their results. data, with the LiDAR operating at a frequency of 10 Hz and the IMU at 100 Hz. Additionally, a higher-precision sensor fusion method with RTK, operating at 50 Hz, is provided as ground truth for quantitatively evaluati"}
{"id": "2507.23660v1_chunk27", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "DAR operating at a frequency of 10 Hz and the IMU at 100 Hz. Additionally, a higher-precision sensor fusion method with RTK, operating at 50 Hz, is provided as ground truth for quantitatively evaluating the performance of different localization systems. We design experiment trajectories in open areas with few tall mechanical structures to ensure GPS accuracy. To reduce potential LiDAR occlu- sions, the LiDAR data is generated from the fused point clouds of two 16-line LiDARs, which are time-synchronized and distortion-corrected. These LiDARs are strategically mounted at the upper-left and lower-right corners of thetest vehicle to maximize coverage. The test vehicle and the resulting LiDAR point cloud data are illustrated in Fig. 2. Implementation Platform and Hardware . Our dataset evaluation and comparison experiments are implemented in C++ using the Robot Operating System (ROS). The experiments are conducted on a computer equipped with an Intel i7-8750H CPU running at 2.20 G Hz, 32 G"}
{"id": "2507.23660v1_chunk28", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "ion and comparison experiments are implemented in C++ using the Robot Operating System (ROS). The experiments are conducted on a computer equipped with an Intel i7-8750H CPU running at 2.20 G Hz, 32 GB of RAM, a GeForce GTX 1050 Ti GPU, and Ubuntu 20.04 as the operating system. Global Prior Map . The port environment is highly dy- namic, characterized by large IGVs, moving containers, mas-TABLE I: Quantitative comparison for Light-LOAM [25], DLL [5], FAST-LIO2 [33], and Point-LIO [34], and our method on three datasets (corresponding to subfigures (d), (g), and (j) in Fig. 4) Dataset Case Max absolute pose error(m) Mean absolute pose error(m) Max lateral error(m) Mean lateral error(m) Max longitudinal error(m) Mean longitudinal error(m) Fig. 4 (d)DLL 451.693 207.601 231.120 63.017 444.835 100.318 FAST-LIO2 8.229 2.753 8.053 1.879 2.845 0.274 Light-LOAM 182.996 149.814 96.159 68.088 176.481 130.285 Point-LIO 55.524 3.590 5.643 1.563 55.236 1.131 Ours 0.742 0.165 0.236 0.031 0.704 0.030 F"}
{"id": "2507.23660v1_chunk29", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "00.318 FAST-LIO2 8.229 2.753 8.053 1.879 2.845 0.274 Light-LOAM 182.996 149.814 96.159 68.088 176.481 130.285 Point-LIO 55.524 3.590 5.643 1.563 55.236 1.131 Ours 0.742 0.165 0.236 0.031 0.704 0.030 Fig. 4 (g)DLL 444.765 174.058 430.615 122.649 403.745 43.740 FAST-LIO2 10.311 3.922 9.819 1.490 10.263 1.657 Light-LOAM 20.374 17.793 20.029 11.800 20.366 8.228 Point-LIO 39.408 20.741 36.930 14.726 38.253 4.734 Ours 0.927 0.200 0.898 0.066 0.533 0.025 Fig. 4 (j)DLL 140.157 68.741 133.230 22.116 139.585 15.966 FAST-LIO2 2.161 0.921 1.290 0.404 2.158 0.169 Light-LOAM 21.753 10.929 0.063 0.013 21.753 4.573 Point-LIO 17.937 3.934 7.833 0.433 16.435 1.680 Ours 0.553 0.262 0.511 0.116 0.472 0.075 sive gantry cranes, and rail cranes in constant motion. This ever-changing nature causes most environmental features to vary over time. Moreover, due to the port’s unbounded and expansive layout, LiDAR data predominantly captures ground-level information. To create a stable and static global prior map, "}
{"id": "2507.23660v1_chunk30", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "tal features to vary over time. Moreover, due to the port’s unbounded and expansive layout, LiDAR data predominantly captures ground-level information. To create a stable and static global prior map, we conduct extensive validation and fine-tuning in real-world scenarios. During the map construction process, we prioritize generating a point cloud map that primarily focuses on ground features while systematically removing all potentially dynamic objects. This approach ensures reliable and consistent pose constraints for the localization algorithm. The final map covers an area of approximately one million square meters as shown in Fig. 3. B. Localization Results and Comparison Localization Results . Through testing and validation, it is observed that Light-LOAM [25] and DLL [5] fail to operate effectively across all port test datasets, showing significant errors. Consequently, the following analysis focuses on the algorithms that functioned properly: our algorithm, FAST- LIO2 [33], and P"}
{"id": "2507.23660v1_chunk31", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "te effectively across all port test datasets, showing significant errors. Consequently, the following analysis focuses on the algorithms that functioned properly: our algorithm, FAST- LIO2 [33], and Point-LIO [34]. The complete test results are shown in Fig. 4. Overall Trajectory Comparison. The trajectory gener- ated by our algorithm closely aligns with the ground truth (as shown in subfigures (a), (d), (g), (j), and (m) in Fig. 4), with near-complete overlap, demonstrating exceptional accuracy in global localization. In contrast, on more complex paths (e.g., subfigures (a), (c), and (e) in Fig. 4), the trajectories of FAST-LIO2 [33] and Point-LIO [34] exhibit varying degrees of deviation. Notably, Point-LIO [34] shows more significant errors, especially in sharp turns or sections with complex mo- tion, revealing its reduced robustness in dynamic scenarios. Our algorithm, however, maintains superior accuracy even under these challenging conditions, with minimal trajectory deviation, s"}
{"id": "2507.23660v1_chunk32", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "mplex mo- tion, revealing its reduced robustness in dynamic scenarios. Our algorithm, however, maintains superior accuracy even under these challenging conditions, with minimal trajectory deviation, showcasing its strong adaptability and reliability. Translational Comparison. The analysis of the transla- tional component, examining accuracy in the x,y, and z directions, is presented in subfigures (b), (e), (h), (k), and (n) in Fig. 4. In the xandydirections, all algorithms perform well, with their estimated curves closely matching the ground truth, reflecting high translational accuracy. However, in the zdirection (height), FAST-LIO2 [33] and Point-LIO [34] show noticeable deviations, as observed in subfigures (f), (h), and (j) in Fig. 4. In contrast, our algorithm exhibitssignificantly better performance with smaller deviations. This discrepancy may stem from substantial sensor noise in height data or inadequate optimization of height estimation in the competing algorithms. Rotation C"}
{"id": "2507.23660v1_chunk33", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "tter performance with smaller deviations. This discrepancy may stem from substantial sensor noise in height data or inadequate optimization of height estimation in the competing algorithms. Rotation Comparison. The rotational component anal- ysis, focusing on angle estimation around the x,y, and z axes, places particular emphasis on the yaw angle, as shown in subfigures (c), (f), (i), (l), and (o) in Fig. 4. Our algorithm demonstrates superior rotational accuracy across all datasets, with its estimated curves closely following the ground truth. While slight jitter is observed, it remains minimal and does not impact overall performance. In contrast, Point-LIO [34] and FAST-LIO2 [33] exhibit significant yaw estimation jitter in certain datasets (e.g., subfigures (k) and (o) in Fig. 4), especially near the end of the tests (the far-right sections of the figures), where error magnitudes increase considerably. This behavior is likely due to the inability of these algorithms to effectively c"}
{"id": "2507.23660v1_chunk34", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "ially near the end of the tests (the far-right sections of the figures), where error magnitudes increase considerably. This behavior is likely due to the inability of these algorithms to effectively correct accumulated angular errors. Quantitative Comparison. We evaluated the performance of DLL [5], FAST-LIO2 [33], Light-LOAM [25], Point-LIO [34], and our approach on three datasets (Fig. 4 (d), (g), and (j)) using metrics such as maximum and mean absolute pose errors, lateral errors, and longitudinal errors. The results in TABLE I, consistently show that our method outperforms all other algorithms across all datasets and metrics. In Fig. 4 (d), our method achieved a maximum absolute pose error of 0.742 m, significantly lower than DLL [5] (451.693 m). Its mean absolute pose error was 0.165 m, far below FAST- LIO2 [33] (2.753 m) and Point-LIO [34] (3.590 m). Similarly, in Fig. 4 (g), our system maintained the smallest maximum (0.927 m) and mean (0.200 m) pose errors, with minimal lateral"}
{"id": "2507.23660v1_chunk35", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "m, far below FAST- LIO2 [33] (2.753 m) and Point-LIO [34] (3.590 m). Similarly, in Fig. 4 (g), our system maintained the smallest maximum (0.927 m) and mean (0.200 m) pose errors, with minimal lateral and longitudinal errors. In Fig. 4 (j), our algorithm continued to excel, achieving a maximum pose error of 0.553 m and a mean pose error of 0.262 m, both significantly better than the competing algorithms. Besides, DLL [5] and Light-LOAM [25] fail with large errors. Overall, our framework consistently demonstrated the highest accuracy and robustness, with significantly lower errors across all datasets. These results highlight the effec- tiveness of its improved optimization strategies and robust error-handling mechanisms in complex environments.V. CONCLUSION In this paper, we present a robust and precise mapping and localization system designed for changing, dynamic outdoor environments. By tightly coupling a LiDAR-inertial odometry with map-based localization, the system addresses insta"}
{"id": "2507.23660v1_chunk36", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "t and precise mapping and localization system designed for changing, dynamic outdoor environments. By tightly coupling a LiDAR-inertial odometry with map-based localization, the system addresses instability and drift caused by environmental changes, in- tegrating offline global map registration with real-time dy- namic mapping for reliable performance in large-scale sce- narios. At its core, the system employs a multi-sensor fusion framework based on the iESKF, ensuring both robustness and accuracy. Extensive real-world experiments demonstrate that our method consistently outperforms state-of-the-art LiDAR localization algorithms, making it a reliable solution for applications like robotics and autonomous driving. REFERENCES [1] J. Levinson, J. Askeland, J. Becker, J. Dolson, D. Held, S. Kammel, J. Z. Kolter, D. Langer, O. Pink, V . Pratt, M. Sokolsky, G. Stanek, D. Stavens, A. Teichman, M. Werling, and S. Thrun, “Towards Fully Autonomous Driving: Systems and Algorithms,” in 2011 IEEE "}
{"id": "2507.23660v1_chunk37", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "ammel, J. Z. Kolter, D. Langer, O. Pink, V . Pratt, M. Sokolsky, G. Stanek, D. Stavens, A. Teichman, M. Werling, and S. Thrun, “Towards Fully Autonomous Driving: Systems and Algorithms,” in 2011 IEEE Intelligent Vehicles Symposium (IV) , 2011, pp. 163–168. [2] A. Dewan, A. Kumar, H. Singh, V . S. Solanki, and P. Kaur, “Advance- ment in SLAM Techniques and Their Diverse Applications,” in 2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART) , 2023, pp. 387–392. [3] D. Kumar and N. Muhammad, “A Survey on Localization for Au- tonomous Vehicles,” IEEE Access , vol. 11, pp. 115 865–115 883, 2023. [4] Z. Du, S. Ji, and K. Khoshelham, “3-D LiDAR-Based Place Recogni- tion Techniques: A Review of the Past Ten Years,” IEEE Transactions on Instrumentation and Measurement , vol. 73, pp. 1–24, 2024. [5] F. Caballero and L. Merino, “DLL: Direct LIDAR Localization. A Map-based Localization Approach for Aerial Robots,” in 2021 IEEE/RSJ International Conference "}
{"id": "2507.23660v1_chunk38", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "Measurement , vol. 73, pp. 1–24, 2024. [5] F. Caballero and L. Merino, “DLL: Direct LIDAR Localization. A Map-based Localization Approach for Aerial Robots,” in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2021, pp. 5491–5498. [6] B. Peng, H. Xie, and W. Chen, “ROLL: Long-Term Robust LiDAR- based Localization With Temporary Mapping in Changing Environ- ments,” in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2022, pp. 2841–2847. [7] K. Koide, M. Yokozuka, S. Oishi, and A. Banno, “GLIM: 3D Range-inertial Localization and Mapping with GPU-accelerated Scan Matching Factors,” Robotics and Autonomous Systems , vol. 179, p. 104750, 2024. [8] C. Campos, R. Elvira, J. J. G ´omez, J. M. M. Montiel, and J. D. Tard ´os, “ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual- Inertial and Multi-Map SLAM,” IEEE Transactions on Robotics , vol. 37, no. 6, pp. 1874–1890, 2021. [9] Y . Yu, P. Yun, B. Xue, J. Jiao, R. Fan, "}
{"id": "2507.23660v1_chunk39", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "M3: An Accurate Open-Source Library for Visual, Visual- Inertial and Multi-Map SLAM,” IEEE Transactions on Robotics , vol. 37, no. 6, pp. 1874–1890, 2021. [9] Y . Yu, P. Yun, B. Xue, J. Jiao, R. Fan, and M. Liu, “Accurate and Robust Visual Localization System in Large-Scale Appearance- Changing Environments,” IEEE/ASME Transactions on Mechatronics , vol. 27, no. 6, pp. 5222–5232, 2022. [10] H. Yin, X. Xu, S. Lu, X. Chen, R. Xiong, S. Shen, C. Stachniss, and Y . Wang, “A Survey on Global LiDAR Localization: Challenges, Advances and Open Problems,” International Journal of Computer Vision , vol. 132, no. 8, pp. 3139–3171, 2024. [11] B. Xue, X. Yan, J. Wu, J. Cheng, J. Jiao, H. Jiang, R. Fan, M. Liu, and C. Zhang, “Visual-Marker-Based Localization for Flat-Variation Scene,” IEEE Transactions on Instrumentation and Measurement , vol. 73, pp. 1–16, 2024. [12] W. Xu and F. Zhang, “FAST-LIO: A Fast, Robust LiDAR-Inertial Odometry Package by Tightly-Coupled Iterated Kalman Filter,” IEEE Roboti"}
{"id": "2507.23660v1_chunk40", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "on Instrumentation and Measurement , vol. 73, pp. 1–16, 2024. [12] W. Xu and F. Zhang, “FAST-LIO: A Fast, Robust LiDAR-Inertial Odometry Package by Tightly-Coupled Iterated Kalman Filter,” IEEE Robotics and Automation Letters , vol. 6, no. 2, pp. 3317–3324, 2021. [13] N. Rufus, U. K. R. Nair, A. V . S. S. B. Kumar, V . Madiraju, and K. M. Krishna, “SROM: Simple Real-time Odometry and Mapping using LiDAR Data for Autonomous Vehicles,” in 2020 IEEE Intelligent Vehicles Symposium (IV) , 2020, pp. 1867–1872. [14] H. Wang, C. Wang, C.-L. Chen, and L. Xie, “F-LOAM : Fast LiDAR Odometry and Mapping,” in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2021, pp. 4390–4396.[15] U. N. C. on Trade and Development, Review of Maritime Transport 2023 , 2023rd ed. United Nations, 2023. [Online]. Available: https://www.un-ilibrary.org/content/books/9789213584569 [16] T. Shan and B. Englot, “LeGO-LOAM: Lightweight and Ground- Optimized LiDAR Odometry and Mapping on Vari"}
{"id": "2507.23660v1_chunk41", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "Nations, 2023. [Online]. Available: https://www.un-ilibrary.org/content/books/9789213584569 [16] T. Shan and B. Englot, “LeGO-LOAM: Lightweight and Ground- Optimized LiDAR Odometry and Mapping on Variable Terrain,” in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2018, pp. 4758–4765. [17] P. Besl and N. D. McKay, “A Method for Registration of 3-D Shapes,” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 14, no. 2, pp. 239–256, 1992. [18] S. Rusinkiewicz and M. Levoy, “Efficient Variants of the ICP Algo- rithm,” in Proceedings Third International Conference on 3-D Digital Imaging and Modeling , 2001, pp. 145–152. [19] Y . Duan, J. Peng, Y . Zhang, J. Ji, and Y . Zhang, “PFilter: Building Persistent Maps through Feature Filtering for Fast and Accurate LiDAR-based SLAM,” in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2022, pp. 11 087–11 093. [20] Z. Wang, L. Yang, F. Gao, and L. Wang, “FEVO-LOAM"}
{"id": "2507.23660v1_chunk42", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "ast and Accurate LiDAR-based SLAM,” in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2022, pp. 11 087–11 093. [20] Z. Wang, L. Yang, F. Gao, and L. Wang, “FEVO-LOAM: Feature Extraction and Vertical Optimized LiDAR Odometry and Mapping,” IEEE Robotics and Automation Letters , vol. 7, no. 4, pp. 12 086– 12 093, 2022. [21] J. Li, X. Zhang, Y . Zhang, Y . Chang, and K. Zhao, “RF-LOAM: Robust and Fast LiDAR Odometry and Mapping in Urban Dynamic Environment,” IEEE Sensors Journal , vol. 23, no. 23, pp. 29 186– 29 199, 2023. [22] D. Chang, S. Huang, R. Zhang, M. Hu, R. Ding, and X. Qin, “WiCRF2: Multi-Weighted LiDAR Odometry and Mapping With Motion Observability Features,” IEEE Sensors Journal , vol. 23, no. 17, pp. 20 236–20 246, 2023. [23] N. Li, Y . Yao, X. Xu, Y . Peng, Z. Wang, and H. Wei, “An Efficient LiDAR SLAM With Angle-Based Feature Extraction and V oxel-Based Fixed-Lag Smoothing,” IEEE Transactions on Instrumentation and Measurement , vol. 73, p"}
{"id": "2507.23660v1_chunk43", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "Xu, Y . Peng, Z. Wang, and H. Wei, “An Efficient LiDAR SLAM With Angle-Based Feature Extraction and V oxel-Based Fixed-Lag Smoothing,” IEEE Transactions on Instrumentation and Measurement , vol. 73, pp. 1–13, 2024. [24] Y . Su, S. Zhang, C. Zhang, and Y . Wu, “A 3D LiDAR Odometry and Mapping Method Based on Clustering-Directed Feature Point,” IEEE Transactions on Vehicular Technology , vol. 73, no. 12, pp. 18 391– 18 401, 2024. [25] S. Yi, Y . Lyu, L. Hua, Q. Pan, and C. Zhao, “Light-LOAM: A Lightweight LiDAR Odometry and Mapping Based on Graph- Matching,” IEEE Robotics and Automation Letters , vol. 9, no. 4, pp. 3219–3226, 2024. [26] D. Rozenberszki and A. L. Majdik, “LOL: LiDAR-only Odometry and Localization in 3D Point Cloud Maps,” in 2020 IEEE International Conference on Robotics and Automation (ICRA) , 2020, pp. 4379– 4385. [27] Y . Wu, C. Zhao, and Y . Lyu, “DMLL: Differential-Map-Aided LiDAR- Based Localization,” IEEE Transactions on Instrumentation and Mea- surement , vol. 72, "}
{"id": "2507.23660v1_chunk44", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "utomation (ICRA) , 2020, pp. 4379– 4385. [27] Y . Wu, C. Zhao, and Y . Lyu, “DMLL: Differential-Map-Aided LiDAR- Based Localization,” IEEE Transactions on Instrumentation and Mea- surement , vol. 72, pp. 1–14, 2023. [28] A. Thakur and P. Rajalakshmi, “LiDAR-Based Optimized Normal Distribution Transform Localization on 3-D Map for Autonomous Navigation,” IEEE Open Journal of Instrumentation and Measurement , vol. 3, pp. 1–11, 2024. [29] Y . Feng, Z. Jiang, Y . Shi, Y . Feng, X. Chen, H. Zhao, and G. Zhou, “Block-Map-Based Localization in Large-Scale Environment,” 2024. [Online]. Available: https://arxiv.org/abs/2404.18192 [30] C. Qin, H. Ye, C. E. Pranata, J. Han, S. Zhang, and M. Liu, “LINS: A Lidar-Inertial State Estimator for Robust and Efficient Navigation,” in2020 IEEE International Conference on Robotics and Automation (ICRA) , 2020, pp. 8899–8906. [31] Y . Wu, T. Guadagnino, L. Wiesmann, L. Klingbeil, C. Stachniss, and H. Kuhlmann, “LIO-EKF: High Frequency LiDAR-Inertial Odometry"}
{"id": "2507.23660v1_chunk45", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": "nference on Robotics and Automation (ICRA) , 2020, pp. 8899–8906. [31] Y . Wu, T. Guadagnino, L. Wiesmann, L. Klingbeil, C. Stachniss, and H. Kuhlmann, “LIO-EKF: High Frequency LiDAR-Inertial Odometry using Extended Kalman Filters,” in 2024 IEEE International Confer- ence on Robotics and Automation (ICRA) , 2024, pp. 13 741–13 747. [32] T. Shan, B. Englot, D. Meyers, W. Wang, C. Ratti, and D. Rus, “LIO- SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping,” in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2020, pp. 5135–5142. [33] W. Xu, Y . Cai, D. He, J. Lin, and F. Zhang, “FAST-LIO2: Fast Direct LiDAR-Inertial Odometry,” IEEE Transactions on Robotics , vol. 38, no. 4, pp. 2053–2073, 2022. [34] D. He, W. Xu, N. Chen, F. Kong, C. Yuan, and F. Zhang, “Point- LIO: Robust High-Bandwidth Light Detection and Ranging Inertial Odometry,” Advanced Intelligent Systems , vol. 5, no. 7, p. 2200459, 2023."}
{"id": "2507.23660v1_chunk46", "paper_id": "2507.23660v1", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "text": " and F. Zhang, “Point- LIO: Robust High-Bandwidth Light Detection and Ranging Inertial Odometry,” Advanced Intelligent Systems , vol. 5, no. 7, p. 2200459, 2023."}
{"id": "2507.23629v1_chunk0", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar Equipped Underwater Robot Teams with Object Graph Matching Yewei Huang1∗, John McConnell2∗, Xi Lin1and Brendan Englot1 Abstract —We present DRACo-SLAM2, a distributed SLAM framework for underwater robot teams equipped with multibeam imaging sonar. This framework improves upon the original DRACo-SLAM by introducing a novel representation of sonar maps as object graphs and utilizing object graph matching to achieve time-efficient inter-robot loop closure detection without relying on prior geometric information. To better-accommodate the needs and characteristics of underwater scan matching, we propose incremental Group-wise Consistent Measurement Set Maximization (GCM), a modification of Pairwise Consistent Measurement Set Maximization (PCM), which effectively handles scenarios where nearby inter-robot loop closures share similar registration errors. The proposed approach is validated through extensiv"}
{"id": "2507.23629v1_chunk1", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "nt Measurement Set Maximization (PCM), which effectively handles scenarios where nearby inter-robot loop closures share similar registration errors. The proposed approach is validated through extensive comparative analyses on simulated and real-world datasets. I. Introduction Exploring the ocean has long been one of humanity’s greatest ambitions, and robust and efficient simultaneous local- ization and mapping (SLAM) algorithms can provide a corner- stone for achieving this efficiently with large teams of robots. Multi-robot SLAM is also valuable for executing other long- term and large-scale marine missions, such as constructing and maintaining offshore energy infrastructure, monitoring water quality over extended periods [1], and conducting targeted sampling near the ocean floor [2]. Although a variety of single-robot SLAM algorithms uti- lizing sonar [3], [4], cameras [5], or their combination [6] have been developed, there are still significant challenges facing multi-robot SLAM in"}
{"id": "2507.23629v1_chunk2", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "hough a variety of single-robot SLAM algorithms uti- lizing sonar [3], [4], cameras [5], or their combination [6] have been developed, there are still significant challenges facing multi-robot SLAM in underwater environments. Chief among these is the limited bandwidth available for wireless communication. For example, the HS underwater acoustic modems 1, commonly used for subsea communication, provide only 62.5 kbps even for short-range transmissions. Therefore, data efficiency is essential for underwater multi-robot SLAM. Although multi-robot SLAM has been applied in underwater environments using cameras [7], [8], the effectiveness of cameras is limited to clear water conditions [9], which are un- common in sandy, near-coastal areas or regions with ongoing construction activities. Thus, a need remains for sonar-based robot teams in many real-world applications. Unlike cameras, which provide clear, high-resolution images, sonar images * Equal contribution. 1Y. Huang, X. Lin and B. Engl"}
{"id": "2507.23629v1_chunk3", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": " need remains for sonar-based robot teams in many real-world applications. Unlike cameras, which provide clear, high-resolution images, sonar images * Equal contribution. 1Y. Huang, X. Lin and B. Englot are with Stevens Institute of Technol- ogy, Hoboken, NJ, USA, {yhuang85, xlin26, benglot }@stevens.edu . 2J. McConnell is with the U.S. Naval Academy, Annapolis, MD, USA, jmcconne@usna.edu . This research was supported in part by NSF Grant IIS- 1652064 and ONR Grant N00014-24-1-2522. 1https://www.evologics.com/acoustic-modem/hs Fig. 1: Example DRACo-SLAM2 result with real sonar data. Optimized trajectories and point clouds from three robots using the proposed DRACo-SLAM2 on a dataset collected at the U.S. Merchant Marine Academy, King’s Point, NY, aligned with a satellite image. Inter-robot measurement constraints are shown in purple. are affected by acoustic noise and have limited resolution. Registering sonar images without prior geometric information is consequently more difficult. T"}
{"id": "2507.23629v1_chunk4", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": " measurement constraints are shown in purple. are affected by acoustic noise and have limited resolution. Registering sonar images without prior geometric information is consequently more difficult. Therefore, effective inter-robot loop closure detection remains a key challenge in underwater sonar-based SLAM systems. We demonstrated the feasibility of performing inter-robot loop closure detection and multi-robot SLAM using only sonar images in DRACo-SLAM [10]. However, the global Iterative Closest Point (Go-ICP) algorithm [11] used in that system is computationally expensive, limiting the overall system to op- erate at relatively low frequencies. In this paper, we introduce DRACo-SLAM2, an enhanced multi-robot SLAM system for underwater environments that achieves greater computational efficiency through object graph matching. DRACo-SLAM2 constructs an object map of the surround- ing environment based on pose estimation from local sonar SLAM, which is then shared among robot team member"}
{"id": "2507.23629v1_chunk5", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "ciency through object graph matching. DRACo-SLAM2 constructs an object map of the surround- ing environment based on pose estimation from local sonar SLAM, which is then shared among robot team members for object graph matching. Relative transformations between object maps serve as initial guesses for ICP registration, elim-arXiv:2507.23629v1 [cs.RO] 31 Jul 2025inating the need for Go-ICP in this approach and significantly reducing computation time. To detect outliers among potential inter-robot loop closures, we propose and employ Group-wise Consistent Measurement Set Maximization (GCM) in lieu of PCM [12]. Our contributions are summarized as follows: •We present the first application of object graph matching for inter-robot loop closure detection using sonar images in underwater environments. •The proposed object graph matching provides a reliable initial guess for refined ICP scan registration, enabling the detection of more inter-robot loop closures in a computationally efficient m"}
{"id": "2507.23629v1_chunk6", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "ents. •The proposed object graph matching provides a reliable initial guess for refined ICP scan registration, enabling the detection of more inter-robot loop closures in a computationally efficient manner. •We introduce the Group-wise Consistent Measurement Set Maximization (GCM) algorithm for robust loop closure selection, targeting cases where multiple loop closures exhibit similar registration errors. Our code for the proposed framework and the datasets described in this paper is publicly released 2. The subsequent sections of this paper are organized as follows: Sec. II provides a review of the background literature, followed by a detailed description of our proposed multi-robot SLAM algorithm in Sec. III. Experimental results are presented in Sec. IV, and the paper concludes with a summary in Sec. V. II. Related Works One key challenge that makes the underwater multi-robot SLAM problem unique, compared to other SLAM problems, is the limited communication bandwidth resulting from "}
{"id": "2507.23629v1_chunk7", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "ummary in Sec. V. II. Related Works One key challenge that makes the underwater multi-robot SLAM problem unique, compared to other SLAM problems, is the limited communication bandwidth resulting from the marine environment. To address this constraint, Bonin-Font et al. [7] and MAM3SLAM [13] propose centralized algorithms, where a server handles all inter-robot data association and optimization under the assumption of “a powerful server without communication restrictions”. However, these methods are only tested on small-range datasets due to the communication bandwidth degradation as robots move farther from the server. Zhang et al. [14] avoids the communication problem by employing a distributed local SLAM data collection approach combined with a GMRBnB-based map registration strategy, similar to the multi-session SLAM frameworks [15], [16]. While these methods are effective for bathymetric mapping tasks, they are less suitable for real-time decision-making applications, such as active"}
{"id": "2507.23629v1_chunk8", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "ilar to the multi-session SLAM frameworks [15], [16]. While these methods are effective for bathymetric mapping tasks, they are less suitable for real-time decision-making applications, such as active planning and exploration. Paull et al. [17] consider a scenario where robots observe one another and share their landmark observations. In their approach, both landmark and robot states are optimized in a distributed manner across team members using a graph- based representation. ¨Ozkahraman et al. [18] explore a similar situation, intentionally designing robot rendezvous strategies to achieve more accurate localization in environments where landmarks are absent. However, achieving robot rendezvous is challenging, particularly for long-term missions, due to factors such as the vastness of marine environments, unpredictable oceanic currents, and time synchronization issues. 2https://github.com/RobustFieldAutonomyLab/DRACO-SLAM2In this paper, we adopt the communication assumptions de- scrib"}
{"id": "2507.23629v1_chunk9", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "ine environments, unpredictable oceanic currents, and time synchronization issues. 2https://github.com/RobustFieldAutonomyLab/DRACO-SLAM2In this paper, we adopt the communication assumptions de- scribed in DRACo-SLAM [10], where robots can communicate with each other (at low bandwidth, using wireless acoustic communication) when within a relatively long distance range. This assumption enables the real-time estimation of both self and neighbor robot states across a relatively large environment. Another key consideration is the inter-robot data association or map merging strategy for multi-robot teams. Most approaches rely on extracting visual features from camera or sonar images. Bryson et al. [15] extract SIFT features from camera images and perform inter-robot structure-from-motion (SfM). Bonin-Font et al. [7] also extract SIFT features from camera images but utilize HALOC as the feature descriptor. MAM3SLAM [13] extracts ORB features and associates keyframes using DBoW2. Zhang et al."}
{"id": "2507.23629v1_chunk10", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": " Bonin-Font et al. [7] also extract SIFT features from camera images but utilize HALOC as the feature descriptor. MAM3SLAM [13] extracts ORB features and associates keyframes using DBoW2. Zhang et al. [14] and Gaspar et al. [19] extract ORB features from sonar images and adopt a similar visual vocabulary strategy. While these feature-based methods are straightforward, their high-dimensional feature descriptors place significant demands on communication bandwidth. To alleviate the communication bandwidth burden caused by high-dimensional feature descriptors, some methods use compact features to represent entire visual or sonar images. Paull et al. [17] extract mine-like objects from sonar images and exchange their positions among team members. Santos et al. [20] extract objects from sonar images and utilize scene graphs to describe and match sonar data. DRACo-SLAM [10] and Sonar-context [21] represent sonar images with compact sonar descriptors for efficient data exchange and loop closu"}
{"id": "2507.23629v1_chunk11", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "mages and utilize scene graphs to describe and match sonar data. DRACo-SLAM [10] and Sonar-context [21] represent sonar images with compact sonar descriptors for efficient data exchange and loop closure candidate detection. While compact features significantly reduce the bandwidth required for inter-robot data exchange, they can lead to errors in regions with repeating patterns (i.e., perceptual aliasing). Some methods perform data association using features ex- tracted from a larger submap to further mitigate perceptual aliasing. Wang et al. [1] apply the DBScan algorithm to cluster and describe the shapes of oil spills in the ocean. Deng et al. [22] utilize a deep learning-based encoder-decoder structure for point cloud transmission. Qi et al. [23] employ a terrain description feature to characterize submaps and match them using a genetic algorithm (GA). However, the GA process is computationally expensive and challenging to tune. Other methods focus on robust loop closure outlier de"}
{"id": "2507.23629v1_chunk12", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": " characterize submaps and match them using a genetic algorithm (GA). However, the GA process is computationally expensive and challenging to tune. Other methods focus on robust loop closure outlier detection algorithms. Most approaches use a RANSAC strategy for map merging. DRACo-SLAM [10] uses pairwise consistent measurement set maximization (PCM) [12] to reduce outliers introduced by repeating patterns. However, despite its emphasis of measurement consistency, PCM is not robust to outliers characterized by high data similarity. To address this issue, Do et al. [24] propose a probabilistic approach specifically tailored for underwater scenarios to robustly reject outliers by seeking both consistency and similarity. Inspired by these methods, we propose DRACo-SLAM2, a multi-robot SLAM framework that uses object graph matching. We cluster objects from the local SLAM map using the DBSCAN clustering algorithm and initially exchange only the object map with team members to reduce computati"}
{"id": "2507.23629v1_chunk13", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "ework that uses object graph matching. We cluster objects from the local SLAM map using the DBSCAN clustering algorithm and initially exchange only the object map with team members to reduce computationalFig. 2: Overview of DRACo-SLAM2 Architecture. Sonar images are captured by a horizontally-oriented sonar. Each robot’s object map is clustered using DBSCAN. The local robot receives the neighboring robot’s object map, aligns it to its local map using graph matching, and requests scans for ICP registration with the graph matching transformation as an initial guess. Inter-robot loop closures are then added to the pose graph for the two-step pose graph optimization (PGO). The local robot’s trajectory and object vertices are shown in blue, while the neighboring robot’s data is in green. Dashed lines indicate detected correspondences between objects in different maps. burden and avoid misaligned loop closures caused by repeating patterns. Additionally, a group-wise consistent measurement se"}
{"id": "2507.23629v1_chunk14", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": " lines indicate detected correspondences between objects in different maps. burden and avoid misaligned loop closures caused by repeating patterns. Additionally, a group-wise consistent measurement set maximization (GCM) is introduced to enhance outlier detection in inter-robot data association, by fully utilizing the information obtained from the object graph matching. III. DRACo-SLAM2 Algorithm In this section, we provide a comprehensive introduction to DRACo-SLAM2, the proposed multi-robot SLAM framework utilizing object graph matching. Fig. 2 presents the complete pipeline of the proposed framework. We define the local robot as robot𝛼. The inter-robot data association process begins by clustering the latest point-cloud map, M𝛼, generated by the local SLAM algorithm into an object map, V𝛼. The object map is then shared among the robot team. When the local robot receives an object map, V𝛽, from a neighboring robot 𝛽, the object maps are matched through object graph matching. A detail"}
{"id": "2507.23629v1_chunk15", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "V𝛼. The object map is then shared among the robot team. When the local robot receives an object map, V𝛽, from a neighboring robot 𝛽, the object maps are matched through object graph matching. A detailed description of the object graph matching process is provided in Sec. III-B. Feature points from the scans corresponding to overlapping objects are then requested from robot 𝛽for performing scan registration. The transformation T𝛽 𝛼, representing the coordinate transformation fromV𝛽toV𝛼, serves as the initial estimate for scan registration, as detailed in Sec. III-C. After registration, incremental group-wise consistent mea- surement set maximization (GCM) is applied to ensure robust- ness by mitigating errors caused by perceptual aliasing. All loop closures that pass the GCM process are subsequently added to the factor graph. A two-step inter-robot pose graph optimization is then performed to achieve a stable result. The detailed process is presented in Sec. III-E.A. Local SLAM, Point-c"}
{"id": "2507.23629v1_chunk16", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "sequently added to the factor graph. A two-step inter-robot pose graph optimization is then performed to achieve a stable result. The detailed process is presented in Sec. III-E.A. Local SLAM, Point-cloud Map and Object Map 1) Local SLAM: We choose Bruce-SLAM [25] as our local SLAM framework. Bruce-SLAM is a sonar SLAM algorithm that uses data exclusively from forward-looking sonar (FLS) and vehicle dead-reckoning measurements. However, the flexibility of our proposed DRACo-SLAM2 system allows for seamless integration with any local sonar SLAM algorithm. As a graph-based SLAM algorithm, Bruce SLAM solves the SLAM optimization problem as a maximum a posteriori (MAP) estimation problem [26]: X∗ 𝛼=arg max X𝛼𝑃(X𝛼|Z𝛼), (1) Z𝛼=Z𝑜 𝛼∪Z𝑠 𝛼∪Z𝑙 𝛼, (2) whereX𝛼represents the states of robot 𝛼’s entire history, andZ𝛼denotes the set of observations, including odometry measurements from the vehicle’s dead reckoning system ( Z𝑜 𝛼), sequential scan matching measurements from neighboring sonar images (Z𝑠"}
{"id": "2507.23629v1_chunk17", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "history, andZ𝛼denotes the set of observations, including odometry measurements from the vehicle’s dead reckoning system ( Z𝑜 𝛼), sequential scan matching measurements from neighboring sonar images (Z𝑠 𝛼), and intra-robot loop closure measurements ( Z𝑙 𝛼). 2) Point-cloud map: To perform sequential scan matching and intra-robot data association, feature points are extracted from sonar images using SOCA-CFAR [27], a variant of the Constant False Alarm Rate (CFAR) technique [28]. Let the set of feature points detected from the sonar image captured at timestamp𝑖by robot𝛼be denoted asF𝛼𝑖. These points are transformed from the sensor frame into the local frame of robot 𝛼, with state x𝛼𝑖∈X 𝛼, and are represented as 𝛼F𝛼𝑖. A local point-cloud map for robot 𝛼, shown as black points in Fig. 2, is thus defined asM𝛼={𝛼F𝛼𝑖|𝑖∈[0,𝑡]}, where𝑡represents the current timestamp. 3) Object map: The DBSCAN clustering algorithm [29] is applied to cluster objects from the latest local point-cloudmap,M𝛼. For eac"}
{"id": "2507.23629v1_chunk18", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": " defined asM𝛼={𝛼F𝛼𝑖|𝑖∈[0,𝑡]}, where𝑡represents the current timestamp. 3) Object map: The DBSCAN clustering algorithm [29] is applied to cluster objects from the latest local point-cloudmap,M𝛼. For each cluster, a bounding rectangle is computed based on the positions of the points in the cluster. The bounding rectangles of local objects are shown in blue in Fig. 2. Each object, denoted as v𝛼𝑖, is described by the center coordinates (𝑥𝛼𝑖,𝑦𝛼𝑖)and the dimensions (length and breadth) (𝑙𝛼𝑖,𝑏𝛼𝑖) of its bounding rectangle. To filter out noise misidentified as objects, only clusters with a number of points 𝑛𝑝> 𝑛 minand a dimension max(𝑙𝛼𝑖,𝑏𝛼𝑖)> 𝑑 minare accepted. The set of all accepted objects forms the object map, V𝛼. B. Object Graph Construction and Matching When the object map V𝛽is received from the neighboring robot𝛽, object graphG𝛽=(V𝛽,E𝛽)is constructed using V𝛽 as vertices in the graph. G𝛽is a directed complete graph, and E𝛽is further defined as: E𝛽={e𝑖 𝑗=(v𝑖,v𝑗,𝑤𝑖 𝑗)|v𝑖,v𝑗∈V 𝛽}, (3) whe"}
{"id": "2507.23629v1_chunk19", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "m the neighboring robot𝛽, object graphG𝛽=(V𝛽,E𝛽)is constructed using V𝛽 as vertices in the graph. G𝛽is a directed complete graph, and E𝛽is further defined as: E𝛽={e𝑖 𝑗=(v𝑖,v𝑗,𝑤𝑖 𝑗)|v𝑖,v𝑗∈V 𝛽}, (3) where𝑤𝑖 𝑗is the Euclidean distance between the centers of objects v𝑖andv𝑗. A local object graph G𝛼is constructed in the same manner. Thus, the object map matching problem is converted into a bipartite graph matching problem between G𝛼 andG𝛽. Inspired by SemanticLoop [30], we formulate this bipartite graph matching problem as a Quadratic Assignment Problem (QAP): A∗ 𝑣=arg max A𝑣∑︁ e𝑖 𝑗∈E𝛼∑︁ e𝑘𝑙∈E𝛽𝑎𝑖𝑘·𝑎𝑗𝑙·𝑢(e𝑖 𝑗,e𝑘𝑙),(4) =arg max A𝑣\u0002 vec(A𝑣)⊤U𝑒vec(A𝑣)\u0003 , (5) 𝑢(e𝑖 𝑗,e𝑘𝑙)=exp\u0000−𝜇|𝑤𝑖 𝑗−𝑤𝑘𝑙|−|𝑙𝑖−𝑙𝑘|−|𝑏𝑖−𝑏𝑘|\u0001.(6) LetA𝑣be the boolean vertex assignment matrix with size 𝑛𝛼× 𝑛𝛽, where𝑛𝛼and𝑛𝛽denote the numbers of vertices in G𝛼and G𝛽, respectively. Eq. (5) provides a vectorized reformulation of Eq. (4), where vec (A𝑣)represents the vectorization of A𝑣. The utility matrix U𝑒contains the utility function 𝑢("}
{"id": "2507.23629v1_chunk20", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "s of vertices in G𝛼and G𝛽, respectively. Eq. (5) provides a vectorized reformulation of Eq. (4), where vec (A𝑣)represents the vectorization of A𝑣. The utility matrix U𝑒contains the utility function 𝑢(·)as its elements, evaluating the similarity between edge pairs. One key difference between our method and SemanticLoop [30] is the calculation of the utility function 𝑢(·). While SemanticLoop considers the semantic classes of objects and the Euclidean distances between their centers, our approach compares object dimensions and corresponding edge weights, with𝜇=4 serving as a scaling factor. We add further constraints to ensure that each vertex in G𝛼matches with at most one vertex inG𝛽: ∀𝑖≤𝑛𝛼,𝑛𝛽∑︁ 𝑗=1𝑎𝑖 𝑗≤1,∀𝑗≤𝑛𝛽,𝑛𝛼∑︁ 𝑖=1𝑎𝑖 𝑗≤1. (7) We relax the boolean constraints on A𝑣, such that 0≤𝑎𝑖 𝑗≤1, yielding A𝑒, which represents a relaxed version of A𝑣. This relaxation allows Eq. (4) to be solved using the spectral method [28]: U𝑒vec(A𝑒)=𝜆maxvec(A𝑒). (8) The optimized solution, vec (A𝑒), is the ei"}
{"id": "2507.23629v1_chunk21", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "ielding A𝑒, which represents a relaxed version of A𝑣. This relaxation allows Eq. (4) to be solved using the spectral method [28]: U𝑒vec(A𝑒)=𝜆maxvec(A𝑒). (8) The optimized solution, vec (A𝑒), is the eigenvector correspond- ing to the largest eigenvalue 𝜆maxofU𝑒.Next, A𝑒, which serves as a vertex similarity weight matrix, can be used to compute A𝑣as follows: A∗ 𝑣=arg min A𝑣𝑛𝛼∑︁ 𝑖=1𝑛𝛽∑︁ 𝑗=1(−𝑐𝑖 𝑗·𝑎𝑖 𝑗), (9) where𝑐𝑖 𝑗denotes the respective element from A𝑒. We solve this Linear Assignment Problem (LAP) using the Jonker-Volgenant algorithm [31]. C. Scan Registration When more than three matched object pairs are obtained from the graph matching process in Sec. III-B, the coordinate transformation T𝛽 𝛼fromV𝛽toV𝛼is computed using affine transformation estimation. To ensure robustness, we apply RANSAC and accept the transformation only if it has more than four inliers. With the transformation T𝛽 𝛼estimated from object maps as geometric priors, we apply ICP registration in a manner similar to its"}
{"id": "2507.23629v1_chunk22", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "AC and accept the transformation only if it has more than four inliers. With the transformation T𝛽 𝛼estimated from object maps as geometric priors, we apply ICP registration in a manner similar to its application in single-robot SLAM [25]. We use a sliding window for the target cloud to improve the registration results. Fig. 3 illustrates the source and target clouds before and after ICP registration, demonstrating that the initial guess provided by object graph matching is sufficient for successful ICP registration. Fig. 3: Graph matching-based ICP registration. The source cloud (orange) is roughly transformed into the local robot coordinate system using the transformation estimated from object graph matching. It is then aligned with the target cloud (black) using ICP with a sliding window of size 3. The registered source cloud is shown in green. D. Group-wise Consistent Measurement Set Maximization A challenging aspect of scan registration is the fact that sepa- rate pairs of scans l"}
{"id": "2507.23629v1_chunk23", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "w of size 3. The registered source cloud is shown in green. D. Group-wise Consistent Measurement Set Maximization A challenging aspect of scan registration is the fact that sepa- rate pairs of scans located in close proximity to one another can give rise to similar registration errors, as illustrated in Figure 4. For sonar scan registration, the overlap percentage between the source and target point clouds is an intuitive indicator of quality. However, as we will discuss in Section IV-A, even a high overlap ratio does not always guarantee minimal registration error. While this is acceptable when using a global PCM, it can lead to errors when only incremental PCM is applied. To address these challenges, we propose a modified approach that we denote Group-wise Consistent Set Measurement Maximization (GCM), inspired by PCM.Fig. 4: Motivation for use of GCM. A series of three inter-robot loop closures, in the same region of the environment (from our USMMA dataset depicted in Fig. 1), are i"}
{"id": "2507.23629v1_chunk24", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "mization (GCM), inspired by PCM.Fig. 4: Motivation for use of GCM. A series of three inter-robot loop closures, in the same region of the environment (from our USMMA dataset depicted in Fig. 1), are impacted by similar point cloud registration errors. Each erroneous registration result is shown in green, with ground truth shown in red. In PCM [12], the consistency of observations between a pair of robots is considered, as illustrated in Fig. 5(a). For two inter- robot loop closure observations, z𝛽𝑖𝛼𝑘andz𝛽𝑗 𝛼𝑙, involving robot 𝛼 and robot𝛽, the method evaluates their consistency using: 𝐶(z𝛽𝑖𝛼𝑘,z𝛽𝑗 𝛼𝑙)= (z𝛽𝑖𝛼𝑘)−1·ˆx𝛽𝑖 𝛽𝑗·z𝛽𝑗 𝛼𝑙·ˆx𝛼𝑙𝛼𝑘 . (10) Where ˆx𝛽𝑖 𝛽𝑗andˆx𝛼𝑙𝛼𝑘are the relative state estimations marginal- ized from the factor graph optimization of local SLAM. In the proposed GCM, we consider the consistency of observations among a group of robots (Fig. 5(b)). For two inter robot loop closure observations, z𝛽𝑖𝛼𝑘between local robot 𝛼and local robot𝛽andz𝛾𝑗 𝛼𝑙between robot 𝛼and robot𝛾, we "}
{"id": "2507.23629v1_chunk25", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "er the consistency of observations among a group of robots (Fig. 5(b)). For two inter robot loop closure observations, z𝛽𝑖𝛼𝑘between local robot 𝛼and local robot𝛽andz𝛾𝑗 𝛼𝑙between robot 𝛼and robot𝛾, we perform: 𝐶(z𝛽𝑖𝛼𝑘,z𝛾𝑗 𝛼𝑙)= (z𝛽𝑖𝛼𝑘)−1·ˆx𝛽𝑖𝛾𝑗·z𝛾𝑗 𝛼𝑙·ˆx𝛼𝑙𝛼𝑘 . (11) Robot𝛼is considered the local robot, so ˆx𝛼𝑙𝛼𝑘can be obtained by marginalizing the factor graph optimization of local SLAM. We further expand ˆx𝛾𝑙 𝛽𝑘: ˆx𝛽𝑖𝛾𝑗=(ˆx𝛼0 𝛽𝑖)−1·ˆx𝛼0𝛾𝑙, (12) =(ˆx𝛽0 𝛽𝑖)−1·ˆx𝛽0𝛼0·(ˆx𝛾0𝛼0)−1·ˆx𝛾0𝛾𝑗. (13) The relative state estimations ˆx𝛾0𝛾𝑙andˆx𝛽0 𝛽𝑘can be marginalized from the local factor graph optimization of neighboring robots, respectively. The relative state estimations ˆ 𝑥𝛽0𝛼0and ˆ𝑥𝛾0𝛼0are the optimized map coordinate transformations obtained from historically accepted loop closures. E. Inter-robot PGO LetR={𝛼,𝛽,𝛾,...}represent the set of robots in the team. Similar to the formulation in Eq. (1), the inter-robot pose graph optimization for the entire robot team can be expressed as: X∗ R=arg max X"}
{"id": "2507.23629v1_chunk26", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "GO LetR={𝛼,𝛽,𝛾,...}represent the set of robots in the team. Similar to the formulation in Eq. (1), the inter-robot pose graph optimization for the entire robot team can be expressed as: X∗ R=arg max X∗ R𝑃(X∗ R|ZR), (14) ZR=Zlocal R∪Zinter R. (15) (a) PCM (b) GCM Fig. 5: Illustration comparing PCM and the proposed GCM . Circles represent pose estimations, arrows indicate measurements, and purple arrows highlight inter-robot loop closure measurements. Here,XR={X𝑖|𝑖∈R} represents the states for all robots in the team. The set of observations for the entire team is denoted by: ZR=Zlocal R∪Zinter R, (16) whereZlocal R={Z𝑖|𝑖∈R} represents the local observations from each robot, and Zinter R={Z𝑖, 𝑗|𝑖,𝑗∈R,𝑖≠𝑗}represents the inter-robot loop closures. Although full PGO in Eq. (14) can yield good results by leveraging all available local and global information, it involves solving a large-scale optimization problem. This process can require significant time to converge, particularly when the ini"}
{"id": "2507.23629v1_chunk27", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "sults by leveraging all available local and global information, it involves solving a large-scale optimization problem. This process can require significant time to converge, particularly when the initial estimate lacks accuracy or when the number of robots in the team grows. To address this challenge, we apply a two-step approach combining global and local PGO, beginning with the local optimization : X∗ 𝛼,X∗ 𝛼,R=arg max X𝛼,X𝛼,R𝑃(X𝛼,X𝛼,R|Z𝛼,Z𝛼,R). (17) Here,Z𝛼,R=Ð 𝑖∈R𝑖≠𝛼Z𝛼,𝑖represents the inter-robot loop closure observations between the local robot 𝛼and all its neigh- boring robots.X𝛼,Rdenotes the states from the neighboring robots that share inter-robot loop closures with the local robot. Next, we proceed with the global optimization for neighbor- ing robots involved in inter-robot data association. Consider a neighboring robot 𝛽that shares inter-robot loop closures Z𝛼,𝛽⊂Z 𝛼,Rwith the local robot 𝛼. The states of 𝛽can be optimized as follows: X∗ 𝛽=arg max X𝛽𝑃(X𝛽|ˆX𝑜 𝛽,X∗ 𝛼,𝛽), (18) w"}
{"id": "2507.23629v1_chunk28", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "association. Consider a neighboring robot 𝛽that shares inter-robot loop closures Z𝛼,𝛽⊂Z 𝛼,Rwith the local robot 𝛼. The states of 𝛽can be optimized as follows: X∗ 𝛽=arg max X𝛽𝑃(X𝛽|ˆX𝑜 𝛽,X∗ 𝛼,𝛽), (18) whereX∗ 𝛼,𝛽⊂X∗ 𝛼,Rrepresents the optimized subset of historical states of robot 𝛽, obtained from Eq. (17), that are associated with robot𝛼, and ˆX𝑜 𝛽represents the estimated relative states marginalized from the local SLAM of the neighboring robot 𝛽. All initial guesses for both local and global PGO are obtained by applying map coordinate transformations based on object graph matching. Inter-robot loop closures are added to the factor graph using a robust noise model to ensure accurate and reliable association. By adopting this two-step PGO approach, we enable faster convergence of the factor graph optimization. This is achieved by updating only a subset of the information at each step, rather than recalculating the entiregraph, which reduces computational overhead and accelerates the optim"}
{"id": "2507.23629v1_chunk29", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "raph optimization. This is achieved by updating only a subset of the information at each step, rather than recalculating the entiregraph, which reduces computational overhead and accelerates the optimization process. F. Communication In this section, we summarize the communication overhead between the local robot and its neighboring robots. As depicted in Fig. 2, at each sonar timestep, the object map is shared with the robot team as an objects message. Additionally, pose estimations are incrementally transmitted to neighboring robots. To optimize the use of communication bandwidth, updates to the historical state estimations for neighboring robots are only performed when significant changes result from loop closures. Once the graphs are matched, the sonar scans associated with the matched objects are requested from the neighboring robots. After the detection of inter-robot loop closures, this information is also shared with the robot neighbors. IV. Experiments and Results We evaluate "}
{"id": "2507.23629v1_chunk30", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": " objects are requested from the neighboring robots. After the detection of inter-robot loop closures, this information is also shared with the robot neighbors. IV. Experiments and Results We evaluate DRACo-SLAM2 using both real-world and simulated data. The real-world data, shown in Fig. 1, was collected using our customized BlueROV2-Heavy (illustrated in Fig. 2) at the U.S. Merchant Marine Academy (USMMA), King’s Point, NY. Details of the BlueROV2 configuration can be found in our previous work [10]. For this study, we utilize the robot’s horizontally-oriented Oculus M750d sonar, along with a Rowe SeaPilot DVL and a VectorNav VN100 MEMS IMU. Two fully simulated datasets, the USMMA dataset and the airplane dataset, were generated using HoloOcean [32]. The USMMA dataset replicates key features of the environment where the real USMMA dataset was collected, including floating docks and repeating circular pier pilings. The airplane dataset is intended to simulate the site of an airplane wr"}
{"id": "2507.23629v1_chunk31", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "res of the environment where the real USMMA dataset was collected, including floating docks and repeating circular pier pilings. The airplane dataset is intended to simulate the site of an airplane wreck located on the seafloor (this dataset is shown in Fig. 2). In all datasets, both real and synthetic, all robots operate at the same, fixed depth (close to the surface in the USMMA datasets, and close to the seafloor in the airplane dataset). To simulate the simultaneous operation of multiple robots, we recorded several dataset sessions and replayed them concurrently on a single computer (for both our real and synthetic datasets), with simulated communications between robots. All experiments were conducted on a single computer equipped with an Intel Xeon E-2276M CPU, 31.1 GB of memory, and running Ubuntu 20.04 with ROS Noetic. A. Performance of Inter-Robot Loop Closure Detection In this section, we evaluate the performance of our object- graph-matching-based inter-robot data association"}
{"id": "2507.23629v1_chunk32", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "running Ubuntu 20.04 with ROS Noetic. A. Performance of Inter-Robot Loop Closure Detection In this section, we evaluate the performance of our object- graph-matching-based inter-robot data association technique. Fig. 6 shows the precision (left) and the number of correct loop closures detected (right) as a function of the minimum acceptable overlap ratio on our two fully simulated datasets. The overlap ratio, 𝑟overlap =𝑛overlap 𝑛total, measures the proportion of points in the target cloud ( 𝑛overlap ) that overlap with the source cloud, relative to the total number of points in the target cloud (𝑛total). A loop closure is considered a true positive if the estimated translation error is within 1.5 m and the angular error is within 15 °of the ground truth. Precision is defined as the ratio of true positives to the total detected loop closures. (a) Result on the airplane dataset. (b) Result on the USMMA dataset. Fig. 6: Performance of inter-robot loop closure detection and regis- tration "}
{"id": "2507.23629v1_chunk33", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "f true positives to the total detected loop closures. (a) Result on the airplane dataset. (b) Result on the USMMA dataset. Fig. 6: Performance of inter-robot loop closure detection and regis- tration on both of our 3-robot fully simulated datasets. Precision (left) and the number of true positive loop closures detected (right) plotted against the overlap ratio parameter 𝑟𝑜𝑣𝑒𝑟𝑙𝑎𝑝 , evaluated using various methods. We tested the proposed algorithm using different sliding win- dow sizes. A window size of 0 indicates no sliding window, while sizes 1 and 3 correspond to the inclusion of 3 and 7 nearby frames for registration, respectively. Across both datasets, the method with a sliding window of size 3 achieved the highest number of true positive loop closures. However, its precision decreased when the minimum overlap ratio threshold was relatively low, which is expected due to the sliding window strategy. To balance these factors, we set the minimum acceptable overlap ratio threshold to 𝜖"}
{"id": "2507.23629v1_chunk34", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": " when the minimum overlap ratio threshold was relatively low, which is expected due to the sliding window strategy. To balance these factors, we set the minimum acceptable overlap ratio threshold to 𝜖overlap =0.9 for all experiments and accepted a loop closure only if 𝑟overlap>𝜖overlap . TABLE I: Runtime for sonar scene descriptor-based inter-robot loop closure and global ICP registration in DRACo-SLAM [10] (referred to as DRACo1), and graph-based inter-robot loop closure detection and ICP registration in the proposed DRACo-SLAM2 (referred to as DRACo2) on both of our 3-robot fully simulated datasets. AlgorithmRuntime (ms) Mean Max DRACo1Ring Key Matching 0.12 2.05 Go-ICP Registration 361.83 2556.38 DRACo2Graph Construction & Matching 5.64 32.76 ICP Registration (Window Size 3) 12.69 358.45 We also summarize the runtime of our proposed DRACo- SLAM2 method (referred to as DRACo2) and its predecessormethod, DRACo-SLAM (referred to as DRACo1) in Tab. I. As shown in Tab. I, the ICP registr"}
{"id": "2507.23629v1_chunk35", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "5 We also summarize the runtime of our proposed DRACo- SLAM2 method (referred to as DRACo2) and its predecessormethod, DRACo-SLAM (referred to as DRACo1) in Tab. I. As shown in Tab. I, the ICP registration module in DRACo2 is, on average, 20 times faster than that in DRACo1 because the time-consuming global registration step is not required in our approach. This enables us to operate at a much higher frequency and detect significantly more loop closures compared to the original method. As shown in Tab. II, our total loop closure detection module achieves a runtime per timestep that is 10 times faster than DRACo1, enabling more time-efficient performance. TABLE II: Mean runtime and mean number of inter-robot loop closure algorithm executions per time step for DRACo1 and DRACo2 on both of our 3-robot fully simulated datasets. Algorithm Mean (s) Mean No. DRACo1Ring Key Matching <0.01 1 Go-ICP Registration 10.11 28 DRACo2Graph Construction & Matching <0.01 1 ICP Registration (Window Size 3"}
{"id": "2507.23629v1_chunk36", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "r 3-robot fully simulated datasets. Algorithm Mean (s) Mean No. DRACo1Ring Key Matching <0.01 1 Go-ICP Registration 10.11 28 DRACo2Graph Construction & Matching <0.01 1 ICP Registration (Window Size 3) 1.55 122 B. Performance of Inter-Robot PGO We also evaluate the performance of inter-robot PGO using both PCM and GCM on our two 3-robot fully simulated datasets. A robust Cauchy noise model [33] from GTSAM [34] is employed for all methods to ensure robust performance. Fig. 7 shows the trajectories optimized locally from robot 𝛼 on the airplane dataset (left) and the USMMA dataset (right). We align the estimated trajectory with the ground truth using EVO 3, treating the robot trajectories from different robots as a single trajectory with different timestamps. The trajectories are generally smooth, although drifts caused by local SLAM are observed in the results for the USMMA dataset. TABLE III: Absolute Trajectory Error (ATE) in meters for the proposed method compared to the full Pose Gr"}
{"id": "2507.23629v1_chunk37", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "mooth, although drifts caused by local SLAM are observed in the results for the USMMA dataset. TABLE III: Absolute Trajectory Error (ATE) in meters for the proposed method compared to the full Pose Graph Optimization (PGO) method on both of our 3-robot fully simulated datasets. AlgorithmUSMMA Airplane 𝛼 𝛽 𝛾 𝛼 𝛽 𝛾 DRACoPCM 1.58 2.43 1.88 1.32 1.36 1.46 GCM 1.43 1.20 1.23 1.33 0.95 1.31 Full PGOPCM 1.27 1.37 1.52 1.28 1.91 1.63 GCM 1.24 1.41 1.56 1.31 1.91 1.65 Additionally, we perform a quantitative analysis. Tab. III presents the Root Mean Square Error (RMSE) of the Absolute Trajectory Error (ATE) for our proposed two-step PGO method (referred to as DRACo) compared to the widely used full PGO with PCM and the proposed GCM. The ATE is also calculated using EVO. For each robot, the estimated and ground truth trajectories are concatenated head-to-tail, starting with the trajectory of the local robot, followed by the trajectories of neighboring robots in alphabetical order. Since EVO is de"}
{"id": "2507.23629v1_chunk38", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": " and ground truth trajectories are concatenated head-to-tail, starting with the trajectory of the local robot, followed by the trajectories of neighboring robots in alphabetical order. Since EVO is designed for 6DoF SLAM, we adapt it for the 3DoF condition by setting 𝑧=0,𝜙=0, and𝜃=0. As shown in Tab. III, our default configuration, DRACo two- step PGO with GCM, achieves the best accuracy in most cases. This is attributed to the introduction of GCM, which efficiently 3https://github.com/MichaelGrupp/evoreduces the influence of loop closures with similar registration errors, an issue that remains inevitable even with a robust noise model. Furthermore, the adoption of the two-step PGO prevents drifts from neighboring robots from affecting the results of the local robot. However, as a trade-off, the improved accuracy from neighboring robots has only a limited influence on the state estimation of the local robot. Fig. 7: Optimized trajectories of different local robots using the proposed DR"}
{"id": "2507.23629v1_chunk39", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "f, the improved accuracy from neighboring robots has only a limited influence on the state estimation of the local robot. Fig. 7: Optimized trajectories of different local robots using the proposed DRACo-SLAM2 on the fully simulated 3-robot airplane (top) and USMMA (bottom) datasets. TABLE IV: Sizes of perception messages for DRACo1 and DRACo2 on the USMMA real-world dataset (with simulated 3-robot comms.). Algorithm Message Type Mean KBits Max KBits DRACo1Ring key Descriptor 0.13 0.13 Point Cloud-float32 9.67 27.90 DRACo2Object Map 1.35 2.25 Point Cloud-float32 9.69 27.90 C. Performance on the Real-world Dataset The proposed algorithm is next evaluated on our real-world dataset from USMMA with simulated 3-robot communications. Fig. 1 illustrates the optimized trajectories, with results from robots𝛼,𝛽, and𝛾marked in blue, green, and orange, respectively. All detected inter-robot loop closures are indicated in purple for clarity. Since ground truth data is unavailable for this real-worl"}
{"id": "2507.23629v1_chunk40", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "om robots𝛼,𝛽, and𝛾marked in blue, green, and orange, respectively. All detected inter-robot loop closures are indicated in purple for clarity. Since ground truth data is unavailable for this real-world dataset, we align the constructed point-cloud map with satellite imagery for a more intuitive and visually interpretable representation of the results. Tab. IV compares the sizes of perception messages trans- mitted by the full version of DRACo-SLAM (DRACo1) and DRACo-SLAM2 (DRACo2) on the USMMA real-world dataset. For DRACo1, the Ring Key Descriptor messages used for initial loop closure candidate detection are compact, withboth mean and maximum sizes under 1 KBits. Object Map messages introduced by DRACo2 are larger but remain well below the 62.5 kbps bandwidth limit of HS underwater acoustic modems operating at a range of 300m. The communication bandwidth required for point-cloud transmission is similar for both methods. V. Conclusion In this paper, we propose DRACo-SLAM2, a distribut"}
{"id": "2507.23629v1_chunk41", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "tic modems operating at a range of 300m. The communication bandwidth required for point-cloud transmission is similar for both methods. V. Conclusion In this paper, we propose DRACo-SLAM2, a distributed multi-robot SLAM framework designed for sonar-equipped underwater robot teams using object graph matching. By clustering the point-cloud map into an object map, exchanging the object maps, and comparing them through graph matching, DRACo-SLAM2 achieves a tenfold speed improvement over our previous work, DRACo-SLAM. Additionally, we introduce the Group-wise Consistent Measurement Set Maximization (GCM) technique to enhance outlier rejection, and address the challenges of similar registration errors across multiple sonar image pairs encountered in underwater mapping scenarios. For future work, we aim to optimize the data updating strategy for object map exchanges, and integrate the algorithm with an autonomous exploration algorithm to enable real-world underwater exploration with robot te"}
{"id": "2507.23629v1_chunk42", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": ", we aim to optimize the data updating strategy for object map exchanges, and integrate the algorithm with an autonomous exploration algorithm to enable real-world underwater exploration with robot teams. References [1] Y. Wang, W. Thanyamanta, and N. Bose, “Cooperation and compressed data exchange between multiple gliders used to map oil spills in the ocean,” Applied Ocean Research , vol. 118, p. 102999, 2022. [2] Y. Zhang, B. Kieft, B. W. Hobson, Q. Shemet, C. M. Preston, C. Wahl, K. J. Pitz, K. J. Benoit-Bird, J. M. Birch, F. P. Chavez, and C. A. Scholin, “Coordinated and collaborative sampling by two long-range autonomous underwater vehicles,” IEEE Journal of Oceanic Engineering , vol. 49, no. 4, pp. 1371–1382, 2024. [3] J. Wang, S. Bai, and B. Englot, “Underwater localization and 3D mapping of submerged structures with a single-beam scanning sonar,” in IEEE Int. Conf. on Robotics and Automation (ICRA) , 2017, pp. 4898–4905. [4] J. McConnell, I. Collado-Gonzalez, P. Szenher, and B."}
{"id": "2507.23629v1_chunk43", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "apping of submerged structures with a single-beam scanning sonar,” in IEEE Int. Conf. on Robotics and Automation (ICRA) , 2017, pp. 4898–4905. [4] J. McConnell, I. Collado-Gonzalez, P. Szenher, and B. Englot, “Large- scale dense 3-D mapping using submaps derived from orthogonal imaging sonars,” IEEE J. Oceanic Engineering , vol. 50, no. 1, pp. 354–369, 2025. [5] J. Song, O. Bagoren, R. Andigani, A. Sethuraman, and K. A. Skinner, “Turtlmap: Real-time localization and dense mapping of low-texture underwater environments with a low-cost unmanned underwater vehicle,” inIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2024, pp. 1191–1198. [6] S. Rahman, A. Quattrini Li, and I. Rekleitis, “SVIn2: A multi-sensor fusion-based underwater SLAM system,” The International Journal of Robotics Research , vol. 41, no. 11-12, pp. 1022–1042, 2022. [7] F. Bonin-Font and A. Burguera, “Towards multi-robot visual graph- slam for autonomous marine vehicles,” Journal of Marine Sci"}
{"id": "2507.23629v1_chunk44", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "ournal of Robotics Research , vol. 41, no. 11-12, pp. 1022–1042, 2022. [7] F. Bonin-Font and A. Burguera, “Towards multi-robot visual graph- slam for autonomous marine vehicles,” Journal of Marine Science and Engineering , vol. 8, no. 6, p. 437, 2020. [8] M. Xanthidis, B. Joshi, M. Roznere, W. Wang, N. Burgdorfer, A. Q. Li, P. Mordohai, S. Nelakuditi, and I. Rekleitis, “Towards mapping of underwater structures by a team of autonomous underwater vehicles,” in The Int. Symposium of Robotics Research . Springer, 2022, pp. 170–185. [9] J. Wang, T. Shan, and B. Englot, “Underwater terrain reconstruction from forward-looking sonar imagery,” in International Conference on Robotics and Automation (ICRA) , 2019, pp. 3471–3477. [10] J. McConnell, Y. Huang, P. Szenher, I. Collado-Gonzalez, and B. Englot, “DRACo-SLAM: Distributed robust acoustic communication-efficient SLAM for imaging sonar equipped underwater robot teams,” in IEEE/RSJ Int. Conf. on Intelligent Robots and Systems , 2022, pp. 8457"}
{"id": "2507.23629v1_chunk45", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "lot, “DRACo-SLAM: Distributed robust acoustic communication-efficient SLAM for imaging sonar equipped underwater robot teams,” in IEEE/RSJ Int. Conf. on Intelligent Robots and Systems , 2022, pp. 8457–8464. [11] J. Yang, H. Li, and Y. Jia, “Go-ICP: Solving 3D registration efficiently and globally optimally,” in IEEE International Conference on Computer Vision , 2013, pp. 1457–1464. [12] J. G. Mangelson, D. Dominic, R. M. Eustice, and R. Vasudevan, “Pairwise consistent measurement set maximization for robust multi-robot map merging,” in IEEE Int. Conf. on Robotics and Automation (ICRA) , 2018, pp. 2916–2923.[13] J. Drupt, A. I. Comport, C. Dune, and V. Hugel, “MAM3SLAM: Towards underwater-robust multi-agent visual SLAM,” Ocean Engineering , vol. 302, p. 117643, 2024. [14] F. Zhang, D. Xu, and C. Cheng, “An underwater distributed SLAM approach based on improved GMRBnB framework,” Journal of Marine Science and Engineering , vol. 11, no. 12, p. 2271, 2023. [15] M. Bryson, M. Johnson-Robers"}
{"id": "2507.23629v1_chunk46", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "nd C. Cheng, “An underwater distributed SLAM approach based on improved GMRBnB framework,” Journal of Marine Science and Engineering , vol. 11, no. 12, p. 2271, 2023. [15] M. Bryson, M. Johnson-Roberson, O. Pizarro, and S. Williams, “Au- tomated registration for multi-year robotic surveys of marine benthic habitats,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2013, pp. 3344–3349. [16] S. B. Williams, O. Pizarro, and B. Foley, “Return to Antikythera: Multi- session slam based AUV mapping of a first century BC wreck site,” in Field and Service Robotics: Results of the 10th International Conference . Springer, 2016, pp. 45–59. [17] L. Paull, G. Huang, M. Seto, and J. J. Leonard, “Communication- constrained multi-AUV cooperative SLAM,” in IEEE International Conference on Robotics and Automation (ICRA) , 2015, pp. 509–516. [18] ¨O.¨Ozkahraman and P. ¨Ogren, “Collaborative navigation-aware coverage in feature-poor environments,” in IEEE/RSJ International "}
{"id": "2507.23629v1_chunk47", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": " Conference on Robotics and Automation (ICRA) , 2015, pp. 509–516. [18] ¨O.¨Ozkahraman and P. ¨Ogren, “Collaborative navigation-aware coverage in feature-poor environments,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2022, pp. 10 066–10 073. [19] A. R. Gaspar and A. Matos, “Feature-based place recognition using forward-looking sonar,” Journal of Marine Science and Engineering , vol. 11, no. 11, p. 2198, 2023. [20] M. M. Santos, G. B. Zaffari, P. O. Ribeiro, P. L. Drews-Jr, and S. S. Botelho, “Underwater place recognition using forward-looking sonar images: A topological approach,” Journal of Field Robotics , vol. 36, no. 2, pp. 355– 369, 2019. [21] H. Kim, G. Kang, S. Jeong, S. Ma, and Y. Cho, “Robust imaging sonar- based place recognition and localization in underwater environments,” in IEEE Int. Conf. on Robotics and Automation , 2023, pp. 1083–1089. [22] Z. Deng, Y. Zhang, H. Yang, and H. Wang, “Underwater point cloud transmission framework: hybr"}
{"id": "2507.23629v1_chunk48", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "ion in underwater environments,” in IEEE Int. Conf. on Robotics and Automation , 2023, pp. 1083–1089. [22] Z. Deng, Y. Zhang, H. Yang, and H. Wang, “Underwater point cloud transmission framework: hybrid encoder implementation based on CNN and transformer,” Measurement Science and Technology , vol. 36, no. 1, p. 015111, 2024. [23] C. Qi, T. Ma, Y. Li, L. Lv, and Y. Ling, “An efficient loop closure detection method for communication-constrained bathymetric cooperative SLAM,” Ocean Engineering , vol. 304, p. 117720, 2024. [24] H. Do, S. Hong, and J. Kim, “Robust loop closure method for multi- robot map fusion by integration of consistency and data similarity,” IEEE Robotics and Automation Letters , vol. 5, no. 4, pp. 5701–5708, 2020. [25] J. Wang, F. Chen, Y. Huang, J. McConnell, T. Shan, and B. Englot, “Virtual maps for autonomous exploration of cluttered underwater environments,” IEEE Journal of Oceanic Engineering , vol. 47, no. 4, pp. 916–935, 2022. [26] M. Kaess, H. Johannsson, R. Ro"}
{"id": "2507.23629v1_chunk49", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": " B. Englot, “Virtual maps for autonomous exploration of cluttered underwater environments,” IEEE Journal of Oceanic Engineering , vol. 47, no. 4, pp. 916–935, 2022. [26] M. Kaess, H. Johannsson, R. Roberts, V. Ila, J. J. Leonard, and F. Dellaert, “iSAM2: Incremental smoothing and mapping using the Bayes tree,” The International Journal of Robotics Research , vol. 31, no. 2, pp. 216–235, 2012. [27] K. El-Darymli, P. McGuire, D. Power, and C. R. Moloney, “Target detection in synthetic aperture radar imagery: A state-of-the-art survey,” Journal of Applied Remote Sensing , vol. 7, no. 1, p. 071598, 2013. [28] M. A. Richards, Fundamentals of radar signal processing . McGraw-Hill New York, 2005, vol. 1. [29] M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al. , “A density-based algorithm for discovering clusters in large spatial databases with noise,” in KDD , vol. 96, no. 34, 1996, pp. 226–231. [30] J. Yu and S. Shen, “SemanticLoop: Loop closure with 3D semantic graph matching,” IEEE Robotics "}
{"id": "2507.23629v1_chunk50", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "ering clusters in large spatial databases with noise,” in KDD , vol. 96, no. 34, 1996, pp. 226–231. [30] J. Yu and S. Shen, “SemanticLoop: Loop closure with 3D semantic graph matching,” IEEE Robotics and Automation Letters , vol. 8, no. 2, pp. 568– 575, 2022. [31] D. F. Crouse, “On implementing 2D rectangular assignment algorithms,” IEEE Transactions on Aerospace and Electronic Systems , vol. 52, no. 4, pp. 1679–1696, 2016. [32] E. Potokar, K. Lay, K. Norman, D. Benham, S. Ashford, R. Peirce, T. B. Neilsen, M. Kaess, and J. G. Mangelson, “Holoocean: A full-featured marine robotics simulator for perception and autonomy,” IEEE Journal of Oceanic Engineering , vol. 49, no. 4, pp. 1322–1336, 2024. [33] G. H. Lee, F. Fraundorfer, and M. Pollefeys, “Robust pose-graph loop- closures with expectation-maximization,” in IEEE/RSJ International Conference on Intelligent Robots and Systems , 2013, pp. 556–563. [34] F. Dellaert and M. Kaess, Factor Graphs for Robot Perception . Foundations and Trend"}
{"id": "2507.23629v1_chunk51", "paper_id": "2507.23629v1", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "text": "ion-maximization,” in IEEE/RSJ International Conference on Intelligent Robots and Systems , 2013, pp. 556–563. [34] F. Dellaert and M. Kaess, Factor Graphs for Robot Perception . Foundations and Trends in Robotics, Vol. 6, 2017. [Online]. Available: http://www.cs.cmu.edu/ ∼kaess/pub/Dellaert17fnt.pdf"}
{"id": "2507.23592v1_chunk0", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "1 Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation Haiyun Zhang1, Stefano Dalla Gasperina1, Saad N. Yousaf1, Toshimitsu Tsuboi2, Tetsuya Narita2, and Ashish D. Deshpande1,3 Abstract —Hand exoskeletons are critical tools for dexterous teleoperation and immersive manipulation interfaces, but achiev- ing accurate hand tracking remains a challenge due to user- specific anatomical variability and donning inconsistencies. These issues lead to kinematic misalignments that degrade tracking performance and limit applicability in precision tasks. We pro- pose a subject-specific calibration framework for exoskeleton- based hand tracking that uses redundant joint sensing and a residual-weighted optimization strategy to estimate virtual link parameters. Implemented on the M AESTRO exoskeleton, our method improves joint angle and fingertip position estimation across users with varying hand geometries. We introduce a data-driven approach to empirically tune"}
{"id": "2507.23592v1_chunk1", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "mented on the M AESTRO exoskeleton, our method improves joint angle and fingertip position estimation across users with varying hand geometries. We introduce a data-driven approach to empirically tune cost function weights using motion capture ground truth, enabling more accurate and consistent calibration across participants. Quantitative results from seven subjects show substantial reductions in joint and fingertip tracking errors compared to uncalibrated and evenly weighted models. Qualitative visualizations using a Unity-based virtual hand further confirm improvements in motion fidelity. The proposed framework generalizes across exoskeleton designs with closed-loop kinematics and minimal sensing, and lays the foundation for high-fidelity teleoperation and learning-from- demonstration applications. Index Terms —Hand Tracking, Hand Exoskeleton, Dexterous Manipulation, Kinematics. I. I NTRODUCTION Achieving human-level dexterity in robotic manipulation remains a key challenge [1], as "}
{"id": "2507.23592v1_chunk2", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "pplications. Index Terms —Hand Tracking, Hand Exoskeleton, Dexterous Manipulation, Kinematics. I. I NTRODUCTION Achieving human-level dexterity in robotic manipulation remains a key challenge [1], as current systems still fall short in achieving the versatility needed for tool use and fine manipulation [2], [3]. Dexterous teleoperation, where humans remotely control robotic hands, offers a promising pathway toward this goal. It is essential for applications such as robotic surgery, space exploration, and disaster response, as well as for collect- ing high-quality demonstrations to train robots via imitation learning [4]. In both real-time control and demonstration settings, the effectiveness of teleoperation hinges on accurately capturing the user’s hand movements, driving growing interest in evaluating and improving hand tracking methods for these applications. Current approaches fall into two main categories: vision- based systems and wearable devices [5], [6]. Vision-based methods, "}
{"id": "2507.23592v1_chunk3", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": " in evaluating and improving hand tracking methods for these applications. Current approaches fall into two main categories: vision- based systems and wearable devices [5], [6]. Vision-based methods, including optical motion capture and camera-based frameworks like MediaPipe [7], offer non-invasive, real-time This project is funded by Sony Group Corporation, Tokyo, Japan. 1Walker Department of Mechanical Engineering, University of Texas at Austin, Austin, TX, USA. 2Sony Group Corporation, Tokyo, Japan. 3Meta Reality Labs Research, Redmond, WA, USA. MCP MCP PIP DIPMCP PIP DIPIPCMC IndexMiddleThumbWristMeasured revolute joint Computed fingertipComputed revolute jointFig. 1: (Left) The M AESTRO hand exoskeleton worn on the thumb, index, and middle fingers. (Right) Simplified kinematic model of the exoskeleton and corresponding rendering of the mapped virtual hand. tracking and can achieve sub-centimeter fingertip precision under ideal conditions. However, they remain sensitive to occlusio"}
{"id": "2507.23592v1_chunk4", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "f the exoskeleton and corresponding rendering of the mapped virtual hand. tracking and can achieve sub-centimeter fingertip precision under ideal conditions. However, they remain sensitive to occlusions, lighting variability, and camera placement, which compromises tracking stability and usable workspace [8]. Additionally, they lack haptic feedback, making them less suitable for force-sensitive teleoperation tasks [9]. In contrast, wearable devices—–such as sensorized gloves and hand exoskeletons–—offer a more direct means of captur- ing finger motion through joint angle or fingertip sensing [10]. Among these, hand exoskeletons are particularly effective in cluttered or occlusion-prone environments [6], [11], and can deliver haptic feedback to keep users actively engaged dur- ing dexterous teleoperation [12]. However, exoskeletons face a persistent challenge in maintaining anatomically accurate tracking across diverse users and donning conditions [13]. Variability in hand size, donning"}
{"id": "2507.23592v1_chunk5", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": " teleoperation [12]. However, exoskeletons face a persistent challenge in maintaining anatomically accurate tracking across diverse users and donning conditions [13]. Variability in hand size, donning inconsistencies, and slip- page at the skin–device interface often introduce kinematic misalignments that degrade tracking accuracy and cause dis- comfort or unintended interaction forces [14]. To mitigate these issues, prior work has employed redundant passive mechanisms [15]–[17] or underactuated fingertip-only designs [18], [19]. While these designs enable self-alignment to accommodate donning variability, they do not guarantee accurate joint-level tracking. The introduced kinematic redun- dancy breaks the one-to-one mapping between exoskeleton and anatomical joints, making the system sensitive to slippage, donning variability, and hand size. As a result, even with high- quality sensors, the estimated joint angles can diverge from the user’s true finger posture, making precise hand tra"}
{"id": "2507.23592v1_chunk6", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "itive to slippage, donning variability, and hand size. As a result, even with high- quality sensors, the estimated joint angles can diverge from the user’s true finger posture, making precise hand tracking difficult [20].arXiv:2507.23592v1 [cs.RO] 31 Jul 20252 To address this gap, we propose a subject-specific cal- ibration framework aimed at reducing hand tracking error in exoskeleton-based telemanipulation. Our method estimates virtual link parameters that account for different hand sizes and donning conditions, thereby improving the accuracy of hand tracking across users. Specifically, we implement and validate this approach on the M AESTRO hand exoskeleton , shown inFig. 1, which em- ploys closed-loop four-bar linkages for passive self-alignment. While this design improves comfort and wearability, it also introduces kinematic redundancy, making the system highly sensitive to hand size, hand anatomy, slippage, and donning conditions [14]. Our proposed calibration mitigates these ef-"}
{"id": "2507.23592v1_chunk7", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "wearability, it also introduces kinematic redundancy, making the system highly sensitive to hand size, hand anatomy, slippage, and donning conditions [14]. Our proposed calibration mitigates these ef- fects by optimizing virtual link lengths and weighting factors to align exoskeleton outputs with anatomical motion. To address these challenges, we introduce a subject-specific, data- driven calibration framework that aligns exoskeleton outputs with anatomical motion by optimizing virtual link lengths and residual weightings. Instead of relying on rigid hardware alignment, the method estimates these parameters from user motion data, compensating for variations in hand size, finger proportions, and donning configuration. The calibration is formulated as an optimization problem that minimizes tracking error via a residual-weighted cost function combining joint and fingertip errors. We further introduce a data-driven approach to optimize these weights, enabling the calibration to adapt to su"}
{"id": "2507.23592v1_chunk8", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "racking error via a residual-weighted cost function combining joint and fingertip errors. We further introduce a data-driven approach to optimize these weights, enabling the calibration to adapt to subject-specific biome- chanics and sensor contributions. This integrated strategy leads to more accurate hand posture estimation and improved tracking fidelity. We implement and evaluate the framework on the M AESTRO hand exoskeleton, demonstrating consistent improvements in joint and fingertip tracking across seven participants with diverse hand geometries. The method is validated both quantitatively and qualitatively, using a virtual hand model rendered in Unity to assess visual plausibility in teleoperation. Our key contributions are: (i) a subject-specific calibration procedure for virtual link estimation under anatomical and donning variability; (ii) a residual-weight optimization method that enhances tracking accuracy over traditional uniform weighting; and (iii) a comprehensive multi"}
{"id": "2507.23592v1_chunk9", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "link estimation under anatomical and donning variability; (ii) a residual-weight optimization method that enhances tracking accuracy over traditional uniform weighting; and (iii) a comprehensive multi-subject evaluation and virtual validation demonstrating improved fidelity for teleoperation. Together, these contributions establish a gen- eralizable calibration framework for improving exoskeleton- based hand tracking. While demonstrated on M AESTRO , the method applies broadly to other wearable hand exoskeletons with kinematic misalignment or minimal sensing, support- ing high-fidelity control in teleoperation and learning-from- demonstration applications. II. M ETHODS A. Hardware Platform The M AESTRO hand exoskeleton, shown in Fig.1, is used in this study as a representative case to present and validate our subject-specific calibration framework. M AESTRO enables motion tracking and haptic feedback for the thumb, index, and middle fingers via a cable-driven [21], [22]. It includes8 a"}
{"id": "2507.23592v1_chunk10", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "ent and validate our subject-specific calibration framework. M AESTRO enables motion tracking and haptic feedback for the thumb, index, and middle fingers via a cable-driven [21], [22]. It includes8 active and 8 passive joints, each instrumented with rotary potentiometers for joint sensing. The index and middle fingers are each equipped with 5 sensors (2 providing redundant measurements), while the thumb has 6 sensors, including two redundant ones. As shown in the figure, the system can measure or estimate the MCP and PIP joints for the fingers, and the CMC, MCP, and IP joints for the thumb. The distal interphalangeal (DIP) joints are computed from the PIP joints using established biomechanical correlations. B. Kinematic Model Each digit of the M AESTRO hand exoskeleton is modeled independently using a dedicated kinematic chain, reflecting the modular mechanical design of the device. These models define the mapping from exoskeleton joint measurements to anatomical joint angles. The thu"}
{"id": "2507.23592v1_chunk11", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "ently using a dedicated kinematic chain, reflecting the modular mechanical design of the device. These models define the mapping from exoskeleton joint measurements to anatomical joint angles. The thumb and fingers differ in mechanical structure but share several underlying kinematic equations. As shown in Fig. 2, each digit consists of multiple four-bar closed-loop chains—two for the index and middle fingers, and three for the thumb. All digits follow a common layout: an initial RRPR loop, followed by one (index/middle) or two (thumb) RRRR loops. For each loop, the anatomical joint angle is estimated by modeling the combined kinematics of the exoskeleton linkages, finger segments, and virtual links. In the following, we present the human–exoskeleton kine- matic model, which maps the exoskeleton’s sensorized joint angles to estimated human joint angles. For the thumb, the sensorized exoskeleton joint angles are denoted as (α2,β2,γ2,δ1,δ2,δ3), and the corresponding anatomical joint angl"}
{"id": "2507.23592v1_chunk12", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "eleton’s sensorized joint angles to estimated human joint angles. For the thumb, the sensorized exoskeleton joint angles are denoted as (α2,β2,γ2,δ1,δ2,δ3), and the corresponding anatomical joint angles are (θ1,θ2,θ3), representing the CMC, MCP, and IP joints, respectively. While similar models apply to the thumb, index, and middle fingers, here we only detail the thumb model for clarity. Complete loop equations and joint mappings are provided in Fig. 2 and Tab. I. Using known geometric parameters, the input angles yield intermediate link angles through closed-form trigonometric expressions, as derived in Eqs. 4–6. Due to the RRPR structure of the first loop, the distances d1andc2are also functions of α2. The analytical solutions are derived using sum-to-product trigonometric identities for compactness. As mentioned above, the exoskeleton includes redundant sensor inputs, which can also be derived from the geometry of the kinematic loops. These redundant joint angles are incorporated i"}
{"id": "2507.23592v1_chunk13", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "r compactness. As mentioned above, the exoskeleton includes redundant sensor inputs, which can also be derived from the geometry of the kinematic loops. These redundant joint angles are incorporated into the calibration cost function to improve estimation accuracy. Here, δ1,δ2, and δ3represent redundant joint angles derived from specific four-bar link constraints, as shown in Fig. 2. Each redundant joint encodes constraints across one or more loops: i) ˆδ1is defined as the angle between links b1andd2 and depends on both the first and second loops; ii) ˆδ2, defined as the angle between a2and the projection of b2along the y-axis ( y3), is specific to the index and middle fingers and is influenced by the second loop geometry, f(β); and iii) ˆδ3the angle between a2andd3is unique to the thumb, and depends on both the second and third loops, f(β,γ). These estimated redundant angles are computed using geo- metric constraints, as:3 Proximal Loop Intermediate Loop Distal LoopMaestro Exoskeleton"}
{"id": "2507.23592v1_chunk14", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "and depends on both the second and third loops, f(β,γ). These estimated redundant angles are computed using geo- metric constraints, as:3 Proximal Loop Intermediate Loop Distal LoopMaestro Exoskeleton Link Virtual Link (XY Coordinate) Virtual Link (Length-Angle)Ground Extension of Ground Human Finger Link Extension of Human Finger Link First Kinematic Loop Second Kinematic Loop Third Kinematic LoopMaestro Exoskeleton Link Virtual Link (XY Coordinate) Virtual Link (Length -Angle)Ground Extension of Ground Human Finger Link Extension of Human Finger Link α2 b1 a1 d1c1 = y2d2a2 c2β2 β3 β4 β5β1 β6y1 x1 L1 = x2x3y3 θ2 θ1α1δ1 δ2 b2 α3 α2d2 b1a2 d3a3 c3b3 b2 c2β2γ2 c1 = y2 d1 L1 = x2x3y3x4y4 θ1β1 β5β6β4β3γ3γ4 γ1γ6 γ5 y1 x1θ2θ3δ1δ3 α3 α1a1 (a) Thumb Kinematic Model α2 b1 a1 d1c1 = y2d2a2 c2β2 β3 β4 β5β1 β6y1 x1 L1 =x2x3y3 θ2 θ1α1δ1 δ2 b2 α3 xy (b) Index (and Middle) Finger Kinematic Model Fig. 2: Kinematic diagram showing the four-bar mechanisms implemented in the human-robot kinematic model o"}
{"id": "2507.23592v1_chunk15", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "4 β5β1 β6y1 x1 L1 =x2x3y3 θ2 θ1α1δ1 δ2 b2 α3 xy (b) Index (and Middle) Finger Kinematic Model Fig. 2: Kinematic diagram showing the four-bar mechanisms implemented in the human-robot kinematic model of the M AESTRO hand exoskeleton. (a) The thumb uses three kinematic loops, whereas (b) the index and middle fingers use two kinematic loops. The first loop is an RRPR mechanism, and both the second and third loops are RRRR mechanisms. Specific definitions of each variable are provided in Tab. I. ˆδ1=2π+θ1+β1−α2−β3 (1) ˆδ2=2π−β4−(π/2−β5) (2) ˆδ3=2π−β4−(π/2−β6)−γ3−(π/2−γ1) (3) where the intermediate variables are presented in Tab. I. While the kinematic model defines a consistent mapping from exoskeleton to anatomical joints, this mapping is sensi- tive to inter-subject variability such as hand anatomy, size, and donning configuration. These factors influence the virtual link geometry and can introduce tracking inaccuracies if left uncal- ibrated. The following section analyzes how variation"}
{"id": "2507.23592v1_chunk16", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "anatomy, size, and donning configuration. These factors influence the virtual link geometry and can introduce tracking inaccuracies if left uncal- ibrated. The following section analyzes how variations in key kinematic parameters affect tracking performance, motivating the need for a subject-specific calibration framework. C. Kinematic Parameters Sensitivity Analysis To motivate the need for a calibration procedure that adjusts the kinematic parameters values, we first analyzed the sensi- tivity of fingertip position to perturbations in the virtual link parameters. Using the kinematics model of the index finger, each of the six 2D virtual link coordinates {x1,y1,x2,y2,x3,y3} was perturbed independently in simulation, and the resulting deviation in fingertip position was computed. The goal was to demonstrate that even small errors in these parameters can lead to significant inaccuracies in fingertip tracking, thereby justifying the need for precise calibration. The results, shown in Fig"}
{"id": "2507.23592v1_chunk17", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "was to demonstrate that even small errors in these parameters can lead to significant inaccuracies in fingertip tracking, thereby justifying the need for precise calibration. The results, shown in Fig. 3, indicate that proximal parameters—particularly x1andx3—have a significantly larger effect on fingertip position than more distal ones such as y1 andy3. This confirms that small errors in estimating virtual link parameters can lead to substantial errors in fingertip -10 -5 0 5 10 Kinematic Parameter Change (%)0102030Fingertip Error (mm)X Position Error Y Position Error Total Position Error(a) Parameter x1 -10 -5 0 5 10 Kinematic Parameter Change (%)0102030Fingertip Error (mm)X Position Error Y Position Error Total Position Error (b) Parameter x3 Fig. 3: Sensitivity analysis in simulation of fingertip position error (mm) as a function of perturbations in select parameters (% variation). position—up to 30 mm for just a 10% perturbation in proximal coordinates. Furthermore, due to the kin"}
{"id": "2507.23592v1_chunk18", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "f fingertip position error (mm) as a function of perturbations in select parameters (% variation). position—up to 30 mm for just a 10% perturbation in proximal coordinates. Furthermore, due to the kinematic design, variations in horizontal parameters (i.e., xvalues) tend to produce larger fin- gertip tracking errors than equivalent changes in vertical ones (yvalues). This is particularly relevant given that horizontal sliding of the device on the back of the hand is more likely than vertical shifting during wear. Such horizontal misalignments directly impact the inferred finger pose, emphasizing the need for personalized calibration of virtual link positions to maintain accurate fingertip estimation. D. Human-Exoskeleton Kinematic Calibration Accurate hand tracking in exoskeletons like M AESTRO is challenging due to non-anatomical joint mappings and user- specific virtual links. These links vary with hand size and donning and cannot be directly measured. We address this by calibrating "}
{"id": "2507.23592v1_chunk19", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "ESTRO is challenging due to non-anatomical joint mappings and user- specific virtual links. These links vary with hand size and donning and cannot be directly measured. We address this by calibrating them from motion data to improve tracking. The goal of the calibration process is to optimize subject- and donning-specific virtual link parameters (e.g., a 1, c1, b2, c3) by minimizing discrepancies between estimated ( ˆθn,ˆδn)4 Proximal Loop (thumb and index) Intermediate Loop (thumb and index) Distal Loop (thumb only) Input: α2 Input: β2,δ1,δ2 Input: γ2,δ3 Parameters: a1,b1,c1,L1,α1 Parameters: a2,b2,d2,β6 Parameters: a3,b3,c3,d3,γ1,γ6 Variables: d1,α3 Variables: c2,β1,β3,β4,β5 Variables: γ3,γ5 Output: θ1 Output: θ2,ˆδ1,ˆδ2 Output: θ3,ˆδ3 Solution: Solution: Solution: θ1=−π+α3 (4) θ2=β1+β5+β6−π (5) θ3=γ1+γ5+γ6−π (6) Where: Where: Where: d1=p a2 1+b2 1−c2 1+2a1b1cos(α2−α1) α3=−2arctan b1−2d1+2a1cos(α2)cos(α1) +2(b1−d1)cos(α2)+2a1cos(α1) +b1cos(2α2) (2cos(α2)+1)·\u0000 −c1+a1sin(α1)+b1sin(α"}
{"id": "2507.23592v1_chunk20", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "β6−π (5) θ3=γ1+γ5+γ6−π (6) Where: Where: Where: d1=p a2 1+b2 1−c2 1+2a1b1cos(α2−α1) α3=−2arctan b1−2d1+2a1cos(α2)cos(α1) +2(b1−d1)cos(α2)+2a1cos(α1) +b1cos(2α2) (2cos(α2)+1)·\u0000 −c1+a1sin(α1)+b1sin(α2)\u0001 c2=p c2 1+(L1−d1)2 β1=arctan\u0010 c1 L1−d1\u0011 β3=arcsin a2sin(β2)q a2 2+d2 2−2a2d2cos(β2)! β4=arccos\u0012 c2 2+d2 2−(a2 2+b2 2)−2c2d2cos(β2) −2a2b2\u0013 β5=arccos\u0012 a2 2+d2 2−(b2 2+c2 2)−2a2d2cos(β2) −2b2c2\u0013 +arcsin b2sin(β6)q b2 2+c2 2−2b2c2cos(β6)!γ3=arcsin a3·sin(γ2)q a2 3+d2 3−2a3d3cos(γ2)! + +arcsin b3·sin(γ5)q b2 3+c2 3−2b3c3cos(γ5)! γ5=arccos\u0012 a2 3+d2 3−(b2 3+c2 3)−2a3d3·cos(γ2) −2b3c3\u0013 TABLE I: Summary of closed-loop kinematic models for computing human finger joint angles from exoskeleton sensor inputs. Each loop models one anatomical joint, using a single exoskeleton joint angle as input and incorporating exoskeleton linkages, anatomical finger segments, and virtual link lengths. The 1st and 2nd loops are used for the index, middle, and thumb digits; the 3rd loop is unique to the thumb. "}
{"id": "2507.23592v1_chunk21", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": " incorporating exoskeleton linkages, anatomical finger segments, and virtual link lengths. The 1st and 2nd loops are used for the index, middle, and thumb digits; the 3rd loop is unique to the thumb. Robot LinkagesAnatomical Finger SegmentsVirtual Link Forward Kinematic Model Hand Tracking from Motion CaptureMinimum Mean Absolute Error SearchUpdated Forward Kinematic Model Optimal Weight Distribution SearchOptimization-Based Calibrationα2，β2，γ2 δ1,δ2,δ3 Human Finger JointsEstimated Redundant JointEstimated Human Finger Joints Error 1 Error 2Updated Virtual LinksEstimated Human Finger Joints Weight DistributionOptimal Virtual Links & Optimal Human Finger Tracking Loop Over All Samples of Random Weight Combination Two-Phase Calibration Calibration: Phase 2 Thumb Index/MiddleCalibration: Phase 1 - +- + Cost Function: Weighted Summation on Errors Random Sampling of WeightsMaestro Data from Hand Motion Tasks Fig. 4: Visualization of the kinematic parameter calibration with weighted optimiza"}
{"id": "2507.23592v1_chunk22", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "e 1 - +- + Cost Function: Weighted Summation on Errors Random Sampling of WeightsMaestro Data from Hand Motion Tasks Fig. 4: Visualization of the kinematic parameter calibration with weighted optimization. The human user performs a two-phase cali- bration, and the weight distribution is adjusted based on human data. and measured joint angles ( δn,ref) or known configurations (θn,ref). The inputs to the calibration are the measured joint angles, while the outputs are the estimated virtual links, which generate updated estimates of the user’s joint angles. The full procedure, illustrated in Fig. 4,consists of two phases: In phase one ( φ1), subjects fully extend their fingers to form a flat-hand posture and hold it for a few seconds while static joint measurements are recorded. This configuration imposes known anatomical constraints, which serve as reference angles (θrefandδref) for calibration: all finger joints are assumed to be at 0°, except for the thumb CMC flexion/extension, which "}
{"id": "2507.23592v1_chunk23", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "ation imposes known anatomical constraints, which serve as reference angles (θrefandδref) for calibration: all finger joints are assumed to be at 0°, except for the thumb CMC flexion/extension, which is anatomically set at 70°. In phase two ( φ2), subjects perform isolated flexion of the MCP joints of the thumb, index, and middle fingers while keeping the IP (thumb) and PIP (index/middle) joints extended. In this case, the reference angles for the MCP jointsare unknown and serve as targets for optimization, while the IP/PIP joints are again assumed to remain at 0°. The two-phase protocol enables global alignment through the flat-hand posture and enforces dynamic consistency via isolated MCP flexion. Together, these conditions provide com- plementary constraints that help the calibration generalize across configurations. An optimization routine then updates the virtual link parameters by minimizing the discrepancy between measured redundant joint angles (or reference postures) and those"}
{"id": "2507.23592v1_chunk24", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "neralize across configurations. An optimization routine then updates the virtual link parameters by minimizing the discrepancy between measured redundant joint angles (or reference postures) and those predicted by the kinematic model. There are two interchangeable representations of virtual link parameters: the length–angle form (a1,α1,c2,β1,b2,β6,b3,γ6), used in the kinematic model, and the XY-coordinate form (x1,y1,x2,y2,x3,y3,x4,y4), used as input for optimization. The conversion between the two is done via trigonometric identities. We adopt the length–angle form for computing joint angles using closed-form geometric relations, and the XY-coordinate form for optimization due to its dimensional consistency. The calibration cost function is defined as a weighted sum of squared errors between estimated joint angles and either known reference or redundant sensor readings, respectively for phase one and two. We define each error term ∆as the discrepancy between the model and ground truth"}
{"id": "2507.23592v1_chunk25", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "een estimated joint angles and either known reference or redundant sensor readings, respectively for phase one and two. We define each error term ∆as the discrepancy between the model and ground truth, as follows: ∆θ(i) n,φn=ˆθ(i) n−θn,ref (7) for anatomical joints with known reference, where the super- script iidentifies samples over the calibration period, nis the joint index, φnis the calibration phase, and θrefis the measured or known reference angle (e.g., 0° during hand flattening) ∆δn,φn(i)=ˆδ(i) n−δ(i) n,ref(8)5 for redundant joints with angular sensors, where δrefis the reference angle measured by the redundant sensors. For the index and middle fingers, the cost function is: f1(x) =1 NN ∑ i=1\u0010 w1(∆θ(i) 1,φ1)2+w2(∆θ(i) 2,φ1)2+w3(∆δ(i) 1,φ1)2+ w4(∆δ(i) 2,φ1)2+w5(∆θ(i) 2,φ2)2+w6(∆δ(i) 1,φ2)2\u0011(9) where x= (x1,y1,x2,y2,x3,y3),wkare weights and Nis the number of samples. To ensure a well-posed optimization, the number of error terms must match the number of virtual link parameters. "}
{"id": "2507.23592v1_chunk26", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": ") 1,φ2)2\u0011(9) where x= (x1,y1,x2,y2,x3,y3),wkare weights and Nis the number of samples. To ensure a well-posed optimization, the number of error terms must match the number of virtual link parameters. For example, six parameters require six independent error terms. While both ∆δ1,φ2and∆δ2,φ2are valid choices, we empirically found that using ∆δ1,φ2results in better calibration. Similarly, the cost function of the thumb is defined as: f2(y) =1 NN ∑ i=1\u0010 w1(∆θ(i) 1,φ1)2+w2(∆θ(i) 2,φ1)2+w3(∆θ(i) 3,φ1)2+ w4(∆δ(i) 1,φ1)2+w5(∆δ(i) 2,φ1)2+w6(∆θ(i) 3,φ2)2+ w7(∆δ(i) 1,φ2)2+w8(∆δ(i) 2,φ2)2\u0011(10) where y= (x1,y1,x2,y2,x3,y3,x4,y4). Subsequently, the global optima of the two problems are obtained as: x∗=argmin xf1(x),y∗=argmin yf2(y) (11) The kinematic calibration procedure relies on optimizing a set of virtual link parameters by minimizing a weighted sum of joint angle errors. The performance of this optimization depends not only on the quality of the input data but also on how the error terms are w"}
{"id": "2507.23592v1_chunk27", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "virtual link parameters by minimizing a weighted sum of joint angle errors. The performance of this optimization depends not only on the quality of the input data but also on how the error terms are weighted in the cost function. While the simulation-based sensitivity analysis, presented in II-C, offers a first-principles rationale for prioritizing proximal pa- rameters, it does not capture real-world factors such as model inaccuracies and inter-subject variability. To address this, we conducted a data-driven weight refinement via motion capture validation (see Section III-A). III. H UMAN -SUBJECT EXPERIMENTS Human-subject experiments were conducted to empirically optimize the calibration cost function weights and evalu- ate hand tracking performance. The study was approved by the Institutional Review Board (IRB ID: STUDY00002527). Seven participants with varying hand sizes (Table II) wore the exoskeleton and completed the protocol shown in Fig. 5. Reflective markers were placed on ana"}
{"id": "2507.23592v1_chunk28", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "ional Review Board (IRB ID: STUDY00002527). Seven participants with varying hand sizes (Table II) wore the exoskeleton and completed the protocol shown in Fig. 5. Reflective markers were placed on anatomical landmarks of the thumb and index finger for motion capture, as shown in Fig. 5a- 5b. First, participants performed the two-phase calibration rou- tine: (i) full finger extension and (ii) MCP flexion of the thumb, index, and middle fingers with IP/PIP joints extended, as described in Sec. II-D. These postures provided constrained joint trajectories for estimating subject-specific kinematic pa- rameters. Second, during the hand tracking task, participants per- formed three representative finger motions: (i) combinedthumb MCP and IP flexion, (ii) index MCP flexion with PIP extended, and (iii) combined index MCP and PIP flexion, as shown in Fig. 5. Each participant performed six trials per motion. These motions were selected to isolate individ- ual joint estimation performance. M AESTR"}
{"id": "2507.23592v1_chunk29", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "iii) combined index MCP and PIP flexion, as shown in Fig. 5. Each participant performed six trials per motion. These motions were selected to isolate individ- ual joint estimation performance. M AESTRO sensor data and motion capture trajectories were recorded simultaneously and synchronized during post-processing. Due to limited visibility and occlusion when wearing the device, the analysis was limited to MCP and IP joints of the thumb, and MCP and PIP joints of the index finger. Due to the shared mechanical and kinematic design between the index and middle fingers, the results obtained for the index finger can also be applied to the middle finger. A. Quantitative Analysis Optimal Weight Search: To enhance calibration robustness against modeling uncertainties and local minima, cost function weights ( wk) were empirically optimized using a data-driven approach. Each subject performed dynamic finger movements while wearing the M AESTRO exoskeleton, and joint angle estimates were compared"}
{"id": "2507.23592v1_chunk30", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "n weights ( wk) were empirically optimized using a data-driven approach. Each subject performed dynamic finger movements while wearing the M AESTRO exoskeleton, and joint angle estimates were compared to ground-truth trajectories. For each subject, 500 candidate weight combinations ( wk∈[0,10]) were randomly sampled across all cost function dimensions. Each weight distribution was used to calibrate virtual link parameters from the two-phase calibration data, and compute the corresponding joint angles and fingertip positions. Motion capture trajectories served as ground truth to identify the weights that minimize the mean absolute error (MAE) in joint angle tracking. The weight distribution yielding the lowest error per subject was selected as optimal. Final weights were obtained by averaging these optimal weights across all participants and applied consistently in subsequent analyses. Performance Validation: Using the averaged optimal weights, each participant’s virtual link parameters"}
{"id": "2507.23592v1_chunk31", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "ng these optimal weights across all participants and applied consistently in subsequent analyses. Performance Validation: Using the averaged optimal weights, each participant’s virtual link parameters were cali- brated based on their two-phase calibration data. The resulting optimized kinematic model was used to estimate joint angles from M AESTRO sensor measurements during dynamic tasks. These estimates were quantitatively validated against motion capture ground truth by computing the mean absolute error (MAE) per joint, averaged across repetitions and subjects. B. Qualitative Analysis After verifying the quantitative performance of our exoskeleton-based hand tracking system, we qualitatively as- sessed how well the calibrated virtual hand resembled the user’s real hand motion in a virtual environment. A custom Unity hand model was developed using direct linear joint- to-joint mapping from the M AESTRO -estimated angles. DIP joints were inferred from corresponding PIP angles via biome"}
{"id": "2507.23592v1_chunk32", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "l environment. A custom Unity hand model was developed using direct linear joint- to-joint mapping from the M AESTRO -estimated angles. DIP joints were inferred from corresponding PIP angles via biome- chanical coupling, and all other joints were fixed. The visual- ization used anatomically plausible joint limits and averaged bone lengths across subjects and conditions, to isolate the ef- fect of calibration. Participants reproduced representative static and dynamic hand gestures—–such as pinching and grasping– —under calibrated and uncalibrated conditions. These gestures were used to generate side-by-side visualization for offline inspection.6 (a) Thumb markers (b) Index markers Human Subjects (N=7) Hand Measurement & Device DonningTwo Phase Calibration: Maestro Data Hand Tracking Task with Motion Capture Calibration & Optimal Weight Distribution SearchUpdated Kinematic ModelOptimal Hand TrackingSampled Calibration DataData Transmission Experimental Sequence Sampled Dynamic Motion Dat"}
{"id": "2507.23592v1_chunk33", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "th Motion Capture Calibration & Optimal Weight Distribution SearchUpdated Kinematic ModelOptimal Hand TrackingSampled Calibration DataData Transmission Experimental Sequence Sampled Dynamic Motion Data Motion Capture Data Updated Virtual Links (c) Experiment workflow Fig. 5: Experimental setup showing motion capture markers placed on the (a) thumb and (b) index finger, along with (c) the experiment workflow for human subject testing. w1: 1,1w2: 2,1 w3: 3,1 w4: 1,1w5: 2,1w6: 3,2w7: 1,2w8: 2,2 0246810 (a) Thumb Weights Distribution w1: 1,1 w2: 2,1 w3: 1,1 w4: 2,1w5: 2,2w6: 1,2 0246810 Sub1 Sub2 Sub3 Sub4 Sub5 Sub6 Sub7 Average (b) Index Weights Distribution Fig. 6: Subject-specific optimized weights distributions for thumb and index finger calibration. The dashed black line represents the averaged distribution across all subjects. IV. R ESULTS A. Quantitative Results Optimal Weight Search: The cost function weights used in the calibration routine is search by averaging the individual opt"}
{"id": "2507.23592v1_chunk34", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "eraged distribution across all subjects. IV. R ESULTS A. Quantitative Results Optimal Weight Search: The cost function weights used in the calibration routine is search by averaging the individual optimal weights distribution. The individual optimal weights- distribution is from each subject by minimizing the joint angle mean absolute error relative to motion capture ground truth. The individual weight distributions for both the thumb and index fingers across all seven subjects, along with the averaged optimal weights, are shown in Fig. 6. While the individual profiles exhibit broadly consistent contours, indicating robust- ness in the optimal weights distribution, some outliers are observed. These deviations likely stem from subject-specific differences in hand size, donning variability, or movement variability during the calibration trials. The averaged results for the thumb (Fig.6a) indicate higher optimal weights for w2 andw8, suggesting that the calibration is more sensitive to pa"}
{"id": "2507.23592v1_chunk35", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "or movement variability during the calibration trials. The averaged results for the thumb (Fig.6a) indicate higher optimal weights for w2 andw8, suggesting that the calibration is more sensitive to parameters contributing to θ2andδ2, both related to the thumb MCP joint. In contrast, the index finger results (Fig.6b) show greater weights for w3andw4, highlighting the sensitivity of the calibration to δ1andδ2, associated with the PIP joints of the index and middle fingers. Overall, the optimized weights emphasize the second kinematic loop parameters—particularlySubject Length Width Size Joint MAE ↓[%] Tip MAE ↓[%] [cm] [cm] T-MCP T-IP I-MCP I-PIP Thumb Index 1 17.0 6.5 S−8.5 64 .7 53 .5 64 .3 32.2 69 .0 2 17.2 8.0 S–M−39.5 11 .4 70 .8 44 .2 5.3 60 .7 3 18.0 9.5 M–L −6.6 12 .5−40.5 57 .0 32.1 58 .0 4 18.5 9.0 M 33.6 48 .6−3.1 61 .7 25.2 63 .8 5 17.0 8.5 M−18.4 3 .2 64 .2 43 .9 41.2 85 .2 6 19.2 9.8 L 59.1 66 .1 73 .6 56 .4 72.1 83 .3 7 18.1 7.8 M−10.8 62 .1 57 .2 35 .4 3.0 66 .0 Avg 17.6 "}
{"id": "2507.23592v1_chunk36", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": " 58 .0 4 18.5 9.0 M 33.6 48 .6−3.1 61 .7 25.2 63 .8 5 17.0 8.5 M−18.4 3 .2 64 .2 43 .9 41.2 85 .2 6 19.2 9.8 L 59.1 66 .1 73 .6 56 .4 72.1 83 .3 7 18.1 7.8 M−10.8 62 .1 57 .2 35 .4 3.0 66 .0 Avg 17.6 8.4 – 37.1 56.7 68.3 52.9 34.8 71.5 TABLE II: Hand dimensions (Length, Width in cm) and percent reduction in joint and fingertip tracking errors after calibration. the thumb MCP and finger PIP joints—as key contributors to accurate hand tracking. Performance Validation: We report mean absolute errors (MAE) between the M AESTRO -based tracking and ground truth for joint angles and fingertip positions. Average results are shown in Fig. 7, while subject-specific results for the three tracking tasks are shown in Fig. 8. Optimal-weighted calibration consistently achieved the lowest MAE, followed by the even-weighted calibration, with the uncalibrated model exhibiting the highest error. This trend holds across joints for the tested digits, validating the benefit of subject-specific weight optimi"}
{"id": "2507.23592v1_chunk37", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": " the even-weighted calibration, with the uncalibrated model exhibiting the highest error. This trend holds across joints for the tested digits, validating the benefit of subject-specific weight optimization. Table II summarizes the results, showing joint-wise and fingertip position error reductions across all participants. De- spite some variability across joints and subjects, calibra- tion consistently led to notable reductions in both joint and fingertip tracking errors. The greatest improvements were observed in the thumb IP and index PIP joints, as well as in the fingertip position of the index finger. Fingertip position accuracy improved similarly, with the index fingertip achieving the highest average error reduction (71.5%), while the thumb fingertip showed a more moderate improvement (34.8%). These gains support the calibration’s effectiveness in enhancing end-effector representation, which is especially relevant for precision manipulation tasks. Overall, the index finger showe"}
{"id": "2507.23592v1_chunk38", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "nt (34.8%). These gains support the calibration’s effectiveness in enhancing end-effector representation, which is especially relevant for precision manipulation tasks. Overall, the index finger showed the most consistent im- provements across subjects, particularly in PIP joint tracking and fingertip accuracy. The thumb also benefited from calibra- tion, though with greater variability—likely due to anatomical differences and higher articulation complexity. A few negative joint error reductions (i.e., increases in error post-calibration) were observed in the thumb MCP and index MCP joints for certain subjects, possibly due to residual alignment errors during marker placement or mechanical coupling not fully captured by the model. Despite inter-subject variability, the averaged error reductions and similarity in weight profiles suggest that the calibration method generalizes well across users with different hand sizes. To illustrate the method’s performance, Fig. 9 shows track- ing res"}
{"id": "2507.23592v1_chunk39", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "tions and similarity in weight profiles suggest that the calibration method generalizes well across users with different hand sizes. To illustrate the method’s performance, Fig. 9 shows track- ing results for Subject 6 across thumb and index. The weighted calibration consistently produces trajectories that align more closely with the ground truth data, compared to both the un- calibrated and even-weighted baselines. This subject serves as a representative example of the effectiveness of our approach in reducing tracking error in dynamic finger motion.7 Thumb MCP Thumb IP Index MCP Index PIP0510152025Finger Joint MAE (°)16.7 14.1 12.025.5 12.2 8.4 6.714.6 10.5 6.1 3.812.0Uncalibrated Even Weights Optimal Weights Thumb Index010203040Fingertip MAE (mm)15.835.5 13.814.7 10.3 10.1 Fig. 7: Hand tracking accuracy results averaged across 7 subjects and all tasks, shown for (left) joint angles (thumb MCP, thumb IP, index MCP, index PIP) and (right) fingertip position (thumb, index). 1234567 Sub"}
{"id": "2507.23592v1_chunk40", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "tracking accuracy results averaged across 7 subjects and all tasks, shown for (left) joint angles (thumb MCP, thumb IP, index MCP, index PIP) and (right) fingertip position (thumb, index). 1234567 Subject01020304050Thumb MCP MAE (°)Uncalibrated Even Weights Optimal Weights 1234567 Subject01020304050Thumb IP MAE (°) 1234567 Subject01020304050Thumb Fingertip MAE (mm) (a) Combined Thumb MCP and IP Flexion 1234567 Subject01020304050Index MCP MAE (°)Uncalibrated Even Weights Optimal Weights 1234567 Subject01020304050Index PIP MAE (°) 1234567 Subject01020304050Index Fingertip MAE (mm) (b) Combined Index MCP and IP Flexion Fig. 8: Hand tracking accuracy results for 7 subjects. (a) Thumb joint angles and fingertip errors across all subjects. (b) Index joint angles and fingertip errors across all subjects for two different index finger motions. B. Qualitative Results To qualitatively assess improvements in hand tracking, we visualized hand tracking results using a virtual Unity hand driven by j"}
{"id": "2507.23592v1_chunk41", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "ubjects for two different index finger motions. B. Qualitative Results To qualitatively assess improvements in hand tracking, we visualized hand tracking results using a virtual Unity hand driven by joint estimates under both calibrated and uncali- brated conditions. Representative postures (e.g., pinch, grasp) were recorded for each subject and used to qualitatively assess tracking fidelity. As shown in Fig. 10, orange hands depict postures based on uncalibrated data, while green hands reflect calibrated estimates. Calibration leads to visibly improved alignment with the real hand, particularly in fine thumb and finger articulations. These visualizations qualitatively confirm the enhanced motion fidelity achieved through the proposed subject-specific cali- bration. A demonstration of the real-time virtual hand motion tracking is provided in the supplementary video. V. D ISCUSSION AND CONCLUSION This study introduced a subject-specific calibration frame- work to enhance hand tracking a"}
{"id": "2507.23592v1_chunk42", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "real-time virtual hand motion tracking is provided in the supplementary video. V. D ISCUSSION AND CONCLUSION This study introduced a subject-specific calibration frame- work to enhance hand tracking accuracy with a sensorized robotic exoskeleton. The method yielded consistent improve- ments in both joint angle and fingertip tracking across par- ticipants with varying hand sizes and donning conditions. 0 10 20 30-50-250255075MCP Joint (°)Motion Capture Uncalibrated Optimal Weights Even Weights 0 10 20 300255075Fingertip X (mm) 0 10 20 30 Time (s)-2502550IP Joint (°) 0 10 20 30 Time (s)-250255075100Fingertip Y (mm)(a) Combined Thumb MCP and IP flexion 0 10 20 30-50-250255075MCP Joint (°)Motion Capture Uncalibrated Optimal Weights Even Weights 0510152025300255075Fingertip X (mm) 0 10 20 30 Time (s)-2502550PIP Joint (°) 051015202530 Time (s)-250255075100Fingertip Y (mm) (b) Combined Index MCP and PIP flexion Fig. 9: Comparison of hand tracking performance for Subject 6 . Each subplot compa"}
{"id": "2507.23592v1_chunk43", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "me (s)-2502550PIP Joint (°) 051015202530 Time (s)-250255075100Fingertip Y (mm) (b) Combined Index MCP and PIP flexion Fig. 9: Comparison of hand tracking performance for Subject 6 . Each subplot compares trajectories obtained from motion capture, uncalibrated tracking, and calibration with even and optimal weights. (a) Index finger pinch (b) Middle finger pinch Fig. 10: Real and virtual hands (Unity rendering) shown side by side for uncalibrated (red) and calibrated (green) tracking. We also introduced an optimal weight distribution for the calibration process, which consistently outperformed even- weighted calibration, particularly in joints affected by closed- loop kinematics. These results suggest that inaccuracies in virtual link parameters, originated from user-specific fit, can significantly affect tracking and should be explicitly calibrated. In a few cases, such as Subject 3, improvements were not observed, likely due to external factors like motion capture oc- clusion or soft "}
{"id": "2507.23592v1_chunk44", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "ficantly affect tracking and should be explicitly calibrated. In a few cases, such as Subject 3, improvements were not observed, likely due to external factors like motion capture oc- clusion or soft tissue artifacts. Moreover, when initial tracking errors were already low, optimization converged near a local minimum, yielding marginal or oscillatory improvements. When compared to existing tracking systems, our method demonstrated competitive performance. Markerless vision- based methods often report fingertip errors exceeding 10–20mm [23], while our calibrated system achieves fingertip MAE of 10mm—sufficient for teleoperation applications [12]. Compared to other exoskeleton-based approaches that rely on structured calibration routines [24], our method avoids8 constrained motion patterns and demonstrates more robust tracking across users. A key contribution is the validation of performance across a diverse user group, in contrast with prior single-subject evaluations. We also provide b"}
{"id": "2507.23592v1_chunk45", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "nd demonstrates more robust tracking across users. A key contribution is the validation of performance across a diverse user group, in contrast with prior single-subject evaluations. We also provide both quantitative and qualitative evidence of improved motion plausibility using a Unity-rendered virtual hand. Importantly, the proposed calibration framework is gener- alizable. Many hand exoskeletons employ 4-bar linkages or underactuated designs due to the challenge of aligning robotic joints with anatomical joints [15]. These designs require virtual kinematic models whose geometry is affected by unmeasur- able donning conditions. Our approach compensates for such inaccuracies by optimizing virtual link lengths to minimize joint estimation error. While additional sensors improve ob- servability and constraint in the optimization, our method is compatible with systems lacking redundancy. By addressing anatomical variability and user-specific fit, this work helps bridge the gap between hu"}
{"id": "2507.23592v1_chunk46", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "lity and constraint in the optimization, our method is compatible with systems lacking redundancy. By addressing anatomical variability and user-specific fit, this work helps bridge the gap between human intent and virtual or robotic hand motion. Despite the promising results, this study presents several limitations. First, the evaluation was limited to isolated pos- tures rather than continuous movements, primarily due to motion capture occlusion issues. While this enabled precise joint-level analysis, it did not fully capture the temporal complexity of real-world manipulation tasks. Second, the cal- ibration assumes a consistent exoskeleton fit and stable sensor behavior, yet in practice, variations in donning, long-term use, and environmental conditions may affect performance and reduce calibration validity over time. In conclusion, this work lays the foundation for extending subject-specific calibration methods to practical teleoperation and robot learning applications. By combinin"}
{"id": "2507.23592v1_chunk47", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": " calibration validity over time. In conclusion, this work lays the foundation for extending subject-specific calibration methods to practical teleoperation and robot learning applications. By combining subject-specific virtual links optimization with a quantitative validation, we present a framework to enhance the fidelity of exoskeleton- based hand tracking. Qualitative assessments using a simulated teleoperation hand model confirmed that calibration leads to more anatomically plausible postures, and showed improved visual alignment post-calibration. Although residual joint an- gle errors remain in the 10-degree range, the resulting motions are visually coherent and consistent. Such improvements in motion fidelity are essential for downstream teleoperation tasks involving fine manipulation, shared autonomy, or learning from demonstration. Future work will focus on evaluating performance in dynamic, task-based scenarios and integrating the framework into imitation learning pipelines to"}
{"id": "2507.23592v1_chunk48", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "n, shared autonomy, or learning from demonstration. Future work will focus on evaluating performance in dynamic, task-based scenarios and integrating the framework into imitation learning pipelines to assess its effect on policy generalization and data efficiency. REFERENCES [1] J. Zhang, H. Zhao, K. Chen, G. Fei, X. Li, Y . Wang, Z. Yang, S. Zheng, S. Liu, and H. Ding, “Dexterous hand towards intelligent manufacturing: A review of technologies, trends, and potential applications,” Robotics andComputer-Integrated Manufacturing, vol. 95, p. 103021, 2025. [2] C. Yu and P. Wang, “Dexterous manipulation for multi-fingered robotic hands with reinforcement learning: A review,” Frontiers in Neurorobotics, vol. 16, p. 861825, 2022. [3] T. Chen, M. Tippur, S. Wu, V . Kumar, E. Adelson, and P. Agrawal, “Visual dexterity: In-hand reorientation of novel and complex object shapes,” Science Robotics, vol. 8, no. 84, p. eadc9244, 2023. [4] W. Si, N. Wang, and C. Yang, “A review on manipulation skill "}
{"id": "2507.23592v1_chunk49", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "al, “Visual dexterity: In-hand reorientation of novel and complex object shapes,” Science Robotics, vol. 8, no. 84, p. eadc9244, 2023. [4] W. Si, N. Wang, and C. Yang, “A review on manipulation skill acquisition through teleoperation-based learning from demonstration,” Cognitive Computation andSystems, vol. 3, no. 1, pp. 1–16, 2021.[5] C. Mizera, T. Delrieu, V . Weistroffer, C. Andriot, A. Decatoire, and J.- P. Gazeau, “Evaluation of hand-tracking systems in teleoperation and virtual dexterous manipulation,” IEEE Sensors Journal, vol. 20, no. 3, pp. 1642–1655, 2019. [6] J. Fu, M. Poletti, Q. Liu, E. Iovene, H. Su, G. Ferrigno, and E. De Momi, “Teleoperation control of an underactuated bionic hand: Comparison be- tween wearable and vision-tracking-based methods,” Robotics, vol. 11, no. 3, p. 61, 2022. [7] A. Vakunov, C.-L. Chang, F. Zhang, G. Sung, M. Grundmann, and V . Bazarevsky, “Mediapipe hands: On-device real-time hand tracking,” inWorkshop onComputer Vision forAR/VR, vol. 2, p. 5,"}
{"id": "2507.23592v1_chunk50", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": ". 61, 2022. [7] A. Vakunov, C.-L. Chang, F. Zhang, G. Sung, M. Grundmann, and V . Bazarevsky, “Mediapipe hands: On-device real-time hand tracking,” inWorkshop onComputer Vision forAR/VR, vol. 2, p. 5, 2020. [8] F. Mueller, D. Mehta, O. Sotnychenko, S. Sridhar, D. Casas, and C. Theobalt, “Real-time hand tracking under occlusion from an egocen- tric rgb-d sensor,” in Proceedings oftheIEEE international conference oncomputer vision, pp. 1154–1163, 2017. [9] H. S. Moon, G. Orr, and M. Jeon, “Hand tracking with vibrotactile feed- back enhanced presence, engagement, usability, and performance in a virtual reality rhythm game,” International Journal ofHuman–Computer Interaction, vol. 39, no. 14, pp. 2840–2851, 2023. [10] T. Du Plessis, K. Djouani, and C. Oosthuizen, “A review of active hand exoskeletons for rehabilitation and assistance,” Robotics, vol. 10, no. 1, p. 40, 2021. [11] K. Darvish, L. Penco, J. Ramos, R. Cisneros, J. Pratt, E. Yoshida, S. Ivaldi, and D. Pucci, “Teleoperation of hu"}
{"id": "2507.23592v1_chunk51", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "eletons for rehabilitation and assistance,” Robotics, vol. 10, no. 1, p. 40, 2021. [11] K. Darvish, L. Penco, J. Ramos, R. Cisneros, J. Pratt, E. Yoshida, S. Ivaldi, and D. Pucci, “Teleoperation of humanoid robots: A survey,” IEEE Transactions onRobotics, vol. 39, no. 3, pp. 1706–1727, 2023. [12] D. Leonardis, M. Gabardi, S. Marcheschi, M. Barsotti, F. Porcini, D. Chiaradia, and A. Frisoli, “Hand teleoperation with combined ki- naesthetic and tactile feedback: A full upper limb exoskeleton interface enhanced by tactile linear actuators,” Robotics, vol. 13, no. 8, p. 119, 2024. [13] K. Xia, X. Chen, X. Chang, C. Liu, L. Guo, X. Xu, F. Lv, Y . Wang, H. Sun, and J. Zhou, “Hand exoskeleton design and human–machine interaction strategies for rehabilitation,” Bioengineering, vol. 9, no. 11, p. 682, 2022. [14] S. N. Yousaf, G. Mukherjee, R. King, and A. D. Deshpande, “Ex- perimental and simulation-based estimation of interface power during physical human-robot interaction in hand exoskeletons"}
{"id": "2507.23592v1_chunk52", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "82, 2022. [14] S. N. Yousaf, G. Mukherjee, R. King, and A. D. Deshpande, “Ex- perimental and simulation-based estimation of interface power during physical human-robot interaction in hand exoskeletons,” IEEE Robotics andAutomation Letters, vol. 9, no. 3, pp. 2575–2581, 2023. [15] M. Sarac, M. Solazzi, D. Leonardis, E. Sotgiu, M. Bergamasco, and A. Frisoli, “Design of an underactuated hand exoskeleton with joint estimation,” in Advances inItalian Mechanism Science: Proceedings oftheFirst International Conference ofIFToMM Italy, pp. 97–105, Springer, 2016. [16] W. Wei, B. Zhou, B. Fan, M. Du, G. Bao, and S. Cai, “An adaptive hand exoskeleton for teleoperation system,” Chinese Journal ofMechanical Engineering, vol. 36, no. 1, p. 60, 2023. [17] H. Zhang, S. Hu, Z. Yuan, and H. Xu, “Doglove: Dexterous manipulation with a low-cost open-source haptic force feedback glove,” arXiv preprint arXiv:2502.07730, 2025. [18] Y . Park, S. Lee, and J. Bae, “Wehaptic-light: A cable slack-based compact ha"}
{"id": "2507.23592v1_chunk53", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "xterous manipulation with a low-cost open-source haptic force feedback glove,” arXiv preprint arXiv:2502.07730, 2025. [18] Y . Park, S. Lee, and J. Bae, “Wehaptic-light: A cable slack-based compact hand force feedback system for virtual reality,” Mechatronics, vol. 79, p. 102638, 2021. [19] E. Amirpour, R. Fesharakifard, H. Ghafarirad, S. M. Rezaei, A. Saboukhi, M. Savabi, and M. R. Gorji, “A novel hand exoskeleton to enhance fingers motion for tele-operation of a robot gripper with force feedback,” Mechatronics, vol. 81, p. 102695, 2022. [20] Y . Yun, P. Agarwal, and A. D. Deshpande, “Accurate, robust, and real-time pose estimation of finger,” Journal ofDynamic Systems, Measurement, andControl, vol. 137, no. 3, p. 034505, 2015. [21] P. Agarwal, J. Fox, Y . Yun, M. K. O’Malley, and A. D. Deshpande, “An index finger exoskeleton with series elastic actuation for rehabilitation: Design, control and performance characterization,” The International Journal ofRobotics Research, vol. 34, no. "}
{"id": "2507.23592v1_chunk54", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "eshpande, “An index finger exoskeleton with series elastic actuation for rehabilitation: Design, control and performance characterization,” The International Journal ofRobotics Research, vol. 34, no. 14, pp. 1747–1772, 2015. [22] P. Agarwal, Y . Yun, J. Fox, K. Madden, and A. D. Deshpande, “Design, control, and testing of a thumb exoskeleton with series elastic actuation,” TheInternational Journal ofRobotics Research, vol. 36, no. 3, pp. 355– 375, 2017. [23] D. Abdlkarim, M. Di Luca, P. Aves, M. Maaroufi, S.-H. Yeo, R. C. Miall, P. Holland, and J. M. Galea, “A methodological framework to assess the accuracy of virtual reality hand-tracking systems: A case study with the meta quest 2,” Behavior research methods, vol. 56, no. 2, pp. 1052–1063, 2024. [24] Y . Su, G. Li, Y . Deng, I. Sarakoglou, N. G. Tsagarakis, and J. Chen, “The joint-space reconstruction of human fingers by using a highly under-actuated exoskeleton,” in 2024 IEEE International Conference on Robotics andAutomation (ICRA)"}
{"id": "2507.23592v1_chunk55", "paper_id": "2507.23592v1", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "text": "u, N. G. Tsagarakis, and J. Chen, “The joint-space reconstruction of human fingers by using a highly under-actuated exoskeleton,” in 2024 IEEE International Conference on Robotics andAutomation (ICRA), pp. 9645–9651, IEEE, 2024."}
{"id": "2507.23784v1_chunk0", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions Jessica Bader1Leander Girrbach1Stephan Alaniz2* Zeynep Akata1 1Technical University of Munich, Helmholtz Munich, Munich Center for Machine Learning (MCML) 2LTCI, T ´el´ecom Paris, Institut Polytechnique de Paris, France jessica.bader@tum.de Abstract Concept Bottleneck Models (CBMs) and other concept- based interpretable models show great promise for mak- ing AI applications more transparent, which is essential in fields like medicine. Despite their success, we demon- strate that CBMs struggle to reliably identify the correct concepts under distribution shifts. To assess the robust- ness of CBMs to concept variations, we introduce SUB: a fine-grained image and concept benchmark containing 38,400 synthetic images based on the CUB dataset. To cre- ate SUB, we select a CUB subset of 33 bird classes and 45 concepts to generate images which substitute a specific concept, such as wing color or belly pattern. We intro- "}
{"id": "2507.23784v1_chunk1", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "d on the CUB dataset. To cre- ate SUB, we select a CUB subset of 33 bird classes and 45 concepts to generate images which substitute a specific concept, such as wing color or belly pattern. We intro- duce a novel Tied Diffusion Guidance (TDG) method to precisely control generated images, where noise sharing for two parallel denoising processes ensures that both the correct bird class and the correct attribute are generated. This novel benchmark enables rigorous evaluation of CBMs and similar interpretable models, contributing to the de- velopment of more robust methods. Our code is available at https://github.com/ExplainableML/sub and the dataset at http://huggingface.co/datasets/Jessica-bader/SUB. . 1. Introduction While deep learning models excel on complex tasks, they are often criticized for lack of transparency in their reason- ing, causing a severe bottleneck in the deployment of deep models in real-world contexts. For example, in the medi- cal field, the model’s reasoning must b"}
{"id": "2507.23784v1_chunk2", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "ed for lack of transparency in their reason- ing, causing a severe bottleneck in the deployment of deep models in real-world contexts. For example, in the medi- cal field, the model’s reasoning must be present in order to be used by physicians. Interpretable models are essential to address these needs. One core method is the Concept Bot- tleneck Model (CBM) [30], which generates intermediate, interpretable concepts to inform the final prediction. CBM evaluation exhibits a limitation, as demonstrated in Fig. 1. We expect the CBM to assign the bottom bird the label yellow crown , but it still identifies a blue crown . In fact, the CBM predicts the exact concept vector associ- *Work was done at TUM and Helmholtz Munich. CBM Blue Jay correct incorrect detected not detected …Blue crown Yellow crown Blue wings White wings Concepts c Blue Jay …Red head White head Red belly White belly Red wings White wings Concepts c Cardinal Predict y correct incorrect detected not detected …Red head White h"}
{"id": "2507.23784v1_chunk3", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "rown Blue wings White wings Concepts c Blue Jay …Red head White head Red belly White belly Red wings White wings Concepts c Cardinal Predict y correct incorrect detected not detected …Red head White head Red belly White belly Red wings White wings Concepts c Cardinal Predict y SUB: Dataset with Attribute Substitutions Benchmarking CBM Generalization Tied Guidance (Ours) T2I Prompting “Blue Jay with a yellow crown” Class: Blue Jay ; Substitution: Blue Crown → Yellow Crown Predict yPredict y CLSCLS CBM Real SUB …Blue crown Yellow crown Blue wings White wings Concepts c Figure 1. (Top) TGD modifies attributes where prompting fails. (Bottom) The CBM generalizes poorly, memorizing the “Blue Jay” concept vector and mis-classifying the modified concept. ated with the Blue Jay class (middle), despite clear differ- ences. This observation suggests that the CBM may ground predictions in extraneous factors, potentially entirely unre- lated to the visible concepts, rather than those in the image. "}
{"id": "2507.23784v1_chunk4", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "ite clear differ- ences. This observation suggests that the CBM may ground predictions in extraneous factors, potentially entirely unre- lated to the visible concepts, rather than those in the image. Ergo, the CBM defaults to predicting the concept vector of the most similar training class. Such behavior raises doubts about the validity of the so-called concept predictions as re- liable interpretability tools. As Koh et al. [30] evaluated their CBM solely on training classes, we cannot distinguish if the model genuinely learned to identify concepts or sim- ply memorized the concept vectors of the target classes. In this work, our objective is to evaluate the general- ization of concept predictions in CBMs and Vision Lan-arXiv:2507.23784v1 [cs.CV] 31 Jul 2025guage Models (VLM) that underpin interpretable architec- tures, particularly when input images contain novel combi- nations of known concepts. More specifically, we focus on small deviations from the training classes, with single co"}
{"id": "2507.23784v1_chunk5", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " interpretable architec- tures, particularly when input images contain novel combi- nations of known concepts. More specifically, we focus on small deviations from the training classes, with single con- cept alterations. In this way, we isolate individual attributes, ensuring the model cannot rely on other cues to shortcut predictions. Furthermore, we propose Tied Diffusion Guid- ance (TDG) to generate these single concept substitutions. As shown in Fig. 1 (top), naively prompting a latent diffu- sion model (LDM) for a “Blue Jay with a yellow crown” does not yield the desired result. Instead, TDG generates a second image where “yellow crown” appears naturally. By tying the diffusion processes, we are able to successfully substitute the desired attribute on the “Blue Jay” class. Using TDG, we create our synthetic dataset: Substitutions on Caltech-U CSD B irds-200-2011 (SUB), consisting of 38,400 images for evaluating interpretable models trained on the CUB dataset [63] or with open voca"}
{"id": "2507.23784v1_chunk6", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "we create our synthetic dataset: Substitutions on Caltech-U CSD B irds-200-2011 (SUB), consisting of 38,400 images for evaluating interpretable models trained on the CUB dataset [63] or with open vocabularies. Leveraging SUB, we find that CBMs and VLMs fail to generalize to novel combinations of known concepts. In particular, we show that a number of CBMs trained with both class- and image-level concept labels, as well as a variety of leading VLMs, cannot reliably detect concepts. This provides strong evidence that these models infer concepts from the predicted class rather than grounding them in the image. In summary, our contributions are the following: (1) We propose a test-time LDM modification to generate novel combinations of known concepts. (2) We release SUB, an evaluation dataset for interpretable models, which is the first photorealistic image dataset to isolate concepts before eval- uating classification. (3) We reveal that existing CBMs and VLMs fail to generalize to new co"}
{"id": "2507.23784v1_chunk7", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "for interpretable models, which is the first photorealistic image dataset to isolate concepts before eval- uating classification. (3) We reveal that existing CBMs and VLMs fail to generalize to new combinations of known con- cepts, raising concerns about their interpretability. 2. Related Work In explainable AI, concept-based models have emerged as a powerful interpretability tool, as prototypical parts [3, 5, 40, 43, 52, 53, 62], sparse auto-encoders [11, 28, 35, 50, 60, 70], self-explaining models [2], or Concept Bottleneck Models (CBM) [30, 42, 44, 59, 66]. CBMs in particular are valued for their ability to pre-define key concepts and enable inter- ventions, making them useful in fields like medicine [1, 10, 41]. Since their creation, CBMs have become more flex- ible by eliminating the need for labeled data [42, 66], fa- cilitating open-vocabulary concept addition and deletion at test time [59], allowing the integration of unsupervised con- cepts [55], improving intervention success"}
{"id": "2507.23784v1_chunk8", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "need for labeled data [42, 66], fa- cilitating open-vocabulary concept addition and deletion at test time [59], allowing the integration of unsupervised con- cepts [55], improving intervention success [56], and more. Nonetheless, follow-up CBM evaluation has revealed that many do not function as intended [23, 34, 37, 48, 57]. Much of this research has focused on information leakage in the pre-defined concepts [21, 34, 36, 37, 68], a phe- nomenon that has been linked to soft labels [34]. Althoughour work does not focus on dataset leakage, these related works have revealed that CBMs tend to focus on incorrect visual cues and are prone to overfitting to irrelevant infor- mation. Other CBM analyses have explored their robust- ness [57], how they respect image locality [48], the impact of concept correlation [23, 49], and their performance on cleaner tasks [19]. Heidemann et al. [23] demonstrated that CBMs struggle with attribute classification, specifi- cally when given highly correlated c"}
{"id": "2507.23784v1_chunk9", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "pt correlation [23, 49], and their performance on cleaner tasks [19]. Heidemann et al. [23] demonstrated that CBMs struggle with attribute classification, specifi- cally when given highly correlated concepts. Different from the previous work, we evaluate novel combinations of known concepts, showing that concept predictions are not grounded in the image. This may reflect limited training- time attribute combinations, indicating insufficient compo- sitional support for generalization. [65]. Tangentially, image generation models have been gain- ing attention for impressive generation capabilities [31, 46, 51]. Research has focused on enhancing prompt- following [4, 6, 17, 18, 64] and improving controllabil- ity [39, 71, 72]. Efforts to support compositionality have ex- plored the combination of models [13], objects [14, 15, 33], attributes [20], relations [32]. Composed GLIDE [33] fa- vors object co-occurrence (e.g., showing multiple birds) through their noise injection method, compared "}
{"id": "2507.23784v1_chunk10", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " of models [13], objects [14, 15, 33], attributes [20], relations [32]. Composed GLIDE [33] fa- vors object co-occurrence (e.g., showing multiple birds) through their noise injection method, compared to TDG which excels at texture edits and attribute manipulation. While CoInD [20] handles attributes, it requires training- time integration, unlike TDG, which adapts at test time. Alongside the improved capabilities of these models, there is increasing exploration into synthetic data, both for training [12, 16, 22, 29, 54] and evaluation [24, 27, 45]. Previous works have explored the use of synthetic datasets specifically to enhance explainability [23, 24]. Synthetic images enable precise manipulations to isolate individual features, which is particularly valuable for evaluating and enforcing explainability. FunnyBirds [24] consists of imag- inary birds created from independent attributes, which are removed and replaced to evaluate model explanations. Sim- ilarly, Heidemann et al. [23] te"}
{"id": "2507.23784v1_chunk11", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " explainability. FunnyBirds [24] consists of imag- inary birds created from independent attributes, which are removed and replaced to evaluate model explanations. Sim- ilarly, Heidemann et al. [23] tested CBMs using a compara- ble dataset. Different from the previous works, the images in SUB are far more natural, bringing them much closer to real-world problems. Since our generated images re- semble the types of birds found in the widely used CUB dataset [63], they can be leveraged to evaluate existing mod- els trained on bird classes, eliminating the need for special- ized training and creating a natural evaluation environment. 3. Tied Diffusion Guidance (TDG) To build our SUB benchmark, we make fine-grained ed- its of individual bird attributes from the CUB dataset [63]. While LDMs follow prompts well, they falter on novel at- tribute combinations. To address this, we propose Tied Dif- fusion Guidance (TDG), a test-time method that enhances attribute-level control in text-to-image LD"}
{"id": "2507.23784v1_chunk12", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "w prompts well, they falter on novel at- tribute combinations. To address this, we propose Tied Dif- fusion Guidance (TDG), a test-time method that enhances attribute-level control in text-to-image LDMs. Latent Diffusion Models (LDMs) generate images byμ : “A blue jay with a yellow crown” : “A bird with a yellow crown” -1-1 iterate T times ×T steps DiT DiT μAverag e Sum Negation Multiplication μ : “A blue jay with a yellow crown” : “A bird with a yellow crown” -1-1×T steps DiT DiT μAverag e Sum Negation Multiplication : “A blue jay with a yellow crown” : “A bird with a yellow crown” -1-1×T steps DiT DiT μAverag e Sum Negation Multiplication μ : “A blue jay with a yellow crown” : “A bird with a yellow crown” -1-1×T steps DiT avg+ + - - : “A blue jay with a yellow crown” : “A bird with a yellow crown” -1-1 ×T steps DiT DiT : “A blue jay with a yellow crown” : “A bird with a yellow crown” -1-1×T steps DiT DiT μAverag e Sum Negation + 2 Noise prediction Mask Denoising step Noise prediction"}
{"id": "2507.23784v1_chunk13", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "low crown” -1-1 ×T steps DiT DiT : “A blue jay with a yellow crown” : “A bird with a yellow crown” -1-1×T steps DiT DiT μAverag e Sum Negation + 2 Noise prediction Mask Denoising step Noise prediction Masking Denoising step - - Shared Weights Tied Diffusion Guidance (TDG) d Mask Masked noise Figure 2. In Tied Diffusion Guidance, two images are generated from related prompts. At each step, the Diffusion Transformer (DiT) predicts the noises ˜ϵR θ,tand˜ϵG θ,tfor each image. We compare these predictions and, thresholded by η, retain the original noise where they differ and average the predictions where they are similar. The modified noises ¯ϵR θ,tand¯ϵG θ,tare subtracted from the images, denoising them. This is repeated for Tsteps with decreasing η, ensuring the images are highly constrained at the start but independent by the end. denoising a sample from Gaussian noise xTto a target im- agex0overTsteps. For text-to-image tasks, they model p(x|c), where cis a text prompt. In practice, ϵθi"}
{"id": "2507.23784v1_chunk14", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "at the start but independent by the end. denoising a sample from Gaussian noise xTto a target im- agex0overTsteps. For text-to-image tasks, they model p(x|c), where cis a text prompt. In practice, ϵθis trained to predict the noise in xtgiven cat each step t, using the loss: min θE(x,c)∼D, ϵ∼N(0,1), th ∥ϵ−ϵθ(xt, c, t)∥2 2i .(1) Diffusion Guidance. The LDM is trained both with text conditions and unconditionally ( c=∅). Classifier-free guidance [25] is applied at inference time using: ˜ϵθ(xt, c, t) =ϵθ(xt, t) +sg(ϵθ(xt, c, t)−ϵθ(xt, t))(2) where guidance scale sgcontrols prompt condition strength. While text-to-image models generally follow prompts well, they often fail at zero-shot compositions, for example, ig- noring the “yellow crown” edit in favor of a typical “Blue Jay” (Fig. 1, top left). As prompts alone do not capture all SUB edits, we propose a test-time adaptation with semantic guidance from a reference image for attribute modifications. Tied Diffusion Guidance (TDG). Our goal"}
{"id": "2507.23784v1_chunk15", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "ft). As prompts alone do not capture all SUB edits, we propose a test-time adaptation with semantic guidance from a reference image for attribute modifications. Tied Diffusion Guidance (TDG). Our goal is to gener- ate an image of a reference classRwith an attribute sub- stitution S, replacing the original attribute S−(e.g., blue crown) with a target attribute S+(e.g., yellow crown), while preserving R’s remaining attributes. To overcome LDMs’ struggles with zero-shot composition, we introduce guid- ance from a guidance classG, where the target attribute S+ is in-distribution and easier to generate. As shown in Fig. 2, we propose to tie the generation of the two images, using G to guide Rto have one attribute substituted according to S.To achieve single-attribute substitution, we generate paired images with separate prompts, cRandcG, related toRandGwith target attribute S+, respectively. We start from the same noise xR T=xG T, and tie the noise predictions element-wise. Given two indepe"}
{"id": "2507.23784v1_chunk16", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "aired images with separate prompts, cRandcG, related toRandGwith target attribute S+, respectively. We start from the same noise xR T=xG T, and tie the noise predictions element-wise. Given two independent noise predictions ˜ϵ(1) and˜ϵ(2), we apply µ(˜ϵ(1),˜ϵ(2), η)i= ( ˜ϵ(1) i+˜ϵ(2) i 2where |˜ϵ(1) i−˜ϵ(2) i| ≤ηthpercentile ˜ϵ(1) i otherwise(3) to obtain the mean noise prediction for all elements i(i.e. image pixels) where the prediction difference is below the ηthpercentile and keep the noise prediction ˜ϵ(1)otherwise. We define an ηschedule that begins with the two predictions strongly tied and is loosened towards the end of generation: η(t, tmin, tmax, k) =  1 ift > t max\u0010 t−tmin tmax−tmin\u0011k iftmin≤t≤tmax 0 ift < t min (4) where tmaxcontrols the length of the initial strict noise tying phase, and kregulates the transition to independent genera- tion (from tminonwards). We apply noise tying symmetrically to both images and update their individual noise predictions using: ¯ϵR "}
{"id": "2507.23784v1_chunk17", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "e tying phase, and kregulates the transition to independent genera- tion (from tminonwards). We apply noise tying symmetrically to both images and update their individual noise predictions using: ¯ϵR θ,t=µ(˜ϵR θ,t,˜ϵG θ,t, η(t, tmin, tmax, k)) (5)200 Bird Classes 312 Attributes Clf. VQA 33 Bird Classes 45 Attributes TDG VQA Human SUB CUB 1,485 Pairings 1,485 Pairings 74,250 Images 768 Pairings 768 Pairings 38,400 Images 831 Pairings 831 Pairings 41,550 Images ≥ 90% Acc. ≥ 90% Agreement ≥ 90% Label Success Image Generation Candidate Selection Filtering Validation Figure 3. Meticulous filtering ensures that SUB is faithful to the target classes and attributes. Starting with the CUB label set [63], we retain only the best images from the most consistent and detectable bird-attribute pairings. ¯ϵG θ,t=µ(˜ϵG θ,t,˜ϵR θ,t, η(t, tmin, tmax, k)) (6) where ˜ϵR θ,t:= ˜ϵθ(xR t, cR, t)and˜ϵG θ,t:= ˜ϵθ(xG t, cG, t). In summary, TDG ties noise predictions for similar pixels but allows divergence wher"}
{"id": "2507.23784v1_chunk18", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " ¯ϵG θ,t=µ(˜ϵG θ,t,˜ϵR θ,t, η(t, tmin, tmax, k)) (6) where ˜ϵR θ,t:= ˜ϵθ(xR t, cR, t)and˜ϵG θ,t:= ˜ϵθ(xG t, cG, t). In summary, TDG ties noise predictions for similar pixels but allows divergence where prompt guidance differs. During generation, we gradually loosen the constraint, ultimately generating the images independently with cRandcG, and ultimately discarding the guide image. 4. SUB Dataset We introduce SUB, a benchmark with fine-grained attribute edits to evaluate concept prediction faithfulness. SUB builds upon CUB [63], comprising 38,400 synthesized im- ages of bird classes with substituted attributes generated with TDG. It consists of 768 unique combinations of CUB species Rand target attributes S+, with 50 images per com- bination. The creation process is detailed in this section and visualized in Fig. 3. CUB Preliminaries. The CUB dataset [63] consists of 11,788 images of 200 bird species, annotated with at- tributes about bird parts and their properties. It is commonly us"}
{"id": "2507.23784v1_chunk19", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "on and visualized in Fig. 3. CUB Preliminaries. The CUB dataset [63] consists of 11,788 images of 200 bird species, annotated with at- tributes about bird parts and their properties. It is commonly used for fine-grained classification and model explainabil- ity. CUB includes 28 attribute groupings A(e.g., leg color, belly pattern), where each image is labeled with 312 binary values representing the presence or absence of individual attributes (e.g., black leg, spotted belly). 4.1. Prompts We generate synthetic images using our novel TDG (de- scribed in Section 3) with the state-of-the-art (SOTA) text- to-image DM FLUX.1-dev [31]. Attribute substitutions S are chosen from three categories: color, shape, and pattern, where a target attribute S+∈ AS\\ {S−}is selected to dif- fer from the reference bird. As the three categories vary in difficulty, prompts are adjusted to achieve the desired mod- ification and avoid attribute ambiguity, requiring a minimal amount of human intervention. The f"}
{"id": "2507.23784v1_chunk20", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "ence bird. As the three categories vary in difficulty, prompts are adjusted to achieve the desired mod- ification and avoid attribute ambiguity, requiring a minimal amount of human intervention. The full details on reference birdsR, guidance birds G, substitutions S, and example prompts cR, cGare found in Appendices A, B, and D.Color is the easiest substitution for FLUX, allowing the generic term G=“bird”. We use cR=“a photo of a {R} with a {S+}” and cG=a photo of a bird with a {S+}. Shape is of medium modification difficulty, hence we explicitly specify the guide bird (e.g. Gcone beak =“song sparrow ”). We discourage excessive modifications by en- couraging the retention of the reference bird’s body shape, using cR=“a photo of a {R} with the body of a {R} and a beak like a {GS+}” and cG=“a photo of a {GS+}”. Texture is the toughest substitution, also requiring a spe- cific guidance bird GS+per target attribute S+.S+is in- cluded in cGto ensure S−fromRis changed to match S+ fromG, rath"}
{"id": "2507.23784v1_chunk21", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "a photo of a {GS+}”. Texture is the toughest substitution, also requiring a spe- cific guidance bird GS+per target attribute S+.S+is in- cluded in cGto ensure S−fromRis changed to match S+ fromG, rather than vice versa. We use cR=“a photo of a{R} with a {S+}like a{GS+}” and cG=“a photo of a {GS+}with a {S+}”. 4.2. Data Filtering We use a filtering mechanism, common in synthetic dataset creation [16, 22], to ensure the faithful representation of ma- nipulated attributes. First, we identify suitable candidates for reference birds and attribute substitutions and generate Nimages for each bird-attribute pairing. Next, we use an automatic visual-question-answering (VQA) evaluation, to- gether with a human validation step to identify and remove images that did not modify S+correctly or deviate from the reference bird R. This process helps quantify the added variety and reliability of our attribute modifications. Candidate Selection. We start by identifying suitable reference birds and attrib"}
{"id": "2507.23784v1_chunk22", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "rom the reference bird R. This process helps quantify the added variety and reliability of our attribute modifications. Candidate Selection. We start by identifying suitable reference birds and attributes. Reference birds must be reli- ably depicted by the generative model. Hence, we generate 20 images per CUB class with FLUX [31] and measure per- class classification accuracy with a CUB pre-trained model1 which achieves 88% accuracy on the CUB test set. We se- lect the 33 classes for which the classifier achieves 100% accuracy on the listed in Appendix A). For attribute candidates, we evaluate a VQA model’s ability to identify them by creating questions for the 28 at- tribute groups Aiwith the prompt 1https : / / huggingface . co / Emiel / cub - 200 - bird - classifier-swinPrompting Tied Guidance R: Cardinal Prompting Tied Guidance Real ( ) Real ( ) G: Hummingbird S: Cone Beak → Needle Beak R: American Goldfinch G: Hummingbird S: Cone Beak → Needle Beak R: European Goldfinch G: Brown "}
{"id": "2507.23784v1_chunk23", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "Guidance R: Cardinal Prompting Tied Guidance Real ( ) Real ( ) G: Hummingbird S: Cone Beak → Needle Beak R: American Goldfinch G: Hummingbird S: Cone Beak → Needle Beak R: European Goldfinch G: Brown Thrasher S: Solid Belly → Spotted Belly R: Horned Puffin G: Song Sparrow S: Solid Belly → Striped Belly R: Blue Jay G: Bird S: Black Beak → Yellow Beak R: Pied Kingfisher G: Bird S: White Breast → Red Breast R: White pelican G: Bird S: White Crown → Pink Crown R: Gadwall G: Bird S: Primarily Buff → Primarily Orange Figure 4. Images generated with TDG (green) are high-quality and more faithfully represent both the reference bird Rand target attribute substitution Sthan those generated with prompting alone (red) . TDG generates an additional guidance image xG, which we discard. What type of {Ai}does this bird have? Please pick between A){a1} A){a2} C){. . .} D) Other where {Ai}corresponds to the i-th attribute group (e.g. “eye color”) and the options aj∈ A icontain all manifes- tations of th"}
{"id": "2507.23784v1_chunk24", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "does this bird have? Please pick between A){a1} A){a2} C){. . .} D) Other where {Ai}corresponds to the i-th attribute group (e.g. “eye color”) and the options aj∈ A icontain all manifes- tations of that attribute group (e.g. “red”, “black”, etc). We also include “Other”, for when none of the attributes match. As model, we use InternVL-2.5-8B [7–9], which yields the best performance-efficiency tradeoff. We run the VQA eval- uation on the full CUB dataset and choose the 45 attributes that obtain accuracy ≥90% when optimizing the answer probability threshold between 60% and95%. VQA Filtering. We construct substitutions Sby pair- ing attribute candidates S+with bird candidates Rthat do not already have this attribute ( S+̸=S−), resulting in 1,485 bird-attribute combinations. We generate images for these combinations using TDG and verify the correctness of S+with a VQA model (InternVL-2.5-8B) using the same prompt as attribute candidate selection. Images with incor- rect attribute predictio"}
{"id": "2507.23784v1_chunk25", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " for these combinations using TDG and verify the correctness of S+with a VQA model (InternVL-2.5-8B) using the same prompt as attribute candidate selection. Images with incor- rect attribute predictions are discarded, and we rank those remaining by target answer confidence (i.e., probability). We retain only the top 10% of images, corresponding to 50out of 500 images generated per pairing. If filtering retains less than 10%, the pairing is eliminated. After this stage, 831 bird-attribute pairings remain for the SUB dataset. 4.3. Human Validation To assess the quality of our synthetic images, we conduct a human study to verify the per-attribute VQA accuracy and the faithfulness of attribute modifications S+to the refer- ence bird classes R. Human annotators evaluate 40 ran- domly selected images per attribute, answering whether the attribute modification was faithfully applied and whether the generated bird deviates significantly from the refer- ence bird. For both questions, there are "}
{"id": "2507.23784v1_chunk26", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "mages per attribute, answering whether the attribute modification was faithfully applied and whether the generated bird deviates significantly from the refer- ence bird. For both questions, there are three possible re- sponses: ”yes”, ”somewhat”, and ”no” (the human anno- tation interface is shown in Appendix E). We discard can- didate attributes with <90% accuracy across all reference birds, based on annotators choosing ”yes”. Additionally, we remove individual bird-attribute pairings with low at- tribute accuracy, excessive modifications beyond the target attribute, or where the reference birds already displayed the target attribute ( S+=S−). The resulting SUB dataset con- sists of 33 CUB bird classes, 15 attribute groupings with 32 unique target attributes, and 768 unique bird-attribute pair- ings, totaling 38,400 images (50 images per pairing).5. Experiments In this section, we first examine the quality of our SUB dataset, followed by an evaluation of CBM models on SUB to assess th"}
{"id": "2507.23784v1_chunk27", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " pair- ings, totaling 38,400 images (50 images per pairing).5. Experiments In this section, we first examine the quality of our SUB dataset, followed by an evaluation of CBM models on SUB to assess their performance independently of the ref- erence bird’s appearance. SUB’s images are generated with FLUX.1-dev [31] with default guidance scale 3.5. For TDG, we use k= 10 ,tmin= 0.2,tmax= 0.6for pattern attributes, and tmax= 0.9for shape and color attributes. These values were selected by looking at 3-5 birds across 2- 4 specific attributes for each category (color, texture, shape). We generated 50 images per pair for all 33×45bird- attribute pairs, and checked that 5+ images per pair for 10+ birds were suitable on 2-3 attributes. Optimizing the hyper- parameters across the full pipeline would be too costly. 5.1. Qualitative TDG Examples from SUB Figure 4 illustrates eight examples generated with TDG. For each image, we include the real reference bird R, and a sample generated by prompting"}
{"id": "2507.23784v1_chunk28", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " be too costly. 5.1. Qualitative TDG Examples from SUB Figure 4 illustrates eight examples generated with TDG. For each image, we include the real reference bird R, and a sample generated by prompting FLUX [31] out-of- the-box to produce the target substitution S+, using the same prompt as the reference image xRfrom TDG (exact prompts are described in Section 4.1). Finally, we present both the reference image xRfrom SUB, and the corre- sponding guidance image xG. From these examples, we ob- serve that TDG is capable of generating realistic attribute- modified images. TDG reliably alters the target attribute’s shape, color, or texture while maintaining the overall recog- nizability of the reference bird, as seen in the top-left image of the Cardinal with a needle-shaped beak. While standard diffusion generates simple modifications successfully, like changing the White Pelican’s crown to pink (bottom left), TDG is capable of modifying more challenging attributes such as texture (second r"}
{"id": "2507.23784v1_chunk29", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " diffusion generates simple modifications successfully, like changing the White Pelican’s crown to pink (bottom left), TDG is capable of modifying more challenging attributes such as texture (second row). Interestingly, although the re- sulting guidance bird image xGis usually visually distinct from xR, it sometimes converges to the same image (bot- tom left, third row on the right). Despite this, TDG remains essential for applying target substitutions. 5.2. VQA and Human Filtering Results VQA filtering eliminates images where TDG substituted improperly. In Fig. 5, we present results for errors flagged by our filtering method and images that passed. InternVL- 2.5-8B filters images where the target attribute is not visi- ble (top left), the wrong attribute is modified (top right), or the model does not make meaningful modifications (bottom left). Through human verification, we further find and re- move bird-attribute combinations where the VQA model is unreliable. The images contained i"}
{"id": "2507.23784v1_chunk30", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "del does not make meaningful modifications (bottom left). Through human verification, we further find and re- move bird-attribute combinations where the VQA model is unreliable. The images contained in SUB (green) are high quality and consistently show the target attribute. 5.3. SUB vs. CUB Label Correctness Although widely used, CUB [63] has faced criticism for la- beling errors [26]. To address inconsistent labeling, CBMs VQA Passed Grey back Multi-colored tail White wings Blue throat Spotted belly Primarily blue Yellow upperparts Red breast VQA Filtered VQA Passed VQA Filtered Human Filtered Figure 5. Many failure cases are flagged by our VQA filtering (red), leaving well-modified images in SUB (green). are trained with per-class attribute vectors determined by majority voting [30]. We ensure that SUB does not inherit these inconsistencies by evaluating attribute quality with human annotations. For CUB, we group images by the pres- ence of an attribute label in the per-class attribu"}
{"id": "2507.23784v1_chunk31", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "ensure that SUB does not inherit these inconsistencies by evaluating attribute quality with human annotations. For CUB, we group images by the pres- ence of an attribute label in the per-class attribute vector and calculate the ratio of images where the per-image la- bel matches (i.e. individual annotations align with the class concepts). For SUB, we calculate average score (“yes” = 1, “somewhat” =0.5, “no” =0) per attribute across the images in the human validation study (see Sec. 4.2). The results for 17 attributes (overlap of 32 SUB attributes and 112 attributes typically used for CBMs) are presented in Fig. 6. As previously mentioned, CUB image-level anno- tations are often inconsistent with class-level attribute vec- tors, with only 57.50% of labels agreeing with the class- level annotations. On the other hand, SUB’s annotations ac- curately represent the images, with 98.90% of all attributes being correctly labeled according to human agreement. 5.4. CBM Evaluation Setup Using SUB"}
{"id": "2507.23784v1_chunk32", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "tations. On the other hand, SUB’s annotations ac- curately represent the images, with 98.90% of all attributes being correctly labeled according to human agreement. 5.4. CBM Evaluation Setup Using SUB as an evaluation set, we assess CBMs ability to generalize to our novel combinations of known concepts. Base CBMs. CBMs can be trained in three ways [30]: 1) jointly, where the image-to-concept and concept-to-label components are trained simultaneously using supervised learning; 2) sequentially, where the image-to-concept com- ponent is trained first, then the concept-to-label component with the former frozen; and 3) independently, where the two components are trained separately, with the class-to-label network receiving only ground-truth concept labels. We ex-Throat Color = yellow Crown Color = white Crown Color = black Breast Color = white Wing Color = blackPrimary Color = brownWing Color = white Crown Color = grey Wing Color = greyForehead Color = greyBill Color = grey Leg Color = blac"}
{"id": "2507.23784v1_chunk33", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "r = white Crown Color = black Breast Color = white Wing Color = blackPrimary Color = brownWing Color = white Crown Color = grey Wing Color = greyForehead Color = greyBill Color = grey Leg Color = black T ail Pattern = solidLeg Color = grey Back Color = grey T ail Pattern = multi-colored0.00.20.40.60.81.0Ratio of correct annotationsData CUB SUBFigure 6. For all 17 attributes included in both SUB and CUB [63], SUB has higher annotation accuracy (std. as error bar). For CUB, we measure agreement between image-level and class-level attributes. For SUB, we use the attribute score from our human validation study. clude sequential training from our evaluation, as it yields the same concept predictions as independent training. Per-Concept CBM. CBMs can be trained individually per concept to improve concept accuracy [23], making the model less likely to look at spurious information, as it does not share weights with other attributes. Concept Embedding Models (CEM). CEMs attempt to overcome the "}
{"id": "2507.23784v1_chunk34", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "e concept accuracy [23], making the model less likely to look at spurious information, as it does not share weights with other attributes. Concept Embedding Models (CEM). CEMs attempt to overcome the interpretability-performance tradeoff seen in CBMs by using a concept vector rather than a binary pre- diction [67]. This vector contains two activations, one sym- bolizing a concept’s presence and the second its absence. Concept Labels. We explore labeling choices. The CBM authors proposed to train the model with fixed at- tribute vectors per class [30]. However, fixing the la- bels across images could hurt generalization by forcing the model to output identical predictions regardless of inter- image fluctuations. We explore models trained with class- defined attribute vectors and with the original per-image la- bels. Moreover, concepts can be supervised by binary at- tribute labels (hard) or by annotator confidence (soft). Human Labeling: SUB. To ensure SUB’s high quality, we establish a"}
{"id": "2507.23784v1_chunk35", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "iginal per-image la- bels. Moreover, concepts can be supervised by binary at- tribute labels (hard) or by annotator confidence (soft). Human Labeling: SUB. To ensure SUB’s high quality, we establish a human baseline where volunteers label our modified attributes. For each bird-attribute pairing, we test three images and prompt humans to select which attribute within the target group is present. We then calculate the accuracy of the human labels with respect to our intended labels. The User Interface is shown in Appendix E. Human Labeling: CUB. We calculate the accuracies on the original CUB classes by generating class-level labels through majority voting across all images in each bird class. We then compute the proportion of image-attribute labels where the image-level annotation agrees with the class-level majority. This helps assess how well the class-level major- ity approximates the image-level attribute values. 5.5. Benchmarking CBMs on SUB We report each model’s performance in de"}
{"id": "2507.23784v1_chunk36", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "th the class-level majority. This helps assess how well the class-level major- ity approximates the image-level attribute values. 5.5. Benchmarking CBMs on SUB We report each model’s performance in detecting the sub- stituted target attribute ( S+) in our synthetic SUB data, aspresented in Table 1. S+evaluates the 17 concepts in the overlap between attributes in SUB (33 attributes) and 112 (of 312) attributes used for CBM training [30]. We also measure the models’ ability to remove the original attribute (S−); 100% accuracy means that the CBM never predicted the removed attribute in the SUB data. For comparison with the accuracy of the classes seen in training, we include the CUB [63] test set on the subset of attributes used in SUB (TA), and the test set accuracy on all CUB concepts ( T). We observe that annotators consistently classify SUB at- tributes, achieving 94.0% accuracy on the target attribute S+, and 96.8% accuracy on not picking the removed at- tribute S−. These accuracies "}
{"id": "2507.23784v1_chunk37", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " We observe that annotators consistently classify SUB at- tributes, achieving 94.0% accuracy on the target attribute S+, and 96.8% accuracy on not picking the removed at- tribute S−. These accuracies are much higher than CUB’s (79.4% for TAand 82.5% for T). Despite all CBMs hav- ing high accuracy in TAandT(up to 96.7%), they gen- eralize poorly to our novel attribute combinations (highest S+:45.7%from CEM [67]). In fact, all tested CBMs de- tectS+less accurately than random chance ( 50%). Also, highS−removal accuracy is often paired with low S+(e.g. per-class soft labels with 91.6%onS−, but11.2%onS+), suggesting that some models may simply have a greater ten- dency to predict false overall. Among all tested CBMs, the CEM is the best at predicting S+with45.7%, albeit still be- low chance, while maintaining relatively high performance of87.2%onS−. All tested CBMs generalize poorly, show- ing that their predictions are not grounded in the target con- cepts and the performance on the train"}
{"id": "2507.23784v1_chunk38", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "hile maintaining relatively high performance of87.2%onS−. All tested CBMs generalize poorly, show- ing that their predictions are not grounded in the target con- cepts and the performance on the training classes is mis- leading about the models’ true interpretability. 5.6. Benchmarking Other VLMs on SUB Many interpretable models rely on VLM backbones such as CLIP [47] to generalize to open-vocabulary settings without explicit training data [38, 42, 50, 66]. If these backbones also poorly ground predictions, all downstream models will be affected. In testing VLMs, we evaluate whether large- scale training can mitigate the challenges CBMs face and recognize SUB attributes, exhibited in Table 2. As our tested VLMs cannot easily make binary predic-Per-Img. SoftSUB CUB S+S−TAT Random Chance 50.0 50.0 50.0 50.0 CBM (ind.) [30] 40.8 51.6 95.9 96.7 CBM (joint) [30] 34.3 54.0 96.1 96.9 CBM (per c.) [23] 0.39 100.0 78.3 79.4 CEM [67] 45.7 87.2 77.9 81.3 CBM (ind.) [30] ✓ 12.9 94.0 84.8 85.5 CBM "}
{"id": "2507.23784v1_chunk39", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "0 50.0 50.0 50.0 CBM (ind.) [30] 40.8 51.6 95.9 96.7 CBM (joint) [30] 34.3 54.0 96.1 96.9 CBM (per c.) [23] 0.39 100.0 78.3 79.4 CEM [67] 45.7 87.2 77.9 81.3 CBM (ind.) [30] ✓ 12.9 94.0 84.8 85.5 CBM (joint) [30] ✓ 5.78 94.0 85.8 86.1 CBM (ind.) [30] ✓ 32.8 73.6 81.6 85.3 CBM (joint) [30] ✓ 27.1 67.1 83.4 86.0 CBM (ind.) [30] ✓ ✓ 11.2 91.6 80.9 82.6 CBM (joint) [30] ✓ ✓ 6.06 92.4 74.9 75.8 Human 94.0 96.8 79.4 82.5 Table 1. We evaluate CBM accuracy on SUB by measuring the substituted attribute ( S+) and the removed attribute ( S−), as well as on CUB ( T) and the SUB attribute label subset ( TA). tions for individual attributes, we classify the target attribute among all within the attribute group or none . The attribute with the highest cosine similarity compared to the image is labeled true while all others are predicted false . These open-set vocabulary models choose from the 312 CUB at- tributes [63], and we evaluate on the cleaner CBM class- aggregated labels [30]. We re-calculate "}
{"id": "2507.23784v1_chunk40", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "ue while all others are predicted false . These open-set vocabulary models choose from the 312 CUB at- tributes [63], and we evaluate on the cleaner CBM class- aggregated labels [30]. We re-calculate random and hu- man baselines given this multiclass classification setting (see Appendix F). Even with large-scale pre-training, CLIP [47], SigLIP [69], and EV A-CLIP [58] continue to face challenges consistently identifying S+. While their overall accuracy is higher (with EV A-CLIP [58] achieving 46.8%), a deeper look reveals interesting patterns. For instance, CLIP, SigLIP, and EV A-CLIP demonstrate a tendency to select the original attribute two to three times more often than random chance ( 9.3%). This suggests a form of hallu- cination, where the models incorrectly identify the original attribute even when it is not present. For example, SigLIP 400m/16 [69], the model with the lowest S−hallucination rate, achieves only 81.7%, incorrectly selecting the original attribute 18.3%of the tim"}
{"id": "2507.23784v1_chunk41", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "tribute even when it is not present. For example, SigLIP 400m/16 [69], the model with the lowest S−hallucination rate, achieves only 81.7%, incorrectly selecting the original attribute 18.3%of the time. This analysis suggests that despite advances in large-scale pre-training, the problem of generalization for individual concept classification remains a persistent hurdle for VLMs. 6. Limitations Since TDG requires some human intervention for prompt creation and filtering verification, future improvements could focus on fully automating these processes. This would enable the creation of more explainable datasets, ad- dressing the limited scope of CUB. Additionally, due to theSUB CUB S+S−TAT Random Chance 9.3 90.7 73.3 74.6 CLIP ViT-B32 [47] 39.2 73.1 78.4 79.7 CLIP ViT-L14 [47] 45.5 73.2 78.6 80.1 SigLIP B/16 [69] 45.2 77.5 78.4 79.2 SigLIP 400m/16 [69] 45.7 81.7 77.6 79.2 SigLIP2 B/16 [61] 40.0 76.6 77.6 79.1 EV A-CLIP [58] 46.8 77.6 78.5 79.4 Human 92.4 97.3 69.3 65.3 Table 2. We evalu"}
{"id": "2507.23784v1_chunk42", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "6 80.1 SigLIP B/16 [69] 45.2 77.5 78.4 79.2 SigLIP 400m/16 [69] 45.7 81.7 77.6 79.2 SigLIP2 B/16 [61] 40.0 76.6 77.6 79.1 EV A-CLIP [58] 46.8 77.6 78.5 79.4 Human 92.4 97.3 69.3 65.3 Table 2. We evaluate VLM accuracy on SUB by measuring the substituted attribute ( S+) and removed attribute ( S−), along with CUB accuracy ( T) and SUB’s attribute subset ( TA). automated filtering system, we cannot guarantee that every image in SUB perfectly represents the target attribute and bird. However, our human validation strongly suggests that SUB is more consistently and accurately labeled than CUB. Lastly, although our human studies involve fewer partici- pants and annotations compared to CUB, we believe that the careful automatic filtering makes validating a subset suf- ficient as opposed to labeling the complete dataset. 7. Conclusion We proposed a new dataset, SUB, to benchmark the ground- ing of attribute predictions in concept models. SUB consists of 38,400 images representing 768 unique bi"}
{"id": "2507.23784v1_chunk43", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "ng the complete dataset. 7. Conclusion We proposed a new dataset, SUB, to benchmark the ground- ing of attribute predictions in concept models. SUB consists of 38,400 images representing 768 unique bird-attribute pairs, where the given attribute is applied to the chosen bird. To generate SUB, we proposed TDG, a test-time adaptation to faithfully generate novel attribute-object combinations by tying them to a second, easier image. Through rigorous fil- tering of the birds, attributes, and resulting images, we en- sured the quality of SUB. We demonstrated that CBM con- cept predictions are poorly grounded in the target concepts and fail to generalize to unseen combinations of known con- cepts. We also revealed that SOTA VLMs experience this issue as well, though to a lesser extent. We hope that SUB will pave the way for the next generation of CBMs with more robust and well-grounded explanations. Acknowledgments This work was partially funded by the ERC (853489 - DEXIM) and the Alfried Kr"}
{"id": "2507.23784v1_chunk44", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "hat SUB will pave the way for the next generation of CBMs with more robust and well-grounded explanations. Acknowledgments This work was partially funded by the ERC (853489 - DEXIM) and the Alfried Krupp von Bohlen und Hal- bach Foundation, which we thank for their generous sup- port. The authors gratefully acknowledge the scientific sup- port and resources of the AI service infrastructure LRZ AI Systems provided by the Leibniz Supercomputing Centre (LRZ) of the Bavarian Academy of Sciences and Human- ities (BAdW), funded by Bayerisches Staatsministerium f ¨ur Wissenschaft und Kunst (StMWK).References [1] Hasan Md Tusfiqur Alam, Devansh Srivastav, Md Abdul Kadir, and Daniel Sonntag. Towards interpretable radiol- ogy report generation via concept bottlenecks using a multi- agentic rag. In arXiv , 2024. 2 [2] David Alvarez-Melis and T. Jaakkola. Towards robust inter- pretability with self-explaining neural networks. In NeurIPS , 2018. 2 [3] Ananthu Aniraj, Cassio F Dantas, Dino Ienco, an"}
{"id": "2507.23784v1_chunk45", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "rXiv , 2024. 2 [2] David Alvarez-Melis and T. Jaakkola. Towards robust inter- pretability with self-explaining neural networks. In NeurIPS , 2018. 2 [3] Ananthu Aniraj, Cassio F Dantas, Dino Ienco, and Diego Marcos. Pdiscoformer: Relaxing part discovery constraints with vision transformers. In ECCV , 2024. 2 [4] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. Sega: Instructing text-to-image models using semantic guidance. InNeurIPS , 2024. 2 [5] Chaofan Chen, Oscar Li, Alina Jade Barnett, Jonathan Su, and Cynthia Rudin. This looks like that: deep learning for interpretable image recognition. In NeurIPS , 2018. 2 [6] Hongyu Chen, Yi-Meng Gao, Min Zhou, Peng Wang, Xubin Li, Tiezheng Ge, and Bo Zheng. Enhancing prompt follow- ing with visual control through training-free mask-guided diffusion. In arXiv , 2024. 2 [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhang- wei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zh"}
{"id": "2507.23784v1_chunk46", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " with visual control through training-free mask-guided diffusion. In arXiv , 2024. 2 [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhang- wei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test- time scaling. In arXiv , 2024. 5 [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhang- wei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. In arXiv , 2024. [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation mod- els and aligning for generic visual-linguistic tasks. In CVPR , 2024. 5 [10] Townim Faisal Chowdhury, Vu Minh Hieu Phan, Kewen Liao, Minh-Son To, Yutong Xie, Anton van den Hengel, Jo- han W. Verjans, and Zhibin Liao. Adacbm: An adapt"}
{"id": "2507.23784v1_chunk47", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "al-linguistic tasks. In CVPR , 2024. 5 [10] Townim Faisal Chowdhury, Vu Minh Hieu Phan, Kewen Liao, Minh-Son To, Yutong Xie, Anton van den Hengel, Jo- han W. Verjans, and Zhibin Liao. Adacbm: An adaptive con- cept bottleneck model for explainable and accurate diagno- sis. In International Conference on Medical Image Comput- ing and Computer-Assisted Intervention , 2024. 2 [11] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. In arXiv , 2023. 2 [12] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip H¨ausser, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learn- ing optical flow with convolutional networks. In ICCV , 2015. 2 [13] Yilun Du and Leslie Pack Kaelbling. Position: Composi- tional generative modeling: A single model is not all you need. In ICML , 2024. 2 [14] Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual gener"}
{"id": "2507.23784v1_chunk48", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " and Leslie Pack Kaelbling. Position: Composi- tional generative modeling: A single model is not all you need. In ICML , 2024. 2 [14] Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation with energy based models. In NeurIPS , 2020. 2[15] Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenen- baum, Sander Dieleman, Rob Fergus, Jascha Narain Sohl- Dickstein, A. Doucet, and Will Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffu- sion models and mcmc. In ICML , 2023. 2 [16] Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E. Gonzalez, and Trevor Darrell. Diversify your vi- sion datasets with automatic diffusion-based augmentation. InNeurIPS , 2023. 2, 4 [17] Luca Vincent Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, and Zeynep Akata. Reno: Enhancing one-step text-to-image models through reward-based noise optimization. In NeurIPS , 2024. 2 [18] Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zho"}
{"id": "2507.23784v1_chunk49", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " and Zeynep Akata. Reno: Enhancing one-step text-to-image models through reward-based noise optimization. In NeurIPS , 2024. 2 [18] Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming text-to-image diffusion for accurate instruction following. In CVPR , 2024. 2 [19] Jack Furby, Daniel Cunnington, Dave Braines, and Alun David Preece. Can we constrain concept bottleneck models to learn semantically meaningful input features? In arXiv , 2024. 2 [20] Sachit Gaudi, Gautam Sreekumar, and Vishnu Naresh Bod- deti. Coind: Enabling logical compositions in diffusion mod- els.ICLR , abs/2503.01145, 2025. 2 [21] Marton Havasi, S. Parbhoo, and Finale Doshi-Velez. Ad- dressing leakage in concept bottleneck models. In NeurIPS , 2022. 2 [22] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip H. S. Torr, Song Bai, and Xiaojuan Qi. Is syn- thetic data from generative models ready for image recogni- tion? In ICLR , 2023. 2, 4 [23] Lena Heidemann, Maureen Mon"}
{"id": "2507.23784v1_chunk50", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "Chuhui Xue, Wenqing Zhang, Philip H. S. Torr, Song Bai, and Xiaojuan Qi. Is syn- thetic data from generative models ready for image recogni- tion? In ICLR , 2023. 2, 4 [23] Lena Heidemann, Maureen Monnet, and Karsten Roscher. Concept correlation and its effects on concept-based models. InWACV , 2023. 2, 7, 8 [24] Robin Hesse, Simone Schaub-Meyer, and Stefan Roth. Fun- nybirds: A synthetic vision dataset for a part-based analysis of explainable ai methods. In ICCV , 2023. 2 [25] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop on Deep Generative Models and Downstream Applications , 2021. 3 [26] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panagiotis G. Ipeirotis, Pietro Perona, and Serge J. Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine- grained dataset collection. In CVPR , 2015. 6 [27] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, "}
{"id": "2507.23784v1_chunk51", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " app and large scale dataset with citizen scientists: The fine print in fine- grained dataset collection. In CVPR , 2015. 6 [27] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR , 2017. 2 [28] Dahye Kim, Xavier Thomas, and Deepti Ghadiyaram. Reve- lio: Interpreting and leveraging semantic information in dif- fusion models. In arXiv , 2024. 2 [29] Jae Myung Kim, Jessica Bader, Stephan Alaniz, Cordelia Schmid, and Zeynep Akata. Datadream: Few-shot guided dataset generation. In ECCV , 2024. 2 [30] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In ICML , 2020. 1, 2, 6, 7, 8[31] Black Forest Labs. Announcing black forest labs. In Black- ForestLabs Blog , 2024. 2, 4, 6 [32] Nan Liu, Shuang Li, Yilun Du, Joshua B. Tenenbaum, and Antonio Torralba. Learnin"}
{"id": "2507.23784v1_chunk52", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "2020. 1, 2, 6, 7, 8[31] Black Forest Labs. Announcing black forest labs. In Black- ForestLabs Blog , 2024. 2, 4, 6 [32] Nan Liu, Shuang Li, Yilun Du, Joshua B. Tenenbaum, and Antonio Torralba. Learning to compose visual relations. NeurIPS , 2021. 2 [33] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B. Tenenbaum. Compositional visual generation with composable diffusion models. ECCV , 2022. 2 [34] Anita Mahinpei, Justin Clark, Isaac Lage, Finale Doshi- Velez, and Weiwei Pan. Promises and pitfalls of black-box concept learning models. In ArXiv , 2021. 2 [35] Alireza Makhzani and Brendan J. Frey. k-sparse autoen- coders. In arXiv , 2013. 2 [36] Emanuele Marconato, Andrea Passerini, and Stefano Teso. Glancenets: Interpretabile, leak-proof concept-based mod- els. In NeurIPS , 2022. 2 [37] Andrei Margeloiu, Matthew Ashman, Umang Bhatt, Yanzhi Chen, Mateja Jamnik, and Adrian Weller. Do concept bot- tleneck models learn as intended? In arXiv , 2021. 2 [38] Sachit Menon and Carl V "}
{"id": "2507.23784v1_chunk53", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " 2 [37] Andrei Margeloiu, Matthew Ashman, Umang Bhatt, Yanzhi Chen, Mateja Jamnik, and Adrian Weller. Do concept bot- tleneck models learn as intended? In arXiv , 2021. 2 [38] Sachit Menon and Carl V ondrick. Visual classification via description from large language models. ICLR , 2023. 7 [39] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, and Bolei Zhou. Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition. In CVPR , 2024. 2 [40] Meike Nauta, J ¨org Schl ¨otterer, Maurice van Keulen, and Christin Seifert. Pip-net: Patch-based intuitive prototypes for interpretable image classification. In CVPR , 2023. 2 [41] Micky C. Nnamdi, Wenqi Shi, Junior Ben Tamo, Henry J. Iwinski, J. Michael Wattenbarger, and May Dongmei Wang. Concept bottleneck model for adolescent idiopathic scoliosis patient reported outcomes prediction. In International Con- ference on Biomedical and Health Informatics (BHI) , 2023. 2 [42] Tuomas P. Oikar"}
{"id": "2507.23784v1_chunk54", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "ncept bottleneck model for adolescent idiopathic scoliosis patient reported outcomes prediction. In International Con- ference on Biomedical and Health Informatics (BHI) , 2023. 2 [42] Tuomas P. Oikarinen, Subhro Das, Lam M. Nguyen, and Tsui-Wei Weng. Label-free concept bottleneck models. In ICLR , 2023. 2, 7 [43] Mateusz Pach, Dawid Rymarczyk, Koryna Lewandowska, Jacek Tabor, and Bartosz Zieli ´nski. Lucidppn: Unambigu- ous prototypical parts network for user-centric interpretable computer vision. In arXiv , 2024. 2 [44] Konstantinos Panousis, Dino Ienco, and Diego Marcos. Coarse-to-fine concept bottleneck models. In NeurIPS , 2024. 2 [45] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In ICCV , 2019. 2 [46] Dustin Podell, Zion English, Kyle Lacey, A. Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR , 2024. 2 [47] Alec Radford, Jong Wook Kim, Chris Hall"}
{"id": "2507.23784v1_chunk55", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "n, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR , 2024. 2 [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML , 2021. 7, 8[48] Naveen Raman, Mateo Espinosa Zarlenga, Juyeon Heo, and Mateja Jamnik. Do concept bottleneck models respect local- ities? In arXiv , 2024. 2 [49] Naveen Raman, Mateo Espinosa Zarlenga, and Mateja Jam- nik. Understanding inter-concept relationships in concept- based models. In ICML , 2024. 2 [50] Sukrut Rao, Sweta Mahajan, Moritz Bohle, and Bernt Schiele. Discover-then-name: Task-agnostic concept bottle- necks via automated concept discovery. In ECCV , 2024. 2, 7 [51] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-re"}
{"id": "2507.23784v1_chunk56", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "iscover-then-name: Task-agnostic concept bottle- necks via automated concept discovery. In ECCV , 2024. 2, 7 [51] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR , 2022. 2 [52] Dawid Rymarczyk, Lukasz Struski, Jacek Tabor, and Bar- tosz Zieli ´nski. Protopshare: Prototypical parts sharing for similarity discovery in interpretable image classification. In ACM SIGKDD Conference on Knowledge Discovery & Data Mining , 2020. 2 [53] Dawid Rymarczyk, Lukasz Struski, Michal G’orszczak, Ko- ryna Lewandowska, Jacek Tabor, and Bartosz Zieli ´nski. In- terpretable image classification with differentiable proto- types assignment. In ECCV , 2021. 2 [54] Mert Bulent Sariyildiz, Alahari Karteek, Diane Larlus, and Yannis Kalantidis. Fake it till you make it: Learning trans- ferable representations from synthetic imagenet clones. In CVPR , 2022. 2 [55] Yoshihide Sawada and Keigo Nakamura. Concept bottlen"}
{"id": "2507.23784v1_chunk57", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "us, and Yannis Kalantidis. Fake it till you make it: Learning trans- ferable representations from synthetic imagenet clones. In CVPR , 2022. 2 [55] Yoshihide Sawada and Keigo Nakamura. Concept bottleneck model with additional unsupervised concepts. In IEEE Ac- cess, 2022. 2 [56] Nishad Singhi, Jae Myung Kim, Karsten Roth, and Zeynep Akata. Improving intervention efficacy via concept realign- ment in concept bottleneck models. In ECCV , 2024. 2 [57] Sanchit Sinha, Mengdi Huai, Jianhui Sun, and Aidong Zhang. Understanding and enhancing robustness of concept- based models. In AAAI , 2022. 2 [58] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv , 2023. 8 [59] Andong Tan, Fengtao Zhou, and Hao Chen. Explain via any concept: Concept bottleneck model with open vocabulary concepts. In ECCV , 2024. 2 [60] Harrish Thasarathan, Julian Forsyth, Thomas Fel, Matthew Kowal, and Konstantinos Derpanis. Universal sparse au- toenc"}
{"id": "2507.23784v1_chunk58", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "pt: Concept bottleneck model with open vocabulary concepts. In ECCV , 2024. 2 [60] Harrish Thasarathan, Julian Forsyth, Thomas Fel, Matthew Kowal, and Konstantinos Derpanis. Universal sparse au- toencoders: Interpretable cross-model concept alignment. In arXiv , 2025. 2 [61] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muham- mad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language en- coders with improved semantic understanding, localization, and dense features. arXiv , 2025. 8 [62] Robert van der Klis, Stephan Alaniz, Massimiliano Mancini, Cassio F Dantas, Dino Ienco, Zeynep Akata, and Diego Mar- cos. Pdisconet: Semantically consistent part discovery for fine-grained recognition. In ICCV , 2023. 2 [63] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. In California In- stitute of Technology Technical Report , 2011. 2, 4, 6, 7, 8["}
{"id": "2507.23784v1_chunk59", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "CCV , 2023. 2 [63] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. In California In- stitute of Technology Technical Report , 2011. 2, 4, 6, 7, 8[64] Ruochen Wang, Ting Liu, Cho-Jui Hsieh, and Boqing Gong. On discrete prompt optimization for diffusion models. In ICML , 2024. 2 [65] Thadd ¨aus Wiedemer, Prasanna Mayilvahanan, Matthias Bethge, and Wieland Brendel. Compositional generalization from first principles. NeurIPS , 2023. 2 [66] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar. Language in a bottle: Language model guided concept bottlenecks for in- terpretable image classification. In CVPR , 2023. 2, 7 [67] Mateo Espinosa Zarlenga, Pietro Barbiero, Gabriele Ciravegna, Giuseppe Marra, Francesco Giannini, Michelan- gelo Diligenti, Zohreh Shams, Fr ´ed´eric Precioso, Stefano Melacci, Adrian Weller, Pietro Lio’, and Mateja Jamnik. Concept embedding models. In NeurIPS , 2022. 7, 8 ["}
{"id": "2507.23784v1_chunk60", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "a, Francesco Giannini, Michelan- gelo Diligenti, Zohreh Shams, Fr ´ed´eric Precioso, Stefano Melacci, Adrian Weller, Pietro Lio’, and Mateja Jamnik. Concept embedding models. In NeurIPS , 2022. 7, 8 [68] Mateo Espinosa Zarlenga, Pietro Barbiero, Zohreh Shams, Dmitry Kazhdan, Umang Bhatt, Adrian Weller, and Mateja Jamnik. Towards robust metrics for concept representation evaluation. In AAAI , 2023. 2 [69] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. InICCV , 2023. 8 [70] Kaichen Zhang, Yifei Shen, Bo Li, and Ziwei Liu. Large multi-modal models can interpret features in large multi- modal models. In arXiv , 2024. 2 [71] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV , 2023. 2 [72] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K. Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. In Ne"}
{"id": "2507.23784v1_chunk61", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": ". In ICCV , 2023. 2 [72] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K. Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. In NeurIPS , 2023. 2Attribute Guidance Bird Needle bill shape Needle bill shape Spotted breast pattern Brown Thrasher Striped breast pattern Song Sparrow Solid tail pattern Gray Catbird Multi-colored tail pattern Cedar Waxwing Table 3. Guidance birds used the the generation of SUB. A. Reference Birds For SUB, we use the following 33 reference birds: West- ern Grebe, Black and white Warbler, European Goldfinch, Pacific Loon, White Pelican, Cedar Waxwing, Gad- wall, Downy Woodpecker, Pileated Woodpecker, Purple Finch, Common Raven, White breasted Nuthatch, North- ern Flicker, Mallard, Tropical Kingbird, Tree Swallow, Song Sparrow, Green Violetear, Gray Catbird, Green Jay, Cardinal, Red bellied Woodpecker, Pied Kingfisher, Ru- fous Hummingbird, Dark eyed Junco, Green Kingfisher, Horned Puffin, Ann"}
{"id": "2507.23784v1_chunk62", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "rd, Tree Swallow, Song Sparrow, Green Violetear, Gray Catbird, Green Jay, Cardinal, Red bellied Woodpecker, Pied Kingfisher, Ru- fous Hummingbird, Dark eyed Junco, Green Kingfisher, Horned Puffin, Anna Hummingbird, Barn Swallow, Ameri- can Goldfinch, Lazuli Bunting, Blue Jay, Painted Bunting. B. Guidance Birds Guidance birds are used for pattern and shape modifica- tions. We include in Table 3 the guidance birds chosen for each attribute when generating SUB. C. Substitutions We use the following list of substitutions in SUB: grey back color, grey bill color, white breast color, red breast color, blue breast color, grey crown color, white crown color, black crown color, pink crown color, yellow eye color, blue eye color, white eye color, grey forehead color, pink leg color, black leg color, grey leg color, green primary color, brown primary color, blue primary color, orange primary color, blue throat color, yellow throat color, green under- parts color, red underparts color, white wing "}
{"id": "2507.23784v1_chunk63", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " grey leg color, green primary color, brown primary color, blue primary color, orange primary color, blue throat color, yellow throat color, green under- parts color, red underparts color, white wing color, grey wing color, black wing color, spotted breast pattern, striped breast pattern, solid tail pattern, multi-colored tail pattern, and needle bill shape. D. Prompts A few example prompts: R=European Goldfinch, S+=Black crown color, G=bird, cR=A photo of a European Goldfinch with black colored feathers on the crown of its head, cG=A photo of a bird with black colored feathers on the crown of its head R=Downy Woodpecker, S+=Red breast color, G= bird, cR=A photo of a Downy Woodpecker with a red Figure 7. First human study interface with binary questions for attribute presence and reference bird faithfulness. colored breast, cG=A photo of a bird with a red colored breast R=Western Grebe, S+=Solid tail pattern, G=Gray Catbird, cR=A photo of a Western Grebe with a solid tail like a Gray C"}
{"id": "2507.23784v1_chunk64", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": " bird faithfulness. colored breast, cG=A photo of a bird with a red colored breast R=Western Grebe, S+=Solid tail pattern, G=Gray Catbird, cR=A photo of a Western Grebe with a solid tail like a Gray Catbird, cG=A photo of a Gray Catbird with a solid tail R=Cardinal, S+=Spotted breast pattern, G=Brown Thrasher, cR=A photo of a Cardinal with a spotted belly like a Brown Thrasher, cG=A photo of a Brown Thrasher with a spotted belly R=Blue Jay, S+=Needle bill shape, G=Humming- bird,cR=A photo of a Blue Jay with the body of a Blue Jay and a beak like a Hummingbird, cG=A photo of a Hummingbird E. Human Verification User Interface Human verification was completed by four volunteers. In Figure 7, we see the user interface used for our first user study, where participants were asked whether S+was present and whether the bird accurately reflected the guide bird. In Figure 8, we show the interface for the second study, where the user is given all options in the target at- tribute group and asked "}
{"id": "2507.23784v1_chunk65", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "resent and whether the bird accurately reflected the guide bird. In Figure 8, we show the interface for the second study, where the user is given all options in the target at- tribute group and asked to label which is present. E.1. Reference Bird Verification The underlying objective of specifying reference birds is to increase the overall diversity in birds exhibiting individual attributes. Specifically, we want to test the accuracy of at- tribute detection when it occurs in combinations not seen during test time. As long as S+is present, it is not imper- ative that every synthetic bird closely match the reference class, but many should. As described in Section 4.3, we verify this on 40 images per attribute, by checking if theFigure 8. Second human study with attribute labeling within full attribute group. Best Classes: White Pelican , Common Raven, Red Bellied Woodpecker, Blue Jay, European Goldfinch Worst Classes: 1. Western Grebe 2. Pacific Loon 3. Painted Bunting 4. Tropical Kingb"}
{"id": "2507.23784v1_chunk66", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "l attribute group. Best Classes: White Pelican , Common Raven, Red Bellied Woodpecker, Blue Jay, European Goldfinch Worst Classes: 1. Western Grebe 2. Pacific Loon 3. Painted Bunting 4. Tropical Kingbird 5. Lazuli Bunting Best Classes: 1. White Pelican 2. Common Raven 3. Red Bellied Woodpecker 4. Blue Jay 5. European Goldfinch Worst Classes: 1. Western Grebe 2. Pacific Loon 3. Painted Bunting 4. Tropical Kingbird 5. Lazuli Bunting Best classes White Pelican Common Raven Red Bellied Woodpecker Worst classes Western Grebe Pacific Loon Green Violetear Figure 9. Histogram of the per-bird accuracy results from the hu- man verification, where participants were asked if the attribute- substituted synthetic image represents the original bird. We see that 27 birds exceed 50% accuracy, showing that our generated dataset is very diverse. synthetic bird is recognizable as the reference bird. We then calculate the percentage of faithful birds of all those gener- ated for each bird class. In Figure "}
{"id": "2507.23784v1_chunk67", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "our generated dataset is very diverse. synthetic bird is recognizable as the reference bird. We then calculate the percentage of faithful birds of all those gener- ated for each bird class. In Figure 9, we show a histogram of this per-bird faithfulness. From this histogram, we can see that 27 out of 33 classes are faithful over half the time, and 10 classes are over 90% faithful. For the attribute spot-ted breast , we show examples from the three most faithful classes, and the three least faithful. While Western Grebe , Pacific Loon , and Green Violetear diverge from the repre- sentative class, we also note that they still provide some di- versity to SUB. F. VLM Random Chance Calculation and La- bel Set For the VLMs, we calculate the probability of getting a sin- gle prediction correct at random if the target label is 1 as 1 |A|+1, where Ais the attribute group corresponding to the target prediction and options aj∈ A are the manifestations of the attribute group. One is added to |A|to "}
{"id": "2507.23784v1_chunk68", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "random if the target label is 1 as 1 |A|+1, where Ais the attribute group corresponding to the target prediction and options aj∈ A are the manifestations of the attribute group. One is added to |A|to account for the additional option none . If the target label is 0, then it is 1−1 |A|+1. For SUB, we calculate the S+random chance baseline across the modified attribute for each image in SUB, as- suming a target label of 1. We calculate S−from only the samples where the class-wise CBM label included a pos- itive label for another attribute within the attribute group, and we consider that attribute with a target label of 0. For CUB, we calculate the random chance baseline across all samples and CBM attributes, with the CBM class- wise labels as targets. For selecting the possible label set Apresented to the VLM, we use the full set of 312 CUB attributes for two reasons: (1) it offers a broader and more challenging set of plausible options than the CBM subset; and (2) the origi- nal labels "}
{"id": "2507.23784v1_chunk69", "paper_id": "2507.23784v1", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "text": "esented to the VLM, we use the full set of 312 CUB attributes for two reasons: (1) it offers a broader and more challenging set of plausible options than the CBM subset; and (2) the origi- nal labels used in CUB collection are well-aligned with ex- pected dataset attributes, increasing the likelihood that the model selects the correct attribute over none ."}
{"id": "2507.23779v1_chunk0", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding Miaosen Zhang Ziqiang Xu Jialiang Zhu Qi Dai Kai Qiu Yifan Yang Chong Luo Tianyi Chen Justin Wagle Tim Franklin Baining Guo Microsoft ABSTRACT With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from \"Iron Man\" , are becoming a reality. GUI grounding is a core component for CUAs to execute actual actions, similar to mechanical control in robotics, and it directly leads to the success or failure of the system. It determines actions such as clicking and typing, as well as related parameters like the coordinates for clicks. Current end-to-end grounding models still achieve less than 65% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from being ready for deployment. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the Phi-Grou"}
{"id": "2507.23779v1_chunk1", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "g ready for deployment. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the Phi-Ground model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under 10Bparameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of 43.2 on ScreenSpot-pro and 27.2 on UI-Vision. We believe that the various details discussed in this paper, along with our successes and failures, not only clarify the construction of grounding models but also benefit other perception tasks. Project homepage: https://zhangmiaosen2000.github.io/Phi-Ground/ Keywords GUI grounding ·AI agent ·Large multi-modal model Ours -4B-7COurs -4B-16C-DPO Ours -4B-16COurs -7B-16C-DPO Ours -7B-16COurs -7B-29C Ours -7B-7COurs -4B-29C UI-TARS -2BUI-TARS -7BUI-TARS -1.5-7B Uground -7BUground -V1-7B SeeClick -10BOS-Atlas -4BOS-Atla"}
{"id": "2507.23779v1_chunk2", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "rs -4B-7COurs -4B-16C-DPO Ours -4B-16COurs -7B-16C-DPO Ours -7B-16COurs -7B-29C Ours -7B-7COurs -4B-29C UI-TARS -2BUI-TARS -7BUI-TARS -1.5-7B Uground -7BUground -V1-7B SeeClick -10BOS-Atlas -4BOS-Atlas -7B Figure 1: Left: The comparison chart of our grounding model results across five GUI grounding benchmarks. Our model, trained specifically for the agent setting, achieved SOTA results on all benchmarks under this focus. Even in the general end-to-end model setting, our model attained SOTA results on three of the benchmarks. Right : The relationship between model performance and computational cost on ScreenSpot-pro demonstrates that our model supports the Pareto frontier, indicating its efficiency. Most GUI research traditionally considers only the parameter count Nfor comparison, but our experiments highlight that computational cost during testing, such as the number of image tokens, also significantly impacts performance. The X-axis in the right figure represents ND, where Dis the nu"}
{"id": "2507.23779v1_chunk3", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "experiments highlight that computational cost during testing, such as the number of image tokens, also significantly impacts performance. The X-axis in the right figure represents ND, where Dis the number of image tokens. Training and inference latency are more linearly correlated with ND than with N. A graph using latency as the X-axis closely resembles the right figure, but latency is often influenced by hardware and acceleration libraries such as vllm, so we did not use latency as X-axis.arXiv:2507.23779v1 [cs.CV] 31 Jul 2025Phi-Ground Tech Report Traditional AdvancedPhysical World Virtual WorldAutomated System Controller Localizing discrete actions from action sets. Fixed environment and tasks “pick!” “jump!” ? Pre -defined tasks The action space is a set of several pre -defined actionsTask -specific robot The physical architecture is only designed for certain range of tasks. Web/APP Agent Planner Localizing discrete actions provided by APIs . Fixed applications and tasks “buy me a"}
{"id": "2507.23779v1_chunk4", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "sTask -specific robot The physical architecture is only designed for certain range of tasks. Web/APP Agent Planner Localizing discrete actions provided by APIs . Fixed applications and tasks “buy me a book on Amazon !” Pre -defined APIs on fixed APP The action is done through APIs by a certain APP.HTML/DOM/API/Code The planner interacts via APIs provided by the target software. LLM “Edict the image with PS and E-mail Jackson !” General Robot General Planner Achieve long -horizon complex physical task planning.Any Physical Tasks Any physical tasks that human can do with their limbs.Humanoid robot possessing operational dexterity on par with or even surpassing that of humans. …… Computer Use Agent General Planner Achieve long -horizon complex task planning on computer.Any Virtual Tasks Any virtual tasks that human can do with computers.Action Grounding Perform any mouse and keyboard operations that a human can achieve. …… Figure 2: Agent evolution across physical and virtual worlds. Trad"}
{"id": "2507.23779v1_chunk5", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "irtual tasks that human can do with computers.Action Grounding Perform any mouse and keyboard operations that a human can achieve. …… Figure 2: Agent evolution across physical and virtual worlds. Traditional systems rely on fixed controllers and pre- defined workflows to execute domain-specific tasks, either in physical environments (e.g., task-specific robots) or virtual environments (e.g., API-based Web/APP agents). In the modern era, intelligent automation has emerged. In the physical world, general-purpose robots perform versatile limb-based operations. In the virtual world, Computer Use Agents (CUAs) achieve human-level behaviors through general purpose planner, GUI grounding, enabling them to complete any virtual task achievable via mouse and keyboard interactions. 1 Introduction Large model based autonomous agents [ 1,2], by leveraging their robust reasoning capabilities and interactions with real-world environments [ 3], enable humans to tackle more complex tasks and significan"}
{"id": "2507.23779v1_chunk6", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ge model based autonomous agents [ 1,2], by leveraging their robust reasoning capabilities and interactions with real-world environments [ 3], enable humans to tackle more complex tasks and significantly enhance productivity. Given that a substantial portion of modern human work is conducted via computers, Computer Use Agents (CUAs) hold immense potential and commercial value [ 4,5]. While CUAs streamline virtual work, robots [ 6,7] simplify physical tasks, as shown in Figure 2, and together, they are poised to drive a revolution in productivity. With the development of reasoning models like O3 [ 8,9,10,11], we are beginning to create prototypes of CUAs, such as OpenAI Operator [ 12] and Claude Computer Use [ 13]. However, there remains a considerable gap before CUAs can be fully commercialized. This is due to the irreversible effects of many computer operations [ 14], such as closing unsaved files, as well as significant concerns regarding user privacy protection [15, 16, 17]. Specifi"}
{"id": "2507.23779v1_chunk7", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "lized. This is due to the irreversible effects of many computer operations [ 14], such as closing unsaved files, as well as significant concerns regarding user privacy protection [15, 16, 17]. Specifically, CUAs can execute actions through the following methods: APIs (e.g., HTML/DOM, Office scripts), scripting tools (such as AutoHotkey, Power Automate, CLI), and simulated input device interactions (such as mouse clicks and keyboard inputs). Traditional approaches based on APIs [ 18,19] and scripts are often constrained by the platform in use, environmental dependencies, and the specific APIs provided by applications [ 20]. In contrast, solutions based on GUI and simulated input device interactions [ 21] are aligned with human operations. This alignment not only facilitates user supervision, enhancing privacy and security, but also theoretically allows for any interaction that a human can perform, without being limited by the application. As a result, GUI-based CUAs are becoming a focal"}
{"id": "2507.23779v1_chunk8", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": ", enhancing privacy and security, but also theoretically allows for any interaction that a human can perform, without being limited by the application. As a result, GUI-based CUAs are becoming a focal point of research in this field, as illustrated in Figure 2. When accomplishing a task, GUI-based CUA can be divided into two steps: temporal planning and grounding [ 22,23]. Planning involves analyzing the task description and the current state to determine the actions that should be taken in the future, while grounding refers to the above mentioned simulation of input devices. Among all interactions, mouse click operations are the most important and common actions for GUI grounding. Since keyboard commands, such as pressing the \"A\" key, are discrete, MLLMs can effectively handle this type of grounding. Hence, our focus is primarily on mouse commands, where the main challenge lies in the fact that mouse command parameters are screen coordinates, and most MLLMs struggle to accurately iden"}
{"id": "2507.23779v1_chunk9", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "f grounding. Hence, our focus is primarily on mouse commands, where the main challenge lies in the fact that mouse command parameters are screen coordinates, and most MLLMs struggle to accurately identify these coordinates [ 24,25,26]. Therefore, specialized training is required for determining the precise click coordinates. 2Phi-Ground Tech Report “Delete the text box first” T=1 T=2 …… T=N -1 T=N“Click the dropdown button for adding shapes”So many dropdown, which to click ???The dropdown button under a blue rectangle and white circle, with the text “Shapes” below … “Click” (A) T emporal planning(B) Spatial planning (C) Action Grounding ……Action Localization [123, 456] Figure 3: Three levels of task of CUAs. Each coral block represent an action step. We focus on the click action, as it is the most important and common operation. Others such as keyboard input can be effectively handled by MLLM. Motivated by the above considerations, this paper conducts a detailed empirical study on the "}
{"id": "2507.23779v1_chunk10", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "the most important and common operation. Others such as keyboard input can be effectively handled by MLLM. Motivated by the above considerations, this paper conducts a detailed empirical study on the training of GUI grounding models. We divide GUI grounding into two components: the first component is spatial planning [ 27,23], which involves identifying which specific element in the image needs to be manipulated to execute a given instruction. The second component is localization, where, after determining the target location, the model needs to output the correct coordinates, as illustrated in Figure 3. Many existing CUA models [ 28,29,30] attempt to accomplish the grounding task in an end-to-end manner. However, due to the spatial planning component, which also demands a strong level of expertise, common sense, and spatial reasoning abilities, there is a need for larger or even deep reasoning models. To achieve better results at this stage, we have adopted a two-phase approach [ 31]. "}
{"id": "2507.23779v1_chunk11", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "xpertise, common sense, and spatial reasoning abilities, there is a need for larger or even deep reasoning models. To achieve better results at this stage, we have adopted a two-phase approach [ 31]. Initially, an advanced MLLM provides a detailed description of the location, followed by our trained grounding model outputting the specific coordinates. In this technical report, we delve into numerous often-overlooked details spanning data, algorithms, and training methodologies. Counterintuitively, we found that many techniques that appear sound and frequently appear in previous work, such as tokenized coordinates [ 32,33,34] , coordinate label smoothing [ 35,36], and loss reweighting, become trivial when dealing with large-scale training. We have retained only the following techniques that remain meaningful under extensive training in the main text. Firstly, at the model input level, we discovered that the order of modality inputs can significantly impact the underlying mechanism of fe"}
{"id": "2507.23779v1_chunk12", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " remain meaningful under extensive training in the main text. Firstly, at the model input level, we discovered that the order of modality inputs can significantly impact the underlying mechanism of feature modeling, leading to notable differences in results. Secondly, data augmentation [ 37] is very common in traditional object detection training, yet is rarely mentioned in the era of large models. Our re-experiments indicate that certain data augmentations can greatly enhance results in high-resolution scenarios (such as ScreenSpot- pro). Thirdly, the academic community predominantly focuses on the out-of-distribution generalization ability of models, with insufficient research on how models can appropriately perform in-domain continual learning in specific scenarios (such as considering only Photoshop software). This area, however, holds significant practical value. We discuss data strategies and training algorithms for in-domain post-training. Lastly, existing GUI grounding work typ"}
{"id": "2507.23779v1_chunk13", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "only Photoshop software). This area, however, holds significant practical value. We discuss data strategies and training algorithms for in-domain post-training. Lastly, existing GUI grounding work typically considers only the size of the parameters when making comparisons. We incorporate the computational load during model inference (primarily influenced by the number of image tokens) into the study of scaling laws[ 38,39,40] and evaluations. Ultimately, we devised an efficient and rational training recipe, collecting over 40M data samples from multiple sources [21, 31, 41]. By scaling up the training volume, we successfully developed the Phi-Ground model family. In our evaluation process, to avoid the limitations of relying on a single benchmark and to prevent systemic overfitting [42,43] due to optimization for a specific benchmark, we consider multiple benchmarks in both ablation studies and final evaluations. We conducted a survey and collected four publicly available test datasets"}
{"id": "2507.23779v1_chunk14", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " due to optimization for a specific benchmark, we consider multiple benchmarks in both ablation studies and final evaluations. We conducted a survey and collected four publicly available test datasets [25, 24, 44, 26], complemented by our internally constructed dataset focusing on commonly used software on Windows, which we refer to as the Gold dataset. This brings our total to five evaluation datasets, whereas most CUA research works [ 30,31,28,29] on grounding typically utilize only one or two of these. The evaluation results demonstrate that, within the Agent setting we are focused on, our Phi-Ground model achieves state-of-the-art results across all benchmarks, with particularly high scores of 55.0 and 36.2 on ScreenSpot-pro [ 25] and UI-Vision [ 24], respectively. Furthermore, in the end-to-end model setting, we also achieved the best results in three of the benchmarks, scoring 43.2 and 27.2 on ScreenSpot-pro and UI-Vision, respectively. These findings indicate that our model poss"}
{"id": "2507.23779v1_chunk15", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "end-to-end model setting, we also achieved the best results in three of the benchmarks, scoring 43.2 and 27.2 on ScreenSpot-pro and UI-Vision, respectively. These findings indicate that our model possesses strong generalization capabilities. We also present a detailed erroneous case study in Section 6.2 and the appendix. We consider GUI grounding to be a classic scenario for multimodal model perception [ 45,46,47], and believe that our experiences can be effectively generalized to other fields involving multimodal perception. We hope our research will benefit related domains. 3Phi-Ground Tech Report Planner’s Input Screenshot: Instruction (short -RE): “I want to setup my firework protection.”Planner’s Output { \"Analyze \": \"The task involves identifying the specific…...\", \"Output \": { \"area_type \": \"text\", \"interactive \": true, \"context \": \"The user is navigating the Windows Settings to ……firewall protection.\", \"functional_reference \": \"The 'Windows Security' option allows users to …… f"}
{"id": "2507.23779v1_chunk16", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "_type \": \"text\", \"interactive \": true, \"context \": \"The user is navigating the Windows Settings to ……firewall protection.\", \"functional_reference \": \"The 'Windows Security' option allows users to …… for the device.\", \"positional_reference \": \"Located in the first section under 'Security' on the right side of the screen, just below the heading 'Privacy & security’., \"appearance_reference \": \"The text 'Windows Security' is accompanied by a shield icon, and …… further settings.\" } } Copy code MLLM: … Text tokens Image tokens Output tokens… = <point> 123, 456 </point> 𝒚𝒕𝒌=𝕀[𝒕=𝒙𝒌] 𝑳𝒐𝒔𝒔𝒙𝒌𝒙𝟎,…=−෍ 𝒕=𝟏𝑽 𝒚𝒕𝒌𝐥𝐨𝐠𝒑(𝒕|𝒙𝟎,…)(d) = <p123><p456><p0> <p1000><p1000> (a) 𝒚𝒕𝒌=𝕀𝒕∈𝒅𝒊𝒈𝒊𝒕× (𝟏−𝒅(𝒕,𝒙𝒌))= <point> 123, …(b) = <p> 123, … 𝒚𝒕𝒌=𝕀[𝒕=𝒙𝒌]×𝝕<p> 1 2 3 𝝕: 1.0 1.0 0.8 0.3(c) Figure 4: In terms of coordinate representation and loss strategies, we experimented with the following approaches: (a) tokenized coordinate representation, (b) label smoothing for coordinates, (c) loss reweighting, and (d) direct textual"}
{"id": "2507.23779v1_chunk17", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "sentation and loss strategies, we experimented with the following approaches: (a) tokenized coordinate representation, (b) label smoothing for coordinates, (c) loss reweighting, and (d) direct textual representation of coordinates. 2 Methodology At the agent level, we adopt a two-stage implementation approach: a large multimodal model (such as GPT-4O) is utilized to generate detailed and specific Reference Expressions (RE), while a smaller multimodal model that we have trained is responsible for generating coordinates based on the RE. At the model level, we fine-tune a MLLM to directly output coordinates, which are generated in text form. The coordinates are represented as relative values scaled by 1000 and then rounded (e.g., a value of 500 corresponds to a coordinate value of 0.5). We present the following discussion and experiments on the aforementioned implementation. Coordinates Representations and Loss During the development, we explored several potential implementation approache"}
{"id": "2507.23779v1_chunk18", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "esent the following discussion and experiments on the aforementioned implementation. Coordinates Representations and Loss During the development, we explored several potential implementation approaches as follows: •Tokenized coordinates. Some existing works represent regions as tokens [ 32,33,34], using learnable special tokens to denote a specific position. However, in GUI grounding, the large image resolution makes using 2D regions as tokens impractical. We attempted to divide both the horizontal and vertical coordinates into 1000 intervals (which is necessary for large screens like 4K displays), with each interval represented by a special token, similar to [ 48,49]. Despite experimenting with various initialization and training strategies, our results indicated that introducing a large number of unlearned tokens during finetuning pretrained MLLM can lead to model collapse and poor performance. For further details, see the Appendix C.1. •Label smoothing. To approximate the classic re"}
{"id": "2507.23779v1_chunk19", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "number of unlearned tokens during finetuning pretrained MLLM can lead to model collapse and poor performance. For further details, see the Appendix C.1. •Label smoothing. To approximate the classic regression loss, we applied label smoothing to the loss of digit tokens, making the magnitude of the loss proportional to the distance between the prediction and the target. For instance, if the target digit is \"5\", we assign smaller labels to \"4\" and \"6\" to indicate their proximity to the target. The detailed implementation can be found in the Appendix C.2. However, the experimental results indicate that this technique may offer improvements only when the training dataset is small and the batch size is minimal. When we eventually increased the batch size to 2048 and the total training samples reached million level, the application of this technique showed no significant impact or improvement. •Loss re-weighting. Following a similar idea as Label smoothing, we assigned different loss weights"}
{"id": "2507.23779v1_chunk20", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "eached million level, the application of this technique showed no significant impact or improvement. •Loss re-weighting. Following a similar idea as Label smoothing, we assigned different loss weights to the digits representing the hundreds, tens, and units places of the coordinates. The results were consistent with the previous findings: not only were the corresponding parameters extremely sensitive, but the technique also lost its advantages as the batch size and training volume increased. See Appendix. C.3. From the above experiments, we found that the most straightforward text output combined with GPT loss (NTP) can effectively facilitate training. We chose to use relative coordinates for expressing positions because their value range is fixed and relatively dense (as the resolution width and height of GUIs are generally greater than 1000 pixels), which facilitates better model learning. Additionally, since relative coordinates are floating-point numbers that are ≤1, the leading \"0"}
{"id": "2507.23779v1_chunk21", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "dth and height of GUIs are generally greater than 1000 pixels), which facilitates better model learning. Additionally, since relative coordinates are floating-point numbers that are ≤1, the leading \"0.\" is insignificant. Therefore, following [31], we scaled these values by multiplying them by 1000. 4Phi-Ground Tech Report Common Crawl IndexURLs DomainSampled URLs Dropped URLsDomain Deduplication - 2.6B URLs - - 46.6M domains -- 475.5M URLs - Same domain share similar layouts WebDriverCrawl & Rendered Data - 285M Webpages - HTML elements Images Attribute Filter: Tag Event Role Class Image & Icon ……Render Space: 1080p 2.5k 4k Render Shape: ………… 1:22:1 Type: Screenshot layout graphRole -based Filtering - 73.5M Webpages - 1. Incomplete webs 2. Naïve layouts 3. Interactive element 4. Icon numbers 5. <div> dedup . 6. Scroll page dedup . …… GPT -4O Labeling - 10.5M pairs - Element Selection - 10.5M Elements - Center point distribution on Image Figure 5: CommonCrawl data processing pipeline. 3"}
{"id": "2507.23779v1_chunk22", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "rs 5. <div> dedup . 6. Scroll page dedup . …… GPT -4O Labeling - 10.5M pairs - Element Selection - 10.5M Elements - Center point distribution on Image Figure 5: CommonCrawl data processing pipeline. 3 Data Preparation 3.1 Processing Open Source Data We utilized a portion of the open-source data from OS-Atlas [ 31], which includes several subsets such as Windows, macOS, Linux, and Fineweb. Additionally, we incorporated data from SeeClick [ 26], E2ISynth [ 41] and GUIAct [ 21]. The total data volume amounts to approximately 10M entries. By employing the prompt outlined in Appendix E.1, we re-annotated all the data using GPT-4O to generate “Long-gold” type reference expressions (as described in Sec 4). Furthermore, we constructed training data by randomly combining three types of reference expressions. 3.2 CommonCrawl Data To acquire larger-scale data for better scaling up of training, we also obtained web pages from CommonCrawl [ 50] and rendered screenshots to generate training data. Ho"}
{"id": "2507.23779v1_chunk23", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "pressions. 3.2 CommonCrawl Data To acquire larger-scale data for better scaling up of training, we also obtained web pages from CommonCrawl [ 50] and rendered screenshots to generate training data. However, the web data contained a significant amount of noisy data that caused training failures. To address this, we constructed a highly specific data cleaning pipeline, as illustrated in Figure 5. Below are the detailed steps of each stage: Index and domain deduplication We utilized the CC-MAIN-2024-46 crawl from CommonCrawl. After a basic deduplication of URLs (exact match), filtering by language (retaining only English), and webpage status (retaining only 2xx, 301, and 302), we were left with 2.6 billion URLs. These 2.6B URLs originate from 45.6 million unique domains, with the number of pages from the same domain displaying a long-tail distribution. For instance, the largest domain contains 204K different pages. We observed that pages from the same domain exhibit strong consistency in "}
{"id": "2507.23779v1_chunk24", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ges from the same domain displaying a long-tail distribution. For instance, the largest domain contains 204K different pages. We observed that pages from the same domain exhibit strong consistency in layout. Therefore, to ensure the generalizability of our model, we performed random sampling so that no more than 50 pages were selected from each domain. After this round of sampling, we were left with 475.45M URLs. Rendering We utilized the Selenium library and Google Chrome Driver to render webpage screenshots. During the rendering process, we randomly selected from three different pixel areas corresponding to 1080p, 2K, and 4K screen resolutions. The aspect ratio of the images was randomly chosen between 2:1 and 1:2. For the elements within the webpage HTML, we designed several rules for filtering and retaining them, as detailed in Appendix D.1.1. This process allowed us to preserve elements that are likely to be interactive components. At this stage, we save webpage screenshots, eleme"}
{"id": "2507.23779v1_chunk25", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "iltering and retaining them, as detailed in Appendix D.1.1. This process allowed us to preserve elements that are likely to be interactive components. At this stage, we save webpage screenshots, element information, and layout graphs (with different types such as interactive text buttons, interactive icon buttons, and images corresponding to specific colors). After this stage, there retained 285M webpages. 5Phi-Ground Tech Report Rule-based filtering Subsequently, we designed more fine-grained filters and deduplication techniques at the webpage and element levels based on the preserved webpages. The specific details are provided in Appendix D.1.2. These filters eliminated many erroneous and overly simplistic webpages. After this phase, 73.5M webpages remained. Element selection and labeling Finally, when selecting elements, we consider the distribution of element centroids and their types, as we found it necessary to do so in Sec. 5.3. Specifically, we discretize and uniformly sample a"}
{"id": "2507.23779v1_chunk26", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "g Finally, when selecting elements, we consider the distribution of element centroids and their types, as we found it necessary to do so in Sec. 5.3. Specifically, we discretize and uniformly sample across various regions of the canvas (see Appendix D.2). During sampling in a discrete area, we prioritize sampling icon elements, as they are less frequent. We sample only one element per webpage screenshot. Consequently, after this stage, 10.5M elements and screenshots remain. Finally, we use GPT-4O to annotate all the data with prompt in Appendix E.1. Filtered Images Kept Images Figure 6: Filtered and kept image of the Web Search Data with the classifier.Table 1: The applications distribution of the search queries. These applications are derived from an internal statistical list of the most frequently used applications on Windows. Domain # Apps Example apps # Queries Office & Productivity 10 Word, Excel 50 Media Playback & Creation 11 VLC, Photoshop, Clipchamp 55 Gaming Platforms 10 Stea"}
{"id": "2507.23779v1_chunk27", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "frequently used applications on Windows. Domain # Apps Example apps # Queries Office & Productivity 10 Word, Excel 50 Media Playback & Creation 11 VLC, Photoshop, Clipchamp 55 Gaming Platforms 10 Steam, League of Legends 50 Social & Communication 10 Discord, Zoom 50 Security & System Tools 10 CCleaner, Malwarebytes 50 Core Windows UI 15 Settings, File Explorer 75 Total 66 - 330 3.3 Web Search Data We construct a complementary, high-resolution screenshot corpus with the Bing Image Search v7 API. We first generate queries from 6 UI domains which covering a total of 66 desktop applications, as shown in Table 1. For each application we manually construct file search phrases such as “Word ribbon interface” or“screen shot of VLC playback controls” , producing 330 queries altogether. Every query retrieves 2048 candidate images and enforces a minimum resolution of200×200px.The request also specifies Bing’s license flag to ensure that all the images we use comply with copyright requirements . E"}
{"id": "2507.23779v1_chunk28", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "retrieves 2048 candidate images and enforces a minimum resolution of200×200px.The request also specifies Bing’s license flag to ensure that all the images we use comply with copyright requirements . Each downloaded image is then scored by a CLIP-based classifier to remove non-screenshot images from the search result. For the final filtered screenshots, we employed OmniParserV2 [ 51] to annotate all bounding boxes. We then used the same method described in 3.2 to label reference expressions and perform rule-based filtering at the bounding box level. This process resulted in approximately 158K data samples. 3.4 Human Labeled Data To address our focus on specific scenarios (Windows and common applications) and to explore in-domain training techniques, we have developed a pipeline for constructing human-labeled data. The data construction process involves three steps: •In the first step, labelers use custom screen recording software to interact with the target scenarios, which include spec"}
{"id": "2507.23779v1_chunk29", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ting human-labeled data. The data construction process involves three steps: •In the first step, labelers use custom screen recording software to interact with the target scenarios, which include specific software or Windows system settings pages. They are required to navigate and operate various pages and functions of the software. The screen recording software automatically captures different pages and retrieves the UIATree of the page. If the software does not implement UIA, we utilize OmniParser-V2 [ 51] to obtain the bounding boxes of elements. •In the second step, recognizing that the acquired bounding boxes contain numerous errors, redundancies, and non-interactive content (such as text portions in Word documents), we developed another software tool to enable labelers to remove erroneous bounding boxes. •Finally, we employ the Long-gold method, as described in Sec. 4.2, to have GPT-4O annotate references for all bounding boxes. 6Phi-Ground Tech Report UI-Vision | “HIDE SIDE PANE"}
{"id": "2507.23779v1_chunk30", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "neous bounding boxes. •Finally, we employ the Long-gold method, as described in Sec. 4.2, to have GPT-4O annotate references for all bounding boxes. 6Phi-Ground Tech Report UI-Vision | “HIDE SIDE PANEL” ScreenSpot -Pro | “up to the previous level directory” ScreenSpot -V2 | “insert a picture” ShowDown | “choose euro currency” Golden | “Select the comic effect ” Figure 7: Examples of benchmarks used in evaluation. Ultimately, we generated 80K training samples derived from a comprehensive suite including Microsoft Office, Windows settings, and over a dozen common software applications such as 7zip, PhotoShop, ClipChamp and audio tools. 4 Evaluation Settings 4.1 Benchmarks To ensure the model’s generalization capability and to avoid systematic overfitting to well-known benchmarks such as ScreenSpot, we have gathered several recent open-source and internally developed evaluation datasets. This approach aims to ensure the comprehensiveness of our testing. ScreenSpot V1 & V2 ScreenSpot [ 26]"}
{"id": "2507.23779v1_chunk31", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "enSpot, we have gathered several recent open-source and internally developed evaluation datasets. This approach aims to ensure the comprehensiveness of our testing. ScreenSpot V1 & V2 ScreenSpot [ 26] is the first realistic GUI grounding benchmark that includes over 600 interface screenshots from mobile, desktop, and web environments. It encompasses two different types of elements, namely, texts and icons. However, due to several incorrect or ambiguous annotations in the original benchmark, OS-Atlas [31] introduces an updated version called ScreenSpot-V2, which corrects various mislabeled instances in the original ScreenSpot dataset. ScreenSpot Pro ScreenSpot-Pro [ 25] focuses on high-resolution GUI tasks in professional applications. It contains 1,581 samples encompassing 23 applications across 3 operating systems, and the micro-elements only cover average 0.07% of the screen, which leads to a highly challenging assessment. UI-Vision UI-Vision [24] provides three fine-to-coarse graine"}
{"id": "2507.23779v1_chunk32", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "across 3 operating systems, and the micro-elements only cover average 0.07% of the screen, which leads to a highly challenging assessment. UI-Vision UI-Vision [24] provides three fine-to-coarse grained tasks—Element Grounding, Layout Grounding, and Action Prediction. Similar to ScreenSpot-pro, this benchmark encompasses another set of practical applications such as education and entertainment. It also categorizes three different types of references for testing purposes. Due to the selection of smaller buttons or those requiring more expert knowledge, this benchmark is equally challenging as ScreenSpot-pro. Moreover, it features a larger sample size, enhancing the stability and reliability of the evaluations. Showdown-click-dev Showdown [ 44] is a collection of 5,679 left clicks of humans performing various tasks in a macOS desktop environment. A subset containing 557 clicks was released on GitHub and huggingface, namely 7Phi-Ground Tech Report Showdown-click-dev. It reported grounding "}
{"id": "2507.23779v1_chunk33", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "erforming various tasks in a macOS desktop environment. A subset containing 557 clicks was released on GitHub and huggingface, namely 7Phi-Ground Tech Report Showdown-click-dev. It reported grounding results and latency of several advanced CUAs like OpenAI Operator [12] and Claude computer use [13]. Gold Dataset (proprietary) We have also internally constructed an evaluation dataset tailored for application scenarios within Windows. This dataset includes six scenarios: Photoshop (213 samples), ClipChamp (179 samples), PowerPoint (82 samples), Excel (107 samples), Word (87 samples), and Windows settings (1266 samples). The bounding boxes and instructions (Short RE) for each scenario were annotated by professional engineers familiar with Windows. While these share the same domain as the human-labeled data in Sec 3.4, the labelers come from different teams and use different computer settings, such as theme colors and screen sizes. In contrast, the selection of elements and the generation "}
{"id": "2507.23779v1_chunk34", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "n-labeled data in Sec 3.4, the labelers come from different teams and use different computer settings, such as theme colors and screen sizes. In contrast, the selection of elements and the generation of REs in our training set’s human-labeled data are automated or AI-generated, lacking expert knowledge. We employ this Gold dataset to evaluate in-domain training research. Additionally, it serves as an effective validation of practicality within Windows scenarios. In subsequent experiments, to facilitate faster testing, we manually extracted a smaller test set of 211 samples from this dataset, referred to as Gold-S . We present examples of these benchmarks in Figure 7. In addition, there are benchmarks like WinClick [ 52] that are still in the process of being open-sourced. As a result, we are unable to present their results in this paper. However, we plan to showcase these results on the project homepage once they are released. 4.2 Evaluation Protocols and metrics Reference expression t"}
{"id": "2507.23779v1_chunk35", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "re unable to present their results in this paper. However, we plan to showcase these results on the project homepage once they are released. 4.2 Evaluation Protocols and metrics Reference expression type A Reference Expression (RE) refers to a segment of referring text provided to a grounding model or GUI agent model concerning a specific interactive area, such as a button. This expression can be an indirect instruction like \"close the webpage\", or a direct and specific description such as \"the blue settings icon in the upper left corner\". The use of different REs during training and testing can significantly impact the model’s performance. Generally, we desire the model to generalize well across various REs. However, when using indirect REs, the model also needs some planning capability. To decouple model capabilities of planning and perception for research and enable efficient application in smaller models, previous works [ 31] introduced a simplified agent setting: employing more po"}
{"id": "2507.23779v1_chunk36", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "To decouple model capabilities of planning and perception for research and enable efficient application in smaller models, previous works [ 31] introduced a simplified agent setting: employing more powerful MLLMs like GPT-4O [ 53] and O4-mini [ 8] for planning and generating more detailed REs, while a smaller grounding model is used to produce coordinates. In this paper, to minimize misunderstanding, we define the following three types of REs. •Short / instruction. \"Short RE\" refers to a more concise form, potentially containing some instructions that require planning. During testing, we use \"Short RE\" to denote the reference provided by the benchmarks. This type of RE will be represented as the \"End-to-end model setting\" in the subsequent results tables. •Long / agent. Following [ 30,31], for short RE, we can utilize advanced MLLM to expand it into three more explicit and detailed types of RE: functional, positional, and appearance. We allow the large model to generate these three typ"}
{"id": "2507.23779v1_chunk37", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " for short RE, we can utilize advanced MLLM to expand it into three more explicit and detailed types of RE: functional, positional, and appearance. We allow the large model to generate these three types of references and concatenate the references together, forming what we call a Long RE. During testing, the input to the MLLM consists of only the screenshot and the short RE, and the output is the Long RE, which is then passed to a smaller model to generate coordinates. In subsequent tables, this approach is referred to as the \"Agent setting\" . The specific system prompt for generating Long RE can be found in Appendix E.2. •Long-gold. In the process of constructing the training dataset, on one hand, there is no short RE provided, while on the other, we can utilize ground truth (GT) bounding boxes. We also instruct the MLLM to generate the aforementioned three types of long REs. The difference lies in the input we provide to the MLLM, which includes a screenshot annotated with the GT bbo"}
{"id": "2507.23779v1_chunk38", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " boxes. We also instruct the MLLM to generate the aforementioned three types of long REs. The difference lies in the input we provide to the MLLM, which includes a screenshot annotated with the GT bbox and a cropped image of the target region. This allows the model to better observe and generate high-quality REs. However, since the generation process relies on the GT, this Long-gold RE is used solely for generating training data and not for evaluation. The specific system prompt can be found in Appendix E.1. Since this paper primarily focuses on model perception rather than planning, and all training utilizes Long-gold RE, unless otherwise specified, all experimental results in Section 5 are based on Long RE generated by GPT-4O. Metrics In all tables within this paper, unless otherwise specified, the reported metric is click accuracy. The benchmark provides a GT bounding box, and the model being tested generates a click coordinate. If the model’s output format is a bounding box, the ce"}
{"id": "2507.23779v1_chunk39", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "cified, the reported metric is click accuracy. The benchmark provides a GT bounding box, and the model being tested generates a click coordinate. If the model’s output format is a bounding box, the center of the box is taken as the click coordinate. A click is considered correct if it falls within the area of the GT bounding box; otherwise, it is deemed incorrect. This method is used to calculate the accuracy. 8Phi-Ground Tech Report Text first input Imag e first inputInstruction -aware featuresGeneral features Image token Text token Figure 8: Illustration of the impact of modal input order on model training. 5 Experiments 5.1 Input / Output Format The order of input modality. We investigated and experimented with the order in which text (or RE) and images are input into the model. The results displayed in Table 2 indicate that inputting text before images yields significantly better outcomes than the reverse order, aligning well with our expectations. Figure 8 illustrates the underlyi"}
{"id": "2507.23779v1_chunk40", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ults displayed in Table 2 indicate that inputting text before images yields significantly better outcomes than the reverse order, aligning well with our expectations. Figure 8 illustrates the underlying mechanism: due to the use of causal masks in transformer decoder’s attention, earlier tokens cannot be updated using later tokens. When images are input into the model first, the image tokens in the later layers, adapted through training, can be understood as being tailored for grounding tasks but unrelated to the RE. Conversely, when the order is reversed, the modeling of image tokens becomes instruction-aware. For perceptual tasks, the effectiveness of image modeling directly influences the final results. This observation bears significant resemblance to existing research related to instruction-aware models [54, 55, 56]. A simple modification of the modality order can effectively achieve this outcome. Table 2: Comparison of input order of modalities. Input format max_crops SSv2-Mobile"}
{"id": "2507.23779v1_chunk41", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "uction-aware models [54, 55, 56]. A simple modification of the modality order can effectively achieve this outcome. Table 2: Comparison of input order of modalities. Input format max_crops SSv2-Mobile SSv2-Desktop SSv2-Web SS-pro Gold-S Image first6 85.1 82.7 81.1 14.6 84.3 15 85.4 83.2 82.6 27.7 86.1 Text first6 87.5 (+2.4) 83.2 (+0.5) 83.1 (+2.0) 18.7 (+4.1) 86.7 (+2.4) 15 87.1 (+1.7) 85.3 (+2.1) 84.2 (+1.6) 30.6 (+2.9) 89.2 (+3.1) Output format. We investigated the impact of output formats on results, where output formats refer to the repre- sentation of coordinates, such as bounding boxes and points. Depending on the scenario, developers might need to employ different output formats. For instance, in scenarios involving the implementation of an OS agent, grounding only requires providing the coordinates for a click, which means delivering a point. However, some developers may wish the model to generate a bounding box. For the output format of boxes, using the center point of a box "}
{"id": "2507.23779v1_chunk42", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ng the coordinates for a click, which means delivering a point. However, some developers may wish the model to generate a bounding box. For the output format of boxes, using the center point of a box can serve to determine the click position, thereby accommodating the previous scenario. We discussed the following formats and implementation strategies: •Point+OmniParser. The model will output text in the following format: < point > mid x, mid y< /point > , where midxandmidyrepresent the coordinates of the central point. In scenarios involving box output, we first utilize OmniParser-V2 [ 51] to annotate all elements within the image. Subsequently, we select the target box using the points provided by the grounding model. •XYXY. The model will output a bounding box with format: < box > x 1, y1, x2, y2< /box > , where the (x1, y1)and the (x2, y2)are the top-left and bottom-right point. •XYWH. The model will output a bounding box with format: < box > x 1, y1, w, h < /box > , where the (x1, "}
{"id": "2507.23779v1_chunk43", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "1, x2, y2< /box > , where the (x1, y1)and the (x2, y2)are the top-left and bottom-right point. •XYWH. The model will output a bounding box with format: < box > x 1, y1, w, h < /box > , where the (x1, y1)is the top-left corner and w, h are the width and height of the box. •MidWH. The model will output a bounding box with format: < box > mix x, mid y, w, h < /box > , where the(midx, mid y)is the center point of the bounding box and w, h are the width and height of the box. Results are shown in the table 3. We found that directly outputting the point format achieved the best Click ACC, while the MidWH format better balanced Click ACC and the precision of the detection box. These conclusions align well with intuition. In terms of output precision for detection boxes, the XYXY format demonstrated the best performance. We observed that integrating OmniParser-V2 did not yield better results. On the one hand, the performance on the 9Phi-Ground Tech Report Table 3: Results of training using dif"}
{"id": "2507.23779v1_chunk44", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ed the best performance. We observed that integrating OmniParser-V2 did not yield better results. On the one hand, the performance on the 9Phi-Ground Tech Report Table 3: Results of training using different output formats. Note that for the point+OP format, the Click ACC reported in the table is derived directly from using the model’s output point as the click target. In contrast, IoU-related metrics are calculated by selecting a box annotated by OmniParserV2 based on the model’s output. The results in parentheses represent the average of the SSv2-desktop and SSv2-web subsets. Output formatScreenSpot-V2 ScreenSpot-pro Click ACC IoU@0.3 IoU@0.5 IoU@0.8 IoU Click ACC IoU@0.5 IoU point+OP 85.5 47.3 (61.8) 39.3 (53.8) 22.0 ( 30.9) 36.7 (48.1) 30.6 22.3 18.8 XYXY 84.7 72.4 (71.2) 59.6 (59.0) 26.6 (28.3) 53.7 (53.7) 30.0 23.8 20.1 XYWH 84.2 73.6 (70.9) 58.9 (57.7) 22.5 (25.2) 52.6 (52.1) 29.4 21.8 18.6 MidWH 85.0 73.6 (71.5) 57.1 (58.1) 21.7 (25.0) 51.6 (52.2) 30.4 22.4 19.2 ScreenSpot-mobil"}
{"id": "2507.23779v1_chunk45", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": ".6 (28.3) 53.7 (53.7) 30.0 23.8 20.1 XYWH 84.2 73.6 (70.9) 58.9 (57.7) 22.5 (25.2) 52.6 (52.1) 29.4 21.8 18.6 MidWH 85.0 73.6 (71.5) 57.1 (58.1) 21.7 (25.0) 51.6 (52.2) 30.4 22.4 19.2 ScreenSpot-mobile subset was exceptionally poor. On other subsets, the results were still slightly lower compared to models that output box coordinates end-to-end. However, for high box precision required situations ( IoU>0.8) and high-resolution scenarios like ScreenSpot-Pro, the gap was reduced, indicating that OmniParser’s implementation may have certain advantages in high-resolution contexts and generate more accurate boxes. 5.2 Data Augmentation Algorithm 1 random crop Require: img, box ,Croping probability: random_crop, Minimal cropping ratio: min_crop Ensure: Cropped image img,Updated box box 1:ifrand()<random_crop then 2: (w, h)←img.size 3: xleft←box[0] 4: xright←1.0−box[2] 5: xcrop _factor ←min_crop +rand()×(1− min_crop ) 6: crop x1←w×xleft×(1−xcrop _factor ) 7: crop x2←w×(box[2] + xright×xcrop _"}
{"id": "2507.23779v1_chunk46", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": ")<random_crop then 2: (w, h)←img.size 3: xleft←box[0] 4: xright←1.0−box[2] 5: xcrop _factor ←min_crop +rand()×(1− min_crop ) 6: crop x1←w×xleft×(1−xcrop _factor ) 7: crop x2←w×(box[2] + xright×xcrop _factor ) 8: 9: ... Same for Y axis ... 10: 11: img←CropImage (img, 12: (crop x1, crop y1, crop x2, crop y2)) 13: box←UpdateBox (box) 14:end ifAlgorithm 2 random resize and padding Require: img, Target dimensions (W, H ), Random re- size probability: random_resize, maximin screen size: max_screen_size Ensure: Output image canvas , Updated box box 1:(w, h)←img.size 2:canvas ←WhiteImage (W, H ) 3:ifrand()<random_resize then 4: Smax←min(1 .0,W w,H h) 5: Smin←W max_screen_size×Smax 6: scale←Smin+rand()×(Smax−Smin) 7: ( ˆw,ˆh)←(w×scale, h ×scale ) 8: img←Resize (img, ( ˆw,ˆh)) 9: pos←(randint (0, W−ˆw),randint (0, H−ˆh)) 10:else 11: Scale the image to fit in canvas... 12: pos←(0,0) 13:end if 14:canvas. paste(img, pos ) 15:box←UpdateBox (box) We investigated the effects of two data augmentation t"}
{"id": "2507.23779v1_chunk47", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ˆw),randint (0, H−ˆh)) 10:else 11: Scale the image to fit in canvas... 12: pos←(0,0) 13:end if 14:canvas. paste(img, pos ) 15:box←UpdateBox (box) We investigated the effects of two data augmentation techniques on the training of UI grounding models: Random Crop When a user’s software interface is larger than their screen, or under certain interface scaling conditions, it may result in the display of an incomplete page. Additionally, the ScreenSpot contains a small number of incomplete UI. Motivated by these observations, we introduce random crop as a data augmentation technique. We define the right edge of the bounding box and the right edge of the image as coordinates 0 and 1, respectively. We then select a random number within the range [min_crop, 1], which determines the new right boundary of the image. The same process is applied to other directions. Notably, we use the same random number for both the left and right sides (as well as the top and bottom), ensuring that the cropping "}
{"id": "2507.23779v1_chunk48", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ary of the image. The same process is applied to other directions. Notably, we use the same random number for both the left and right sides (as well as the top and bottom), ensuring that the cropping is proportional. This approach maintains the positional integrity of objects, such that if a box is located on the left side of the original image, it will remain on the left side of the cropped image. This helps avoid potential errors or changes in positional references. The detailed implementation is shown in Algorithm 1. 10Phi-Ground Tech Report Random Resize When a user reduces the software interface size or when the screen resolution is very high, items may become very small. To address this issue, we introduce a random resize data augmentation technique. The core of this data augmentation method is to shrink the image and place it onto a fixed-size white canvas. The canvas size is generally related to the design of the LMM model, particularly how it partitions the image into patches."}
{"id": "2507.23779v1_chunk49", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "n method is to shrink the image and place it onto a fixed-size white canvas. The canvas size is generally related to the design of the LMM model, particularly how it partitions the image into patches. We set a maximum screen size (e.g., 4K), and the canvas size is randomly selected between the size of the images in the training set and the maximum screen size. The purpose of this approach is to leverage the inherent size of the training set images (for instance, if the training set images are already large, excessive resizing should be avoided). For more details, please refer to Algorithm 2. Table 4: Data augmentation ablation experiments. Data Augmentation SSv2-desktop SSv2-web SSv2-mobile SS-pro Gold-S base 85.9 84.7 88.1 24.8 90.0 base + R-crop 86.2 84.9 87.9 23.6 89.8 base + R-resize 85.9 85.1 88.3 32.8 (+8.0) 89.1 base + R-crop + R-resize 86.5 85.1 88.3 32.7 (+7.9) 91.0 We conducted several sets of hyperparameter experiments, and the optimal combination is as follows: The probabil"}
{"id": "2507.23779v1_chunk50", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "5.9 85.1 88.3 32.8 (+8.0) 89.1 base + R-crop + R-resize 86.5 85.1 88.3 32.7 (+7.9) 91.0 We conducted several sets of hyperparameter experiments, and the optimal combination is as follows: The probability for Random Resize is set to 100%, with a maximum screen size of 4096, while the probability for random cropping is 0.3 with a min_crop = 0.7. We utilized these hyperparameters in all subsequent experiments. Table 5 presents the results of the ablation study on data augmentation. The results indicate that in high-resolution testing environments such as ScreenSpot-pro, employing random resize significantly improves performance. On the other hand, random cropping does not have a substantial impact in various scenarios. 5.3 Data Distribution Windows MacOS Linux GUIAct Seeclick FinewebBase Figure 9: Data distribution.Table 5: Data ablation experiments. \"Bing\" refers to our BingSearch dataset, \"CC\" denotes the CommonCrawl dataset, and \"CC\" and \"CC-re\" represent the datasets before and after "}
{"id": "2507.23779v1_chunk51", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "re 9: Data distribution.Table 5: Data ablation experiments. \"Bing\" refers to our BingSearch dataset, \"CC\" denotes the CommonCrawl dataset, and \"CC\" and \"CC-re\" represent the datasets before and after applying our proposed re-sampling technique, respectively. ID Data SSv2 SS-pro Gold-S 1 base 85.3 32.3 89.1 297%base + 3%Bing 85.5 32.7 89.6 370%base + 30%CC 85.8 31.5 90.0 470%base + 30%CC-Re 85.5 33.1 89.6 567%base + 3%Bing + 30%CC-Re 86.1 33.3 89.7 We analyzed the distribution of the relative positions of the center points of all detection boxes within each dataset, as illustrated in Figure 10. Our findings reveal that data from different sources exhibit distinct distribution patterns. For instance, we observed that web-rendering data such as FineWeb, SeeClick, and our CommonCrawl data (when not resampled) exhibit an identical distribution pattern, as shown in the first graph of Figure 10(a). These datasets almost completely lack buttons on the right side, which can be attributed to the"}
{"id": "2507.23779v1_chunk52", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "n not resampled) exhibit an identical distribution pattern, as shown in the first graph of Figure 10(a). These datasets almost completely lack buttons on the right side, which can be attributed to the common web design practice of placing sidebars on the left and reserving the right side for scroll bars. However, such a distribution is not universal. For instance, in other datasets, the distribution tends to be more uniform. Our BingSearch demonstrates the best uniformity and diversity. Different software exhibits its unique distribution, as evidenced by our human-labeled data. Therefore, we proposed an algorithm to resample our own CommonCrawl data. The basic idea of the algorithm is to divide the image into a 50×50grid and sample a fixed number of points from each grid cell. This approach ensures that the central points are uniformly distributed by area, as detailed in Appendix D.2. During the resampling process, we introduced a sampling factor to balance the trade-off between sampli"}
{"id": "2507.23779v1_chunk53", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "h ensures that the central points are uniformly distributed by area, as detailed in Appendix D.2. During the resampling process, we introduced a sampling factor to balance the trade-off between sampling rate and uniformity, as illustrated in Figure 10(b). To validate the generated data and the effects of the aforementioned data resampling, we first constructed a base setting using open-source data, as shown in Figure 9. The data proportions are roughly proportional to the size of the subset. 11Phi-Ground Tech Report Linux MacOS Windows Seeclick /Fineweb GUIAct Human -ClipChamp Human -Office Human -PhotoShop BingSearch CommonCrawl R=0.25; RES=36% R=0.3; RES=45% R=0.2; RES=30% R=0.1; RES=9% R=0.15; RES=18% SS-pro Gold SSv2 -mobile SSv2 -desktop SSv2 -Web(a) The center point distribution of different training datasets (b) The center point distribution and remaining data proportion (RES) when changing sampling factor (R) (c) The center point distribution of different benchmarks Figure 10: "}
{"id": "2507.23779v1_chunk54", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ifferent training datasets (b) The center point distribution and remaining data proportion (RES) when changing sampling factor (R) (c) The center point distribution of different benchmarks Figure 10: Center point distribution of training and evaluation data. We sampled 5M data points from this distribution to train the model. Based on this, we gradually replaced with our data, such as replacing 3% of the BingSearch data (3% of 5M is approximately equivalent to 1 epoch of BingSearch) or 30% of the CommonCrawl data. The results are shown in Table 5. Comparing experiments ID-1, ID-2, and ID-3, the model’s performance on specific benchmarks improved after incorporating our data. Since we primarily replaced web data (Fineweb and SeeClick), this indicates that our data surpasses the web data from open-source datasets, mainly due to the greater variety of our data. Comparing experiments ID-3 and ID-4, there is a significant improvement on the ScreenSpot-pro dataset. This suggests that a unifo"}
{"id": "2507.23779v1_chunk55", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "m open-source datasets, mainly due to the greater variety of our data. Comparing experiments ID-3 and ID-4, there is a significant improvement on the ScreenSpot-pro dataset. This suggests that a uniform distribution of boxes is more beneficial for generalization in high-resolution scenarios. Finally, combining all our data achieved the best results (ID-5). 5.4 In-domain Post-training In this section, we will utilize our human-labeled data and explore fine-tuning strategies for target scenarios. In practical applications, developers might have a small set of target software that they wish to cover with their agent. We will use Adobe Photoshop (PS) as an example. Our human-labeled data includes screen recordings processed using the method described in Sec. 3.4. The bounding boxes are derived from UIA-tree, while the references are sourced from GPT-4o. For our evaluation set, we focus on the PS subset of our Gold dataset, as well as the PS subset from ScreenSpot-pro. It is important to no"}
{"id": "2507.23779v1_chunk56", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " from UIA-tree, while the references are sourced from GPT-4o. For our evaluation set, we focus on the PS subset of our Gold dataset, as well as the PS subset from ScreenSpot-pro. It is important to note that our Gold dataset was provided by another team, and both the bounding box and reference annotations were done by humans. Therefore, there may be discrepancies with our training set. Domain Finetuning When performing domain fine-tuning, we consider the following strategies: 12Phi-Ground Tech Report • Strategy A involves directly incorporating domain-specific data into the pre-trained model. •Strategy B entails performing SFT of a pre-trained model (without using domain data) using the domain data. •Strategy C first introduces a small proportion of domain data during the pre-training phase, followed by using a larger proportion of domain data during the SFT phase. We set the pre-training data size to 5M and the SFT data size to 200K. Training a subset for too many epochs (e.g., more t"}
{"id": "2507.23779v1_chunk57", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "e, followed by using a larger proportion of domain data during the SFT phase. We set the pre-training data size to 5M and the SFT data size to 200K. Training a subset for too many epochs (e.g., more than 10) can lead to overfitting and other issues. Thus, we limit the number of epochs of domain data for both the pre-training and SFT stages to approximately 3 epochs. Due to the difference in total data volume, the proportion of domain-specific data will vary accordingly. Table 6: Comparison of data selection strategies during in-domain post-training and pretraining. StrategyPretrain Finetune General Domain base PS (r / ep) base PS (r / ep) SSv2 SSpro Gold-PS SSpro-PS - 100% 0% / 0 - - 86.1 33.3 68.1 39.2 A-1 99.5% 0.5% / 2.2 - - 86.2 32.7 71.8 41.2 A-2 99.0% 1.0% / 4.5 - - 86.0 34.4 72.7 41.2 B-1 100% 0% / 0 88.0% 12.0% / 2.1 84.2 31.4 72.0 41.2 B-2 100% 0% / 0 76.0% 24.0% / 4.3 84.4 30.7 74.2 41.2 C 99.5% 0.5% / 2.2 88.0% 12.0% / 2.1 85.8 34.2 74.8 43.1 The results, as shown in Table 6"}
{"id": "2507.23779v1_chunk58", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": ".2 B-1 100% 0% / 0 88.0% 12.0% / 2.1 84.2 31.4 72.0 41.2 B-2 100% 0% / 0 76.0% 24.0% / 4.3 84.4 30.7 74.2 41.2 C 99.5% 0.5% / 2.2 88.0% 12.0% / 2.1 85.8 34.2 74.8 43.1 The results, as shown in Table 6, indicate that directly incorporating domain-specific data into the training set has a minimal impact on the model’s general capabilities. However, the improvement in performance for the specific domain is quite limited. This may be due to the relatively small proportion of domain-specific data, which constitutes only a minor part of the optimization objective. In the case of Strategy B, where domain-specific data is introduced only during the fine-tuning phase, we observed a significant improvement in in-domain results. However, we also noted a marked decline in the model’s general capabilities after fine-tuning. This suggests that incorporating previously unseen domain data during fine-tuning may lead to overfitting and catastrophic forgetting of pre-trained knowledge. Strategy C effect"}
{"id": "2507.23779v1_chunk59", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ies after fine-tuning. This suggests that incorporating previously unseen domain data during fine-tuning may lead to overfitting and catastrophic forgetting of pre-trained knowledge. Strategy C effectively balances general capabilities and in-domain performance. It maintains strong general abilities from the pre-training phase while also achieving the best results within the domain. During our fine-tuning process, we found that the gradient norm for Strategy C was smaller than that for Strategy B. This suggests that the decline in general capabilities observed in Strategy B may also stem from training instability caused by the introduction of new data. Since Strategy C is exposed to domain data during the pre-training phase, it experiences a more tempered impact during fine-tuning. Post-training Algorithms Beyond the impact at the data level, we also explored the effects of training algorithms during the post-training phase. We primarily considered the following categories of algorithm"}
{"id": "2507.23779v1_chunk60", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "aining Algorithms Beyond the impact at the data level, we also explored the effects of training algorithms during the post-training phase. We primarily considered the following categories of algorithms: Supervised Fine- Tuning, Curriculum Learning, and Reinforcement Learning. For reinforcement learning, we further examined several algorithms, including reject sampling finetuning (which is not strictly traditional reinforcement learning but has been shown by many studies [ 57,58,59,60] to have similar characteristics and effects), DPO [ 61], REINFORCE [ 62], PPO [ 63], REINFORCE++ [ 64] and GRPO [ 65]. However, we failed to make positive increase on results with PPO, REINFORCE++, and GRPO, primarily due to three reasons: (1) The pure perception tasks lacked textual exploration and reasoning and hence, in this situation, the essence of using RL loss will be more closely aligned with the original SFT loss. (2) Although some previous work [ 66,67,68] has shown that RL can provide benefits "}
{"id": "2507.23779v1_chunk61", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " and hence, in this situation, the essence of using RL loss will be more closely aligned with the original SFT loss. (2) Although some previous work [ 66,67,68] has shown that RL can provide benefits in purely perceptual tasks, these studies typically begin with relatively low baselines or apply RL directly from scratch without prior SFT. In contrast, our work involves pre-training on in-domain tasks to an optimal level, leaving little room for further optimization. (3) The absence of exploration and reasoning led to low diversity in the answers among rollouts for the same sample, frequently resulting in all rollouts being either entirely correct or entirely incorrect (about 70% probability). This made the training of algorithms like GRPO trivial and further hindered the effective training of the critic model in PPO. This prompted us to consider using simpler RL algorithms such as reject sampling finetuning and DPO, as well as manual intervention in the selection of rollout samples. We"}
{"id": "2507.23779v1_chunk62", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " of the critic model in PPO. This prompted us to consider using simpler RL algorithms such as reject sampling finetuning and DPO, as well as manual intervention in the selection of rollout samples. We employed a unified framework, as illustrated in Figure 11, to integrate SFT, DPO, curriculum learning, and reject sampling finetuning. We ensured that the total number of samples used was consistent with the 200K samples used in SFT. Taking DPO as an example, the effective training samples only include \"Case 1\" in Figure 11, resulting in less than 100K samples being used for training. We implemented DPO using the trl library [ 69], which also includes several variants of DPO, such as sigmoid [ 61](original DPO implementation), hinge [ 70], IPO [ 71], exo [ 72], nca [ 73], 13Phi-Ground Tech Report prompts N outputs model X correct Y incorrectall correct all incorrectCase 1: Case 2: Case 3: SFT loss DPO lossGround Truth DPO Curriculum Learning Reject sampling finetuning Figure 11: ■:DPO. ■:"}
{"id": "2507.23779v1_chunk63", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "port prompts N outputs model X correct Y incorrectall correct all incorrectCase 1: Case 2: Case 3: SFT loss DPO lossGround Truth DPO Curriculum Learning Reject sampling finetuning Figure 11: ■:DPO. ■:Curriculum learning. ■:Reject sampling finetuning. X+Y=NTable 7: Results of different post-training algorithms which further optimized a pre-trained model that had reached saturation. To our surprise, we found that for purely perceptual tasks, DPO could still enhance the results of the pre-trained model. However, this conclusion primarily stems from data-level adaptability and robustness, which is fundamentally different from the mechanisms of LLM reasoning. AlgrithmGeneral Domain SSv2 SSpro Gold-S Gold-PS SSpro-PS Base Model 86.2 32.7 88.5 71.8 41.2 SFT 85.8 34.2 89.1 74.8 43.1 Curriculum-L 86.1 33.7 89.4 75.1 45.1 Reject Sampling 86.2 34.5 88.6 75.5 45.1 DPO-sigmoid 86.2 35.2 90.5 76.8 49.0 DPO-IPO 86.4 34.8 90.3 75.5 43.1 DPO-NCA 85.2 33.9 88.7 75.7 47.1 DPO-DiscoPoP 85.4 34.8 91.0 76.3"}
{"id": "2507.23779v1_chunk64", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "6.1 33.7 89.4 75.1 45.1 Reject Sampling 86.2 34.5 88.6 75.5 45.1 DPO-sigmoid 86.2 35.2 90.5 76.8 49.0 DPO-IPO 86.4 34.8 90.3 75.5 43.1 DPO-NCA 85.2 33.9 88.7 75.7 47.1 DPO-DiscoPoP 85.4 34.8 91.0 76.3 49.0 robust-dpo [ 74], SPPO [ 75], AOT [ 76], DiscoPoP [ 77], and APO [ 78]. For each variant of DPO, as well as for SFT, curriculum learning, and reject sampling, we conducted hyperparameter tuning. This included a grid search for learning rates ranging from 3e-6 to 3e-4, as well as an exploration of the hyperparameters most influential in each algorithm, such as βin DPO. In Table 7, we present the results of various algorithms, including the top-performing variants of DPO. The results indicate that RL can consistently outperform SFT in purely perceptual tasks, even when starting from a highly optimized pre-trained checkpoint, which is a non-trivial conclusion. As previously mentioned, purely perceptual tasks lack text-level reasoning, exploration, and reflection. While existing studies "}
{"id": "2507.23779v1_chunk65", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ly optimized pre-trained checkpoint, which is a non-trivial conclusion. As previously mentioned, purely perceptual tasks lack text-level reasoning, exploration, and reflection. While existing studies [ 66,67,68] have shown that reinforcement learning can enhance models that are either not pre-trained or insufficiently pre-trained, this improvement likely stems from mechanisms similar to those used by SFT. It remains uncertain whether reinforcement learning is beneficial for a model that has been pre-trained to its maximum potential. In conjunction with the results of curriculum learning, we summarize the principles underlying the advantages of RL in post-training as follows, which are fundamentally different from those in RL work related to LLM reasoning: •Due to the targeted nature of rollouts selection by these algorithms, they exhibit greater robustness in the data distribution during post-training. The advantages of RL in post-training may stem from the adaptive data distribution s"}
{"id": "2507.23779v1_chunk66", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "louts selection by these algorithms, they exhibit greater robustness in the data distribution during post-training. The advantages of RL in post-training may stem from the adaptive data distribution selection, akin to curriculum learning. •Similarly, because DPO selects samples that include both correct and incorrect outputs, it may mitigate the impact of simplistic data and, more importantly, erroneous ground truth. •Compared to SFT and curriculum learning, which use ground truth coordinates for training, RL relies solely on the model it-self’s outputs. This results in a more gradual and stable training process. 5.5 Scaling Scaling settings. We have set up the training settings for the scaling experiments by integrating all the conclusions drawn from previous sections. Specifically, we combined all available datasets at a certain ratio. Note that the Seeclick dataset was excluded after experimentation due to the excessive number of elements on average in each screenshot. For instance,"}
{"id": "2507.23779v1_chunk67", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "mbined all available datasets at a certain ratio. Note that the Seeclick dataset was excluded after experimentation due to the excessive number of elements on average in each screenshot. For instance, in the case of 40M training, Table 8 shows the proportion of each dataset used. For other training volumes, such as 20M, the proportions are calculated accordingly based on the weights. During the training process, we further increased the batch size to 8K. As illustrated in Figure 12, the training loss curve is exceptionally smooth and stable. We also observed some fluctuations in the performance across different benchmarks during training, which led us to conduct tests at regular intervals and ultimately select the checkpoint that performed best across all benchmarks. Table 9 presents the model cards for all the models trained. 14Phi-Ground Tech Report Table 8: Detailed training data proportion for Phi-Ground-4B-16C with 40M training volume. †: The number of samples here does not refer "}
{"id": "2507.23779v1_chunk68", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "odel cards for all the models trained. 14Phi-Ground Tech Report Table 8: Detailed training data proportion for Phi-Ground-4B-16C with 40M training volume. †: The number of samples here does not refer to the quantity of images or elements. In fact, because each element has three types of references—positional, appearance, and functional—we randomly combine them during training as model inputs. This combina- tion could involve one, two, or all three types. Consequently, a single element can be paired with various references, resulting in multiple sam- ples. This explains why the numbers here differ from those described earlier. Dataset Samples †Epoch Weight BingSearch 158 K 7.6 3.0% Linux 149 K 3.6 1.3% MacOS 69 K 3.9 0.7% Windows 3.5 M 1.1 10.0% GUIAct 155 K 3.4 1.3% E2ISynth 180 K 2.9 1.3% Fineweb 9.0 M 0.9 21.0 % CommonCrawl 25.3 M 1.0 60.0% Human 160 K 3.5 1.4% /uni00000013/uni00000011/uni00000014/uni00000018/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni"}
{"id": "2507.23779v1_chunk69", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " Fineweb 9.0 M 0.9 21.0 % CommonCrawl 25.3 M 1.0 60.0% Human 160 K 3.5 1.4% /uni00000013/uni00000011/uni00000014/uni00000018/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000016/uni00000013/uni00000013/uni00000011/uni00000016/uni00000018/uni00000013/uni00000011/uni00000017/uni00000013/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048 /uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056/uni00000003/uni00000046/uni00000058/uni00000055/uni00000059/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000016/uni00000011/uni00000018/uni0000001"}
{"id": "2507.23779v1_chunk70", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "i00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000016/uni00000011/uni00000018/uni00000017/uni00000011/uni00000013/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048 /uni0000000b/uni00000045/uni0000000c/uni00000003/uni0000002a/uni00000055/uni00000044/uni00000047/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000003/uni00000051/uni00000052/uni00000055/uni00000050/uni00000003/uni00000046/uni00000058/uni00000055/uni00000059/uni00000048 /uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni0000001b/uni00000013/uni0000001b/uni00000015/uni0000001b/uni00000017/uni0000001b/uni00000019/uni0000001b/uni0000001b/uni0000001c/uni00000013/uni0000001c/uni00000015"}
{"id": "2507.23779v1_chunk71", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "00000048/uni00000053/uni00000056/uni0000001b/uni00000013/uni0000001b/uni00000015/uni0000001b/uni00000017/uni0000001b/uni00000019/uni0000001b/uni0000001b/uni0000001c/uni00000013/uni0000001c/uni00000015/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048 /uni0000000b/uni00000046/uni0000000c/uni00000003/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni00000036/uni00000003/uni00000048/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000046/uni00000058/uni00000055/uni00000059/uni00000048/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/u"}
{"id": "2507.23779v1_chunk72", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048 /uni0000000b/uni00000047/uni0000000c/uni00000003/uni00000036/uni00000046/uni00000055/uni00000048/uni00000048/uni00000051/uni00000036/uni00000053/uni00000052/uni00000057/uni00000010/uni00000053/uni00000055/uni00000052/uni00000003/uni00000048/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000046/uni00000058/uni00000055/uni00000059/uni00000048 Figure 12: Training and evaluation curves for Phi-Ground-4B-16C. Scaling effect of parameters-computation trade-off. Scaling laws [ 38,39,40] typically examine the relationship between model parameters and training data given a fixed training budget. In our scenario, we further consider the computational cost during te"}
{"id": "2507.23779v1_chunk73", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ling laws [ 38,39,40] typically examine the relationship between model parameters and training data given a fixed training budget. In our scenario, we further consider the computational cost during testing, which is not only related to the number of parameters but is also directly influenced by the number of image tokens. In perception tasks, the number of image tokens is highly correlated with the model’s capability (see Table 2). In this section, we aim to investigate the relationship among model parameters, the number of image tokens, and training data volume. This is particularly relevant in real-world applications where developers are concerned with both training performance under limited resources and application latency (rather than just parameter count). Conducting such experiments can offer more economical strategies. Table 9: Models card of Phi-Ground family. Name Model details Training details Image tokens Phi-Ground-4B-7C Pretrained with Phi-3.5-Vision-Instruct as base mode"}
{"id": "2507.23779v1_chunk74", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " can offer more economical strategies. Table 9: Models card of Phi-Ground family. Name Model details Training details Image tokens Phi-Ground-4B-7C Pretrained with Phi-3.5-Vision-Instruct as base model, which is a 4.1B VLM with CLIP as image encoder. During training we set the num_crops of the model to 6 and make sure the image’s resolution is2×3. The model will devide the image into 2×3 + 1 = 7 crops, in which there is a global crop.Trained with 40M data; BS=8192; LR=8e-5. The training cost 200 A100 GPU days in total.1045 Phi-Ground-4B-16C Same with the above but change the image resolution to 3×5and num_crops to 15.Trained with 40M data; BS=8192; LR=8e-5. The training cost 440 A100 GPU days in total.2353 Phi-Ground-4B-16C-DPO Same with Phi-Ground-4B-16C Three rounds of DPO finetuning with more desk- top data from Phi-Ground-4B-16C. See Sec. 5.4 and Table 11 for more details.2353 Phi-Ground-4B-29C Same with Phi-Ground-4B-16C except for changing the image resolution to 4×7and num_crops"}
{"id": "2507.23779v1_chunk75", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "h more desk- top data from Phi-Ground-4B-16C. See Sec. 5.4 and Table 11 for more details.2353 Phi-Ground-4B-29C Same with Phi-Ground-4B-16C except for changing the image resolution to 4×7and num_crops to 28.Trained with 20M data; BS=8192; LR=8e-5. The training cost 415 A100 GPU days in total.4237 Phi-Ground-7B-7C A variation of Phi-4-MM [ 79] with 7B parameters. The model use SigLip as the image encoder. The input image’s resolution is 2×3 and num_crops of the model is 6 .Trained with 30M data; BS=8192; LR=1e-5. The training cost 350 A100 GPU days in total.1841 Phi-Ground-7B-16C Same with Phi-Ground-7B-7C except for changing the image reso- lution to 3×5and num_crops to 15.Trained with 15M data; BS=8192; LR=1e-5. The training cost 450 A100 GPU days in total.4161 Phi-Ground-7B-16C-DPO Same with Phi-Ground-7B-16C. Three rounds of DPO finetuning with more desk- top data from Phi-Ground-7B-16C. See Sec. 5.4 and Table 11 for more details.4161 Phi-Ground-7B-29C Same with Phi-Ground-7B-7C exc"}
{"id": "2507.23779v1_chunk76", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "with Phi-Ground-7B-16C. Three rounds of DPO finetuning with more desk- top data from Phi-Ground-7B-16C. See Sec. 5.4 and Table 11 for more details.4161 Phi-Ground-7B-29C Same with Phi-Ground-7B-7C except for changing the image reso- lution to 4×7and num_crops to 28.Trained with 7.5M data; BS=8192; LR=1e-5. The training cost 390 A100 GPU days in total.7505 Specifically, we used the 4.1B Phi-3.5-Vision-Instruct model and the 7B Phi-4-MM model as the base models for training. For each model, we configured three different image settings corresponding to varying numbers of image 15Phi-Ground Tech Report tokens. Specifically, the phi model family scales an image and crops it into a grid-shaped square, such as 336×336for Phi-3.5-V . By configuring the model’s num_crops parameter, we can adjust the maximum number of patches into which an image can be cropped. In our three image settings, we pad the images to shapes of 3×2,5×3, and 7×4using white padding, and set num_crops to 6, 15, and 28, res"}
{"id": "2507.23779v1_chunk77", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "e maximum number of patches into which an image can be cropped. In our three image settings, we pad the images to shapes of 3×2,5×3, and 7×4using white padding, and set num_crops to 6, 15, and 28, respectively. Under these three configurations, the final images are divided into 7, 16, and 29 patches. This is because the model also includes a default global image patch. During training, we fixed the total training budget for all models to 450 ( ±50) NVIDIA A100-80G GPU days. Table 9 provides more detailed information on the image tokens and actual training durations for each model. In this manner, we trained six models under a fixed training budget. Our aim is to understand which model design can most effectively achieve optimal performance given the available computational resources. /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni000000"}
{"id": "2507.23779v1_chunk78", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "0000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000005a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000003/uni00000012/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000003/uni0000000b/uni00000037/uni00000029/uni0000002f/uni00000032/uni00000033/uni00000056/uni0000000c/uni00000011/uni0000001b/uni00000013/uni0000001b/uni00000015/uni0000001b/uni00000017/uni0000001b/uni00000019/uni0000001b/uni0000001b/uni0000001c/uni00000013/uni0000001c/uni00000015 /uni00000017/uni00000025/uni00000010/uni0000001a/uni00000026/uni00000017/uni00000025/uni00000010/uni0000001a/uni00000026 /uni00000017/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026/uni00000017/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026 /uni00000017/uni00000025/un"}
{"id": "2507.23779v1_chunk79", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "010/uni0000001a/uni00000026 /uni00000017/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026/uni00000017/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026 /uni00000017/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026/uni00000017/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026 /uni0000001a/uni00000025/uni00000010/uni0000001a/uni00000026/uni0000001a/uni00000025/uni00000010/uni0000001a/uni00000026 /uni0000001a/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026/uni0000001a/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026 /uni0000001a/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026/uni0000001a/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026 /uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000036/uni00000046/uni00000055/uni00000048/uni00000048/uni00000051/uni00000036/uni00000053/uni00000052/uni00000057/uni00000010/uni00000039/uni00000015/uni00000017/uni00000025/uni00000003/uni0000005a/uni00000012/uni000"}
{"id": "2507.23779v1_chunk80", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "6/uni00000055/uni00000048/uni00000048/uni00000051/uni00000036/uni00000053/uni00000052/uni00000057/uni00000010/uni00000039/uni00000015/uni00000017/uni00000025/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000011/uni00000003/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000051/uni00000048/uni00000055 /uni0000001a/uni00000025/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000011/uni00000003/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000051/uni00000048/uni00000055 /uni00000017/uni00000025/uni00000003/uni0000005a/uni00000011/uni00000003/uni00000032/uni00000017/uni00000010/uni00000050/uni0000004c/uni00000051/uni0000004c /uni0000001a/uni00000025/uni00000003/uni0000005a/uni00000011/uni00000003/uni00000032/uni00000017/uni00000010/uni00000050/uni0000004c/uni00000051/uni0000004c /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni0000"}
{"id": "2507.23779v1_chunk81", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "0000004c /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000005a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000003/uni00000012/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000003/uni0000000b/uni00000037/uni00000029/uni0000002f/uni00000032/uni00000033/uni00000056/uni0000000c/uni00000011/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni00000017/uni00000018/uni00000018/uni00000013/uni00000018/uni00000018 /uni00000017/uni00000025/uni00000010/uni0000001a/uni00000026/uni00000017/uni00000025/uni00000010/uni0000001a/uni00000026 /uni00000017/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026/uni00000017/uni00000025/uni00000010/uni00000014/uni00000019/u"}
{"id": "2507.23779v1_chunk82", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "00026/uni00000017/uni00000025/uni00000010/uni0000001a/uni00000026 /uni00000017/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026/uni00000017/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026 /uni00000017/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026/uni00000017/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026 /uni0000001a/uni00000025/uni00000010/uni0000001a/uni00000026/uni0000001a/uni00000025/uni00000010/uni0000001a/uni00000026/uni0000001a/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026/uni0000001a/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026 /uni0000001a/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026/uni0000001a/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026 /uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000036/uni00000046/uni00000055/uni00000048/uni00000048/uni00000051/uni00000036/uni00000053/uni00000052/uni00000057/uni00000010/uni00000053/uni00000055/uni00000052/uni00000014/uni00"}
{"id": "2507.23779v1_chunk83", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "0c/uni00000003/uni00000036/uni00000046/uni00000055/uni00000048/uni00000048/uni00000051/uni00000036/uni00000053/uni00000052/uni00000057/uni00000010/uni00000053/uni00000055/uni00000052/uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000005a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000003/uni00000012/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000003/uni0000000b/uni00000037/uni00000029/uni0000002f/uni00000032/uni00000033/uni00000056/uni0000000c/uni00000011/uni00000015/uni00000013/uni00000015/uni00000015/uni00000015/uni00000017/uni00000015/uni00000019/uni00000015/uni0000001b/uni00000016/uni00000013/uni00000016/uni00000015/uni00000016/uni00000017/uni00000016/uni00000019 /uni00000017/uni00000025/uni00000010/uni0000001a/uni000000"}
{"id": "2507.23779v1_chunk84", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "i00000015/uni00000019/uni00000015/uni0000001b/uni00000016/uni00000013/uni00000016/uni00000015/uni00000016/uni00000017/uni00000016/uni00000019 /uni00000017/uni00000025/uni00000010/uni0000001a/uni00000026/uni00000017/uni00000025/uni00000010/uni0000001a/uni00000026 /uni00000017/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026/uni00000017/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026 /uni00000017/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026/uni00000017/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026 /uni0000001a/uni00000025/uni00000010/uni0000001a/uni00000026/uni0000001a/uni00000025/uni00000010/uni0000001a/uni00000026 /uni0000001a/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026/uni0000001a/uni00000025/uni00000010/uni00000014/uni00000019/uni00000026 /uni0000001a/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026/uni0000001a/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026 /uni0000000b/uni00000046/uni0000000c"}
{"id": "2507.23779v1_chunk85", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "000019/uni00000026 /uni0000001a/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026/uni0000001a/uni00000025/uni00000010/uni00000015/uni0000001c/uni00000026 /uni0000000b/uni00000046/uni0000000c/uni00000003/uni00000038/uni0000002c/uni00000010/uni00000039/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051 Figure 13: Illustration of the evaluation results in relation to the training computation load. The Y-axis represents the benchmark scores in click accuracy, while the X-axis denotes the training computation per sample in TFLOPs. This training computation is estimated using the formula FLOPs = 6ND, where Nis the number of image tokens and D is the number of model parameters. Figure 13 presents the training results of these six models. Based on our evaluations, the inference time generally aligns with the relationship depicted on the x-axis of the graph. Many current studies typically report only the number of parameters when discussing model performance, without emphasiz"}
{"id": "2507.23779v1_chunk86", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "time generally aligns with the relationship depicted on the x-axis of the graph. Many current studies typically report only the number of parameters when discussing model performance, without emphasizing computational aspects, such as the number of image tokens. In our experiments, where the model architecture is fixed, we observe that for more advanced and challenging benchmarks like ScreenSpot-pro and UI-Vision, the impact of image tokens is significant. Specifically, when the number of image tokens is low, it may become a bottleneck, resulting in the inability to perceive small objects and thus reducing the score. When the number of image tokens exceeds 2000, their impact gradually diminishes, meaning further increases in image tokens do not yield substantial marginal benefits akin to scaling laws. In some test datasets that do not require high resolution, such as ScreenSpot-V2, neither the model size nor the number of image tokens significantly affects performance. Furthermore, whe"}
{"id": "2507.23779v1_chunk87", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "caling laws. In some test datasets that do not require high resolution, such as ScreenSpot-V2, neither the model size nor the number of image tokens significantly affects performance. Furthermore, when the number of image tokens exceeds the benchmark requirements (as previously mentioned bottleneck), the impact on perception is very limited, as illustrated by the results in the Figure 13 with o4-mini as the planner. However, the difference between using a planner and not using one is quite significant. Scaling post-training We attempted to extend the duration of DPO post-training to observe its effects. Our findings indicate that conducting long-epoch Offline DPO training directly can lead to an initial increase in model performance followed by a decline, as illustrated by the gray line in Figure 14. We believe this phenomenon may be related to distribution shifting [ 80,81], a concept frequently discussed in previous research. Consequently, we plan to update the rollouts more frequent"}
{"id": "2507.23779v1_chunk88", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " Figure 14. We believe this phenomenon may be related to distribution shifting [ 80,81], a concept frequently discussed in previous research. Consequently, we plan to update the rollouts more frequently. Given our stringent rollout selection criteria, we opted to conduct multiple rounds of DPO to approximate the effects of Online DPO [ 80] efficiently. Specifically, we performed a new rollout every 100 steps and initiated a new round of training. Ultimately, after three rounds of DPO, we achieved the best results, as shown in Figure 14. 16Phi-Ground Tech Report /uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000017/uni00000019/uni00000017/uni0000001a/uni00000017/uni0000001b/uni00000017/uni0000001c/uni00000018/uni00000013/uni00000018/uni00000014 /uni00000036/uni00000046/uni00000055/uni00000048/uni00000048/uni00"}
{"id": "2507.23779v1_chunk89", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "7/uni00000019/uni00000017/uni0000001a/uni00000017/uni0000001b/uni00000017/uni0000001c/uni00000018/uni00000013/uni00000018/uni00000014 /uni00000036/uni00000046/uni00000055/uni00000048/uni00000048/uni00000051/uni00000036/uni00000053/uni00000052/uni00000057/uni00000010/uni00000053/uni00000055/uni00000052/uni00000032/uni00000049/uni00000049/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000003/uni00000027/uni00000033/uni00000032 /uni00000050/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000010/uni00000057/uni00000058/uni00000055/uni00000051/uni00000003/uni00000027/uni00000033/uni00000032 /uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000016/uni00000014/uni00000016/uni00000015/uni00000016/uni00000016/uni00000016/uni00000017/uni00000016/uni00000018 /uni00000038/uni0000002c/uni00000010/uni00000039/uni00"}
{"id": "2507.23779v1_chunk90", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "8/uni00000053/uni00000056/uni00000016/uni00000014/uni00000016/uni00000015/uni00000016/uni00000016/uni00000016/uni00000017/uni00000016/uni00000018 /uni00000038/uni0000002c/uni00000010/uni00000039/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000032/uni00000049/uni00000049/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000003/uni00000027/uni00000033/uni00000032 /uni00000050/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000010/uni00000057/uni00000058/uni00000055/uni00000051/uni00000003/uni00000027/uni00000033/uni00000032 /uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni0000001b/uni00000018/uni0000001b/uni00000019/uni0000001b/uni0000001a/uni0000001b/uni0000001b /uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000048/uni00000051/uni00000003/uni0000000b/uni0000004c/uni00000051/uni00"}
{"id": "2507.23779v1_chunk91", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "8/uni0000001b/uni00000019/uni0000001b/uni0000001a/uni0000001b/uni0000001b /uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000048/uni00000051/uni00000003/uni0000000b/uni0000004c/uni00000051/uni00000010/uni00000047/uni00000052/uni00000050/uni00000044/uni0000004c/uni00000051/uni0000000c/uni00000032/uni00000049/uni00000049/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000003/uni00000027/uni00000033/uni00000032 /uni00000050/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000010/uni00000057/uni00000058/uni00000055/uni00000051/uni00000003/uni00000027/uni00000033/uni00000032 Figure 14: Multi-turns DPO vs. Offline DPO for in-domain post-training. In the post-training phase, as previously mentioned, we increased the proportion of in-domain data, which primarily consists of Web Search and human-labeled data. After training, the results for in-domain data (our Gold dataset) improved significantly. Surprisingly, several general benchmarks, such as ScreenSpot-Pro and UI-Vision, als"}
{"id": "2507.23779v1_chunk92", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ch and human-labeled data. After training, the results for in-domain data (our Gold dataset) improved significantly. Surprisingly, several general benchmarks, such as ScreenSpot-Pro and UI-Vision, also showed noticeable improvements. This could be partly because these benchmarks have some overlap with our target software (e.g., PPT), and partly because the distribution of desktop applications is similar to that of these benchmarks. Other benchmarks not shown here maintained their original scores or experienced slight improvements after DPO, with detailed results available in the experimental tables in Appendix B. 6 Evaluation and Case Study 6.1 UI Grounding Benchmark Results Table 10 presents the test results of several open-source models with fewer than 10Bparameters on our selected five GUI grounding test sets. The results in the upper block were obtained using the benchmark’s built-in reference expressions, typically an instruction or short REs. In contrast, the lower block shows th"}
{"id": "2507.23779v1_chunk93", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "GUI grounding test sets. The results in the upper block were obtained using the benchmark’s built-in reference expressions, typically an instruction or short REs. In contrast, the lower block shows the results when we used o4-mini to generate Long REs, which were then tested by the grounding models. Table 10: The comparison of results across five GUI grounding test sets, which were tested by us, is presented. More detailed tables of results for additional open-source and closed-source models can be found in Appendix B. modelScreenSpot-V2 ScreenSpot-pro UI-Vision ShowDown Gold Desktop Web Mobile A VG A VG basic functional spatial A VG A VG Gold-S ALL End-to-end model setting (Use short REs) SeeClick-9.6B [26] 64.6 49.7 43.9 55.1 1.1 9.4 4.7 2.1 5.4 24.6 51.7 20.4 UGround-7B [30] 73.2 78.3 72.7 76.1 16.5 11.5 12.2 2.8 8.8 46.5 74.9 54.9 UGround-v1-7B [30] 87.1 86.1 89.4 87.7 31.1 15.4 17.1 6.25 12.9 57.8 84.4 66.4 OS-Atlas-4B [31] 73.5 59.6 74.5 71.9 3.7 - - - - 15.8 47.9 22.0 OS-Atlas-7"}
{"id": "2507.23779v1_chunk94", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "2.7 76.1 16.5 11.5 12.2 2.8 8.8 46.5 74.9 54.9 UGround-v1-7B [30] 87.1 86.1 89.4 87.7 31.1 15.4 17.1 6.25 12.9 57.8 84.4 66.4 OS-Atlas-4B [31] 73.5 59.6 74.5 71.9 3.7 - - - - 15.8 47.9 22.0 OS-Atlas-7B [31] 85.5 77.2 84.0 84.1 18.9 12.2 11.2 3.67 9.0 41.1 66.4 48.8 UI-TARS-2B [28] 87.2 79.7 82.8 84.7 27.7 - - - - 59.8 79.2 60.1 UI-TARS-7B [28] 93.0 90.2 89.4 91.6 35.7 20.1 24.3 8.4 17.6 66.1 87.2 76.8 UI-TARS-1.5-7B [29] 86.9 87.6 90.0 89.0 42.6 28.8 27.5 10.7 22.3 67.2 86.7 77.2 Phi-Ground-4B-16C-DPO 84.4 86.4 78.1 84.1 38.0 33.4 34.3 5.8 24.5 58.2 87.2 78.2 Phi-Ground-7B-16C-DPO 83.3 84.8 79.3 83.8 43.2 36.8 37.1 7.6 27.2 62.5 84.4 79.6 Agent setting (Use long REs) with O4-mini as planner SeeClick-9.6B 43.5 64.4 43.3 53.5 1.2 5.2 5.0 3.2 4.5 19.6 39.4 15.6 UGround-7B 84.6 90.0 88.5 89.0 23.7 25.1 22.8 11.3 19.7 62.4 83.2 59.5 UGround-v1-7B 89.8 92.2 91.9 92.1 32.5 30.5 29.3 13.9 24.6 66.7 88.0 73.8 OS-Atlas-4B 41.9 64.6 57.7 57.4 2.0 - - - - 18.0 37.0 18.2 OS-Atlas-7B 80.6 86.2 88.9 "}
{"id": "2507.23779v1_chunk95", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "23.7 25.1 22.8 11.3 19.7 62.4 83.2 59.5 UGround-v1-7B 89.8 92.2 91.9 92.1 32.5 30.5 29.3 13.9 24.6 66.7 88.0 73.8 OS-Atlas-4B 41.9 64.6 57.7 57.4 2.0 - - - - 18.0 37.0 18.2 OS-Atlas-7B 80.6 86.2 88.9 84.7 21.1 17.2 16.6 7.5 13.8 45.5 68.3 52.2 UI-TARS-2B 85.5 92.2 90.0 90.4 35.9 - - - - 66.4 85.6 77.1 UI-TARS-7B 91.5 93.1 92.9 93.0 40.6 33.2 33.4 16.7 27.8 69.8 90.9 81.5 UI-TARS-1.5-7B 89.9 92.7 92.0 92.2 48.8 35.1 35.1 17.9 29.4 71.6 90.4 80.3 Phi-Ground-4B-16C-DPO 92.8 92.7 92.4 92.3 51.5 43.8 42.1 21.0 35.6 73.5 95.2 88.4 Phi-Ground-7B-16C-DPO 92.6 92.6 93.0 93.4 55.0 44.2 43.8 20.5 36.2 73.9 93.8 88.2 17Phi-Ground Tech Report Our grounding model is trained specifically for the agent setting, meaning that the training dataset primarily consists of various combinations of REs. As a result, our model achieves significant advantages and SOTA results across all benchmarks in the agent setting. Specifically, ScreenSpot-pro achieved an accuracy of 55.0. UI-Vision also attained a result of"}
{"id": "2507.23779v1_chunk96", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "t, our model achieves significant advantages and SOTA results across all benchmarks in the agent setting. Specifically, ScreenSpot-pro achieved an accuracy of 55.0. UI-Vision also attained a result of 36.2, which is the highest for this benchmark. Furthermore, our results on the Showdown benchmark surpass those of commercial models like OpenAI Operator and Claude Computer Use (see Table 15 in the Appendix). In the end-to-end model setting, our model consistently outperforms others on the ScreenSpot-Pro, UI-Vision, and our Gold dataset, although its performance on ScreenSpot-V2 is relatively average. ScreenSpot V1 and V2 (with V2 sharing most of its data with V1) have long been crucial benchmarks for GUI grounding testing. We observed that some models perform well on ScreenSpot-V2 but do not demonstrate the same significant advantages on newly emerging benchmarks. This could be a result of developers optimizing their models based on a single benchmark over time. In our approach, we did "}
{"id": "2507.23779v1_chunk97", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "t demonstrate the same significant advantages on newly emerging benchmarks. This could be a result of developers optimizing their models based on a single benchmark over time. In our approach, we did not include any mobile data in the training set (as this is not our focus scenario), nor did we balance the training set with icon and text-based buttons (since the scenarios faced by product users are not balanced either). However, these techniques might significantly impact the accuracy on ScreenSpot-V1 and V2. Throughout our development process, the selection and ablation of each technique were carefully considered across multiple benchmarks. As a result, our model exhibits more balanced performance and better generalization. 6.2 Error Analysis 0% 38% 57% 72% 78% 86% 100% Proportion:Samples left: 1534 951 659 423 335 218 0(A) Solved examples with instruction (short RE)(B) Solved examples with O4 -mini as planner to generate long RE (C) Solved examples with human expert as planner(D) Pas"}
{"id": "2507.23779v1_chunk98", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " left: 1534 951 659 423 335 218 0(A) Solved examples with instruction (short RE)(B) Solved examples with O4 -mini as planner to generate long RE (C) Solved examples with human expert as planner(D) Pass@8 solved (F) Remaining e rror (E)Not covered languag e instruction Phi-Ground Resultinstruction Phi-Ground ResultO4-miniinstruction O4-mini … instruction O4-mini Long -Gol dHumanGT bbox Result Phi-Ground Instruction : key in keywords for the spreadsheet O4-mini: …… A long, empty white rectangle outlined by a green selection highlight, directly to the right of the gray label “Keywords:”, inside a …… Instruction : view settings of the other screen O4-mini: …… the secondary monitor icon is immediately to the right of the …… The icon is a small rectangular grayscale monitor graphic labeled DELL U2723QE …… O4-mini : …… It’s located at the top right corner of the editor …… Human: …… immediately to the bottom of the search input field , between the the …… vertical rectangles overlaid by a magni"}
{"id": "2507.23779v1_chunk99", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "U2723QE …… O4-mini : …… It’s located at the top right corner of the editor …… Human: …… immediately to the bottom of the search input field , between the the …… vertical rectangles overlaid by a magnifying glass at the lower right corner . Instruction : click layout in vivado Figure 15: Types and Proportions of Errors on the ScreenSpot-pro Benchmark. In each image, the red rectangles represent the regions corresponding to the ground truth. Red circles indicate erroneous outputs from the previous stage, while green circles denote correct outputs from the current stage. The centers of the green circles fall within the ground truth boundaries. To avoid obstructing the image content, we have enlarged the green circles in some of the images. To analyze and illustrate the errors made by current grounding models as a case study, we selected a challenging benchmark, ScreenSpot-Pro, as an example. We employed the Phi-Ground-4B-DPO as the grounding model and designed a cascading approach to sequ"}
{"id": "2507.23779v1_chunk100", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "grounding models as a case study, we selected a challenging benchmark, ScreenSpot-Pro, as an example. We employed the Phi-Ground-4B-DPO as the grounding model and designed a cascading approach to sequentially process the benchmark data. Specifically, as shown in Figure 15, Stage A involves using the benchmark-provided instructions as reference expressions (also known as short REs) for the total of 1,534 test samples from ScreenSpot-Pro. As previously mentioned, our model successfully resolved 38% of the test cases, leaving 951 incorrect samples. In Stage B , we used O4-mini as a planner for the remaining 951 samples to generate long REs as input for the model. This approach further resolved 292 samples, leaving 659 samples unresolved. Across both stages, we successfully addressed 57% of the benchmark samples. In Stage C , for the remaining 659 samples, we intended to use human experts as planners to generate REs to observe the potential error rates in the planner section. However, this"}
{"id": "2507.23779v1_chunk101", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "e benchmark samples. In Stage C , for the remaining 659 samples, we intended to use human experts as planners to generate REs to observe the potential error rates in the planner section. However, this approach was deemed too costly. Therefore, we first had O4-mini generate REs using the Long-Gold method (see Sec. 4.2), which involves disclosing the GT bbox during RE generation. Human experts then reviewed the samples, GT, and REs generated by O4-mini to correct any errors and 18Phi-Ground Tech Report produce the final REs for model input. This stage resolved an additional 236 samples. In Stage D , we recognized that for REs of similar quality, different styles and emphases might lead to varying results. Thus, we repeated the RE and grounding process of Stage C seven times for the remaining 423 samples, resolving an additional 88 samples. Ultimately, 335 samples remained with errors largely unrelated to RE quality. We will analyze the errors and their implications in each stage in detai"}
{"id": "2507.23779v1_chunk102", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " 423 samples, resolving an additional 88 samples. Ultimately, 335 samples remained with errors largely unrelated to RE quality. We will analyze the errors and their implications in each stage in detail below. We first observe that end-to-end grounding models lack spatial reasoning capabilities, as illustrated in the example from Stage B of Figure 15. When certain keywords appear in the instruction, such as \"keyword\" in Example 1 or \"screen\" in Example 2, the grounding model tends to directly highlight the locations of these words in the image. However, in Example 1, the interactive region is actually the white rectangular input box, and clicking on the text of the label might result in a failed interaction. Such spatial reasoning requires a degree of common sense, rather than being purely a grounding task. The introduction of a planner addresses this type of task effectively by directly describing \"white rectangle\" in the RE. We refer to such errors as \" planning omissions ,\" which acc"}
{"id": "2507.23779v1_chunk103", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "a grounding task. The introduction of a planner addresses this type of task effectively by directly describing \"white rectangle\" in the RE. We refer to such errors as \" planning omissions ,\" which account for 19% of the total sample and 30.7% of the total errors. However, the planning of O4-mini may also encounter errors, particularly in scenarios where the target area contains multiple similar regions or when specialized application knowledge is required. In such cases, the planner’s hallucina- tions can lead to mistakes. For instance, as illustrated in Figure 15 Stage C, the markdown display button is typically located in the upper right corner. However, in the example shown, two work pages are open, causing the button to be centered. This resulted in an incorrect RE by O4-mini, leading to erroneous grounding. After manually correcting the erroneous RE, our model was able to produce the correct result. We refer to this type of error as a \" planning error ,\" which accounts for 15.4% o"}
{"id": "2507.23779v1_chunk104", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "to erroneous grounding. After manually correcting the erroneous RE, our model was able to produce the correct result. We refer to this type of error as a \" planning error ,\" which accounts for 15.4% of the total samples and 24.8% of the total errors. For the remaining samples, we observed that the grounding model might exhibit a preference for a certain style of RE, even when the quality is consistent, as evidenced by the pass@8 metric indicating new correct samples. However, this influence is minor, affecting only 5.7% of the samples and accounting for 9.3% of the errors. Additionally, we found that 117 samples (7.6% of the samples and 12.3% of the errors) were impacted because the target area or its vicinity contained languages not covered by our model, such as Chinese. Due to the strict selection of training data, primarily from the CommonCrawl dataset, which exclusively includes only English data, the model failed to correctly recognize many straightforward and easy situations due "}
{"id": "2507.23779v1_chunk105", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " selection of training data, primarily from the CommonCrawl dataset, which exclusively includes only English data, the model failed to correctly recognize many straightforward and easy situations due to language issues. Ultimately, 218 samples remained as particularly challenging data, making up another 14.2% of the samples and 22.9% of the errors. For the remaining errors, we provide a more detailed case study in Appendix F. We categorize these errors into several identifiable types: •Accuracy issues arise due to excessively extreme screen sizes and shapes. For instance, screens with an ultra-wide aspect ratio may result in output coordinates that deviate from the intended target area. •Language descriptions fail to adequately constrain areas of spatial planning, such as when generating tables and instructing to click on a blank cell in the 13th row and 8th column. Such specific spatial tasks present significant challenges for both the planner and the grounding model. • Regions that a"}
{"id": "2507.23779v1_chunk106", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ables and instructing to click on a blank cell in the 13th row and 8th column. Such specific spatial tasks present significant challenges for both the planner and the grounding model. • Regions that are difficult to describe using natural language. 7 Social Impacts and Open Questions With the development of CUA, we have both expectations and concerns regarding this direction. Primarily, there is the issue of user privacy. During our training process, we have verified the legality of the licenses for the open-source datasets used, and ensured that licenses are valid in Bing search filtering and web filtering. However, when CUA is successfully deployed in user environments in the future, the need for grounding and planning may require screenshots of users’ screens to be uploaded to the cloud, potentially leading to privacy breaches. Throughout the entire research and product deployment process, we may need to establish relevant protocols, legal frameworks, or algorithms to ensure the pro"}
{"id": "2507.23779v1_chunk107", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": ", potentially leading to privacy breaches. Throughout the entire research and product deployment process, we may need to establish relevant protocols, legal frameworks, or algorithms to ensure the protection of user privacy. Secondly, there is the issue of accountability for erroneous actions performed by CUAs. There are instances where CUAs might execute irreversible and harmful operations, such as closing software without saving files or even deleting important documents. At the system level, we need to explore human-computer collaboration methods that allow CUAs to efficiently replace human labor while ensuring human oversight. From the perspective of GUI grounding, we have observed that errors due to incorrect grounding can have more severe consequences. This is because a trained grounding model, when making mistakes, still outputs interactively meaningful regions rather than blank areas, thus increasing the likelihood of irreversible impacts. For instance, the multiplication symbo"}
{"id": "2507.23779v1_chunk108", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ding model, when making mistakes, still outputs interactively meaningful regions rather than blank areas, thus increasing the likelihood of irreversible impacts. For instance, the multiplication symbol on a calculator might be mistakenly interpreted as a command to close software due to similar symbols, leading to unintended software closure. Some recent studies [ 82,83] have attempted to use MLLM to verify actions post-grounding, but these have shown limited 19Phi-Ground Tech Report effectiveness and increased time costs. Developing a benchmark to evaluate the potential harmfulness of GUI grounding models could also be highly beneficial. 8 Conclusion In conclusion, we have developed the Phi-Ground model family, which significantly enhances GUI grounding capa- bilities by improving the perception of interactive elements in digital interfaces. Our comprehensive empirical study identified critical factors such as data distribution, input/output formats, and computational efficiency that "}
{"id": "2507.23779v1_chunk109", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ception of interactive elements in digital interfaces. Our comprehensive empirical study identified critical factors such as data distribution, input/output formats, and computational efficiency that influence model performance. Using a two-stage approach, we combined advanced MLLMs for generating detailed REs with a specialized grounding model for precise coordinate output, achieving state-of-the-art results across various benchmarks, including challenging ones like ScreenSpot-pro and UI-Vision. While our models demonstrate promising results, we acknowledge the societal implications of deploying CUAs, especially regarding user privacy and error accountability. Our research not only advances GUI grounding but also offers insights applicable to other multimodal perception tasks, contributing to the development of more reliable and efficient CUAs. References [1]Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A s"}
{"id": "2507.23779v1_chunk110", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "g to the development of more reliable and efficient CUAs. References [1]Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science , 2024. [2]Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. Science China Information Sciences , 2025. [3]Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, et al. Exploring large language model based intelligent agents: Definitions, methods, and prospects. arXiv preprint arXiv:2401.03428 , 2024. [4]Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: A survey. arXiv preprint arXiv:2"}
{"id": "2507.23779v1_chunk111", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "2024. [4]Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: A survey. arXiv preprint arXiv:2411.18279 , 2024. [5]Pascal J Sager, Benjamin Meyer, Peng Yan, Rebekka von Wartburg-Kottler, Layan Etaiwi, Aref Enayati, Gabriel Nobel, Ahmed Abdulkadir, Benjamin F Grewe, and Thilo Stadelmann. A comprehensive survey of agents for computer use: Foundations, challenges, and future directions. arXiv preprint arXiv:2501.16150 , 2025. [6]Mohsen Soori, Behrooz Arezoo, and Roza Dastres. Artificial intelligence, machine learning and deep learning in advanced robotics, a review. Cognitive Robotics , 2023. [7]Demetris Vrontis, Michael Christofi, Vijay Pereira, Shlomo Tarba, Anna Makrides, and Eleni Trichina. Artificial intelligence, robotics, advanced technologies and human resource management: a systematic review. Artificial intelligence and international HRM , 2023. [8] OpenAI. Introducing openai"}
{"id": "2507.23779v1_chunk112", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "china. Artificial intelligence, robotics, advanced technologies and human resource management: a systematic review. Artificial intelligence and international HRM , 2023. [8] OpenAI. Introducing openai o3 and o4-mini, 2025. [9] Anthropic. Claude sonnet 4, 2025. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025. [11] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 , 2025. [12] OpenAI. Operator system card, 2025. [13] Anthropic. Introducing computer use, a new claude 3.5 sonnet, and claude 3.5 haiku, 2024. [14] Michael S Greenberg, Jennifer C Byington, and David G Harper. Mobile agents and security. IEEE Communica- tions magazine , 1998. [15] Jose M Such, A"}
{"id": "2507.23779v1_chunk113", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "claude 3.5 sonnet, and claude 3.5 haiku, 2024. [14] Michael S Greenberg, Jennifer C Byington, and David G Harper. Mobile agents and security. IEEE Communica- tions magazine , 1998. [15] Jose M Such, Agustín Espinosa, and Ana García-Fornes. A survey of privacy in multi-agent systems. The Knowledge Engineering Review , 2014. [16] Sohye Lim and Hongjin Shim. No secrets between the two of us: Privacy concerns over using ai agents. Cyberpsychology: Journal of Psychosocial Research on Cyberspace , 2022. [17] K Cartrysse and JCA Van Der Lubbe. Privacy in mobile agents. In IEEE First Symposium onMulti-Agent Security and Survivability, 2004 . IEEE, 2004. 20Phi-Ground Tech Report [18] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. ICLR , 2018. [19] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. NIP"}
{"id": "2507.23779v1_chunk114", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "s using workflow-guided exploration. ICLR , 2018. [19] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. NIPS , 2023. [20] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. ICML , 2025. [21] Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317 , 2024. [22] Segev Shlomov, Aviad Sela, Ido Levy, Liane Galanti, Roy Abitbol, et al. From grounding to planning: Bench- marking bottlenecks in web agents. arXiv preprint arXiv:2409.01927 , 2024. [23] Suyu Ye, Haojun Shi, Darren Shih, Hyokun Yun, Tanya Roosta, and Tianmin Shu. Realwebassist: A benchmark for long-horizon web assistance with real-world users. "}
{"id": "2507.23779v1_chunk115", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "v preprint arXiv:2409.01927 , 2024. [23] Suyu Ye, Haojun Shi, Darren Shih, Hyokun Yun, Tanya Roosta, and Tianmin Shu. Realwebassist: A benchmark for long-horizon web assistance with real-world users. arXiv preprint arXiv:2504.10445 , 2025. [24] Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan A Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, M Tamer Özsu, Aishwarya Agrawal, David Vazquez, et al. Ui-vision: A desktop-centric gui benchmark for visual perception and interaction. arXiv preprint arXiv:2503.15661 , 2025. [25] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981 , 2025. [26] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. In Proceedings of the 62nd Annual Meeting of the Association for Computa"}
{"id": "2507.23779v1_chunk116", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 2024. [27] Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239 , 2025. [28] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326 , 2025. [29] ByteDance Seed. Ui-tars-1.5. https://seed-tars.com/1.5 , 2025. [30] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. ICLR , 2025"}
{"id": "2507.23779v1_chunk117", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "25. [30] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. ICLR , 2025. [31] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: A foundation action model for generalist gui agents. ICLR , 2025. [32] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In ECCV . Springer, 2022. [33] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. NIPS , 2023. [34] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and "}
{"id": "2507.23779v1_chunk118", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "n-ended decoder for vision-centric tasks. NIPS , 2023. [34] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. ICLR , 2024. [35] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. ICLR , 2023. [36] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100 , 2022. [37] Parvinder Kaur, Baljit Singh Khehra, and Er Bhupinder Singh Mavi. Data augmentation for object detection: A review. In 2021 IEEE International Midwest Symposium on Circuits and Systems (MWSCAS) . IEEE, 2021. [38] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Ya"}
{"id": "2507.23779v1_chunk119", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "rnational Midwest Symposium on Circuits and Systems (MWSCAS) . IEEE, 2021. [38] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409 , 2017. 21Phi-Ground Tech Report [39] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020. [40] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. In NIPS , 2022. [41] Xinyi Liu, Xiaoyi Zhang, Ziyun Zhang, and Yan Lu. Ui-e2i-synth: Advancing gui grounding with large-scale instruction synthesis. arXiv preprint arXiv:2504.11257 , 2025. [42] Hugh Z"}
{"id": "2507.23779v1_chunk120", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "In NIPS , 2022. [41] Xinyi Liu, Xiaoyi Zhang, Ziyun Zhang, and Yan Lu. Ui-e2i-synth: Advancing gui grounding with large-scale instruction synthesis. arXiv preprint arXiv:2504.11257 , 2025. [42] Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, William Song, Tiffany Zhao, Pranav Raja, Charlotte Zhuang, Dylan Slack, et al. A careful examination of large language model performance on grade school arithmetic. NIPS , 2024. [43] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, et al. Skywork: A more open bilingual foundation model. arXiv preprint arXiv:2310.19341 , 2023. [44] General Agents Team. The showdown computer control evaluation suite, 2025. [45] Zhipeng Huang, Zhizheng Zhang, Yiting Lu, Zheng-Jun Zha, Zhibo Chen, and Baining Guo. Visualcritic: Making lmms perceive visual quality like humans. arXiv preprint arXiv:2403.12806 , 2024. [46] Rizhao Cai, Zirui Song, Dayan Guan, Zhenhao Chen, Yaohang Li, Xing Luo, "}
{"id": "2507.23779v1_chunk121", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "n, and Baining Guo. Visualcritic: Making lmms perceive visual quality like humans. arXiv preprint arXiv:2403.12806 , 2024. [46] Rizhao Cai, Zirui Song, Dayan Guan, Zhenhao Chen, Yaohang Li, Xing Luo, Chenyu Yi, and Alex Kot. Benchlmm: Benchmarking cross-style visual capability of large multimodal models. In ECCV . Springer, 2024. [47] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In ECCV . Springer, 2024. [48] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195 , 2023. [49] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In ICML . PMLR, 2022. [50] "}
{"id": "2507.23779v1_chunk122", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In ICML . PMLR, 2022. [50] Common crawl - open repository of web crawl data, 2025. [51] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203 , 2024. [52] Zheng Hui, Yinheng Li, Tianyi Chen, Colby Banbury, Kazuhito Koishida, et al. Winclick: Gui grounding with multimodal large language models. arXiv preprint arXiv:2503.04730 , 2025. [53] OpenAI. Hello gpt-4o, 2024. [54] Samuel Lavoie, Polina Kirichenko, Mark Ibrahim, Mahmoud Assran, Andrew Gordon Wilson, Aaron Courville, and Nicolas Ballas. Modeling caption diversity in contrastive vision-language pretraining. arXiv preprint arXiv:2405.00740 , 2024. [55] Ziqiang Xu, Qi Dai, Tian Xie, Yifan Yang, Kai Qiu, DongDong Chen, Zuxuan Wu, and Chong Luo. Viarl: Adaptive temporal grounding via vis"}
{"id": "2507.23779v1_chunk123", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "anguage pretraining. arXiv preprint arXiv:2405.00740 , 2024. [55] Ziqiang Xu, Qi Dai, Tian Xie, Yifan Yang, Kai Qiu, DongDong Chen, Zuxuan Wu, and Chong Luo. Viarl: Adaptive temporal grounding via visual iterated amplification reinforcement learning. arXiv preprint arXiv:2505.15447 , 2025. [56] Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Siddharth Garg, and Farshad Khorrami. Emma: Efficient visual alignment in multi-modal llms. arXiv preprint arXiv:2410.02080 , 2024. [57] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. [58] Walter R Gilks and Pascal Wild. Adaptive rejection sampling for gibbs sampling. Journal of the Royal Statistical Society: Series C (Applied Statistics) , 41(2), 1992. [59] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Chris"}
{"id": "2507.23779v1_chunk124", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "tion sampling for gibbs sampling. Journal of the Royal Statistical Society: Series C (Applied Statistics) , 41(2), 1992. [59] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021. [60] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022. 22Phi-Ground Tech Report [61] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. NIPS , 2023. [62] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement"}
{"id": "2507.23779v1_chunk125", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "n. Direct preference optimization: Your language model is secretly a reward model. NIPS , 2023. [62] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning , 8(3), 1992. [63] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017. [64] Jian Hu. Reinforce++: A simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262 , 2025. [65] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024. [66] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement l"}
{"id": "2507.23779v1_chunk126", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620 , 2025. [67] Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810 , 2025. [68] Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: A generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458 , 2025. [69] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https: //github.com/huggingface/trl , 2020. [70] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human fee"}
{"id": "2507.23779v1_chunk127", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " learning. https: //github.com/huggingface/trl , 2020. [70] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425 , 2023. [71] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics . PMLR, 2024. [72] Haozhe Ji, Cheng Lu, Yilin Niu, Pei Ke, Hongning Wang, Jun Zhu, Jie Tang, and Minlie Huang. Towards efficient exact optimization of language model alignment. In ICML , 2024. [73] Huayu Chen, Guande He, Lifan Yuan, Ganqu Cui, Hang Su, and Jun Zhu. Noise contrastive alignment of language models with explicit rewards. Advances in Neural Information Processing Systems , 37:117784–117812, 2024. [74] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. P"}
{"id": "2507.23779v1_chunk128", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "rastive alignment of language models with explicit rewards. Advances in Neural Information Processing Systems , 37:117784–117812, 2024. [74] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust dpo: Aligning language models with noisy feedback. In International Conference on Machine Learning , pages 42258–42274. PMLR, 2024. [75] Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. arXiv preprint arXiv:2405.00675 , 2024. [76] Igor Melnyk, Youssef Mroueh, Brian Belgodere, Mattia Rigotti, Apoorva Nitsure, Mikhail Yurochkin, Kristjan Greenewald, Jiri Navratil, and Jarret Ross. Distributional preference alignment of llms via optimal transport. NIPS , 2024. [77] Chris Lu, Samuel Holt, Claudio Fanconi, Alex Chan, Jakob Foerster, Mihaela van der Schaar, and Robert Lange. Discovering preference optimization algorithms with and for large language models. NIPS , 2024. [78] Karel D’Oosterli"}
{"id": "2507.23779v1_chunk129", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "udio Fanconi, Alex Chan, Jakob Foerster, Mihaela van der Schaar, and Robert Lange. Discovering preference optimization algorithms with and for large language models. NIPS , 2024. [78] Karel D’Oosterlinck, Winnie Xu, Chris Develder, Thomas Demeester, Amanpreet Singh, Christopher Potts, Douwe Kiela, and Shikib Mehri. Anchored preference optimization and contrastive revisions: Addressing underspecification in alignment. ACL, 2025. [79] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743 , 2025. [80] Biqing Qi, Pengfei Li, Fangyuan Li, Junqi Gao, Kaiyan Zhang, and Bowen Zhou. Online dpo: Online direct preference optimization with fast-slow chasing. arXiv preprint arXiv:2406.05534 , 2024. [81] Yi Ren and Danica J Sutherland. Learning dynamics of llm fine"}
{"id": "2507.23779v1_chunk130", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ng, and Bowen Zhou. Online dpo: Online direct preference optimization with fast-slow chasing. arXiv preprint arXiv:2406.05534 , 2024. [81] Yi Ren and Danica J Sutherland. Learning dynamics of llm finetuning. ICLR , 2024. [82] Tiange Luo, Lajanugen Logeswaran, Justin Johnson, and Honglak Lee. Visual test-time scaling for gui agent grounding. arXiv preprint arXiv:2505.00684 , 2025. [83] Jungjae Lee, Dongjae Lee, Chihun Choi, Youngmin Im, Jaeyoung Wi, Kihong Heo, Sangeun Oh, Sunjae Lee, and Insik Shin. Safeguarding mobile gui agent via logic-based action verification. arXiv preprint arXiv:2503.18492 , 2025. 23Phi-Ground Tech Report A Experiment Settings Due to resource constraints and the evolution of the development process, different ablation experiments may have utilized varying hyper-parameters and data configurations. We have documented the detailed setup for each ablation experiment in the table below. Table 11: Detailed training data configuration and hyper-parameters. EXP Data con"}
{"id": "2507.23779v1_chunk131", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "meters and data configurations. We have documented the detailed setup for each ablation experiment in the table below. Table 11: Detailed training data configuration and hyper-parameters. EXP Data configure Hyper parameters Tab. 2, Tab. 3Training samples: 2M Dataset: [(SeeClick, 0.075 ep), (Fineweb, 0.062 ep), (Windows, 0.088 ep), (MaxOS, 0.77 ep), (Linux, 0.45 ep), (GUIAct, 0.44 ep)]lr=8e-5; batch size=2048; weight decay=0.01; random-resize=0.5 max-grad-norm=0.1; warmup-steps=50 Tab. 4, Tab. 17Training samples: 5M Dataset: [(SeeClick, 0.19 ep), (Fineweb, 0.16 ep), (Windows, 0.22 ep), (MaxOS, 0.77 ep), (Linux, 0.65 ep), (GUIAct, 0.63 ep)] (BingSearch, 0.95 ep)lr=8e-5; batch size=2048; weight decay=0.01; max-grad-norm=0.1; warmup-steps=100 Tab. 5Training samples: 5M Base: [(SeeClick, 0.19 ep), (Fineweb, 0.16 ep), (Windows, 0.23 ep), (MaxOS, 0.77 ep), (Linux, 0.65 ep), (GUIAct, 0.63 ep)]lr=8e-5; batch size=2048; weight decay=0.01; max-grad-norm=0.1; warmup-steps=100 Tab. 6Training sample"}
{"id": "2507.23779v1_chunk132", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "p), (Fineweb, 0.16 ep), (Windows, 0.23 ep), (MaxOS, 0.77 ep), (Linux, 0.65 ep), (GUIAct, 0.63 ep)]lr=8e-5; batch size=2048; weight decay=0.01; max-grad-norm=0.1; warmup-steps=100 Tab. 6Training samples: 5M A-pretrain: [(SeeClick, 0.13 ep), (Fineweb, 0.11 ep), (Windows, 0.15 ep), (MaxOS, 0.51 ep), (Linux, 0.43 ep), (GUIAct, 0.52 ep), (BingSearch, 0.95 ep), (CC, 0.06 ep)]lr=8e-5; batch size=2048; weight decay=0.01; max-grad-norm=0.1; warmup-steps=100 Phi-Ground pre-train In Table 8 and 9weight decay=0.01; max-grad-norm=0.1; warmup-ratio=6%; Others in Table 9 Phi-Ground DPOTraining samples: 400K for each round DPO: [(Human, 0.5ep), (Windows, 0.03 ep), (MaxOS, 0.14 ep), (Linux, 0.2 ep), (GUIAct, 0.2 ep), (BingSearch, 0.33 ep), (CC, 0.004 ep)]lr=1e-5; batch size=256; weight decay=0.01; max-grad-norm=0.1; warmup-steps=100 24Phi-Ground Tech Report B Detailed Evaluation Results Table 12: Detailed ScreenSpot-V2 results. ModelDesktop Web Mobile A VG.Text Icon/Widget Text Icon/Widget Text Icon/Wi"}
{"id": "2507.23779v1_chunk133", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "-norm=0.1; warmup-steps=100 24Phi-Ground Tech Report B Detailed Evaluation Results Table 12: Detailed ScreenSpot-V2 results. ModelDesktop Web Mobile A VG.Text Icon/Widget Text Icon/Widget Text Icon/Widget End-to-end model setting (Use short REs) SeeClick 78.4 50.7 70.1 29.3 55.2 32.5 55.1 UGround-7B*85.1 61.2 84.6 71.9 84.3 61.1 76.1 UGround-v1-7B*83.6 90.5 85.8 86.3 95.5 83.2 87.7 OS-Atlas-4B 87.2 59.7 72.7 46.4 85.9 63.1 71.9 OS-Atlas-7B 95.2 75.8 90.7 63.6 90.6 77.3 84.1 UI-TARS-2B 95.2 79.1 90.7 68.6 87.2 78.3 84.7 UI-TARS-7B 96.9 89.1 95.4 85.0 93.6 85.2 91.6 UI-TARS-1.5-7B*92.2 81.5 91.0 84.2 95.5 84.5 89.0 Phi-Ground-4B-7C 87.6 75.7 93.6 71.4 94.1 54.3 80.8 Phi-Ground-4B-16C 90.2 77.1 92.3 76.8 93.7 63.5 83.4 Phi-Ground-4B-16C-DPO 91.7 77.1 94.4 78.3 94.1 62.0 84.1 Phi-Ground-4B-29C 90.7 80.0 92.3 76.8 92.0 63.5 83.4 Phi-Ground-7B-7C 88.1 70.0 93.2 71.4 94.4 60.6 81.3 Phi-Ground-7B-16C 83.9 72.1 94.0 73.4 94.4 63.5 81.8 Phi-Ground-7B-16C-DPO 90.2 76.4 93.6 75.9 96.5 62.0 83.8 Ph"}
{"id": "2507.23779v1_chunk134", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " 90.7 80.0 92.3 76.8 92.0 63.5 83.4 Phi-Ground-7B-7C 88.1 70.0 93.2 71.4 94.4 60.6 81.3 Phi-Ground-7B-16C 83.9 72.1 94.0 73.4 94.4 63.5 81.8 Phi-Ground-7B-16C-DPO 90.2 76.4 93.6 75.9 96.5 62.0 83.8 Phi-Ground-7B-29C 97.2 77.9 91.9 71.9 95.1 63.5 82.5 Agent setting (Use long REs) with GPT-4O as planner SeeClick*55.7 28.8 43.6 24.1 67.9 55.3 48.1 UGround-7B*90.2 68.3 91.9 72.9 95.8 76.4 84.3 UGround-v1-7B*83.1 92.9 85.8 84.3 97.6 85.6 88.5 OS-Atlas-4B*49.5 30.1 67.0 48.7 78.6 58.7 58.7 OS-Atlas-7B*85.0 66.9 84.3 68.6 94.4 75.5 80.9 UI-TARS-2B*91.2 77.7 90.6 77.3 95.8 81.7 86.9 UI-TARS-7B*93.3 83.5 91.5 78.3 96.2 85.0 88.8 UI-TARS-1.5-7B*92.8 80.6 91.5 75.4 96.5 84.1 87.8 Phi-Ground-4B-7C 92.7 76.2 94.6 75.4 95.8 76.0 85.5 Phi-Ground-4B-16C 90.2 80.0 92.7 72.9 96.5 76.9 86.0 Phi-Ground-4B-16C-DPO 91.2 80.0 92.7 73.4 97.2 76.9 86.4 Phi-Ground-4B-29C 90.2 79.3 92.7 74.5 96.2 76.4 86.1 Phi-Ground-7B-7C 90.2 80.7 92.3 74.4 97.6 78.8 86.8 Phi-Ground-7B-16C 89.6 81.4 93.6 73.4 96.5 79.3 86.7 Ph"}
{"id": "2507.23779v1_chunk135", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "-DPO 91.2 80.0 92.7 73.4 97.2 76.9 86.4 Phi-Ground-4B-29C 90.2 79.3 92.7 74.5 96.2 76.4 86.1 Phi-Ground-7B-7C 90.2 80.7 92.3 74.4 97.6 78.8 86.8 Phi-Ground-7B-16C 89.6 81.4 93.6 73.4 96.5 79.3 86.7 Phi-Ground-7B-16C-DPO 92.2 80.0 93.2 73.9 92.2 80.0 87.2 Phi-Ground-7B-29C 91.2 80.7 93.2 75.4 97.2 77.9 87.0 Agent setting (Use long REs) with O4-mini as planner SeeClick*60.6 26.3 76.8 51.9 57.1 29.5 53.5 UGround-7B*92.6 76.6 97.5 82.5 92.9 84.0 89.0 UGround-v1-7B*92.6 86.9 97.9 86.4 93.8 91.0 92.1 OS-Atlas-4B*50.5 33.3 73.9 55.3 63.0 52.4 57.4 OS-Atlas-7B*86.0 75.2 94.7 77.7 86.0 81.8 84.7 UI-TARS-2B*93.6 77.4 97.9 86.4 93.4 86.5 90.4 UI-TARS-7B*93.1 89.8 98.2 87.9 94.7 91.0 93.0 UI-TARS-1.5-7B*93.6 86.1 97.5 87.9 92.9 91.0 92.2 Phi-Ground-4B-7C 93.6 84.7 94.7 87.5 96.8 84.0 91.0 Phi-Ground-4B-16C 93.6 88.3 93.4 88.5 97.9 85.9 91.9 Phi-Ground-4B-16C-DPO 94.6 90.1 97.5 87.9 98.7 86.0 92.3 Phi-Ground-4B-29C 93.1 91.2 95.6 87.5 97.5 85.0 92.1 Phi-Ground-7B-7C 93.6 89.1 95.1 87.0 98.3 85.9 92"}
{"id": "2507.23779v1_chunk136", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "B-16C 93.6 88.3 93.4 88.5 97.9 85.9 91.9 Phi-Ground-4B-16C-DPO 94.6 90.1 97.5 87.9 98.7 86.0 92.3 Phi-Ground-4B-29C 93.1 91.2 95.6 87.5 97.5 85.0 92.1 Phi-Ground-7B-7C 93.6 89.1 95.1 87.0 98.3 85.9 92.1 Phi-Ground-7B-16C 93.6 90.5 94.7 88.0 96.8 85.0 91.9 Phi-Ground-7B-16C-DPO 95.1 90.1 96.6 88.5 99.6 86.4 93.4 Phi-Ground-7B-29C 94.7 86.1 94.2 88.5 97.9 84.5 91.7 25Phi-Ground Tech Report Table 13: ScreenSpot-Pro results. ModelDevelopment Creative CAD Scientific Office OS A VG. Text Icon A VG. Text Icon A VG. Text Icon A VG. Text Icon A VG. Text Icon A VG. Text Icon A VG. Text Icon A VG. End-to-end model setting (Use short REs) SeeClick [26] 0.6 0.0 0.3 1.0 0.0 0.6 2.5 0.0 1.9 3.5 0.0 2.0 1.1 0.0 0.9 2.8 0.0 1.5 1.8 0.0 1.1 OS-Atlas-4B [31] 7.1 0.0 3.7 3.0 1.4 2.3 2.0 0.0 1.5 9.0 5.5 7.5 5.1 3.8 4.8 5.6 0.0 3.1 5.0 1.7 3.7 Show-UI-2B 16.9 1.4 9.4 9.1 0.0 5.3 2.5 0.0 1.9 13.2 7.3 10.6 15.3 7.5 13.5 10.3 2.2 6.6 10.8 2.6 7.7 CogAgent-18B 14.9 0.7 8.0 9.6 0.0 5.6 7.1 3.1 6.1 22.2 1.8 13.4 "}
{"id": "2507.23779v1_chunk137", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "1 3.8 4.8 5.6 0.0 3.1 5.0 1.7 3.7 Show-UI-2B 16.9 1.4 9.4 9.1 0.0 5.3 2.5 0.0 1.9 13.2 7.3 10.6 15.3 7.5 13.5 10.3 2.2 6.6 10.8 2.6 7.7 CogAgent-18B 14.9 0.7 8.0 9.6 0.0 5.6 7.1 3.1 6.1 22.2 1.8 13.4 13.0 0.0 10.0 5.6 0.0 3.1 12.0 0.8 7.7 Aria-UI 16.2 0.0 8.4 23.7 2.1 14.7 7.6 1.6 6.1 27.1 6.4 18.1 20.3 1.9 16.1 4.7 0.0 2.6 17.1 2.0 11.3 UGround-7B 26.6 2.1 14.7 27.3 2.8 17.0 14.2 1.6 11.1 31.9 2.7 19.3 31.6 11.3 27.0 17.8 0.0 9.7 25.0 2.8 16.5 Claude Computer Use [13] 22.0 3.9 12.6 25.9 3.4 16.8 14.5 3.7 11.9 33.9 15.8 25.8 30.1 16.3 26.9 11.0 4.5 8.1 23.4 7.1 17.1 OS-Atlas-7B 33.1 1.4 17.7 28.8 2.8 17.9 12.2 4.7 10.3 37.5 7.3 24.4 33.9 5.7 27.4 27.1 4.5 16.8 28.1 4.0 18.9 UI-TARS-2B 47.4 4.1 26.4 42.9 6.3 27.6 17.8 4.7 14.6 56.9 17.3 39.8 50.3 17.0 42.6 21.5 5.6 14.3 39.6 8.4 27.7 UGround-v1-7B - - 35.5 - - 27.8 - - 13.5 - - 38.8 - - 48.8 - - 26.1 - - 31.1 UI-TARS-7B 58.4 12.4 36.1 50.0 9.1 32.8 20.8 9.4 18.0 63.9 31.8 50.0 63.3 20.8 53.5 30.8 16.9 24.5 47.8 16.2 35.7 UI-TARS-1.5-7B*"}
{"id": "2507.23779v1_chunk138", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "d-v1-7B - - 35.5 - - 27.8 - - 13.5 - - 38.8 - - 48.8 - - 26.1 - - 31.1 UI-TARS-7B 58.4 12.4 36.1 50.0 9.1 32.8 20.8 9.4 18.0 63.9 31.8 50.0 63.3 20.8 53.5 30.8 16.9 24.5 47.8 16.2 35.7 UI-TARS-1.5-7B*58.4 14.1 37.2 57.6 14.7 39.6 39.1 15.6 33.3 68.3 23.4 49.0 73.6 34.7 65.0 49.5 17.2 35.1 57.5 18.2 42.6 Phi-Ground-4B-7C 32.5 5.6 19.5 26.8 7.0 18.5 10.2 4.7 8.8 39.9 17.3 30.0 46.6 34.0 43.8 27.1 16.9 22.4 29.8 12.0 23.0 Phi-Ground-4B-16C 59.1 18.8 39.6 48.0 9.1 31.7 15.7 9.4 14.2 53.8 25.5 41.5 69.0 42.0 62.9 45.8 21.3 34.7 47.5 19.0 36.7 Phi-Ground-4B-16C-DPO 64.3 15.3 40.6 46.5 12.6 32.3 20.3 12.5 18.4 54.5 26.4 42.3 69.0 42.0 62.9 44.9 24.7 35.7 49.0 20.0 38.0 Phi-Ground-4B-29C 64.9 15.3 40.9 48.5 7.7 31.4 21.8 9.4 18.8 58.7 23.6 43.5 71.8 36.0 63.8 47.7 18.0 34.2 51.3 16.5 38.0 Phi-Ground-7B-7C 57.8 12.5 35.9 39.9 9.8 27.3 17.8 6.3 14.9 53.2 24.6 40.7 63.2 22.0 54.0 42.1 19.1 31.6 44.6 15.2 33.4 Phi-Ground-7B-16C 68.2 16.7 43.3 53.5 11.2 35.8 26.4 17.2 24.1 58.7 25.5 44.3 76.4 42.0 "}
{"id": "2507.23779v1_chunk139", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "nd-7B-7C 57.8 12.5 35.9 39.9 9.8 27.3 17.8 6.3 14.9 53.2 24.6 40.7 63.2 22.0 54.0 42.1 19.1 31.6 44.6 15.2 33.4 Phi-Ground-7B-16C 68.2 16.7 43.3 53.5 11.2 35.8 26.4 17.2 24.1 58.7 25.5 44.3 76.4 42.0 68.8 53.3 22.5 39.3 55.2 20.0 41.8 Phi-Ground-7B-16C-DPO 70.8 16.7 44.6 56.6 13.3 38.4 26.9 17.2 24.5 58.0 29.1 45.5 76.4 44.0 69.2 55.1 25.8 41.8 56.4 21.8 43.2 Phi-Ground-7B-29C 66.9 20.1 44.3 57.1 9.8 37.2 26.9 10.9 23.0 60.5 25.5 47.8 79.3 40.0 70.5 57.9 38.0 19.8 57.8 19.0 43.0 Agent setting (Use long REs) with GPT-4O as planner SeeClick*1.3 1.4 1.3 1.0 0.0 0.6 1.0 0.0 0.8 2.8 0.0 1.6 0.6 0.0 0.4 0.0 0.0 0.0 1.1 0.3 0.8 UGround-7B*33.1 2.1 18.1 33.3 4.9 21.4 14.7 1.6 11.5 40.6 5.5 25.3 37.9 8.0 31.2 24.3 2.2 14.3 30.4 3.8 20.3 UGround-v1-7B*46.8 2.8 25.5 37.4 4.9 23.8 16.8 3.1 13.4 53.8 15.5 37.2 53.4 18.0 45.5 34.6 5.6 21.4 39.7 7.3 27.3 OS-Atlas-4B*2.6 0.0 1.3 2.5 0.0 1.5 1.5 0.0 1.1 3.5 0.0 2.0 2.3 2.0 2.2 2.8 0.0 1.5 2.5 0.2 1.6 OS-Atlas-7B*34.5 0.0 17.5 29.8 3.5 18.8 7.2 3.3 6.3 "}
{"id": "2507.23779v1_chunk140", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "5.5 37.2 53.4 18.0 45.5 34.6 5.6 21.4 39.7 7.3 27.3 OS-Atlas-4B*2.6 0.0 1.3 2.5 0.0 1.5 1.5 0.0 1.1 3.5 0.0 2.0 2.3 2.0 2.2 2.8 0.0 1.5 2.5 0.2 1.6 OS-Atlas-7B*34.5 0.0 17.5 29.8 3.5 18.8 7.2 3.3 6.3 36.6 7.4 24.0 35.3 10.4 29.8 19.6 6.9 13.9 26.7 4.4 18.2 UI-TARS-2B*48.1 6.2 27.9 42.4 9.1 28.4 13.2 3.1 10.7 65.0 14.5 43.1 53.4 18.0 45.5 29.0 11.2 20.9 41.2 9.8 29.2 UI-TARS-7B*56.5 10.4 34.2 49.0 14.0 34.3 18.3 4.7 14.9 69.2 24.5 49.8 61.5 20.0 52.2 33.6 13.5 24.5 47.5 14.5 34.9 UI-TARS-1.5-7B*57.1 13.2 35.9 49.5 14.7 34.9 28.4 12.5 24.5 67.8 23.6 48.6 67.8 42.0 62.1 40.2 18.0 30.1 51.4 18.5 38.8 Phi-Ground-4B-7C 42.9 6.3 25.2 33.8 13.3 25.2 15.2 1.6 11.9 55.9 16.4 38.7 48.3 36.0 15.5 33.6 12.4 24.0 37.3 12.7 27.9 Phi-Ground-4B-16C 49.4 16.0 33.2 49.0 16.8 35.5 17.8 15.6 17.2 59.4 20.9 42.7 62.1 50.0 59.4 48.6 21.3 36.2 46.6 20.7 36.7 Phi-Ground-4B-16C-DPO 48.1 16.0 32.6 52.0 18.9 38.1 18.8 14.1 17.6 61.5 25.5 45.8 63.2 44.0 58.9 49.5 24.7 38.3 47.8 21.8 37.9 Phi-Ground-4B-29C 54.5 17."}
{"id": "2507.23779v1_chunk141", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "7 62.1 50.0 59.4 48.6 21.3 36.2 46.6 20.7 36.7 Phi-Ground-4B-16C-DPO 48.1 16.0 32.6 52.0 18.9 38.1 18.8 14.1 17.6 61.5 25.5 45.8 63.2 44.0 58.9 49.5 24.7 38.3 47.8 21.8 37.9 Phi-Ground-4B-29C 54.5 17.4 36.6 50.0 14.0 34.9 20.8 7.8 17.6 67.8 20.9 47.4 67.8 34.0 60.3 44.9 15.7 31.6 50.1 17.3 37.8 Phi-Ground-7B-7C 74.2 20.3 48.0 53.3 19.0 38.9 25.0 14.1 22.3 68.3 26.7 50.6 75.2 33.3 66.0 58.8 20.5 41.1 57.3 21.5 43.6 Phi-Ground-7B-16C 53.2 16.0 35.2 52.0 18.2 37.8 21.8 12.5 19.5 68.5 22.7 48.6 63.2 42.0 58.4 42.1 14.6 29.6 49.4 17.8 38.0 Phi-Ground-7B-16C-DPO 55.8 16.7 36.9 54.0 20.3 39.9 25.4 17.2 23.4 65.7 25.5 48.2 65.5 46.0 61.2 43.0 18.0 31.6 51.1 21.8 39.9 Phi-Ground-7B-29C 55.8 18.8 37.9 56.6 15.4 39.3 24.4 14.1 21.8 68.5 25.5 49.8 61.5 32.0 54.9 48.6 16.9 34.2 51.7 19.5 39.4 Agent setting (Use long REs) with O4-mini as planner SeeClick*3.3 0.0 1.7 0.5 0.0 0.3 1.5 0.0 1.2 2.1 1.0 1.6 3.7 0.0 2.9 0.0 0.0 0.0 1.9 0.2 1.2 UGround-7B*41.1 4.9 23.5 34.4 5.6 22.3 14.8 3.1 11.9 40.8 9.5 2"}
{"id": "2507.23779v1_chunk142", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ting (Use long REs) with O4-mini as planner SeeClick*3.3 0.0 1.7 0.5 0.0 0.3 1.5 0.0 1.2 2.1 1.0 1.6 3.7 0.0 2.9 0.0 0.0 0.0 1.9 0.2 1.2 UGround-7B*41.1 4.9 23.5 34.4 5.6 22.3 14.8 3.1 11.9 40.8 9.5 27.5 47.2 11.1 39.3 35.3 3.4 20.5 34.6 6.0 23.7 UGround-v1-7B*62.9 4.9 34.7 42.1 7.0 27.3 21.4 6.2 17.7 51.4 19.0 37.7 60.9 20.0 51.9 47.1 11.4 30.5 46.3 10.2 32.5 OS-Atlas-4B*3.3 0.7 2.0 2.1 0.0 1.2 2.1 0.0 1.5 4.2 0.0 2.4 4.4 2.2 3.9 2.9 0.0 1.6 3.1 0.3 2.0 OS-Atlas-7B*41.4 1.4 21.6 30.4 5.6 19.9 10.8 8.1 10.1 38.0 13.5 27.6 37.3 13.3 32.0 25.7 6.8 16.9 29.8 7.0 21.1 UI-TARS-2B*64.2 6.3 36.1 51.8 10.6 34.4 18.9 9.4 16.5 63.4 23.8 46.6 65.8 20.0 55.8 43.1 12.5 28.9 50.2 12.8 35.9 UI-TARS-7B*66.2 11.9 39.8 55.9 14.1 38.3 25.0 10.9 21.5 72.5 28.6 53.8 71.4 22.2 60.7 51.0 12.5 33.2 55.8 16.2 40.6 UI-TARS-1.5-7B*67.5 23.8 46.3 59.5 23.9 44.5 43.4 20.3 37.7 70.4 29.5 53.0 83.2 42.2 74.3 58.8 22.7 42.1 63.0 25.7 48.8 Phi-Ground-4B-7C 69.5 14.7 42.9 44.6 20.4 34.4 23.0 4.7 18.5 69.7 23.8 50.2 72."}
{"id": "2507.23779v1_chunk143", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " UI-TARS-1.5-7B*67.5 23.8 46.3 59.5 23.9 44.5 43.4 20.3 37.7 70.4 29.5 53.0 83.2 42.2 74.3 58.8 22.7 42.1 63.0 25.7 48.8 Phi-Ground-4B-7C 69.5 14.7 42.9 44.6 20.4 34.4 23.0 4.7 18.5 69.7 23.8 50.2 72.0 51.1 67.5 52.0 22.7 38.4 53.3 20.6 40.8 Phi-Ground-4B-16C 74.2 29.4 52.4 59.5 31.0 47.5 29.6 18.8 26.9 69.0 27.6 51.4 82.6 48.9 75.2 67.6 33.0 51.6 61.9 30.3 49.8 Phi-Ground-4B-16C-DPO 76.2 33.6 55.4 62.1 31.9 49.4 32.1 15.6 28.1 69.0 31.4 53.0 82.6 53.3 76.2 66.7 36.4 52.6 63.1 32.8 51.5 Phi-Ground-4B-29C 76.2 22.4 50.0 63.1 27.5 48.1 29.6 14.1 25.8 76.8 28.6 56.3 83.2 40.0 73.8 63.7 20.5 43.7 63.8 24.9 48.9 Phi-Ground-7B-7C 52.0 14.6 33.9 41.4 11.9 29.0 18.8 7.8 16.1 62.2 20.0 43.9 58.1 28.0 51.3 35.5 14.6 26.0 43.9 15.3 33.0 Phi-Ground-7B-16C 76.8 32.2 55.1 66.2 35.9 53.4 34.7 17.2 30.4 73.9 35.2 57.5 83.2 51.1 76.2 69.6 31.8 52.1 65.8 33.4 53.4 Phi-Ground-7B-16C-DPO 80.8 32.9 57.5 66.7 37.3 54.3 35.7 18.8 31.5 77.5 35.2 59.5 83.9 57.8 78.2 71.6 31.8 53.2 67.6 34.6 55.0 Phi-Ground-7B-"}
{"id": "2507.23779v1_chunk144", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "3.9 35.2 57.5 83.2 51.1 76.2 69.6 31.8 52.1 65.8 33.4 53.4 Phi-Ground-7B-16C-DPO 80.8 32.9 57.5 66.7 37.3 54.3 35.7 18.8 31.5 77.5 35.2 59.5 83.9 57.8 78.2 71.6 31.8 53.2 67.6 34.6 55.0 Phi-Ground-7B-29C 78.1 29.4 54.4 67.2 31.0 51.9 40.3 18.8 35.0 77.5 35.2 59.5 83.2 51.1 76.2 72.5 30.7 53.2 68.2 31.5 54.2 26Phi-Ground Tech Report Basic Functional Spatial Model Ed Br De Pr Cr En Overall Ed Br De Pr Cr En Overall Ed Br De Pr Cr En Overall Final Avg (215) (56) (376) (605) (438) (82) (1772) (215) (56) (376) (605) (438) (82) (1772) (212) (31) (338) (740) (586) (28) (1935) Closed-Source VLMs GPT-4o 2.23 0.00 1.86 1.16 1.14 4.88 1.58 1.40 0.00 3.19 0.83 0.91 3.66 1.52 0.94 0.00 1.48 1.22 0.51 3.57 1.03 1.38 Gemini-1.5-pro 0.47 0.00 1.60 0.83 0.46 0.00 0.79 0.00 1.79 0.27 0.17 0.46 0.00 0.28 0.94 0.00 0.89 0.54 0.34 0.00 0.57 0.55 Gemini-Flash-2.0 0.00 0.00 0.27 0.66 0.68 0.00 0.45 0.47 1.79 0.00 0.66 0.23 0.00 0.40 0.00 0.00 0.00 0.14 0.00 0.00 0.05 0.30 Claude-3.5-Sonnet 3.26 16.1 5.32 6.9"}
{"id": "2507.23779v1_chunk145", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "4 0.00 0.89 0.54 0.34 0.00 0.57 0.55 Gemini-Flash-2.0 0.00 0.00 0.27 0.66 0.68 0.00 0.45 0.47 1.79 0.00 0.66 0.23 0.00 0.40 0.00 0.00 0.00 0.14 0.00 0.00 0.05 0.30 Claude-3.5-Sonnet 3.26 16.1 5.32 6.94 1.83 4.88 5.08 5.12 19.6 4.79 5.95 2.51 4.88 5.19 2.83 9.68 5.03 2.43 2.56 7.14 3.15 4.47 Claude-3.7-Sonnet 6.51 12.5 7.98 11.24 9.13 11.0 9.48 5.12 7.14 8.24 9.92 6.16 4.88 7.73 6.60 9.68 7.69 7.43 7.85 10.7 7.60 8.27 Open-Source VLMs Qwen-2.5VL-7B 0.47 0.00 1.33 1.65 0.68 1.22 1.24 0.47 0.00 0.80 1.16 0.46 1.22 0.79 0.47 0.00 1.48 0.00 0.51 0.00 0.51 0.85 InternVL2-8B 0.00 0.00 0.00 0.02 0.00 0.14 0.11 0.00 0.00 0.27 0.00 0.00 1.22 0.11 0.00 0.00 0.00 0.14 0.00 0.00 0.05 0.09 InternVL2.5-8B 0.93 8.93 3.46 2.31 1.37 4.88 2.48 1.40 7.14 3.72 2.81 1.60 6.10 2.82 0.94 3.23 1.78 0.68 0.68 3.57 0.98 2.09 Qwen-2VL-7B 2.79 7.14 3.72 3.97 0.68 12.2 3.44 2.79 12.5 3.19 3.97 0.68 6.10 3.22 0.47 3.23 2.37 1.08 0.51 3.57 1.45 2.70 MiniCPM-V-8B 4.19 21.4 7.71 7.44 3.65 18.3 7.11 4.19 19.6 6.38 4.63 "}
{"id": "2507.23779v1_chunk146", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "0.98 2.09 Qwen-2VL-7B 2.79 7.14 3.72 3.97 0.68 12.2 3.44 2.79 12.5 3.19 3.97 0.68 6.10 3.22 0.47 3.23 2.37 1.08 0.51 3.57 1.45 2.70 MiniCPM-V-8B 4.19 21.4 7.71 7.44 3.65 18.3 7.11 4.19 19.6 6.38 4.63 2.97 11.0 5.30 0.47 3.23 1.78 0.27 0.17 3.57 1.45 4.34 Open-Source GUI Agents ( ≤10B) ShowUI-2B 5.12 16.1 9.84 9.09 3.42 19.5 8.07 5.12 12.5 9.31 8.60 4.11 15.9 7.67 0.94 9.68 2.96 2.70 0.68 3.57 2.07 5.94 AriaUI-25.3B 10.7 23.2 13.0 12.9 8.22 20.7 12.2 12.6 19.6 15.4 14.6 10.5 22.0 14.0 3.77 9.68 4.44 4.86 2.22 7.14 3.98 10.1 UGround-v1-7B 11.6 35.7 19.7 15.0 11.0 18.3 15.4 15.4 33.9 22.3 16.5 11.6 19.5 17.1 4.25 6.45 9.76 6.35 4.44 14.3 6.25 12.9 OSAtlas-7B 10.7 23.2 13.3 12.6 8.22 22.0 12.2 11.6 16.1 11.4 12.7 7.53 13.4 11.2 3.77 6.45 5.62 3.65 2.22 7.14 3.67 9.02 UGround-7B 10.2 23.2 14.9 10.6 7.53 19.5 11.5 12.1 25.0 15.2 11.2 7.99 20.7 12.2 2.36 0.00 4.14 2.70 2.22 7.14 2.79 8.83 Aguvis-7B 16.7 37.5 22.3 16.2 12.6 26.8 17.8 17.2 35.7 21.5 18.0 13.0 24.4 18.3 5.19 9.68 6.51 4.05 4.78 "}
{"id": "2507.23779v1_chunk147", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "10.6 7.53 19.5 11.5 12.1 25.0 15.2 11.2 7.99 20.7 12.2 2.36 0.00 4.14 2.70 2.22 7.14 2.79 8.83 Aguvis-7B 16.7 37.5 22.3 16.2 12.6 26.8 17.8 17.2 35.7 21.5 18.0 13.0 24.4 18.3 5.19 9.68 6.51 4.05 4.78 14.3 5.06 13.7 UI-TARS-7B 15.4 41.1 21.8 21.2 13.2 39.0 20.1 20.5 41.1 25.5 26.5 16.0 45.1 24.3 6.60 12.9 11.0 9.19 5.80 17.9 8.37 17.6 UI-TARS-1.5-7B*22.1 59.6 29.5 31.8 19.8 50.0 28.8 24.3 56.9 26.2 31.2 19.1 40.7 27.5 8.96 20.0 10.9 11.6 8.70 29.6 10.7 22.3 CogAgent-9B 11.2 14.3 12.5 13.7 8.68 15.9 12.0 11.6 14.3 11.4 14.7 8.22 18.3 12.2 3.30 0.00 1.18 4.05 1.37 7.14 2.63 8.94 SeeClick-9.6B 7.44 23.2 13.0 8.43 5.48 17.1 9.42 4.65 7.14 5.32 3.97 4.34 7.32 4.68 0.47 6.45 3.25 1.22 2.73 3.57 2.07 5.39 Our Phi-Ground Models Phi-Ground-4B-7C 26.5 42.9 27.2 31.6 18.7 38.3 27.5 27.4 46.4 26.9 34.9 21.0 35.8 29.2 6.13 6.45 5.92 4.60 2.90 18.5 4.71 20.5 Phi-Ground-4B-16C 27.0 50.0 30.9 37.2 24.4 40.7 32.0 28.4 48.2 32.2 39.3 21.2 43.2 32.5 5.66 3.23 7.10 5.82 3.58 3.70 5.28 23.3 Phi-Ground-4B-16"}
{"id": "2507.23779v1_chunk148", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " 21.0 35.8 29.2 6.13 6.45 5.92 4.60 2.90 18.5 4.71 20.5 Phi-Ground-4B-16C 27.0 50.0 30.9 37.2 24.4 40.7 32.0 28.4 48.2 32.2 39.3 21.2 43.2 32.5 5.66 3.23 7.10 5.82 3.58 3.70 5.28 23.3 Phi-Ground-4B-16C-DPO 29.8 48.2 30.7 39.7 24.9 45.7 33.4 32.1 51.8 32.4 41.2 23.1 45.7 34.3 6.13 0.00 9.17 5.68 4.27 3.70 5.79 24.5 Phi-Ground-4B-29C 29.3 53.6 32.3 34.5 22.4 48.1 31.6 27.0 46.4 31.6 38.5 20.1 37.0 31.3 7.08 3.23 4.14 3.65 2.56 7.41 3.83 22.2 Phi-Ground-7B-7C 28.4 53.6 32.8 34.7 23.7 45.7 31.9 25.6 53.6 33.0 37.2 21.0 39.5 31.5 6.11 9.74 5.31 5.80 5.60 14.8 5.90 23.1 Phi-Ground-7B-16C 35.3 53.6 38.9 38.8 24.0 50.5 35.8 32.6 62.5 36.7 38.7 23.7 43.2 34.8 6.13 3.23 7.40 6.08 6.32 3.70 6.31 25.6 Phi-Ground-7B-16C-DPO 35.3 57.1 37.1 40.3 26.0 56.8 36.8 34.4 62.5 39.1 40.8 25.8 50.6 37.1 7.51 6.50 8.33 6.80 8.41 3.70 7.60 27.2 Phi-Ground-7B-29C 33.5 46.4 36.3 34.0 23.3 51.9 33.0 28.8 51.8 36.2 34.7 25.8 43.2 33.0 8.02 6.45 7.69 4.86 4.78 0.00 5.63 23.9 GPT-4O as planner UGround-v1-7B*21.9 44.6"}
{"id": "2507.23779v1_chunk149", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " 8.33 6.80 8.41 3.70 7.60 27.2 Phi-Ground-7B-29C 33.5 46.4 36.3 34.0 23.3 51.9 33.0 28.8 51.8 36.2 34.7 25.8 43.2 33.0 8.02 6.45 7.69 4.86 4.78 0.00 5.63 23.9 GPT-4O as planner UGround-v1-7B*21.9 44.6 27.2 21.8 17.6 25.9 22.8 26.0 33.9 28.5 22.0 16.9 27.2 23.2 7.08 9.68 13.0 7.31 6.48 14.8 8.17 18.1 OSAtlas-7B*11.7 30.4 19.1 14.1 8.03 22.5 14.3 13.1 30.4 18.1 13.5 9.40 21.0 14.3 4.74 9.68 8.04 3.52 3.95 3.70 4.68 11.1 UGround-7B*17.2 25.0 22.4 17.9 12.6 24.7 18.0 19.5 26.8 22.1 18.5 12.6 24.7 18.5 6.13 9.68 10.4 5.28 5.63 3.70 6.41 14.3 UI-TARS-7B*23.3 44.6 28.8 27.6 19.4 46.9 26.7 24.7 42.9 29.0 28.1 16.9 37.0 26.0 12.3 9.68 14.2 10.8 5.98 22.2 10.2 21.0 UI-TARS-1.5-7B*24.9 47.3 32.3 29.3 22.1 50.6 29.1 26.5 47.3 28.2 31.0 20.5 39.5 28.2 9.43 9.68 16.0 9.47 9.04 18.5 10.6 22.6 SeeClick-9.6B*3.72 8.93 4.01 3.80 4.34 3.70 4.12 3.72 5.36 4.52 3.47 3.21 3.70 3.73 0.47 3.23 3.55 1.49 1.71 3.70 1.86 3.24 Phi-Ground-4B-7C 26.0 50.0 32.0 32.9 23.7 48.1 30.8 28.4 42.9 30.9 32.4 21.5 42.0 29.6 "}
{"id": "2507.23779v1_chunk150", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "6B*3.72 8.93 4.01 3.80 4.34 3.70 4.12 3.72 5.36 4.52 3.47 3.21 3.70 3.73 0.47 3.23 3.55 1.49 1.71 3.70 1.86 3.24 Phi-Ground-4B-7C 26.0 50.0 32.0 32.9 23.7 48.1 30.8 28.4 42.9 30.9 32.4 21.5 42.0 29.6 9.91 12.9 16.3 10.1 8.70 14.8 10.9 23.8 Phi-Ground-4B-16C 27.0 46.4 33.3 35.7 26.5 53.1 33.0 29.3 41.1 32.7 37.9 23.7 48.2 32.8 11.3 16.1 15.7 9.91 8.01 11.1 10.6 25.5 Phi-Ground-4B-16C-DPO 26.2 51.8 33.6 38.1 26.0 56.8 34.0 29.3 41.1 34.0 39.1 23.8 45.0 33.4 10.4 12.9 18.3 12.6 7.40 18.5 11.9 26.4 Phi-Ground-4B-29C 28.4 48.2 33.3 36.5 26.5 48.1 33.3 29.8 44.6 33.5 36.0 24.9 48.1 32.8 11.8 12.9 15.7 11.0 9.04 3.70 11.2 25.8 Phi-Ground-7B-7C 30.7 51.8 33.1 32.4 27.4 51.9 32.6 29.3 50.0 32.4 33.2 24.0 42.0 31.2 11.8 16.1 11.2 9.30 9.21 14.8 10.1 24.6 Phi-Ground-7B-16C 29.8 51.8 37.1 35.2 28.1 54.3 34.6 29.8 51.8 37.5 36.2 24.2 49.4 33.8 12.3 16.1 14.8 10.5 9.74 11.1 11.3 26.6 Phi-Ground-7B-16C-DPO 30.7 50.0 38.7 38.0 27.9 61.7 36.2 32.1 50.0 39.1 39.0 25.1 50.6 35.6 12.7 19.4 15.1 11.6 9.40 "}
{"id": "2507.23779v1_chunk151", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": ".3 34.6 29.8 51.8 37.5 36.2 24.2 49.4 33.8 12.3 16.1 14.8 10.5 9.74 11.1 11.3 26.6 Phi-Ground-7B-16C-DPO 30.7 50.0 38.7 38.0 27.9 61.7 36.2 32.1 50.0 39.1 39.0 25.1 50.6 35.6 12.7 19.4 15.1 11.6 9.40 14.8 11.9 27.9 Phi-Ground-7B-29C 31.6 48.2 36.7 33.4 28.8 60.5 34.0 29.3 46.4 36.4 35.7 26.7 49.4 33.8 11.3 19.4 13.6 10.9 9.91 14.8 11.3 26.4 O4-mini as planner UGround-v1-7B*31.3 56.4 32.8 29.4 24.0 43.8 30.5 32.5 44.6 32.5 27.2 23.2 42.5 29.3 11.3 16.1 19.8 12.6 11.8 37.0 13.9 24.6 OSAtlas-7B*16.4 32.7 21.9 15.8 11.0 30.0 17.2 17.9 30.4 20.5 13.9 11.6 33.3 16.6 6.67 12.9 13.4 5.27 5.87 29.6 7.48 13.8 UGround-7B*23.4 41.8 29.0 23.3 19.5 45.0 25.1 25.5 33.9 26.8 19.6 17.4 42.5 22.8 10.4 19.4 15.1 10.3 9.76 22.2 11.3 19.7 UI-TARS-7B*30.4 54.5 36.3 32.5 26.3 53.8 33.2 35.8 50.0 35.2 34.6 23.2 51.2 33.4 16.0 25.8 22.5 16.9 11.6 44.4 16.7 27.8 UI-TARS-1.5-7B*30.8 61.8 36.6 35.4 27.4 60.0 35.1 36.8 55.4 35.2 36.5 24.9 60.0 35.1 17.5 25.8 23.7 16.2 15.2 44.4 17.9 29.4 SeeClick-9.6B*5.61 10.9 5."}
{"id": "2507.23779v1_chunk152", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "1.2 33.4 16.0 25.8 22.5 16.9 11.6 44.4 16.7 27.8 UI-TARS-1.5-7B*30.8 61.8 36.6 35.4 27.4 60.0 35.1 36.8 55.4 35.2 36.5 24.9 60.0 35.1 17.5 25.8 23.7 16.2 15.2 44.4 17.9 29.4 SeeClick-9.6B*5.61 10.9 5.46 5.15 3.72 7.50 5.21 4.25 7.14 6.56 4.57 3.52 10.0 5.03 1.42 9.68 3.85 2.84 3.08 11.1 3.16 4.47 Phi-Ground-4B-7C 35.5 60.0 36.3 40.2 27.7 60.0 37.3 39.2 58.9 36.3 39.6 26.3 60.0 37.1 16.5 22.6 25.4 19.2 13.4 44.4 18.6 31.0 Phi-Ground-4B-16C 37.9 63.6 40.2 46.7 32.8 63.8 42.1 44.8 60.7 38.0 43.5 30.8 60.0 40.7 17.9 29.0 24.3 20.8 15.1 29.6 19.6 34.1 Phi-Ground-4B-16C-DPO 37.4 63.6 41.8 48.3 35.5 67.5 43.8 67.6 62.5 39.1 44.6 31.9 63.8 42.1 19.0 29.0 26.9 22.3 15.1 40.7 21.0 35.6 Phi-Ground-4B-29C 36.4 61.8 40.7 45.7 32.3 67.5 41.7 39.2 60.7 40.2 41.6 29.1 63.8 39.6 17.5 32.3 26.3 20.0 14.9 37.0 19.7 33.7 Phi-Ground-7B-7C 38.3 65.5 41.3 40.4 32.6 63.7 40.2 46.5 43.6 40.2 37.6 37.6 45.0 39.7 17.9 29.0 27.2 19.5 13.9 44.4 19.5 33.1 Phi-Ground-7B-16C 36.9 65.5 44.0 46.0 35.1 66.3 43.3 42.3 45"}
{"id": "2507.23779v1_chunk153", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": ".0 19.7 33.7 Phi-Ground-7B-7C 38.3 65.5 41.3 40.4 32.6 63.7 40.2 46.5 43.6 40.2 37.6 37.6 45.0 39.7 17.9 29.0 27.2 19.5 13.9 44.4 19.5 33.1 Phi-Ground-7B-16C 36.9 65.5 44.0 46.0 35.1 66.3 43.3 42.3 45.5 42.6 42.0 43.4 45.0 42.8 17.0 29.0 27.5 21.0 15.6 48.2 20.6 35.6 Phi-Ground-7B-16C-DPO 38.8 67.3 44.0 47.0 35.3 70.9 44.2 45.1 49.1 44.0 43.7 43.0 42.5 43.8 17.0 29.0 26.6 21.5 15.2 48.1 20.5 36.2 Phi-Ground-7B-29C 38.8 63.6 42.9 43.7 35.3 70.0 42.7 40.4 49.1 43.7 39.3 42.3 40.0 41.4 13.7 29.0 26.6 20.5 14.6 40.7 19.5 34.5 Table 14: Results of UI-Vision [ 24]. The final column shows the overall average. Abbreviated category labels: Ed (Education), Br (Browsers), De (Development), Pr (Productivity), Cr (Creativity), En (Entertainment). The best model within each size category is highlighted in bold , and the runner-up is underlined . We tested Agent setting results of UI-Vision of several open-source GUI models ( ≤10Bparameters). 27Phi-Ground Tech Report Table 15: Showdown-click-dev resu"}
{"id": "2507.23779v1_chunk154", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "in bold , and the runner-up is underlined . We tested Agent setting results of UI-Vision of several open-source GUI models ( ≤10Bparameters). 27Phi-Ground Tech Report Table 15: Showdown-click-dev results. †: For the latency of the models we tested, we report the inference speed of the models accelerated using the vllm Python library if supported, otherwise we report the latency using huggingface transformers, marked with ’hf’. This may be faster than the results provided by the benchmark itself, but the comparison between the models we tested remains fair. For the settings with GPT-4O and O4-mini as planners, we directly added 2.5 seconds (aligned with the original benchmark) and 8 seconds (our tested average level, which may be highly dependent on the endpoint) to the original model latency, respectively. *: Results from the original GitHub repository. Model Accuracy(%) ↑Latency†(ms)↓Model Accuracy(%) ↑Latency†(ms)↓ End-to-end model setting (Use short REs) GPT-4O*5.21 2500 UI-TARS-1.5"}
{"id": "2507.23779v1_chunk155", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "cy, respectively. *: Results from the original GitHub repository. Model Accuracy(%) ↑Latency†(ms)↓Model Accuracy(%) ↑Latency†(ms)↓ End-to-end model setting (Use short REs) GPT-4O*5.21 2500 UI-TARS-1.5-7B 67.15 445 Qwen2.5-VL-73B-Instruct*24.78 3790 UGround-7B 46.50 1871 (hf) Gemini 2.0 Flash*33.39 3069 UGround-v1-7B 57.81 209 UI-TARS-72B-SFT*54.40 1977 Phi-Ground-4B-7C 54.40 122 Claude 3.7 Sonnet (Computer Use)*53.68 9656 Phi-Ground-4B-16C 59.96 212 Molmo-72B-0924*54.76 6599 Phi-Ground-4B-16C-DPO 58.17 212 Operator (OpenAI CUA)*64.27 6385 Phi-Ground-4B-29C 55.11 401 seeclick 24.60 847 (hf) Phi-Ground-7B-7C 57.45 168 OS-ATLAS-4B 15.80 1288 (hf) Phi-Ground-7B-16C 61.04 313 OS-ATLAS-7B 41.11 1788 (hf) Phi-Ground-7B-16C-DPO 62.48 313 UI-TARS-2B 59.78 186 Phi-Ground-7B-29C 61.22 603 UI-TARS-7B 66.07 237 Agent setting (Use long REs) with GPT-4O as planner seeclick 15.62 3347 (hf) Phi-Ground-4B-7C 60.68 2622 OS-ATLAS-4B 13.46 3788 (hf) Phi-Ground-4B-16C 62.84 2712 OS-ATLAS-7B 40.22 4288 (hf) "}
{"id": "2507.23779v1_chunk156", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "66.07 237 Agent setting (Use long REs) with GPT-4O as planner seeclick 15.62 3347 (hf) Phi-Ground-4B-7C 60.68 2622 OS-ATLAS-4B 13.46 3788 (hf) Phi-Ground-4B-16C 62.84 2712 OS-ATLAS-7B 40.22 4288 (hf) Phi-Ground-4B-16C-DPO 62.59 2712 UI-TARS-2B 58.89 2687 Phi-Ground-4B-29C 61.04 2901 UI-TARS-7B 61.58 2745 Phi-Ground-7B-7C 59.25 2668 UI-TARS-1.5-7B 61.40 2950 Phi-Ground-7B-16C 63.02 2813 UGround-7B 52.96 4371 (hf) Phi-Ground-7B-16C-DPO 64.39 2813 UGround-v1-7B 57.99 2717 Phi-Ground-7B-29C 61.93 3103 Agent setting (Use long REs) with O4-mini as planner seeclick 19.60 8847 (hf) Phi-Ground-4B-7C 69.60 8122 OS-ATLAS-4B 17.99 9288 (hf) Phi-Ground-4B-16C 72.12 8212 OS-ATLAS-7B 45.50 9788 (hf) Phi-Ground-4B-16C-DPO 73.51 8212 UI-TARS-2B 66.37 8188 Phi-Ground-4B-29C 69.96 8401 UI-TARS-7B 69.78 8243 Phi-Ground-7B-7C 71.94 8168 UI-TARS-1.5-7B 71.58 8454 Phi-Ground-7B-16C 72.12 8313 UGround-7B 62.41 9871 (hf) Phi-Ground-7B-16C-DPO 73.87 8313 UGround-v1-7B 66.73 8214 Phi-Ground-7B-29C 71.40 8603 28P"}
{"id": "2507.23779v1_chunk157", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "Phi-Ground-7B-7C 71.94 8168 UI-TARS-1.5-7B 71.58 8454 Phi-Ground-7B-16C 72.12 8313 UGround-7B 62.41 9871 (hf) Phi-Ground-7B-16C-DPO 73.87 8313 UGround-v1-7B 66.73 8214 Phi-Ground-7B-29C 71.40 8603 28Phi-Ground Tech Report Table 16: Gold dataset evaluation results. modelGold dataset Gold-SPhotoShop ClipChamp Excel PowerPoint Word Windows Setting A VG. End-to-end model setting (Use short REs) SeeClick-10B [44] 1.41 14.53 21.50 13.41 11.63 59.76 20.37 51.66 UGround-7B [30] 29.11 65.36 48.60 58.54 41.86 86.01 54.91 74.88 UGround-v1-7B [30] 49.30 79.33 57.01 69.51 47.67 95.73 66.42 84.36 OS-ATLAS-4B [31] 5.63 21.23 14.95 13.41 16.28 60.55 22.01 47.87 OS-ATLAS-7B [31] 32.86 70.39 27.10 31.71 39.53 90.91 48.75 66.35 UI-TARS-2B [28] 46.95 67.60 55.14 57.32 48.84 84.90 60.12 79.15 UI-TARS-7B [28] 56.34 76.54 72.90 85.37 75.58 93.75 76.75 87.20 UI-TARS-1.5-7B [29] 63.85 65.92 83.18 80.49 81.40 88.62 77.24 86.73 Phi-Ground-4B-7C 46.01 62.57 52.34 65.85 58.14 91.86 62.8 76.78 Phi-Ground-4B-16C 58."}
{"id": "2507.23779v1_chunk158", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "6.34 76.54 72.90 85.37 75.58 93.75 76.75 87.20 UI-TARS-1.5-7B [29] 63.85 65.92 83.18 80.49 81.40 88.62 77.24 86.73 Phi-Ground-4B-7C 46.01 62.57 52.34 65.85 58.14 91.86 62.8 76.78 Phi-Ground-4B-16C 58.22 73.18 76.64 81.71 76.74 93.28 76.63 85.78 Phi-Ground-4B-16C-DPO 64.79 68.72 78.50 87.80 75.58 93.99 78.23 87.20 Phi-Ground-4B-29C 56.81 68.72 80.37 87.80 73.26 94.15 76.85 83.41 Phi-Ground-7B-7C 57.28 69.83 66.36 81.71 68.60 91.94 72.62 85.31 Phi-Ground-7B-16C 69.95 69.83 77.57 87.8 77.91 93.28 79.39 84.83 Phi-Ground-7B-16C-DPO 72.30 69.27 72.90 90.24 77.91 95.18 79.63 84.36 Phi-Ground-7B-29C 67.61 67.60 79.44 90.24 72.09 96.28 78.88 84.83 Agent setting (Use long REs) with GPT-4O as planner SeeClick-10B 1.41 9.50 11.21 6.10 2.33 48.46 13.17 32.23 UGround-7B 38.97 64.25 57.94 56.10 53.49 86.88 59.60 81.52 UGround-v1-7B 52.11 73.18 67.29 74.39 62.79 95.57 70.89 84.83 OS-ATLAS-4B 7.98 18.44 15.89 15.85 10.47 61.34 21.66 38.39 OS-ATLAS-7B 31.92 62.57 26.17 41.46 50.00 87.67 49.97 67.30 UI-T"}
{"id": "2507.23779v1_chunk159", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "88 59.60 81.52 UGround-v1-7B 52.11 73.18 67.29 74.39 62.79 95.57 70.89 84.83 OS-ATLAS-4B 7.98 18.44 15.89 15.85 10.47 61.34 21.66 38.39 OS-ATLAS-7B 31.92 62.57 26.17 41.46 50.00 87.67 49.97 67.30 UI-TARS-2B 55.40 76.54 71.96 80.49 69.77 93.91 74.68 87.20 UI-TARS-7B 62.91 74.86 78.50 82.93 81.40 94.39 79.17 85.31 UI-TARS-1.5-7B 66.67 62.57 85.05 84.15 87.21 86.88 78.76 88.63 Phi-Ground-4B-7C 65.26 71.51 68.22 78.05 75.58 96.76 75.90 88.63 Phi-Ground-4B-16C 70.89 78.77 84.11 86.59 83.72 96.92 83.50 91.47 Phi-Ground-4B-16C-DPO 76.06 77.65 87.85 89.02 84.88 97.71 85.52 91.00 Phi-Ground-4B-29C 70.89 70.95 78.50 91.46 80.23 96.36 81.40 89.57 Phi-Ground-7B-7C 69.48 75.42 75.70 87.80 74.42 97.15 80.00 88.63 Phi-Ground-7B-16C 73.24 75.42 89.72 91.46 81.40 96.36 84.60 90.05 Phi-Ground-7B-16C-DPO 76.06 75.98 86.92 89.02 86.05 97.94 85.33 91.00 Phi-Ground-7B-29C 71.83 73.18 84.11 89.02 77.91 97.00 82.17 88.15 Agent setting (Use long REs) with O4-mini as planner SeeClick-10B 2.35 20.11 9.43 4.94 6."}
{"id": "2507.23779v1_chunk160", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "06 75.98 86.92 89.02 86.05 97.94 85.33 91.00 Phi-Ground-7B-29C 71.83 73.18 84.11 89.02 77.91 97.00 82.17 88.15 Agent setting (Use long REs) with O4-mini as planner SeeClick-10B 2.35 20.11 9.43 4.94 6.90 49.72 15.57 39.42 UGround-7B 39.44 70.39 56.60 54.32 48.28 87.75 59.46 83.17 UGround-v1-7B 52.11 77.09 67.92 80.25 68.60 96.76 73.79 87.98 OS-ATLAS-4B 6.57 17.88 6.60 16.05 9.20 53.04 18.22 37.02 OS-ATLAS-7B 35.68 65.92 33.96 39.51 49.43 88.62 52.19 68.27 UI-TARS-2B 57.28 82.12 73.58 81.48 73.26 94.86 77.10 85.58 UI-TARS-7B 67.14 82.12 77.36 88.89 77.91 95.26 81.45 90.87 UI-TARS-1.5-7B 63.38 74.86 85.85 81.48 87.21 89.09 80.31 90.38 Phi-Ground-4B-7C 70.42 82.12 73.58 81.48 73.26 99.13 80.00 92.79 Phi-Ground-4B-16C 75.59 88.27 83.96 87.65 82.56 98.97 86.17 95.19 Phi-Ground-4B-16C-DPO 79.81 88.83 88.68 88.89 84.88 99.21 88.38 95.19 Phi-Ground-4B-29C 73.71 87.71 83.96 90.12 82.56 98.97 86.17 90.87 Phi-Ground-7B-7C 71.83 85.47 78.30 88.89 80.23 99.29 84.00 92.79 Phi-Ground-7B-16C 76.53 89.3"}
{"id": "2507.23779v1_chunk161", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "8.83 88.68 88.89 84.88 99.21 88.38 95.19 Phi-Ground-4B-29C 73.71 87.71 83.96 90.12 82.56 98.97 86.17 90.87 Phi-Ground-7B-7C 71.83 85.47 78.30 88.89 80.23 99.29 84.00 92.79 Phi-Ground-7B-16C 76.53 89.39 86.79 91.36 79.07 98.81 86.99 92.31 Phi-Ground-7B-16C-DPO 81.22 89.94 85.85 91.36 81.40 99.21 88.16 93.75 Phi-Ground-7B-29C 76.53 86.03 84.91 90.12 80.23 98.10 85.99 92.31 29Phi-Ground Tech Report C Coordinates Representations and Loss As discussed in the main text, we experimented with various coordinate representations and loss function designs. We found that these techniques can accelerate training when dealing with small datasets. However, when the training dataset exceeds 1 million samples, these methods do not exhibit significant improvements. Consequently, the content presented in this section highlights approaches that failed to scale. We disclose these findings to help future researchers avoid similar pitfalls. Overall, the development of all the techniques discussed in this sec"}
{"id": "2507.23779v1_chunk162", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "s section highlights approaches that failed to scale. We disclose these findings to help future researchers avoid similar pitfalls. Overall, the development of all the techniques discussed in this section stems from the following considerations: Unlike regression loss, modeling with natural language treats the difference between \"19\" and \"20\" as a gap of two tokens, which should theoretically be equivalent in distance to that between \"18\" and \"19\". Furthermore, differences in the units, tens, and hundreds places should have varying impacts—we might tolerate errors in the units place, but errors in the hundreds place are entirely unacceptable. These aspects highlight a disparity to regression loss and our expectations. However, experimental results indicate that using the most straightforward next token prediction and expressing coordinates in natural language is well enough, and there is no significant difference in outcomes among these techniques when the batch size is extremely large"}
{"id": "2507.23779v1_chunk163", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "rd next token prediction and expressing coordinates in natural language is well enough, and there is no significant difference in outcomes among these techniques when the batch size is extremely large and the training volume is very high. C.1 Tokenized Coordinates It has been observed that in most LLM, numbers are tokenized by digits, meaning the number 123 would be tokenized into three separate tokens: \"1\", \"2\", and \"3\". This form of representation offers limited interpretability in the context of images. In previous work, many researchers have modeled regions within an image using newly introduced special tokens. We also attempted this approach; however, when processing screenshots, the images often have extremely high resolutions, and buttons are relatively small. If we divide both the height and width into 1,000 discrete intervals and assign a new token to each square region, this would add 1 million tokens to the model, which is entirely impractical. Therefore, we opted to model t"}
{"id": "2507.23779v1_chunk164", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "height and width into 1,000 discrete intervals and assign a new token to each square region, this would add 1 million tokens to the model, which is entirely impractical. Therefore, we opted to model the coordinate values from 1 to 1,000 as new tokens, for example < p123>for value 123, using two tokens to represent a single position: < p123>< p 456>. In this way, we added only 1000 special tokens to the model. We discussed various strategies for initializing these 1000 tokens. To better illustrate our point, we first introduce the following definition: let the function emb:V→Rnrepresent the retrieval of the embedding for a specific token v∈Vfrom a pre-trained model. Then we define the following variables: Crand=random (mean =MEAN ({emb(v), v∈V}), std =STD ({emb(v), v∈V})) Cdigit=random (mean =MEAN ({emb(v), v∈digit}), std =STD ({emb(v), v∈digit})) Mdigit=1 |digit|X v∈dightemb(v) Ep=emb(\"point\" ) We then consider the following five initialization methods: •The implementation in the Huggi"}
{"id": "2507.23779v1_chunk165", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "n =MEAN ({emb(v), v∈digit}), std =STD ({emb(v), v∈digit})) Mdigit=1 |digit|X v∈dightemb(v) Ep=emb(\"point\" ) We then consider the following five initialization methods: •The implementation in the Hugging Face Transformers library (hf) calculates the mean and variance of all pre-trained embeddings to generate a random vector, which is then used to initialize all newly added tokens. •We adapted the hf method by restricting the embeddings used for calculating the mean and variance to only digit tokens, naming this approach R-digit. •The digit-mean method directly uses the mean of the embeddings of digit tokens from the pre-trained model to initialize larger number tokens. •The term main-digit refers to using the embedding of the digit in the hundredths place as the initialization. For example, for the number 234, the initialization would use the embedding of the digit \"2.\" •Prefix-learning method first freeze all parameters except the embedding of newly added tokens, and then used the lear"}
{"id": "2507.23779v1_chunk166", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " for the number 234, the initialization would use the embedding of the digit \"2.\" •Prefix-learning method first freeze all parameters except the embedding of newly added tokens, and then used the learned embedding to initialize in the later training. Additionally, we handle <point>, </point> and digits 0 to 9 differently. The specific assignments can be found in Table 17, where the training results are also presented. We set the training volume for these experiments to 5 million samples and detailed setting can be found in Table 11. Regrettably, the results in Table 17 indicate that all initialization methods significantly underperform compared to directly using natural language to express coordinates. Additionally, during training, we observed slow convergence and significant fluctuations in the gradient norm. These phenomena suggest that the introduction of too many special tokens, which have not undergone large-scale pre-training, can interact adversely with the pre-trained paramete"}
{"id": "2507.23779v1_chunk167", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " in the gradient norm. These phenomena suggest that the introduction of too many special tokens, which have not undergone large-scale pre-training, can interact adversely with the pre-trained parameters, leading to training collapse. However, when high resolution is required, 1000 special tokens become necessary. Thus, in the field of UI grounding, this technique appears to be less practical. 30Phi-Ground Tech Report Table 17: The details and result of different initialization methods for newly added special tokens. method nameInitial value for special tokens <point> </point> <p 0> <p 1> . . . <p 9> <p 10> <p 11> . . . <p 999> Gold-S hf Crand Crand Crand Crand Crand Crand Crand Crand 81.7 R-digit Ep Ep Cdigit Cdigit Cdigit Cdigit Cdigit Cdigit 83.2 digit-mean Ep Ep emb (\"0\")emb (\"0\") emb (\"9\") Mdigit Mdigit Mdigit 74.6 main-digit Ep Ep emb (\"0\")emb (\"0\") emb (\"0\")emb (\"0\")emb (\"0\") emb (\"9\") 12.6 prefix-learning - - - - - - - - 82.3 Natural language - - - - - - - - 88.9 C.2 Label Smoot"}
{"id": "2507.23779v1_chunk168", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "\"9\") Mdigit Mdigit Mdigit 74.6 main-digit Ep Ep emb (\"0\")emb (\"0\") emb (\"0\")emb (\"0\")emb (\"0\") emb (\"9\") 12.6 prefix-learning - - - - - - - - 82.3 Natural language - - - - - - - - 88.9 C.2 Label Smoothing It is fairly intuitive to consider that applying label smoothing to digit tokens might produce an effect similar to that of regression loss. Specifically, for instance, when the ground truth token is the digit 5, the standard Cross-Entropy loss assigns a label of 1 to digit 5 and 0 to all other tokens. However, if we apply label smoothing and assign a certain degree of smoothing to digits 4 and 6, we effectively inform the model that digits 5 and 4, 6 are numerically close. This approach results in an outcome akin to regression loss. In fact, it is possible to derive how to set label smoothing parameters to achieve equivalence with using regression loss. We will next derive the formula for our label smoothing technique. First, we provide the following notations. Let x= (x0, x1, ...x n"}
{"id": "2507.23779v1_chunk169", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "thing parameters to achieve equivalence with using regression loss. We will next derive the formula for our label smoothing technique. First, we provide the following notations. Let x= (x0, x1, ...x n)be a tokenized sentence and xnis a digit token whose digit value(integer number) is T,tis the token id of xn. We usually model the probability given by language model as p(x) =p(x0)p(x1|x0). . . p(xn|x0, x1, . . . , x n−1). Define Vas the vocabulary set, Vdas the set of digit token and V=Vd∪Vt. Now let’s recall the formulation of classical language modeling loss: Llm=−(logp(x0) + log p(x1|x0) +. . .+ log p(xn|x0, . . . , x n−1)) The last term: Ln=−logp(xn|x0, . . . , x n−1) :=−logpn=−|V|X i=1y(i)logp(i), where p(i)is the softmax result of last hidden activations and yiis the label. In general cross-entropy loss, the label is just a one-hot encoding: y(i)=\u001a1ifi=t 0else In the following, we will derive how to approximate the regression loss with an appropriate designed label y(i). For k∈Vdl"}
{"id": "2507.23779v1_chunk170", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " cross-entropy loss, the label is just a one-hot encoding: y(i)=\u001a1ifi=t 0else In the following, we will derive how to approximate the regression loss with an appropriate designed label y(i). For k∈Vdlet the digit number of digit token id kbeK. We design a regularized MSE loss as: LMSE =Ek∈Vd[(K−T)2] +ψEk∈Vt[1], where the second term is a punishment for other none-digit tokens and ψis the punishment factor. We can calculate that: LMSE =Ek∈Vd[(K−T)2] +ψEk∈Vt[1] =X k∈Vd(K−T)2p(k)+ψX k∈Vtp(k) =X k∈Vd/{t}(K−T)2p(k)−ψX k∈Vd/{t}p(k)+ψX k∈Vd/{t}p(k)+ψX k∈Vtp(k) =X k∈Vd/{t}[(K−T)2−ψ]p(k)+ψ(1−p(t)) =−ψ  p(t)+X k∈Vd/{t}[1−(K−T)2 ψ]p(k)−1   31Phi-Ground Tech Report This will be equivalent to optimize loss: ˜LMSE =−PV i=1˜y(i)p(i), where: ˜y(i)=  1 ifi=t 1−d(K,T ) ψifi∈Vd\\ {t} 0 i∈Vt(1) d(K, T)is the distance function, for MSE, it should be d(K, T) = (K−T)2. We directly use Equation 1 for label smoothing, denoted as ˆLMSE =−PV i=1˜y(i)logp(i). It is important to note that this differs fr"}
{"id": "2507.23779v1_chunk171", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " T)is the distance function, for MSE, it should be d(K, T) = (K−T)2. We directly use Equation 1 for label smoothing, denoted as ˆLMSE =−PV i=1˜y(i)logp(i). It is important to note that this differs from ˜LMSE . However, by leveraging the convexity of the logarithmic function, we can easily demonstrate that ˆLMSE serves as an upper confidence bound for ˜LMSE . From a practical standpoint, the information embedded in Equation 1 suggests that the label values are larger for positions closer to the target, which is highly intuitive. Table 18: The scaling effect of label smoothing technique is very limited. d(K, T) ψN= 50K, BS = 64 N= 1M BS = 2048 ScreenSpot-V2 Gold-S ScreenSpot-V2 Gold-S d(K, T) = (K−T)210 52.5 66.9 81.7 85.9 d(K, T) = (K−T)230 55.8 66.2 83.3 88.1 d(K, T) =|K−T| 10 54.6 67.6 84.3 87.8 d(K, T) =|K−T| 30 60.1 69.6 84.1 87.6 no label smoothing 52.3 63.2 84.2 88.8 The results, as shown in Table 18, indicate that this technique demonstrates a clear advantage when dealing with s"}
{"id": "2507.23779v1_chunk172", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "4.3 87.8 d(K, T) =|K−T| 30 60.1 69.6 84.1 87.6 no label smoothing 52.3 63.2 84.2 88.8 The results, as shown in Table 18, indicate that this technique demonstrates a clear advantage when dealing with smaller datasets and smaller batch sizes. However, when we increased the training data size to 1 million and the batch size to 2048, we observed little to no improvement, with results falling within the margin of fluctuation. This suggests that while the approach may offer some acceleration benefits in resource-constrained scenarios, it holds limited significance for large-scale training. In such cases, with larger batch sizes, the model’s optimization direction aligns with the regression loss, reducing the scaling advantage of this technique. C.3 Loss Re-weighting We attempt to assign different weights to the loss of different tokens. For instance, we assign higher weights to tokens in the tens place compared to those in the units place. This approach ensures that the model prioritizes the"}
{"id": "2507.23779v1_chunk173", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "t weights to the loss of different tokens. For instance, we assign higher weights to tokens in the tens place compared to those in the units place. This approach ensures that the model prioritizes the correctness of the tens place over the units place. If expressed formally using equations, it can be represented as follows: Lre=−nX t=0wtlogp(xt|xt−1..., x 0). Initially, we set wt= 1.0when xtis not a digit token. Then we consider different settings of weights for digit tokens, as shown in Table 19. Table 19: Experiment on selecting parameters for loss reweighting. weights for digit position N= 50K, BS = 64 N= 1M BS = 2048 hundrads tens units ScreenSpot-V2 Gold-S ScreenSpot-V2 Gold-S 1.0 1.0 1.0 52.3 63.2 84.2 88.8 2.0 1.5 1.0 6.3 16.2 15.5 22.7 4.0 2.0 1.0 0.0 0.0 0.0 0.0 1.0 1/10 1 /100 55.4 66.8 84.5 85.8 1.0 1/√ 10 1 /10 54.7 63.6 83.3 86.7 1.0 1/ln 10 1 /(ln 10)257.6 64.8 84.1 86.7 The first block in the table represents the control group without using reweighting techniques. We fir"}
{"id": "2507.23779v1_chunk174", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": ".4 66.8 84.5 85.8 1.0 1/√ 10 1 /10 54.7 63.6 83.3 86.7 1.0 1/ln 10 1 /(ln 10)257.6 64.8 84.1 86.7 The first block in the table represents the control group without using reweighting techniques. We first confirm that the weight of digit tokens cannot exceed 1.0, even though these format-related tokens appear in almost all data. We found that if the weight of digit tokens is even slightly greater than 1.0, it causes the model to output in an unexpected format 32Phi-Ground Tech Report during testing. This results in parsing errors, causing the test results to be nearly zero. In contrast, in models trained under normal conditions, the proportion of parsing errors is nearly zero. When the weight of digit tokens is proportionally adjusted to smaller than 1.0 and the model is trained accordingly, we obtain results similar to those described in Section C.2. Specifically, when training resources are extremely limited, we observe that the reweighting technique accelerates model convergence and a"}
{"id": "2507.23779v1_chunk175", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " we obtain results similar to those described in Section C.2. Specifically, when training resources are extremely limited, we observe that the reweighting technique accelerates model convergence and achieves consistently better results in low-sample scenarios. However, when the data size and batch size increase significantly, the benefits of this technique are minimal or unstable. Such results are insufficient to support us to widespread this technique. The reason for this phenomenon may be that when the training volume and batch size increase, learning based on higher numeric values exhibits greater certainty and stability (or lower perplexity), which facilitates more effective learning. In contrast, learning based on lower numeric values might fluctuate due to errors present in the dataset, leading to slower learning. This process is similar to our reweighting technique, and therefore, as the training volume and batch size increase, this technique is effectively replaced. D Data Pre-"}
{"id": "2507.23779v1_chunk176", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "et, leading to slower learning. This process is similar to our reweighting technique, and therefore, as the training volume and batch size increase, this technique is effectively replaced. D Data Pre-processing Details D.1 CommonCrawl Data Pre-processing D.1.1 Rendering Resolutions and Filtering Rendering resolutions. When rendering each web page, we initially select a screen size with equal probability (1/3 chance) from the options of 1080p, 2.5k, and 4k. The corresponding screen area ( Space ) are1920×1080 ,2560×1440 , and3840×2160 , respectively. For a given screen area, we then randomly choose an aspect ratio (Rw, Rh)from the following set of aspect ratios: (Rw, Rh)∈\u001a (1 +i N,2−i N)|i= 0,1, ..., N\u001b . The final screen size (W, H )used for rendering can be calculated using the following formula: (W, H ) = (Rw×S, R w×S), S =&r Space RwRh' Filtering. During webpage rendering, we implemented certain filtering processes. Unlike the filtering described in Section D.1.2, this stage of filt"}
{"id": "2507.23779v1_chunk177", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "formula: (W, H ) = (Rw×S, R w×S), S =&r Space RwRh' Filtering. During webpage rendering, we implemented certain filtering processes. Unlike the filtering described in Section D.1.2, this stage of filtering requires the relevant code (such as JavaScript) used in browser rendering. In contrast, the subsequent filtering refers to offline filtering conducted after the necessary information has been stored. Therefore, even though both processes involve rule-based filtering, they are described in two separate sections. During webpage rendering, we can use JavaScript to obtain interactive information, and based on this information, we have established the following filtering rules for all HTML elements. We first retain only those elements that meet any of the following conditions: •Interactive Tags: The HTML tag name of the element is one of ’button’ ,’input’ ,’textarea’ ,’select’ ,’a’,’form’ •Event Attribute: The elements have specific JavaScript methods (functions) attached, such as ’onclic"}
{"id": "2507.23779v1_chunk178", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "gs: The HTML tag name of the element is one of ’button’ ,’input’ ,’textarea’ ,’select’ ,’a’,’form’ •Event Attribute: The elements have specific JavaScript methods (functions) attached, such as ’onclick’ , ’onmousedown’ ,’onmouseup’ ,’onmouseover’ ,’onmouseout’ ,’onkeydown’ •Role attribute: Elements with the following role attributes are generally interactive: ’button’ ,’link’ ,’textbox’ , ’menuitem’ ,’option’ ,’checkbox’ ,’radio’ ,’tab’ ,’switch’ . •Interactive class: The class name of the element is a string type and the class name is one of ’btn’ ,’button’ , ’input’ ,’link’ ,’nav’ ,’menu’ ,’item’ . •Is icon: Tag name is one of ’i’,’span’ ,’svg’ and the class name is one of ’fa’,’fas’,’far’,’fal’,’fab’ , ’material-icons’ • Is image: The tag name is ’img’ Subsequently, we remove all elements that are not visible on the screen, such as those that require scrolling to be viewed. We then store all relevant information of the elements that meet the criteria, in JSON format, along with the "}
{"id": "2507.23779v1_chunk179", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ts that are not visible on the screen, such as those that require scrolling to be viewed. We then store all relevant information of the elements that meet the criteria, in JSON format, along with the rendered images. This includes details such as bounding box coordinates, HTML code, and various attributes. We also store a layout diagram, which matches the size of the rendered screenshot. However, different types of elements are represented by different colors occupying their respective areas; for example, interactive text is marked in red, and images in cyan. This type of layout diagram allows for the expression of button positions without focusing on the content itself and is used for data deduplication at the webpage level. 33Phi-Ground Tech Report D.1.2 Offline Rule-based Filtering Once the data has been stored, we consider the following filtering rules: Boxes deduplication. If a box completely encompasses multiple other boxes, we first remove the outer box. Such boxes are typically"}
{"id": "2507.23779v1_chunk180", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " the data has been stored, we consider the following filtering rules: Boxes deduplication. If a box completely encompasses multiple other boxes, we first remove the outer box. Such boxes are typically div containers of a module area, containing multiple related buttons. When one box contains another (determined by an IoU greater than a certain threshold), we remove the larger box. This situation is common in web design with nested containers and errors generated by OmniParser when using it to create boxes. Remove empty boxes. In both webpage rendering and OmniParser, there are instances where certain boxes appear in a blank, solid-colored area devoid of any content. For each candidate box, we crop the corresponding region from the screenshot and use the pixel standard deviation to directly determine if the area is solid-colored. If it is deemed to be solid-colored, we delete the box. Text content recognition. Sometimes, the text content within a screenshot, such as a long sentence, is "}
{"id": "2507.23779v1_chunk181", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " determine if the area is solid-colored. If it is deemed to be solid-colored, we delete the box. Text content recognition. Sometimes, the text content within a screenshot, such as a long sentence, is recognized as an element. We wish to retain buttons with text but not these non-interactive content texts. To achieve this, we use the aspect ratio of the box as a filtering criterion. If the aspect ratio of a box exceeds a certain threshold, we discard that box. D.2 Re-sampling algorithm Algorithm 3 Re-sampling algorithm Require: The center point set of training dataset C={(xi, yi)}using relative coordinates. Segmentation granularity N, M , Sampling factor ψ. Ensure: Sampled center point set ˆC 1:all_box ← {(i, j) :list()|i∈ {0,1, ...N−1}, j∈ {0,1, ...M−1}} 2:for(x, y)inCdo 3: locate ←(int(x//(1.0/N)),int(y//(1.0/M))) 4: all_box [locate ].append ((x, y)) 5:end for 6:dist←[len(v)|v∈all_box.values() ] 7:dist.sort() 8:keep_number ←dist[int(N×M×ψ)] 9:ˆC←list() 10:forvinall_box do 11: N_Sample"}
{"id": "2507.23779v1_chunk182", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "/(1.0/N)),int(y//(1.0/M))) 4: all_box [locate ].append ((x, y)) 5:end for 6:dist←[len(v)|v∈all_box.values() ] 7:dist.sort() 8:keep_number ←dist[int(N×M×ψ)] 9:ˆC←list() 10:forvinall_box do 11: N_Sample ←min(len(v),keep_number ) 12: ˆC.extend (random_sample (v,N_Sample )) 13:end for 34Phi-Ground Tech Report E Prompts E.1 Reference expression generation prompt. Prompt for generating Long-Gold RE / Training data generation System prompt: —————————————————————————————————————— User will provide you with a screenshot, in which a specific area will be highlighted with a red rectangular box. We will also provide a cropped image of the corresponding area, and (optionally) additional information related to the area to help you understand it. Your task is to generate several references regarding the target area on the original image. Specific task requirements are as follows: You will analyze and output a dictionary in JSON file format. The key-value pairs included are as follows: •area_type: Cho"}
{"id": "2507.23779v1_chunk183", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "target area on the original image. Specific task requirements are as follows: You will analyze and output a dictionary in JSON file format. The key-value pairs included are as follows: •area_type: Choose one from ’icon’, ’text’. These represent whether the target area is an indicative icon, text. •interactive: bool, indicating whether this element in the screenshot scenario is an interactive element (e.g., clickable, inputable, etc.). If it is static text or an image, then it is not interactive. •context: Generate a background context describing why the current screen and area would be used. For example, if the area is the close button of a image file, the context could be that the user is editing a file and has completed their task at this moment. • functional_reference: A reference about the target area, involving the function of the target area. •positional_reference: A reference about the target area by describing the position of the target area, such as layout and nearby elements."}
{"id": "2507.23779v1_chunk184", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " the target area, involving the function of the target area. •positional_reference: A reference about the target area by describing the position of the target area, such as layout and nearby elements. •appearance_reference: A reference about the target area by describing the appearance of the target area. Ensure that anyone can uniquely identify this area in the screenshot through any one of the references. Don’t mention red rectangular box. Your output references should only include the element description itself and follow the requirements. Do not start with \"the target element\" or \"the element\" Your output should follow this format strictly: # Analyze A f r e e form a n a l y z e of t h e s c r e e n s h o t and t a s k . # Output ‘ ‘ ‘ j s o n { \" a r e a _ t y p e \" : . . . , \" i n t e r a c t i v e \" : . . . , \" c o n t e x t \" : . . . , \" f u n c t i o n a l _ r e f e r e n c e \" : . . . , . . . , \" a p p e a r a n c e _ r e f e r e n c e \" : . . . } ‘ ‘ ‘ 35Phi-Ground Tech Repo"}
{"id": "2507.23779v1_chunk185", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "e r a c t i v e \" : . . . , \" c o n t e x t \" : . . . , \" f u n c t i o n a l _ r e f e r e n c e \" : . . . , . . . , \" a p p e a r a n c e _ r e f e r e n c e \" : . . . } ‘ ‘ ‘ 35Phi-Ground Tech Report Model input: —————————————————————————————————————— # Screenshot with highlight # Cropped target image Model output: —————————————————————————————————————— # Analyze The screenshot shows a PowerPoint presentation interface. The highlighted area is located on the toolbar of the PowerPoint application and is labeled as \"Shapes.\" This area is used to insert various shapes into the presentation slides, an essential feature for users looking to enhance their slides with graphical elements. The toolbar section where the \"Shapes\" button is located is a common area for formatting and editing tools, making it easily accessible for users during the slide creation process. # Output { \" a r e a _ t y p e \" : \" i c o n \" , \" i n t e r a c t i v e \" : t r u e , \" c o n t e x t \" : \" While working on "}
{"id": "2507.23779v1_chunk186", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " making it easily accessible for users during the slide creation process. # Output { \" a r e a _ t y p e \" : \" i c o n \" , \" i n t e r a c t i v e \" : t r u e , \" c o n t e x t \" : \" While working on a PowerPoint p r e s e n t a t i o n , t h e u s e r i s l i k e l y l o o k i n g t o add g r a p h i c a l e l e m e n t s or i l l u s t r a t i o n s t o t h e i r s l i d e s t o enhance t h e v i s u a l a p p e a l or t o convey i n f o r m a t i o n more e f f e c t i v e l y . \" , \" f u n c t i o n a l _ r e f e r e n c e \" : \" Shapes b u t t o n i s used t o add v a r i o u s g r a p h i c a l s h a p e s t o a s l i d e i n a PowerPoint p r e s e n t a t i o n . \" , \" p o s i t i o n a l _ r e f e r e n c e \" : \" Located on t h e t o o l b a r under t h e ’Home’ tab , t o t h e r i g h t of t h e ’ Layout ’ b u t t o n and t o t h e l e f t of t h e ’ Arrange ’ b u t t o n . \" , \" a p p e a r a n c e _ r e f e r e n c e \" : \" C o n t a i n s a w h i t e c i r c l e and a b l u e"}
{"id": "2507.23779v1_chunk187", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "g h t of t h e ’ Layout ’ b u t t o n and t o t h e l e f t of t h e ’ Arrange ’ b u t t o n . \" , \" a p p e a r a n c e _ r e f e r e n c e \" : \" C o n t a i n s a w h i t e c i r c l e and a b l u e square , wi th t h e l a b e l ’ Shapes ’ u n d e r n e a t h . \" } 36Phi-Ground Tech Report E.2 Planner’s generation prompt for agent setting. Prompt for generating Long RE / Agent settings for evaluation System prompt: —————————————————————————————————————— User will provide you with a screenshot and a short instruction related to the area to help you understand it.Your task is to classify the target area and generate several references regarding the target area on the original image. Specific task requirements are as follows: You will analyze and output a dictionary in JSON file format. The key-value pairs included are as follows: •context: Generate a background context describing why the current screen and area would be used. For example, if the area is the close button of a image fil"}
{"id": "2507.23779v1_chunk188", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "e key-value pairs included are as follows: •context: Generate a background context describing why the current screen and area would be used. For example, if the area is the close button of a image file, the context could be that the user is editing a file and has completed their task at this moment. • functional_reference: A reference about the target area, involving the function of the target area. •positional_reference: A reference about the target area by describing the position of the target area, such as layout and nearby elements. •appearance_reference: A reference about the target area by describing the appearance of the target area. Ensure that anyone can uniquely identify this area in the screenshot through anyone of the references. Ensure that the reference is complete and independent. Your output should follow this format strictly: # Analyze A f r e e form a n a l y z e of t h e s c r e e n s h o t and t a s k . # Output ‘ ‘ ‘ j s o n { \" c o n t e x t \" : . . . , \" f u n c "}
{"id": "2507.23779v1_chunk189", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "ndent. Your output should follow this format strictly: # Analyze A f r e e form a n a l y z e of t h e s c r e e n s h o t and t a s k . # Output ‘ ‘ ‘ j s o n { \" c o n t e x t \" : . . . , \" f u n c t i o n a l _ r e f e r e n c e \" : . . . , . . . , \" a p p e a r a n c e _ r e f e r e n c e \" : . . . } ‘ ‘ ‘ Model input: —————————————————————————————————————— # Screenshot # Instruction adjust the options of \"show previous\" 37Phi-Ground Tech Report Model output: —————————————————————————————————————— # Analyze The screenshot displays a macOS System Settings window focused on the \"Notifications\" section. The user intends to adjust the \"Show previews\" option, which is part of the Notification Center settings. This area allows users to control how notifications are displayed on their device, particularly when the screen is unlocked. The screen is divided into two main sections: the left pane with different settings categories and the right pane showing detailed options for the selected c"}
{"id": "2507.23779v1_chunk190", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " particularly when the screen is unlocked. The screen is divided into two main sections: the left pane with different settings categories and the right pane showing detailed options for the selected category, which is \"Notifications\" in this case. # Output { \" c o n t e x t \" : \" The u s e r i s c o n f i g u r i n g n o t i f i c a t i o n s e t t i n g s t o d e t e r m i n e how p r e v i e w s of n o t i f i c a t i o n s a r e d i s p l a y e d on t h e i r d e v i c e when t h e s c r e e n i s u n l o c k e d . \" , \" f u n c t i o n a l _ r e f e r e n c e \" : \" The ’Show previews ’ o p t i o n a l l o w s t h e u s e r t o c o n t r o l whether n o t i f i c a t i o n p r e v i e w s a r e shown on t h e s c r e e n when i t i s u n l o c k e d . \" , \" p o s i t i o n a l _ r e f e r e n c e \" : \" The ’Show previews ’ o p t i o n i s l o c a t e d i n t h e r i g h t pane of t h e System S e t t i n g s window under ’ N o t i f i c a t i o n Center ’ s e t t i n g s , j u s t b"}
{"id": "2507.23779v1_chunk191", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " r e n c e \" : \" The ’Show previews ’ o p t i o n i s l o c a t e d i n t h e r i g h t pane of t h e System S e t t i n g s window under ’ N o t i f i c a t i o n Center ’ s e t t i n g s , j u s t below t h e d e s c r i p t i v e t e x t a b o u t n o t i f i c a t i o n s . \" , \" a p p e a r a n c e _ r e f e r e n c e \" : \" The ’Show previews ’ o p t i o n i s a dropdown menu l o c a t e d n e x t t o t h e t e x t ’Show previews ’ wi th t h e c u r r e n t s e t t i n g ’When Unlocked ’ . \" } F More Cases Study Error grounding case study with human level reference. Case-1: Similar Icons Reference: —————————————————————————————————————— Toggles visibility of all annotations (dimensions, notes, symbols) in the graphics area. Located in the heads-up view toolbar at the top center of the viewport, immediately to the right of the Temporary Axes button and left of the Measure button. Blue uppercase letter A on a light gray square background. —————————————————————————————————————— Outpu"}
{"id": "2507.23779v1_chunk192", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " viewport, immediately to the right of the Temporary Axes button and left of the Measure button. Blue uppercase letter A on a light gray square background. —————————————————————————————————————— Output: —————————————————————————————————————— 38Phi-Ground Tech Report Error grounding case study with human level reference. Case-2: Precision Error Reference: —————————————————————————————————————— Convert Entities command projects selected edges or curves from the model into the active sketch as sketch entities.Located on the Quick Access Toolbar at the very top of the SOLIDWORKS window, immediately to the right of the Redo icon and before the application menu bar.Grey square button showing a diagonal blue dashed line connecting two white square endpoints. —————————————————————————————————————— Output: —————————————————————————————————————— Error grounding case study with human level reference. Case-3: Lack spatial reasoning Reference: —————————————————————————————————————— Grid cell for se"}
{"id": "2507.23779v1_chunk193", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " Output: —————————————————————————————————————— Error grounding case study with human level reference. Case-3: Lack spatial reasoning Reference: —————————————————————————————————————— Grid cell for selecting a 9x7 table dimension when inserting a new table. Cell in the ninth column of the seventh row within the table size preview grid under the Table button in the Insert tab. Light-blue interior square outlined by a white inner border and a thicker blue outer border. —————————————————————————————————————— Output: —————————————————————————————————————— 39Phi-Ground Tech Report Error grounding case study with human level reference. Case-4: Two same area Reference: —————————————————————————————————————— scatter plot template used to generate an X-Y scatter diagram from worksheet data. icon located in the top section of the 2D plot palette, in the row of scatter type graphs beneath the line and bar icons; it is the second icon from the left in that row. white rectangular button with severa"}
{"id": "2507.23779v1_chunk194", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": "cated in the top section of the 2D plot palette, in the row of scatter type graphs beneath the line and bar icons; it is the second icon from the left in that row. white rectangular button with several solid black dots arranged randomly inside and the Chinese label displayed underneath —————————————————————————————————————— Output: —————————————————————————————————————— Error grounding case study with human level reference. Case-5: Interactive area Reference: —————————————————————————————————————— Toggles the applied Digital Glitch effect on or off for live comparison in the Effect Controls panel. Found immediately to the left of the \"Digital Glitch\" effect name under the adjustment layer’s effects list in the top-left panel. Small grey lowercase \"fx\" icon on a dark background matching the style of other effect-toggle buttons. —————————————————————————————————————— Output: —————————————————————————————————————— 40"}
{"id": "2507.23779v1_chunk195", "paper_id": "2507.23779v1", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "text": " style of other effect-toggle buttons. —————————————————————————————————————— Output: —————————————————————————————————————— 40"}
{"id": "2507.23773v1_chunk0", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "SIMURA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model Mingkai Deng⋄,†∗Jinyu Hou⋄,†∗Yilin Shen‡Hongxia Jin‡ Graham Neubig†Zhiting Hu⋄,▲Eric Xing⋄,† ⋄Institute of Foundation Models, Mohamed bin Zayed University of Artificial Intelligence †School of Computer Science, Carnegie Mellon University ‡Samsung Research ▲Halıcıo ˘glu Data Science Institute, UC San Diego {mingkaid, jinyuhou}@cs.cmu.edu, eric.xing@mbzuai.ac.ae Abstract AI agents built on large language models (LLMs) hold enormous promise, but current practice focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also suffers from the fundamental limitations of autoregressive LLMs. On the other hand, humans are general agents who reason by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SIMURA, a goal-oriented architecture for generalized agentic reason"}
{"id": "2507.23773v1_chunk1", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "on by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SIMURA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of optimal agent in any environment, SIMURAovercomes the limitations of au- toregressive reasoning by introducing a world model for planning via simulation. The generalized world model is implemented using LLM, which can flexibly plan in a wide range of environments using the concept-rich latent space of natural lan- guage. Experiments on difficult web browsing tasks show that SIMURAimproves the success of flight search from 0% to 32.2%. World-model-based planning, in particular, shows consistent advantage of up to 124% over autoregressive planning, demonstrating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently "}
{"id": "2507.23773v1_chunk2", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "strating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently in all environments. To start, we make REASONER AGENT -WEB, a web-browsing agent built on SIMURAwith pretrained LLMs, available as a research demo for public testing. 1 Introduction AI agents powered by large language models (LLMs) hold tremendous potential for handling tasks that require flexible decision making. Recently, there have been great advancements in agents specialized in web and computer automation [ 1,2,3,4], internet research [ 5,6,7], social simulation [8], software development [ 9,10], scientific research [ 11,12], and so on. Despite the promise, current LLMs often prove insufficient for solving complex agentic tasks, suffering from issues such as hallucination, repetitions, or failure at complex planning [ 13,14]. To address these issues, many approaches focus on creating "}
{"id": "2507.23773v1_chunk3", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "icient for solving complex agentic tasks, suffering from issues such as hallucination, repetitions, or failure at complex planning [ 13,14]. To address these issues, many approaches focus on creating agents tailored to specific tasks like the above examples. However, this strategy have some inherent drawbacks. Economically, redesigning custom agents for every task is ∗Co-first author Preprint. Under review.arXiv:2507.23773v1 [cs.AI] 31 Jul 2025Flight Searching : “I’d like to find a round -trip ticket from Chicago to Dubai next month, traveling in economy class, with non -stop flights only, departing after 8 AM, and returning within two weeks. ” Online Shopping : “I want to buy a black mattress. Can you look at Amazon, eBay, and Mattress Firm and give me one good option from each? ” News Research : “Find and summarize one article from each of these local news outlets on Trump’s inauguration: Times of San Diego, The Tennessee Tribune, and MinnPost ?”Go to Google Flights Fill in origin an"}
{"id": "2507.23773v1_chunk4", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "search : “Find and summarize one article from each of these local news outlets on Trump’s inauguration: Times of San Diego, The Tennessee Tribune, and MinnPost ?”Go to Google Flights Fill in origin and destination Fill in departure and return dates Filter search result and respond Go to Amazon website Search for mattress on eBay Go on Mattress Firm website Search and compose response Go on Times of San Diego Browse The Tennessee Tribune Search on MinnPost Review and compose responseFigure 1: Demo of tasks performed using a web-browsing agent built on SIMURAwith simulative planning using a LLM-based world model. costly and not scalable from a business standpoint. Intellectually, narrowly focused solutions offer no clear path towards general and transferrable intelligence. [ 15] Technically, autoregressive LLMs rely on linear, step-by-step reasoning that often leads to errors that propagate through their thought trajectories [ 16,17]. Humans, in contrast, are generalist problem-solvers t"}
{"id": "2507.23773v1_chunk5", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " autoregressive LLMs rely on linear, step-by-step reasoning that often leads to errors that propagate through their thought trajectories [ 16,17]. Humans, in contrast, are generalist problem-solvers that can reason and plan to achieve goals in diverse environments. Using a single cognitive system, we adapt to different tasks not only by linear reasoning, but also by imagining potential outcomes, simulating possibilities using a mental world model, and planning accordingly. [18] Moving towards a more general and powerful AI agent, we introduce SIMURA(Simulative Reasoning Architecture), a goal-oriented architecture for generalized agentic reasoning. SIMURAmitigates the limitations of LLM autoregressive reasoning by introducing world model as the engine for planning via simulation. Specifically, a policy module first proposes a few potential actions, aimed at achieving specific goals based on agent identity and environment. Then, the world model simulates the outcomes of those proposed ac"}
{"id": "2507.23773v1_chunk6", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ly, a policy module first proposes a few potential actions, aimed at achieving specific goals based on agent identity and environment. Then, the world model simulates the outcomes of those proposed actions. Finally, a critic module evaluates these outcomes against the initial goals in order to select the best action from the candidates. Because simulating the full details of the world is infeasible and unnecessary for planning, we extract only the relevant information using natural language as a compact but complete representation, and simulate the next world in this latent space. To ensure robustness from observation noise and distracting execution details, we further propose a hierarchical architecture that isolates perception, simulative planning, and action selection which ensures adaptability and consistency across diverse tasks. Experiments on a range of web browsing tasks show SIMURAimproving substantially compared to baselines, increasing the success rate of flight search from "}
{"id": "2507.23773v1_chunk7", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "tability and consistency across diverse tasks. Experiments on a range of web browsing tasks show SIMURAimproving substantially compared to baselines, increasing the success rate of flight search from 0%to32.2% , with reasoning by WM simulation outperforming LLM autoregressive reasoning by up to 124% . Figure 1 shows examples of the agent performing multi-website, long-range task such as flight searching, online shopping, and news research. For evaluation and demonstration purposes, we implemented SIMURAas an open-source library available via LLM Reasoners [ 19].2The resulting web agent, REASONER AGENT -WEB[20], is 2https: //github.com/maitrix-org/llm-reasoners/tree/main/examples/ReasonerAgent-Web 2available as a research preview.3We are actively expanding the system to address broader challenges and to further demonstrate its generality across a wider range of task domains. 2 Related Work LLM-Based Agents LLM-based agents have rapidly evolved into versatile systems capable of autonomou"}
{"id": "2507.23773v1_chunk8", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "lenges and to further demonstrate its generality across a wider range of task domains. 2 Related Work LLM-Based Agents LLM-based agents have rapidly evolved into versatile systems capable of autonomous behavior across a range of environments. One major approach to build such systems focuses on data collection in the targeted environment followed by model training. Notable examples include AutoWebGLM [ 21], AgentQ [ 22], UI-TARS [ 23], etc. Prompt-based workflows, on the other hand, have also shown strong potential when equipped with carefully designed modules, as demonstrated by recent work such as AWM [ 24], VOYAGER [ 25], and so on. SIMURAis built on prompt-based workflows but can leverage observation data for targeted improvement of its world model [ 26], leading to reduced reliance on human demonstration and strong generalizability to new tasks [18], which is an exciting next step. World-Model-Based Agents Model-based planning for agents have long been frequently discussed and stud"}
{"id": "2507.23773v1_chunk9", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "human demonstration and strong generalizability to new tasks [18], which is an exciting next step. World-Model-Based Agents Model-based planning for agents have long been frequently discussed and studied. Early work demonstrated the success of this approach by testing in classic games like go, chess, shogi and Atari. [ 27,28]. Later on, world model was used for policy optimization and experimented on control tasks. [ 29,30] In recent years, with the boost in foundation model’s capabilities, world-model-based planning was applied to more complex problems like math reasoning [31], playing Minecraft [ 32], and web browsing [ 33]. However, these world models typically represent and predict the world states using holistic continuous embeddings, which suffer from noise and high variability which detracts from robust and stable decision-making [ 34].SIMURAinstead adopts natural language as a discrete, concept-based latent space for consistent representation and prediction, which shows more ge"}
{"id": "2507.23773v1_chunk10", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " detracts from robust and stable decision-making [ 34].SIMURAinstead adopts natural language as a discrete, concept-based latent space for consistent representation and prediction, which shows more general applicability across tasks in practice. Web Browsing Agents Web browsing and navigation were chosen to evaluate SIMURAdue to their realism and the complex decision-making they demand across diverse, dynamic interfaces. Recent years have seen the emergence of several prominent web-browsing agents, from proprietary ones such as OpenAI’s Operator [ 1], Anthropic’s Computer Use [ 6], and Google-DeepMind’s Project Mariner [2], and open-source ones including OpenHand’s BrowsingAgent [ 35], WebV oyager [ 36], CogAgent [37] and WebAgent [ 38]. These agents are typically built on simple ReAct-based [ 17] autoregressive reasoning which have difficulty recovering from previous mistakes; their often specialized design also preclude these approaches from generalizing to other task domains like so"}
{"id": "2507.23773v1_chunk11", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ed [ 17] autoregressive reasoning which have difficulty recovering from previous mistakes; their often specialized design also preclude these approaches from generalizing to other task domains like social interactions and the physical world. Numerous benchmarks have been introduced to evaluate these web agents, including WebArena [ 3], WebV oyager [ 36] MiniWoB++ [ 39], Mind2Web [ 40], and WebShop [ 41]. Despite wide adoption, these benchmarks are usually either built in simulated and simplified environments, based on outdated questions, or lacks convincing method of measuring task completion, which detract from the goal of evaluating practically useful web agents. To address these challenges, we build FlightQA, an new dataset for evaluating agent ability in real-time complex website navigation. More details are included in Section 4.1. Generalist Agents There have been various attempts of building generalist agents recently. One major approach focuses on creating a multi-agent system "}
{"id": "2507.23773v1_chunk12", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ation. More details are included in Section 4.1. Generalist Agents There have been various attempts of building generalist agents recently. One major approach focuses on creating a multi-agent system that consists of a unified interface on top of a few specialist agents that collaborates to decompose and complete complex tasks. [ 42,43,44,45] Although this approach could lead to impressive performance on benchmarks, it has a few inherent limitations. First of all, tasks in reality could be versatile and may constantly require new specialist agents to be added to the system to achieve optimal performance, which is not efficient. Moreover, independently trained specialist agents for different domains are unable to leverage shared experience in the way that world model training enables. Finally, error propagation along the interaction trajectory remains an open challenge and is further complicated by the presence of multiple agents. Another popular approach utilizes framework similar to t"}
{"id": "2507.23773v1_chunk13", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ly, error propagation along the interaction trajectory remains an open challenge and is further complicated by the presence of multiple agents. Another popular approach utilizes framework similar to the CodeActAgent [ 46]. These agents [ 35,47,48] suffer from inaccurate code plans and have limited ability to revise or correct prior errors as well. SIMURA, on the other hand, is able to avoid these limitations by working as a monolithic architecture in which world model act as a central planning component. 3https://easyweb.maitrix.org/ 3𝜋 Agent 𝑎𝑡𝑠𝑡+1 Goal World State 𝑔 𝑠𝑡 𝑇 𝜇 Universe Critic 𝑉Figure 2: A possible definition of an optimal agent 3 S IMURA: Generalized Architecture for Optimal Goal-Oriented Agent 3.1 Formulation of Agent-Environment Model We first present our formulation of an optimal goal-oriented agent following the agent-environment model presented in [ 49]: We consider an agent πwith identity i(e.g., name, description) and goal g acting in environment µ(e.g., web brows"}
{"id": "2507.23773v1_chunk14", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " optimal goal-oriented agent following the agent-environment model presented in [ 49]: We consider an agent πwith identity i(e.g., name, description) and goal g acting in environment µ(e.g., web browser, physical world, the entire universe) with action space Aand state space S. Formally, at each time step t, the agent πtakes the current state st∈ S and outputs the next action at∈ A following a policy distribution pπ(at|st), while the environment µ takes the current state stand action at, and outputs the next state st+1∈ S based on the distribution pµ(st+1|st, at). We can thus denote the distribution of the interaction trajectory up to timestep T, or (at, st+1, . . . , a T−1, sT)given the current state stas below: pπ µ(at, st+1, . . . , a T−1, sT|st) =T−1Y k=tpπ(ak|sk)|{z} agentpµ(sk+1|sk, ak)| {z } environment(1) In each state st, the agent also receives a reward r(g, st)based on its goal g. We evaluate the agent by its discounted cumulative reward, denoted asP∞ k=tγkr(g, sk)(with the "}
{"id": "2507.23773v1_chunk15", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ak)| {z } environment(1) In each state st, the agent also receives a reward r(g, st)based on its goal g. We evaluate the agent by its discounted cumulative reward, denoted asP∞ k=tγkr(g, sk)(with the discount parameter γt decaying to zero with time, i.e., limt→∞γt= 0). Note that this reward function can be dense (e.g., gaming scores), but perhaps frequently sparse (e.g., curing a disease). The agent’s long-term success can thus be measured by its expected future discounted reward, also known as value function [50], which satisfies the following recurrence: Vg π,µ(st):=Eπ,µ\"∞X k=tγkr(g, sk) st# = lim T→∞X (at,st+1,...,s T)TX k=tγkr(g, sk)pπ µ(at, st+1, . . . , s T|st) =X (at,st+1,...,s T) T−1X k=tγkr(g, sk) +γTVg π,µ(sT) | {z } goal progress! pπ µ(at, st+1, . . . , s T|st) | {z } trajectory,(2) Which indicates that the value function in state stcan be expressed in terms of the value function at possible future states sTweighted by their probabilities. 3.2 Definition of Optimal Agent Bas"}
{"id": "2507.23773v1_chunk16", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": ",(2) Which indicates that the value function in state stcan be expressed in terms of the value function at possible future states sTweighted by their probabilities. 3.2 Definition of Optimal Agent Based on Equations 1 and 2, we can define the optimal agent π∗ µin environment µas one that maximizes the value function, written formally as below: π∗ µ:= arg max πVg π,µ. (3) 4Some simple derivation will show that the optimal agent in state stwill follow the following decision ruleπ∗ µwhen planning for actions at:T−1: π∗ µ(st) = arg max at:T−1|{z} possible actionsX st+1:T T−1X k=tγkr(g, sk) +γTVg π,µ(sT) | {z } goal progress!T−1Y i=tpµ(si+1|si, ai)|{z } universe response(4) In practice, agents often samples promising action candidates using a policy function ˜πthrough the distribution p˜π(at|st). Building the optimal agent thus requires capabilities for proposing possible actions ( ˜π), predicting their outcomes ( µ), and evaluating goal progress ( r, V), respectively. Note that typical rea"}
{"id": "2507.23773v1_chunk17", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "). Building the optimal agent thus requires capabilities for proposing possible actions ( ˜π), predicting their outcomes ( µ), and evaluating goal progress ( r, V), respectively. Note that typical reactive agents that output the next action directly can be seen as taking the first sample from ˜π(similar to “System 1” in humans which makes fast, instinctive reactions [ 51]), without simulating and evaluating the outcomes using µandV(similar to “System 2” responsible for deliberate decision- making). In terms of LLM-based agents, this can also be seen as the agent generating a plan using autoregressive LLMs, which has no way of correcting errors during the sampling process. 3.3 World Model for Generalized Simulative Reasoning Note that the optimal decision-making process defined in Equation 4 requires the agent to have access to the ground-truth world state sand the environment µto experience and optimize over. However, these are often not available aside from simple scenarios like Go an"}
{"id": "2507.23773v1_chunk18", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "4 requires the agent to have access to the ground-truth world state sand the environment µto experience and optimize over. However, these are often not available aside from simple scenarios like Go and Chess games [ 52,53] – imagine building an spacecraft to land on Mars, or simply a humanoid robot relying on noisy sensors in daily environments. World Model (WM) thus arises as a crucial component for predicting any environment’s response to a general agent. Specifically, a WM foperates on an internal representation of the world state, denoted as a belief state ˆst, which is derived from sensory inputs otvia an Encoder h(unlike the optimal agent described in §3.2 which has direct access to the true world state st). Given proposed action at, the WM predicts the next belief state ˆst+1according to the distribution pf(ˆst+1|ˆst, at). The predicted belief state then allows the agent to propose the next action, continuing the cycle of prediction and action up to the desired time horizon T. T"}
{"id": "2507.23773v1_chunk19", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ng to the distribution pf(ˆst+1|ˆst, at). The predicted belief state then allows the agent to propose the next action, continuing the cycle of prediction and action up to the desired time horizon T. Thus, a WM here essentially functions as a generative model of possible future world states, which enables simulative reasoning, or “thought experiments\". Formally, for the optimal agent π∗ fequipped with WMfin belief state ˆst, we define the simulation-based decision rule in Equation 6 as follows: π∗ f(ˆst) = arg max at:T−1|{z} possible actionsX ˆst+1:T T−1X k=tγkr(g,ˆsk) +γTVg π,f(ˆsT) | {z } goal progress!T−1Y i=tpf(ˆsi+1|ˆsi, ai)|{z} simulation with world model(5) A general-purpose WM fhere enables simulation of diverse possibilities across a wide range of domains, enabling agents to reason about outcomes without direct interaction with the environment. 3.4 Design of Simulative Reasoning Agent Using LLM-Based World Model In this subsection, we present our design of a generalizable simul"}
{"id": "2507.23773v1_chunk20", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " about outcomes without direct interaction with the environment. 3.4 Design of Simulative Reasoning Agent Using LLM-Based World Model In this subsection, we present our design of a generalizable simulative reasoning agent using large language models (LLMs) as building blocks due to the latter’s strength in a wide range of capabilities such as summarization, commonsense knowledge, reflection, and tool use, which are gained from large-scale pretraining and instruction tuning. In particular, we provide detailed discussion on design decisions that enable robust and general applicability across environments and tasks. Discrete, Hierarchical State Representation via Natural Language The dominant approach to encoding observation ot(e.g., webpages, video streams) has been to directly pass all input tokens into an LLM to form continuous embeddings ˆsz t. While technically preserving all information, real- world sensory readings often suffer from inherent noise and high variability (e.g., ads on"}
{"id": "2507.23773v1_chunk21", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "t tokens into an LLM to form continuous embeddings ˆsz t. While technically preserving all information, real- world sensory readings often suffer from inherent noise and high variability (e.g., ads on a webpage, varying weather and lighting conditions in video), which can make them brittle for reasoning over. Human cognition, on the other hand, has evolved to counter this variability by categorizing raw perception into discrete concepts [34], which are often encoded in language, symbols or structured thoughts. Indeed, natural language is inherently hierarchical, capable of encoding concepts from concrete ones (e.g., apple) to highly abstract ones (e.g., religion). Discrete representations are also complete in general [ 49], which ensures no information is necessarily lost in the compression process. 5𝜇inaccessible𝜋 Agent 𝑎𝑡′Ƹ𝑠𝑡+1′Goal Critic𝑓 𝑉Belief 𝑔 𝑇𝑜𝑡World Model Ƹ𝑠𝑡 Encoder ℎ agent estimation of world state Observation𝑇′ 𝑎𝑡continuous or discrete internal representations sensory da"}
{"id": "2507.23773v1_chunk22", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "n process. 5𝜇inaccessible𝜋 Agent 𝑎𝑡′Ƹ𝑠𝑡+1′Goal Critic𝑓 𝑉Belief 𝑔 𝑇𝑜𝑡World Model Ƹ𝑠𝑡 Encoder ℎ agent estimation of world state Observation𝑇′ 𝑎𝑡continuous or discrete internal representations sensory data contains incomplete informationactual actionsimulative reasoning to find the best action UniverseFigure 3: An agent in real world where groundtruth world state and universe are unavailable to experience or experiment, so world model is crucial for simulation. As discussed in §3.4, separation of simulated actions a′ tfor planning and concrete actions atfor execution facilitates transfer and hierarchical planning, leading to more diverse and grounded actions which lead to better task success. Implementing this form of perception, we propose to represent the world state ˆstusing a discrete natural language summary ˆsc tgenerated by an encoder LLM h, formally expressed as below: ph(ˆst|ot) =NtY i=1ph(ˆst,i|ˆst,<i, ot), (6) Where each ˆst,iis a natural language token. Likewise, we also imple"}
{"id": "2507.23773v1_chunk23", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ural language summary ˆsc tgenerated by an encoder LLM h, formally expressed as below: ph(ˆst|ot) =NtY i=1ph(ˆst,i|ˆst,<i, ot), (6) Where each ˆst,iis a natural language token. Likewise, we also implement the WM fusing an LLM which predicts the next state ˆst+1as a natural language sequence ˆsc t+1, formally as below: pf(ˆst+1|ˆst, at) =Nt+1Y i=1ph(ˆst+1,i|ˆst+1,<i,ˆst, at) (7) Such a concept-based representation allows the other modules like policy to operate on a more structured latent space, which we find empirically to reduce hallucination and enable more robust planning, leading to better task performance in practice. Hierarchical Planning via Simulated Actions The customary approach to decision-making with world models has been to perform simulations or rollouts based on the specific action space A(π) afforded to the agent. While this approach indeed captures all the execution details, the specific idiosyncracies of individual action spaces (e.g., parameter ordering, format, and "}
{"id": "2507.23773v1_chunk24", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " action space A(π) afforded to the agent. While this approach indeed captures all the execution details, the specific idiosyncracies of individual action spaces (e.g., parameter ordering, format, and scale) may distract from transferring knowledge across different action spaces, environments, and tasks for generalizable reasoning. The real world may contain a richer range of intentions than what a particular action space offers (e.g., clicking on a flight may mean either exploring the pricing or committing to the option). Last but not least, the sequential roll-out over atomic actions can be inefficient and increase opportunities for error accumulation across multi-step, low-level predictions (e.g., swooshing of liquids with each muscle twitch), when higher-level dynamics over more abstract actions (e.g., spilling water due to tilting the glass) remain stable and predictable. To close this gap, we adopt a hierarchical architecture which separates high-level, flexible planning from low-"}
{"id": "2507.23773v1_chunk25", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "t actions (e.g., spilling water due to tilting the glass) remain stable and predictable. To close this gap, we adopt a hierarchical architecture which separates high-level, flexible planning from low-level, rigorous execution [ 54]. As illustrated in Figure 3, the agent’s policy p˜π(a′ t|ˆst)and world model pf(ˆst+1|ˆst, a′ t)operate over simulated actions a′ tfrom a separate action space A′, while another actor pν(at|a′ t,ˆst)is responsible for selecting the concrete action at∈ A conditioned on the selected simulated action a′ t. This divide-and-conquer approach allows for more generalized reasoning disentangled from the exact details of the concrete action space and enables representation of a richer set of intentions. Furthermore, each simulated action a′ tmay represent multiple execution steps in the environment (e.g., “explore the website” vs “click on the link”), which shortens the number of rollout steps for higher efficiency and fewer chances for error accumulation. In practice"}
{"id": "2507.23773v1_chunk26", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "on steps in the environment (e.g., “explore the website” vs “click on the link”), which shortens the number of rollout steps for higher efficiency and fewer chances for error accumulation. In practice, we represent simulated actions a′ tusing natural language due to its generality and expressivity, and find it results in more diverse and grounded action proposals, leading to better task success. Having discussed our major designs, we proceed to describe the full decision process of the SIMURA architecture: As illustrated in Figure 3, given observation ot(e.g., webpage screenshots and/or accessibility tree), SIMURAfirst infers the world state ˆstusing the encoder h, and then selects the 6 Observation Encoder Actor Belief State Planner Simulated Action Action Plan Candidates Next State Predictions State Values Optimize Policy World Model Critic Identity Goal Agent Simulative Reasoning Environment Figure 4: Optimal agent architecture design with conditional probability annotation best sim"}
{"id": "2507.23773v1_chunk27", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ctions State Values Optimize Policy World Model Critic Identity Goal Agent Simulative Reasoning Environment Figure 4: Optimal agent architecture design with conditional probability annotation best simulated action a′∗ tthrough the planner. Inside the planner, the architecture performs simulative reasoning by proposing actions a′ tusing policy ˜πand predicting the next state ˆst+1using the world model f, and evaluating goal progressPT′−1 k=tγkr(g,ˆsk)+γT′Vg π,f(ˆsT′)using critic vupon reaching state ˆsT′at the planning horizon T′. This can repeat multiple times until the planner selects the action sequence a′∗ t:T′−1with the highest expected success and passes the first step a∗ tto actor ν which finally outputs the concrete action at. Formally, SIMURAcan be seen as solving the following multi-level optimization problem: ˆst= arg max ˆsph(ˆs|ot)|{z} encoder(Perception) a′∗ t:T′−1= arg max a′ t:T−1|{z} sampled from policy ˜πX ˆst+1:T′v(ˆsT′)|{z} criticT′−1Y k=tpf(ˆsk+1|ˆsk, a′ k)|{z } wor"}
{"id": "2507.23773v1_chunk28", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ti-level optimization problem: ˆst= arg max ˆsph(ˆs|ot)|{z} encoder(Perception) a′∗ t:T′−1= arg max a′ t:T−1|{z} sampled from policy ˜πX ˆst+1:T′v(ˆsT′)|{z} criticT′−1Y k=tpf(ˆsk+1|ˆsk, a′ k)|{z } world model(Planning) (8) at= arg max apν(a|ˆst, a′∗ t)|{z} actor(Acting) In practice, we implement each of these components by zero-shot prompting pretrained LLMs. While these LLMs alone are often insufficient for many complex agentic tasks, SIMURA’s divide-and- conquer approach combines existing LLM strengths like instruction-following, summarization, reflec- tion, and tool use to allow agentic behavior to emerge. Benefiting from massive web-scale pretraining on next-token prediction p(xt|x<t), which is formally akin to world modeling, LLMs possess significant potential to serve as world models with natural-language state and action spaces [ 31,55]. We approximately infer the world state ˆstand action atby sampling from the LLM-based encoder and actor distributions phandpν, respectively. Fo"}
{"id": "2507.23773v1_chunk29", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " with natural-language state and action spaces [ 31,55]. We approximately infer the world state ˆstand action atby sampling from the LLM-based encoder and actor distributions phandpν, respectively. For planning, we optimize over the sampled actions a′ t:T′−1using readily available tree search algorithms like Depth-First Search (DFS) and Monte-Carlo Tree Search (MCTS). 4 Experiments Our proposed SIMURAarchitecture is generally applicable to various environments and tasks. As our first step, we evaluate our implementation on web browsing as an example due to both its practical value and its technical challenge. Web browser is an indispensable portal for individuals to perform 7Goal Observation \"Find the cheapest one-way flight from Boston to Los Angeles next Wednesday\" \"The current webpage is a Google flight interface that has ...\"\" Encoder Actor Environment Identity \"An information and automation assistant who navigates ...\" Memory \"... We filled out flight information and clicked on th"}
{"id": "2507.23773v1_chunk30", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "s a Google flight interface that has ...\"\" Encoder Actor Environment Identity \"An information and automation assistant who navigates ...\" Memory \"... We filled out flight information and clicked on the 'Search' botton ...\" User Belief State \"Select the 'Cheapest' tab\" Planner \"click('128')\" Simulated Action ActionFigure 5: LLM-based implementation of our proposed agent model for web-related tasks (e.g. multi- website QA, flight search, etc). Planner is where we implement our proposed world-model-based planning. We also implement a baseline that simply samples the plan from a language model (i.e., autoregressive planning). many tasks in real life (e.g., gather information, book travels, and submit applications). Whereas many existing products do access the internet [ 56,57, etc.], they typically use specialized tools (e.g., search engines and data APIs) to capture a subset of web browser capabilities (i.e., reading) while falling short of the full functionality (e.g., access content not"}
{"id": "2507.23773v1_chunk31", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "cally use specialized tools (e.g., search engines and data APIs) to capture a subset of web browser capabilities (i.e., reading) while falling short of the full functionality (e.g., access content not exposed to search engines or predefined APIs like flight and hotel databases). We argue that an agent that takes advantage of the full browser will push the envelope in AI’s abilities to serve human needs. Despite the richness and flexibility, the web browser is a highly challenging environment for agentic reasoning due to its immense complexity, long-horizon nature, partial observability, and multimodal- ity [3,58]. We evaluate our architecture in 3 types of web browsing tasks: 1) complex website navigation, 2) multi-hop, multi-website QA, and 3) general web automation. For the baselines, we compare against: 1.BrowsingAgent from OpenHands [ 35], a representative open-web agent which generates chain-of-thought before selecting an action 2.SIMURA(our architecture) with autoregressive plann"}
{"id": "2507.23773v1_chunk32", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " compare against: 1.BrowsingAgent from OpenHands [ 35], a representative open-web agent which generates chain-of-thought before selecting an action 2.SIMURA(our architecture) with autoregressive planning (i.e., commit to the first sample from our policy module) instead of our proposed simulation-based planning with world model. Formally, the planning process is simplified to the following: a′∗ t= arg max a′ tp˜π(a′ t|ˆst) Implementation for Web Browsing Figure 5 presents our implementation when applied to web browsing. We use prompts tailored to the web environments in this example, but plan to extend to other environments and move towards training a single agent model that can act optimally in all environments, which is an exciting next step. At each step t, the agent receives the observation otas the HTML-based accessibility tree visible through the browser’s viewport (an example is provided in 8Figure 6: Overview of performance comparison between SIMURAand baselines. The full archit"}
{"id": "2507.23773v1_chunk33", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "tion otas the HTML-based accessibility tree visible through the browser’s viewport (an example is provided in 8Figure 6: Overview of performance comparison between SIMURAand baselines. The full architecture shows clear advantage over the baseline BrowsingAgent, improving the performance on complex website navigation from 0% to 32.2%. Our proposed world model reasoning for planning also consistently improves over simple planning with autoregressive LLM by up to 124%. Appendix A). The agent then uses encoder LLM hto summarizes the observation as ˜st∼ph(· |ot), and then add it to a selective memory of past summaries and simulated actions {m(˜sk, a′∗ k)}t−1 k=1to form the estimated world state ˆst= [m(˜s1, a′∗ 1), . . . , m (˜st−1, a′∗ t−1),˜st]for planning. During planning, we sample Msimulated actions a′ tfrom the policy ˜π, cluster them into distinct actions, and use the world model fto predict the next summary as ˜st+1∼pf(· |ˆst, a′ t)to form the next state ˆst+1= [m(˜s1, a′∗ 1), . . ."}
{"id": "2507.23773v1_chunk34", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ed actions a′ tfrom the policy ˜π, cluster them into distinct actions, and use the world model fto predict the next summary as ˜st+1∼pf(· |ˆst, a′ t)to form the next state ˆst+1= [m(˜s1, a′∗ 1), . . . , m (˜st, a′ t),˜st+1]; this repeats until the planning horizon T. To evaluate the terminal state ˆsTwith critic v, we prompt the LLM to generate qualitative answers and convert them into numerical scores (e.g., “success” receives a score of 1), and repeat for Ntimes to capture the fine-grained differences between states. Following previous work [ 59,33], we set M=N= 20 and T=t+ 1, and use DFS as the search algorithm. We implement the planning process using LLM Reasoners [ 19], a library for LLM-based complex reasoning using advanced algorithms. After the planner selects the simulated action a′∗ t, we update the memory with m(˜st, a′∗ t). For the actor ν, we additionally include the observation text otin the prompt to ensure the action grounding. All the prompts are included in Appendix B"}
{"id": "2507.23773v1_chunk35", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "a′∗ t, we update the memory with m(˜st, a′∗ t). For the actor ν, we additionally include the observation text otin the prompt to ensure the action grounding. All the prompts are included in Appendix B. Overview of Results An overview of our results is presented in Figure 6. Across all 3 categories of tasks, our architecture shows a clear advantage over the baseline BrowsingAgent, specifically increase the success rate on complex website navigation from 0% to 32.2%. Our proposed world model reasoning for planning also consistently improves over simple planning with autoregressive LLMs by up to 124%. In the subsections below, we will introduce the evaluation settings and discuss the results in more detail. 4.1 Complex Website Navigation A distinguishing feature of web agents is the ability to gather live information (e.g., flights, stock prices, social media) not present in the training data of foundation models due to its rapid update [ 60]. For many questions (e.g., the price of the ea"}
{"id": "2507.23773v1_chunk36", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "o gather live information (e.g., flights, stock prices, social media) not present in the training data of foundation models due to its rapid update [ 60]. For many questions (e.g., the price of the earliest flight tomorrow), LLMs without such grounding often result in hallucinations (see Figure 7 for an example). In practice, however, obtaining the information is challenging, as many websites are very complex and difficult to navigate (e.g., execute a flight search query on a travel website and filter through the results), which calls for superb reasoning skills on the part of the agent. Dataset Due to the lack of benchmarks for complex website navigation on the open internet, we created our own dataset using flight search as an example, since it is an important part of travel planning, a major use case for web agents. We formulate flight search as the user asking for a flight satisfying a list of constraints (e.g., one-way, from New York to Los Angeles), and the agent operating a flig"}
{"id": "2507.23773v1_chunk37", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "major use case for web agents. We formulate flight search as the user asking for a flight satisfying a list of constraints (e.g., one-way, from New York to Los Angeles), and the agent operating a flight search engine to find such a flight and returning it to the user. To evaluate the agent’s reasoning 9Figure 7: Faced with the question “What is the earliest-arriving flight tomorrow from Pittsburgh to Zurich?” ChatGPT-4o browsed the frontpage of Kayak.com and hallucinated a flight that arrives at 10:45am on the following day as the answer ( left). Performing the search on Kayak.com, however, shows that the earliest-arriving flight lands in Zurich at 6:10am on the next day ( right ). The question was asked on December 17th, 2024. Constraint Generation Question Generation ['round-trip', 'San Francisco to Paris', 'departing in two weeks'] Constraint Extension Number of Constraints: 3 ['round-trip', 'San Francisco to Paris', 'departing in two weeks', 'returning after a 10-day stay'] Could y"}
{"id": "2507.23773v1_chunk38", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "Francisco to Paris', 'departing in two weeks'] Constraint Extension Number of Constraints: 3 ['round-trip', 'San Francisco to Paris', 'departing in two weeks', 'returning after a 10-day stay'] Could you find a round-trip flight from San Francisco to Paris departing in two weeks? Could you find a round-trip flight from San Francisco to Paris departing in two weeks and returning after a 10-day stay? Repeat K Times Repeat N Times FlightQA Dataset (N x (K+1) Questions) Figure 8: Illustration of the data generation process for the FlightQA dataset. We first prompt a LLM to generate Nlists of Cstarting constraints (Constraint Generation). Then, we prompt the LLM to iteratively add constraints to the lists one by one, repeating for Ktimes (Constraint Extension). Finally, we prompt the LLM to convert each constraint list into a question in natural language (Question Generation). ability, we further produce questions with varying number of constraints by iteratively adding to the list, which en"}
{"id": "2507.23773v1_chunk39", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ert each constraint list into a question in natural language (Question Generation). ability, we further produce questions with varying number of constraints by iteratively adding to the list, which enables a counterfactual analysis that controls for the confounding effect of specific constraint configurations (e.g., an agent with perfect reasoning should still be able to answer the same question with one more constraint; an agent of rote memorization will likely fail when the question changes slightly). We illustrate our data collection process in Figure 8. To ensure scalability and controllability, we prompt a LLM to first generate a list of Cstarting constraints, repeating for Ntimes. After that, we prompt the LLM to iteratively add constraints to the lists one at a time, repeating for Ktimes. Finally, we prompt the LLM to convert each constraint list into a question in natural language. In practice, we set C= 3,N= 15 , and K= 5, which results in FlightQA, a dataset consisting of 90 "}
{"id": "2507.23773v1_chunk40", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "times. Finally, we prompt the LLM to convert each constraint list into a question in natural language. In practice, we set C= 3,N= 15 , and K= 5, which results in FlightQA, a dataset consisting of 90 questions with 15 sequences of constraint lists where the number of constraints increases from 3 to 8. We use gpt-4o to perform all the data generation steps. The initial question generation and question expansion prompts are included in Appendix C Evaluation Because FlightQA involves querying live information from the open internet, it is impossible to establish ground truth answers due to the constantly evolving flight pricing and availability. Inspired by previous work on evaluation for generated text [ 61], we propose to evaluate the agent response based on two quality aspects: groundedness for whether the response is supported by the interaction history and relevance for whether the response satisfies user constraints to the 10Performance (%) Outcomes (%) Method Correct Grounded Relev"}
{"id": "2507.23773v1_chunk41", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "s for whether the response is supported by the interaction history and relevance for whether the response satisfies user constraints to the 10Performance (%) Outcomes (%) Method Correct Grounded RelevantResponse ReturnedBrowser CrashedMax Steps ReachedRepetitive ActionsAction Errors OpenHands BrowsingAgent 0.0 0.0 0.0 0.0 3.3 3.3 0.0 93.3 SIMURA(Ours) Autoregressive Planning 14.4 15.6 14.4 16.7 0.0 37.8 44.4 1.1 – with o1†1.1 1.1 1.1 1.1 11.1 40.0 37.8 10.0 – with o3-mini†3.3 4.4 3.3 4.4 3.3 51.1 32.2 8.9 World Model Planning 32.2**36.7 32.2 38.9 1.1 40.0 18.9 1.1 Table 1: Performance and outcome statistics for the FlightQA dataset. Our architecture increases the correct rate from 0% in OpenHands BrowsingAgent to 32.2%. Reasoning by world model simulation also clearly outperforms autoregressive reasoning by 124%. ** indicates being significantly higher than the second-best method at the statistical significance level of 0.01 ( p <0.01) based on pairwise t-test.†We implement the autoreg"}
{"id": "2507.23773v1_chunk42", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ssive reasoning by 124%. ** indicates being significantly higher than the second-best method at the statistical significance level of 0.01 ( p <0.01) based on pairwise t-test.†We implement the autoregressive planner with o1 and o3-mini, respectively. extent allowed by the results (e.g., if the search results do not include any flight that satisfies all user constraints). Due to the strong ability of LLMs in evaluating generated text [ 62], we prompt LLMs to assess the two quality aspects of the agent response. Specifically, we include all browser observations in the agent’s trajectory over Tsteps (o1, o2, . . . o T), the constraint list, the question, and the agent response, and ask the LLM to provide judgment on the groundedness and relevance of the response. We further define an answer to be correct when it is both grounded and relevant. We also include the evaluation prompt in Appendix C. Experiment Setup We ran the experiments and evaluation using gpt-4o between November 24th, 2024"}
{"id": "2507.23773v1_chunk43", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "o be correct when it is both grounded and relevant. We also include the evaluation prompt in Appendix C. Experiment Setup We ran the experiments and evaluation using gpt-4o between November 24th, 2024 and December 9th, 2024. For the environment, we use BrowserGym [ 63], a popular open-source browser sandbox. We stop each run when the agent provides a response or after the agent takes 30 actions, whichever comes first. We also mark the run as failed when the agent repeats the same action for 3 times consecutively or when the agent causes more than 3 errors while interacting with the browser. Results We present our Complex Website Navigation results in Table 1. Compared to Browsin- gAgent which fails completely in this task, our full architecture improves the correct rate from 0% to 32.2%. Within our architecture, our proposed world-model-based planning shows superior performance over autoregressive reasoning with a 124% improvement (significant at the 0.01 level). The other components i"}
{"id": "2507.23773v1_chunk44", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ithin our architecture, our proposed world-model-based planning shows superior performance over autoregressive reasoning with a 124% improvement (significant at the 0.01 level). The other components in our architecture, which communicate using the concept-based latent space of model-generated language (e.g., observation summary and selective memory), also result in more coherent behavior by reducing the action error rate in BrowsingAgent from 93.3% to 1.1%. How- ever, the autoregressive reasoning still results in frequent repetitions, which is mitigated by world model-based planning (44.4% →18.9%). Analysis of Reasoning Ability To compare the reasoning abilities of autoregressive and world- model planners within our architecture, we visualize the percentage of correct responses vs number of constraints in Figure 9. As the questions in FlightQA are generated based on iteratively expanded constraint lists, this analysis should faithfully reflect the effect of increasing constraints while"}
{"id": "2507.23773v1_chunk45", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "f constraints in Figure 9. As the questions in FlightQA are generated based on iteratively expanded constraint lists, this analysis should faithfully reflect the effect of increasing constraints while controlling for other confounders such as specific constraint sets. Based on our data samples, world model planning shows consistent advantage over autoregressive planning as we increase the number of constraints, showing signs of improved reasoning ability. The performance for both methods decreases with more constraints initially but then increases sharply at 7 constraints before dropping again, which may reflect memorization in the backend LLM or implicit constraints in questions with fewer explicit constraints. 4.2 Multi-Hop, Multi-Website QA Another type of challenging questions for web agents is those that require gathering information about multiple entities over multiple websites. For instance, given the question “ What are the 11Figure 9: % correct and % response returned vs. num"}
{"id": "2507.23773v1_chunk46", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "agents is those that require gathering information about multiple entities over multiple websites. For instance, given the question “ What are the 11Figure 9: % correct and % response returned vs. number of constraints for FlightQA. Based on our data samples, world model planning consistently outperforms autoregressive planning as we increase the number of constraints, showing signs of improved reasoning ability. availabilities of the top-10 restaurants in Paris for a dinner next week? ”, an agent must first find the top-10 restaurants in Paris, then look up the availability of each restaurant, and finally compile the information into a response to the user. Whereas complex website navigation stresses the depth of individual websites, multi-hop, multi-website QA concerns the breadth of websites to navigate over long-horizon interactions. Dataset To evaluate agent abilities for multi-hop, multi-website QA, we adopt the FanOutQA [ 64] dataset, which consists of questions of exactly this "}
{"id": "2507.23773v1_chunk47", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "sites to navigate over long-horizon interactions. Dataset To evaluate agent abilities for multi-hop, multi-website QA, we adopt the FanOutQA [ 64] dataset, which consists of questions of exactly this nature. Due to resource constraints, we evaluate on the first 100 examples of the dev set. As the results show, however, the smaller sample size is sufficient to show statistically significant differences between methods. Experiment Setup We ran the experiments using gpt-4o-2024-05-13 between November 10th, 2024 and December 8th, 2024. We noticed that our architecture with world-model-based planning deteriorates in performance when using the newer versions of gpt-4o , which may be due to additional training which changed the model’s response patterns to the same prompts. We operate the browser using the same rules as in experiments for Complex Website Navigation. Results We present our results on Multi-Hop, Multi-Website QA in Table 2. Again, our method increases the accuracy from 17.0% to"}
{"id": "2507.23773v1_chunk48", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " using the same rules as in experiments for Complex Website Navigation. Results We present our results on Multi-Hop, Multi-Website QA in Table 2. Again, our method increases the accuracy from 17.0% to 29.8% and world model planning improves over autoregressive planning by 48.6% (p-value = 0.011). BrowsingAgent achieves fair performance even though it cannot memorize information from different websites, often due to some questions in the dataset being answerable based on information from a single Wikipedia page (e.g., What are the publication dates for all of the Harry Potter books? ). Despite this, our architecture improves over BrowsingAgent even without world model planning by dramatically reducing action errors (43% →10%). Browser crashes make a sizable contribution to agent failures (24% for our architecture), indicating room for improvement in the tooling for open-web navigation. 4.3 General Web Automation Last but not least, web agents are often tasked with performing various wor"}
{"id": "2507.23773v1_chunk49", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "4% for our architecture), indicating room for improvement in the tooling for open-web navigation. 4.3 General Web Automation Last but not least, web agents are often tasked with performing various work tedious to human users (e.g., online shopping, managing social media). These tasks often require the ability to interact with a range of websites of moderate complexity. As an example, given the question “ Summarize customer reviews for Amazon Echo Dot 3rd generation ,” the agent should navigate a shopping website to locate and go over all the customer reviews of said product before summarizing the content for the user. 12Performance (%) Outcomes (%) Method Acc. Acc. (Strict)Response ReturnedBrowser CrashedMax Steps ReachedRepetitive ActionsAction ErrorParsing Error OpenHands BrowsingAgent 17.0 4.0 32.0 17.0 8.0 0.0 43.0 0.0 SIMURA(Ours) Autoregressive Planning 20.2 3.0 37.0 24.0 10.0 18.0 10.0 1.0 World Model Planning 29.8* 4.0 55.0 24.0 12.0 8.0 1.0 0.0 Table 2: Performance and outcome"}
{"id": "2507.23773v1_chunk50", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "7.0 4.0 32.0 17.0 8.0 0.0 43.0 0.0 SIMURA(Ours) Autoregressive Planning 20.2 3.0 37.0 24.0 10.0 18.0 10.0 1.0 World Model Planning 29.8* 4.0 55.0 24.0 12.0 8.0 1.0 0.0 Table 2: Performance and outcome statistics for the FanOutQA dataset. Acc. (Strict) refers to the percentage of responses that exactly match the groundtruth. Our architecture clearly outperforms the baseline BrowsingAgent. Reasoning by world model increases the response rate and fact-level accuracy vs. autoregressive planning by 48.6% and 47.5%, respectively. * indicates being significantly higher than the second-best method at the 0.05 level based on pairwise t-test. Dataset To evaluate general web automation capabilities, we adopt the WebArena [ 3] benchmark, a standard environment for testing web agents which features a range of simulated websites including a Reddit-like social forum, a shopping site, a GitLab-based code management platform, a map, and a Wikipedia-like encyclopedia. Following the evaluation for Multi-"}
{"id": "2507.23773v1_chunk51", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ange of simulated websites including a Reddit-like social forum, a shopping site, a GitLab-based code management platform, a map, and a Wikipedia-like encyclopedia. Following the evaluation for Multi-Hop, Multi-Website QA, we take a random subset of 100 examples. Experiment Setup We run the experiments using gpt-4o over BrowserGym accessed via the OpenHands platform which provides a uniform evaluation procedure. Because WebArena demands a specific response format for evaluation, we rewrote the agent description to steer the agent answer format accordingly (Appendix B.1). We keep all other environment rules the same as previous experiments, except for setting the maximum allowed steps to 15 which is consistent with the default setting of WebArena. Results We present our results on General Web Automation in Table 3. Continuing the patterns from previous experiments, our proposed architecture improves over BrowsingAgent by up to 91.7%, while within our architecture, world model reasoning "}
{"id": "2507.23773v1_chunk52", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "eb Automation in Table 3. Continuing the patterns from previous experiments, our proposed architecture improves over BrowsingAgent by up to 91.7%, while within our architecture, world model reasoning improves over autoregressive reasoning by 21.1%, highlighting the comparative advantage under the given experimental setup. Method Success Rate (%) OpenHands BrowsingAgent 12.0 SIMURA(Ours) Autoregressive Planning 19.0 Ours (World Model Planning) 23.0 Table 3: Results on a random 100-sample subset of WebArena. Our architecture improves over BrowsingAgent by up to 91.7%, while world model planning improves over autoregressive planning by 21.1%. 5 Limitations Due to the modular pipeline and thorough exploration of multiple plans in world model planning, the current agent takes longer than typical LLM agents to run. Speeding up world-model-based reasoning with appropriate caching and parallelization strategies is an important part of our future work. Agent capabilities can be limited by the t"}
{"id": "2507.23773v1_chunk53", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "l LLM agents to run. Speeding up world-model-based reasoning with appropriate caching and parallelization strategies is an important part of our future work. Agent capabilities can be limited by the tooling. For example, with open-source browser environments, web agents are often blocked by Captcha or anti-scraping tools from certain websites. Deeper integration with user browser can help solve this issue. As agent-based automation become more integrated into browsing and computer-use workflows, we also encourage conversations around fair use and protocols around agent access of certain websites. We are currently only using the text portion of the webpage observations, which can miss information like images and layout information (e.g., occlusions). While existing work are experimenting with visual-based web browsing, it is still 13challenging to combine multimodal perception and planning, which we are excited to keep working on. 6 Conclusion In this paper, we have presented SIMURA, a "}
{"id": "2507.23773v1_chunk54", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "th visual-based web browsing, it is still 13challenging to combine multimodal perception and planning, which we are excited to keep working on. 6 Conclusion In this paper, we have presented SIMURA, a general goal-oriented architecture for optimal agent decision-making. Empowered by simulation-based planning using world model and modeling of agent internal belief space activities using natural language as latent representation, we see significant and strong improvements on a range of tasks in web browsing experiments, with world model-based reasoning showing improved reasoning capacity compared to LLM autoregressive reasoning. We are very excited about the possibilities for a single, general, superintelligent agent, but are also keenly aware of the risks for individuals and societies. On the capability side, we aim to test on more types of environments (e.g., software development) and continue developing functional components that strengthen the agent (e.g., multi-agent interaction and "}
{"id": "2507.23773v1_chunk55", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "e capability side, we aim to test on more types of environments (e.g., software development) and continue developing functional components that strengthen the agent (e.g., multi-agent interaction and long-term memory). On the safety / alignment side, we look forward to engaging the community in discussions about how to ensure such an agent stays aligned with our shared values, priorities, and welfare. Acknowledgement This work was supported in part by the Samsung GRO Project “Efficient Designs for Generative and Agent LLM Development.” We thank Zhoujun Cheng, Shibo Hao, and Xinyu Pi from MixLab; Han Guo, Nicholas Ho, and Bowen Tan from SAILING Lab; Li Erran Li from AWS, Zora Wang from NeuLab; and Sarah Cheah and Hector Ren from MBZUAI for their insightful feedback and discussions. We are also grateful for their helpful suggestions throughout the project. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily"}
{"id": "2507.23773v1_chunk56", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "lso grateful for their helpful suggestions throughout the project. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Samsung. References [1]OpenAI. Computer -using agent (cua). https://openai.com/index/ computer-using-agent/ , January 2025. Research preview of “Operator”, published January 23, 2025. [2]DeepMind. Project mariner. https://deepmind.google/models/project-mariner/ , 2024. Accessed: 2025-07-16. [3]Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854 , 2023. [4]Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Oswo"}
{"id": "2507.23773v1_chunk57", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, 2024. [5]OpenAI. Introducing Deep Research. https://openai.com/index/ introducing-deep-research/ , February 2025. Deep Research agent release an- nouncement. [6]Anthropic. Introducing computer use, a new claude 3.5 sonnet, and claude 3.5 haiku. https: //www.anthropic.com/news/3-5-models-and-computer-use , October 22 2024. Public beta “computer use” feature for Claude 3.5 Sonnet. [7]Google. Gemini Deep Research: Your Personal Research Assistant. https://gemini. google/overview/deep-research/?hl=en , December 2024. Overview of Gemini Deep Research agent feature. [8]Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactiv"}
{"id": "2507.23773v1_chunk58", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " 2024. Overview of Gemini Deep Research agent feature. [8]Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior, 2023. 14[9]Anysphere Inc. Cursor: The ai code editor. https://cursor.com , 2025. Accessed: 2025-07- 16. [10] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for ai software developers as generalist agents, 2025. [11] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, A"}
{"id": "2507.23773v1_chunk59", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "u, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, Yossi Matias, Andrew Carroll, Kavita Kulkarni, Nenad Tomasev, Yuan Guan, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Tiago R D Costa, José R Penadés, Gary Peltz, Yunhan Xu, Annalisa Pawlosky, Alan Karthikesalingam, and Vivek Natarajan. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864 , February 2025. URL: https://storage.googleapis.com/coscientist_paper/ai_coscientist.pdf . [12] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search, 2025. [13] Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, Yuheng Cheng, Suyuchen Wang, Xiao"}
{"id": "2507.23773v1_chunk60", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "very via agentic tree search, 2025. [13] Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, Yuheng Cheng, Suyuchen Wang, Xiaoqiang Wang, Yuyu Luo, Haibo Jin, Peiyan Zhang, Ollie Liu, Jiaqi Chen, Huan Zhang, Zhaoyang Yu, Haochen Shi, Boyan Li, Dekun Wu, Fengwei Teng, Xiaojun Jia, Jiawei Xu, Jinyu Xiang, Yizhang Lin, Tianming Liu, Tongliang Liu, Yu Su, Huan Sun, Glen Berseth, Jianyun Nie, Ian Foster, Logan Ward, Qingyun Wu, Yu Gu, Mingchen Zhuge, Xiangru Tang, Haohan Wang, Jiaxuan You, Chi Wang, Jian Pei, Qiang Yang, Xiaoliang Qi, and Chenglin Wu. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems, 2025. [14] Trishna Chakraborty, Udita Ghosh, Xiaopan Zhang, Fahim Faisal Niloy, Yue Dong, Jiachen Li, Amit K. Roy-Chowdhury, and Chengyu Song. Heal: An empirical study on hallucinations in embodied agents driven by large language models, 2025. "}
{"id": "2507.23773v1_chunk61", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " Xiaopan Zhang, Fahim Faisal Niloy, Yue Dong, Jiachen Li, Amit K. Roy-Chowdhury, and Chengyu Song. Heal: An empirical study on hallucinations in embodied agents driven by large language models, 2025. [15] Cewu Lu and Shiquan Wang. The general-purpose intelligent agent. Engineering , 6(3):221–226, 2020. [16] Jacob Andreas. Language models as agent models, 2022. [17] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations , 2023. [18] Yann LeCun. A path towards autonomous machine intelligence. OpenReview preprint, June 2022. Version 0.9.2, June 27 2022. [19] Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221 ,"}
{"id": "2507.23773v1_chunk62", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221 , 2024. [20] Brandon Chiou, Mason Choey, Mingkai Deng, Jinyu Hou, Jackie Wang, Ariel Wu, Frank Xu, Zhiting Hu, Hongxia Jin, Li Erran Li, Graham Neubig, Yilin Shen, and Eric P. Xing. Reasoneragent: A fully open source, ready-to-run agent that does research in a web browser and answers your queries, February 2025. [21] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: A large language model-based web navigating agent, 2024. [22] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents, 2024. 15[23] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda"}
{"id": "2507.23773v1_chunk63", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "n, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents, 2024. 15[23] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. Ui-tars: Pioneering automated gui interaction with native agents, 2025. [24] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory, 2024. [25] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. V oyager: An open-ended embodied agent with large language models, 2023. [26] Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, "}
{"id": "2507.23773v1_chunk64", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " and Anima Anandkumar. V oyager: An open-ended embodied agent with large language models, 2023. [26] Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232 , 2024. [27] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action- conditional video prediction using deep networks in atari games, 2015. [28] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. Nature , 588(7839):604–609, December 2020. [29] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization, 2021. [30] Nicklas Hansen, Xiaolon"}
{"id": "2507.23773v1_chunk65", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ature , 588(7839):604–609, December 2020. [29] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization, 2021. [30] Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive control, 2022. [31] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model, 2023. [32] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models, 2024. [33] Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, and Yu Su. Is your llm secretly a world model of the internet? model-based planning for web agents, 2025. [34] Lisa Feldman Barrett. How emotions are made: The secret life of the brain . Pan Macmillan, 2017. [35] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge,"}
{"id": "2507.23773v1_chunk66", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "agents, 2025. [34] Lisa Feldman Barrett. How emotions are made: The secret life of the brain . Pan Macmillan, 2017. [35] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024. [36] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models, 2024. [37] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2024. [38] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdar"}
{"id": "2507.23773v1_chunk67", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2024. [38] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis, 2024. 16[39] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In International Conference on Learning Representations (ICLR) , 2018. [40] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web, 2023. [41] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents, 2023. [42] Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Q"}
{"id": "2507.23773v1_chunk68", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ebshop: Towards scalable real-world web interaction with grounded language agents, 2023. [42] Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, and Guohao Li. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation, 2025. [43] Xun Jiang, Feng Li, Han Zhao, Jiahao Qiu, Jiaying Wang, Jun Shao, Shihao Xu, Shu Zhang, Weiling Chen, Xavier Tang, Yize Chen, Mengyue Wu, Weizhi Ma, Mengdi Wang, and Tianqiao Chen. Long term memory: The foundation of ai self-evolution, 2025. [44] Wentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorchestra: A hierarchical multi-agent framework for general-purpose task solving, 2025. [45] Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang, Zhu, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter C"}
{"id": "2507.23773v1_chunk69", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "sk solving, 2025. [45] Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang, Zhu, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter Chang, Ricky Loynd, Robert West, Victor Dibia, Ahmed Awadallah, Ece Kamar, Rafah Hosn, and Saleema Amershi. Magentic-one: A generalist multi-agent system for solving complex tasks, 2024. [46] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents, 2024. [47] Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunismäki. ‘smolagents‘: a smol library to build great agentic systems. https://github. com/huggingface/smolagents , 2025. [48] Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, Xing Zhou, Dongrui Liu, Ling Yang, Yue Wu, Kaixuan Huang, Shilong Liu, Hongru Wang, and Mengdi Wang. Alita: Generalist agent ena"}
{"id": "2507.23773v1_chunk70", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "an, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, Xing Zhou, Dongrui Liu, Ling Yang, Yue Wu, Kaixuan Huang, Shilong Liu, Hongru Wang, and Mengdi Wang. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution, 2025. [49] Eric Xing, Mingkai Deng, Jinyu Hou, and Zhiting Hu. Critiques of world models. arXiv preprint arXiv:2507.05169 , 2025. [50] Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction , volume 1. MIT press Cambridge, 1998. [51] Daniel Kahneman. Thinking, fast and slow . macmillan, 2011. [52] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess- che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas- tering the game of go with deep neural networks and tree search. nature , 529(7587):484–489, 2016. [53] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez"}
{"id": "2507.23773v1_chunk71", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " tering the game of go with deep neural networks and tree search. nature , 529(7587):484–489, 2016. [53] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815 , 2017. [54] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin , 2(4):160–163, 1991. [55] Zhiting Hu and Tianmin Shu. Language models, agent models, and world models: The law for machine reasoning and planning. arXiv preprint arXiv:2312.05230 , 2023. 17[56] OpenAI. Introducing chatgpt search. https://openai.com/index/ introducing-chatgpt-search/ , 2024. Accessed: 2024-12-19. [57] Perplexity. Getting started with perplexity. https://www.perplexity.ai/hub/blog/ getting-started-with-perplexity , 2024. Accessed: 2024-12-19. [58] Yu Gu, Boyuan Zheng"}
{"id": "2507.23773v1_chunk72", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "2024. Accessed: 2024-12-19. [57] Perplexity. Getting started with perplexity. https://www.perplexity.ai/hub/blog/ getting-started-with-perplexity , 2024. Accessed: 2024-12-19. [58] Yu Gu, Boyuan Zheng, Boyu Gou, Kai Zhang, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, and Yu Su. Is your llm secretly a world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559 , 2024. [59] Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents. arXiv preprint arXiv:2407.01476 , 2024. [60] Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. Assistantbench: Can web agents solve realistic and time-consuming tasks? arXiv preprint arXiv:2407.15711 , 2024. [61] Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P Xing, and Zhiting Hu. Compression, transduction, and creation: A unified framework for evaluating natural language generation. arXiv preprint arXiv:210"}
{"id": "2507.23773v1_chunk73", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P Xing, and Zhiting Hu. Compression, transduction, and creation: A unified framework for evaluating natural language generation. arXiv preprint arXiv:2109.06379 , 2021. [62] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G- eval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 2511–2522, Singapore, December 2023. Association for Computational Linguistics. [63] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web agents at solving common knowledge work tasks? In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International"}
{"id": "2507.23773v1_chunk74", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ommon knowledge work tasks? In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning , volume 235 of Proceedings of Machine Learning Research , pages 11642–11662. PMLR, 21–27 Jul 2024. [64] Andrew Zhu, Alyssa Hwang, Liam Dugan, and Chris Callison-Burch. FanOutQA: A multi-hop, multi-document question answering benchmark for large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 18–37, Bangkok, Thailand, August 2024. Association for Computational Linguistics. 18A Details on Web Browsing Environment Example Observation URL https://www.google.com/travel/flights Scroll Position: 0, Window Height: 720, Webpage Height: 3024, Remaining Pixels: 2304, Scrolling Progress: 23.8% RootWebArea ’Google Flights -"}
{"id": "2507.23773v1_chunk75", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "mple Observation URL https://www.google.com/travel/flights Scroll Position: 0, Window Height: 720, Webpage Height: 3024, Remaining Pixels: 2304, Scrolling Progress: 23.8% RootWebArea ’Google Flights - Find Cheap Flight Options & Track Prices’ [149] banner ’’ [160] button ’Main menu’, clickable, expanded=False [161] image ’’ [168] link ’Google’, clickable StaticText ’Skip to main content’ StaticText ’Accessibility feedback’ [186] navigation ’’ [189] link ’Travel’ [193] image ’’ [197] link ’Explore’ [201] image ’’ [205] link ’Flights’ [209] image ’’ [213] link ’Hotels’ [217] image ’’ [221] link ’Vacation rentals’ [225] image ’’ [235] button ’Change appearance’, hasPopup=’menu’, expanded=False [240] image ’’ [249] button ’Google apps’, clickable, expanded=False [250] image ’’ [251] link ’Sign in’, clickable [342] image ’’ StaticText ’Flights’ [346] search ’Flight’ [355] combobox ’Change ticket type. \\u200bRound trip’, live=’polite’, relevant=’additions text’, hasPopup=’listbox’, expanded="}
{"id": "2507.23773v1_chunk76", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " in’, clickable [342] image ’’ StaticText ’Flights’ [346] search ’Flight’ [355] combobox ’Change ticket type. \\u200bRound trip’, live=’polite’, relevant=’additions text’, hasPopup=’listbox’, expanded=False, controls=’i9’ [364] image ’’ [399] button ’1 passenger, change number of passengers.’, hasPopup=’dialog’ [404] image ’’ [406] image ’’ [522] combobox ’Change seating class. \\u200bEconomy’, live=’polite’, relevant=’additions text’, hasPopup=’listbox’, expanded=False, controls=’i22’ [529] image ’’ [576] combobox ’Where from?’ value=’Pittsburgh’, clickable, autocomplete=’inline’, hasPopup=’menu’, expanded=False [580] image ’’ [628] button ’Swap origin and destination.’, disabled=True [631] image ’’ [638] combobox ’Where to?’, clickable, focused, autocomplete=’inline’, hasPopup=’menu’, expanded=False [641] image ’’ generic ’’, hidden=True [690] image ’’ [691] textbox ’Departure’, clickable, describedby=’i32’ [712] textbox ’Return’, clickable, describedby=’i32’ generic ’’, hidden=True [8"}
{"id": "2507.23773v1_chunk77", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "nded=False [641] image ’’ generic ’’, hidden=True [690] image ’’ [691] textbox ’Departure’, clickable, describedby=’i32’ [712] textbox ’Return’, clickable, describedby=’i32’ generic ’’, hidden=True [857] button ’Explore destinations’ [866] Section ’’ [867] heading ’Find cheap flights from Pittsburgh to anywhereMore information on suggested flights.’ [871] button ’More information on suggested flights.’, hasPopup=’menu’ [873] image ’’ [904] list ’’, clickable [905] listitem ’’ StaticText ’Pittsburgh’ [907] listitem ’’ [908] button ’Cleveland’ [909] listitem ’’ [910] button ’Columbus’ [911] listitem ’’ [912] button ’Akron’ StaticText ’San Francisco’ StaticText ’\\$128’ StaticText ’Jan 9 — Jan 16’ StaticText ’1 stop’ StaticText ’ ·’ StaticText ’10 hr 30 min’ StaticText ’New York’ StaticText ’\\$68’ StaticText ’Dec 7 — Dec 14’ 19B Prompts for Web Browsing Implementation Prompt for Agent Identity # Name: Web Browsing Agent # Description: An information and automation assistant who responds to"}
{"id": "2507.23773v1_chunk78", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "$68’ StaticText ’Dec 7 — Dec 14’ 19B Prompts for Web Browsing Implementation Prompt for Agent Identity # Name: Web Browsing Agent # Description: An information and automation assistant who responds to user instructions by browsing the internet. The assistant strives to answer each question accurately, thoroughly, efficiently, and politely, and to be forthright when it is impossible to answer the question or carry out the instruction. The assistant will end the task once it sends a message to the user. # Observation Space: The text representation and screenshot of the part of webpage visible in the viewport of a browser. Here is an abstract description of the information available in the webpage text representation: - Identification Information: - URL: The web address that specifies the location of the webpage. -Document Properties: Attributes such as scroll position and viewport dimensions that describe the current viewing context. - Structural Hierarchy: -Root Element: The primary con"}
{"id": "2507.23773v1_chunk79", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ation of the webpage. -Document Properties: Attributes such as scroll position and viewport dimensions that describe the current viewing context. - Structural Hierarchy: -Root Element: The primary container for the webpage, indicating its overall theme or purpose. -Nested Elements: A hierarchy of sections, containers, and components that organize content logically (e.g., headers, footers, sidebars). - Interactive Components: -Links: Elements that can be clicked to navigate to other pages or sections, often labeled descriptively. -Buttons: Interactive controls that trigger actions (e.g., submitting forms, opening menus). - Content Types: -Text: Main content, headings, and subheadings that provide information and context. -Images and Media: Visual elements that enhance the understanding or appeal of the content. -Forms and Inputs: Fields for user input, including text boxes, dropdowns, and checkboxes. - Functional Areas: -Navigation Menus: Organized sets of links that allow users to expl"}
{"id": "2507.23773v1_chunk80", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ppeal of the content. -Forms and Inputs: Fields for user input, including text boxes, dropdowns, and checkboxes. - Functional Areas: -Navigation Menus: Organized sets of links that allow users to explore different sections of the site. -Search Interface: Components that enable users to search for content within the site, including input fields and associated buttons. - State Information: -Visibility and Expand/Collapse States: Indicators showing whether certain elements are active, visible, or in a collapsed state, impacting user interaction. -Focus States: Information on which elements are currently focused, important for keyboard navigation and accessibility. 20Prompt for Agent Identity (Continued) - Accessibility Features: -Role and Description Information: Metadata that provides context about the purpose of elements, useful for screen readers and assistive technologies. - General User Considerations: - Navigation: Recognizing how to traverse the webpage using links and buttons. - I"}
{"id": "2507.23773v1_chunk81", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "t about the purpose of elements, useful for screen readers and assistive technologies. - General User Considerations: - Navigation: Recognizing how to traverse the webpage using links and buttons. - Interactivity: Understanding how to engage with forms, search fields, and dynamic components. -Content Engagement: Identifying and interpreting various content types to glean necessary information. # Action Space: 13 different types of actions are available. noop(wait_ms: float = 1000) Examples: noop() noop(500) send_msg_to_user(text: str) Examples: send_msg_to_user(’Based on the results of my search, the city was built in 1751.’) scroll(delta_x: float, delta_y: float) Examples: scroll(0, 200) scroll(-50.2, -100.5) fill(bid: str, value: str) Examples: fill(’237’, ’example value’) fill(’45’, ’multi-line\\nexample’) fill(’a12’, ’example with \"quotes\"’) select_option(bid: str, options: str | list[str]) Examples: select_option(’a48’, ’blue’) select_option(’c48’, [’red’, ’green’, ’blue’]) click(b"}
{"id": "2507.23773v1_chunk82", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "lti-line\\nexample’) fill(’a12’, ’example with \"quotes\"’) select_option(bid: str, options: str | list[str]) Examples: select_option(’a48’, ’blue’) select_option(’c48’, [’red’, ’green’, ’blue’]) click(bid: str, button: Literal[’left’, ’middle’, ’right’] = ’left’, modifiers: list[typing.Literal[’Alt’, ’Control’, ’Meta’, ’Shift’]] = []) Examples: click(’a51’) click(’b22’, button=’right’) click(’48’, button=’middle’, modifiers=[’Shift’]) dblclick(bid: str, button: Literal[’left’, ’middle’, ’right’] = ’left’, modifiers: list[typing.Literal[’Alt’, ’Control’, ’Meta’, ’Shift’]] = []) Examples: dblclick(’12’) dblclick(’ca42’, button=’right’) dblclick(’178’, button=’middle’, modifiers=[’Shift’]) 21Prompt for Agent Identity (Continued) hover(bid: str) Examples: hover(’b8’) press(bid: str, key_comb: str) Examples: press(’88’, ’Backspace’) press(’a26’, ’Control+a’) press(’a61’, ’Meta+Shift+t’) focus(bid: str) Examples: focus(’b455’) clear(bid: str) Examples: clear(’996’) drag_and_drop(from_bid: str,"}
{"id": "2507.23773v1_chunk83", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "r) Examples: press(’88’, ’Backspace’) press(’a26’, ’Control+a’) press(’a61’, ’Meta+Shift+t’) focus(bid: str) Examples: focus(’b455’) clear(bid: str) Examples: clear(’996’) drag_and_drop(from_bid: str, to_bid: str) Examples: drag_and_drop(’56’, ’498’) upload_file(bid: str, file: str | list[str]) Examples: upload_file(’572’, ’my_receipt.pdf’) upload_file(’63’, [’/home/bob/Documents/image.jpg’, ’/home/bob/Documents/- file.zip’]) Only a single action can be provided at once. Example: fill(’a12’, ’example with \"quotes\"’) # Instruction: {user_instruction} # Current Date and Time: {current_datetime} Prompt for Encoder # Observation: {observation} # State: Describe all the elements in the current webpage observation. Note any dialogs, progress indicators, or error messages. Include any interactive elements and their values or if they are blank. Note any detailed information such as facts, entities, or data that are relevant to the task. Report any error messages like whether the last action wa"}
{"id": "2507.23773v1_chunk84", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " elements and their values or if they are blank. Note any detailed information such as facts, entities, or data that are relevant to the task. Report any error messages like whether the last action was correct. Try to be as comprehensive and detailed as possible. Wrap your response in the tag <state> and </state>. 22Prompt for Policy {memory} # Current State: {state} # Intent: Describe the action the assistant should take next to carry out the user’s instruction. Avoid using phrases such as \"To accomplish the goal,\" \"I will,\" \"To proceed.\". Avoid ending with phrases like \"to execute the search.\" Describe one action at a time and avoid combining multiple steps. Refrain from mentioning specific element IDs as they may change during execution. Limit your response to one phrase and include any details that help select the correct action. Be creative and propose novel methods to achieve the goal. Avoid creating accounts without user permission or providing personal information. Concrete exa"}
{"id": "2507.23773v1_chunk85", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " details that help select the correct action. Be creative and propose novel methods to achieve the goal. Avoid creating accounts without user permission or providing personal information. Concrete example would be \"Go to the home page of Google Flights.\" and \"Click on the ’Search’ button.\" Wrap your response in the following format: <think> Your thoughts and reasoning process </think> <intent> Description of the action to perform next </intent> Prompt for World Model {memory} # Current State: {state} # Memory Update: {memory_update} # Action Intent: {plan} # Next State: Describe all the elements in the webpage after the agent attempts to carry out the intent. Note that the execution may not be successful, so you will have to infer the result of the action. Note any dialogs, progress indicators, or error messages. Include any interactive elements and their values or if they are blank. Note any detailed information such as facts, entities, or data that are relevant to the task. Report an"}
{"id": "2507.23773v1_chunk86", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "tors, or error messages. Include any interactive elements and their values or if they are blank. Note any detailed information such as facts, entities, or data that are relevant to the task. Report any error messages displayed. Try to be as comprehensive and detailed as possible. Wrap your response in the following format: <next_state> Follow the format of the current state description. Use present tense. Avoid starting phrases like \"Based on the interaction history, current state, and current intent\". </next_state> 23Prompt for Critic {memory} # Final State: {state} # Task Success and Progress: Your task is to evaluate the performance of the agent. Given the agent’s instruction, interaction history, the final state of the webpage, and the agent’s responses to the user if any, your goal is to decide whether the agent’s execution is successful or not. If the current state is a failure but it looks like the agent is on the right track towards success, you should also output as such. Wrap"}
{"id": "2507.23773v1_chunk87", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " to decide whether the agent’s execution is successful or not. If the current state is a failure but it looks like the agent is on the right track towards success, you should also output as such. Wrap your response in the following format: <think> Your thoughts and reasoning process </think> <status> \"success\" or \"failure\" </status> <on_the_right_track> \"yes\" or \"no\" </on_the_right_track> Prompt for Memory Update {memory} # State: {state} # Action Intent: {plan} # Memory Update: Summarize the changes in the webpage observation that should be remembered for achieving your goal and for predicting the next state. Note any new elements, any elements no longer visible, or any changes in the content of existing elements. Also note if there is no change. Include any other inferred information that may help you decide the next action, such as whether an action intent is successful, or whether progress has been made or reversed. Do not include your next planned actions. Revise your belief from "}
{"id": "2507.23773v1_chunk88", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "t may help you decide the next action, such as whether an action intent is successful, or whether progress has been made or reversed. Do not include your next planned actions. Revise your belief from previous history if the current state contradicts it. Wrap your response in the tag <memory_update> and </memory_update>. 24Prompt for Actor {memory} # Observation: {observation} # Current State: {state} # Current Intent: {plan} # Action: Choose an API call that will carry out the intent when executed in the webpage. Use only one action at a time. You must not enclose bid inputs in [brackets] but instead in ’single quotes’. Interact only with elements in the current step observation. Your response will be executed as a Python function call, so ensure it adheres to the format and argument data type specifications defined in the action space. Wrap your response in the tag <action> and </action>. 25Prompt for Action Clustering Here is the action space for a browser agent to navigate in a webp"}
{"id": "2507.23773v1_chunk89", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "type specifications defined in the action space. Wrap your response in the tag <action> and </action>. 25Prompt for Action Clustering Here is the action space for a browser agent to navigate in a webpage: 16 different types of actions are available: noop(wait_ms: float = 1000) send_msg_to_user(text: str) scroll(delta_x: float, delta_y: float) fill(bid: str, value: str) select_option(bid: str, options: str | list[str]) click(bid: str, button: Literal[’left’, ’middle’, ’right’] = ’left’, modifiers: list[typing.Literal[’Alt’, ’Control’, ’Meta’, ’Shift’]] = []) dblclick(bid: str, button: Literal[’left’, ’middle’, ’right’] = ’left’, modifiers: list[typing.Literal[’Alt’, ’Control’, ’Meta’, ’Shift’]] = []) hover(bid: str) press(bid: str, key_comb: str) focus(bid: str) clear(bid: str) drag_and_drop(from_bid: str, to_bid: str) upload_file(bid: str, file: str | list[str]) go_back() go_forward() goto(url: str) Only a single action can be provided at once. Example: fill(’a12’, ’example with \"quote"}
{"id": "2507.23773v1_chunk90", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "p(from_bid: str, to_bid: str) upload_file(bid: str, file: str | list[str]) go_back() go_forward() goto(url: str) Only a single action can be provided at once. Example: fill(’a12’, ’example with \"quotes\"’) Below, you will find lists of intents, or natural language descriptions of actions that, when executed, will translate to one of the function calls above. The intents will be provided in the following JSON format: ‘‘‘json { \"intent_id\": \"intent description\" } ‘‘‘ Your task is to cluster list of intents into semantically equivalent groups, where each group represents intents that lead to the same action when executed (i.e., navigating to the Google homepage is translated to goto(’https://www.google.com’)) and would therefore correspond to the same API call in a Playwright browser. Intents that use different wording but convey the same action should be grouped together. Try to minimize the number of clusters. 26Prompt for Action Clustering (Continued) Represent the clustering results us"}
{"id": "2507.23773v1_chunk91", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "that use different wording but convey the same action should be grouped together. Try to minimize the number of clusters. 26Prompt for Action Clustering (Continued) Represent the clustering results using a JSON object where each cluster has a unique identifier, and each identifier maps to a list of actions in that cluster. See below for an abstract example: ‘‘‘json { \"cluster_id\": { \"intent\": \"representative intent name for this cluster\", \"candidates\": [ \"<list of intent ids that belong to this cluster> ] } } ‘‘‘ Concrete Example 1: Dictionary of Intents: ‘‘‘json { \"0\": \"Navigate to the Google homepage by entering its URL.\", \"1\": \"Go to the Google homepage.\", \"2\": \"Go to the Google homepage\", \"3\": \"Go to the Google homepage by navigating to ’https://www.google.com’\", \"4\": \"Go to the home page of Google\" } ‘‘‘ [\"Navigate to the Google homepage by entering its URL.\", \"Go to the Google home- page.\", \"Go to the Google homepage\", \"Go to the Google homepage by navigating to ¨https://www.goog"}
{"id": "2507.23773v1_chunk92", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "age of Google\" } ‘‘‘ [\"Navigate to the Google homepage by entering its URL.\", \"Go to the Google home- page.\", \"Go to the Google homepage\", \"Go to the Google homepage by navigating to ¨https://www.google.com ¨\", \"Go to the home page of Google\"] Clustering Results: ‘‘‘json { \"cluster_1\": { \"intent\": \"Navigate to the Google homepage\", \"candidates\": [0, 1, 2, 3, 4] } } ‘‘‘ Concrete Example 2: Dictionary of Intents: {action_candidate_json} Clustering Results: 27B.1 Adaptation for WebArena Evaluation Agent Description for WebArena Evaluation An information and automation assistant that interacts with the browser and responds to user instructions. The response follows the following rules: 1. When the intent is a question, and a complete answer to the question has been found, then send the answer to the user; 2. the intent wants to locate specific information or navigate to a particular section of a site, and the current page satisfies, then stop and tell the user you found the required inform"}
{"id": "2507.23773v1_chunk93", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "to the user; 2. the intent wants to locate specific information or navigate to a particular section of a site, and the current page satisfies, then stop and tell the user you found the required information; 3. the intent want to conduct an operation, and has been done, then stop and tell the user the operation has been completed. The assistant should try to achieve the goal in the current site without navigating to sites like Google. Be forthright when it is impossible to answer the question or carry out the task. The assistant will end the task once it sends a message to the user. 28C Prompts for Generating and Evaluating on the FlightQA Dataset Prompt for Generating Initial Constraints and Questions System : You are a creative writer who is an expert at crafting questions to help train assistants who answer user queries. Current date and time: {current_datetime} Instruction : Your task is to create a robust benchmark for evaluating an AI’s ability to search for flights through a plat"}
{"id": "2507.23773v1_chunk94", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "ssistants who answer user queries. Current date and time: {current_datetime} Instruction : Your task is to create a robust benchmark for evaluating an AI’s ability to search for flights through a platform like Google Flights. To ensure the dataset effectively represents real-world use cases. Here are some important factors to consider: 1. Diversity of Queries - Range of Destinations: Include both common and obscure destinations to test how well the model handles varying levels of demand. - Dates and Duration: Include different date ranges, including last-minute flights, peak travel dates (like holidays), and extended trips. Ensure there’s a variety in trip duration as well. - Passenger Variability: Include solo travelers, families, and group travel (e.g., one adult vs. two adults and two children) since these change the search parameters and price results. - Class and Preference: Vary preferences like cabin class (economy, business, first) and filters (non-stop, one stop, preferred air"}
{"id": "2507.23773v1_chunk95", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "hildren) since these change the search parameters and price results. - Class and Preference: Vary preferences like cabin class (economy, business, first) and filters (non-stop, one stop, preferred airlines, etc.). - Budget Constraints: Test price sensitivity by setting different budget limits to see how well the AI handles trade-offs. 2. Complexity of Requirements - Multi-Leg Flights: Add queries for multi-city trips or those requiring complex layovers. - Dynamic Constraints: Include queries with dynamic constraints, such as “find the cheapest flight but depart between 8-10 AM,” to see if the model can adapt its search within specific time frames. - Conditional Preferences: Test cases where users might want options based on multiple conditions, like “either find the cheapest non-stop or the shortest two-stop option.” In practice, the questions typically vary in the following dimensions: - Ticket type (round-trip, one-way, etc.) - Routes (origin and destination) - Layover location(s) - "}
{"id": "2507.23773v1_chunk96", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "e shortest two-stop option.” In practice, the questions typically vary in the following dimensions: - Ticket type (round-trip, one-way, etc.) - Routes (origin and destination) - Layover location(s) - Dates (departure and/or return) - Flight time (departure and arrival) - Total flight time - Airlines - Cabin class (economy, business, etc.) - Aircraft - Eco-friendly options (CO2 Emissions) Given a number of constraints, you should first provide a list of constraints, with the number of constraints equal to the specification. After that, you will generate a question a typical user will ask which imposes those constraints. You should re- peat this for at least 7 times to generate a set of questions with simple language. Make sure that the number of constraints in the question matches the number of constraints specified. Do not include constraints about the number of passengers. If the constraint is a date, you can use relative dates (e.g., \"tomorrow\", \"next month\", \"after 8 PM\", etc.). Avo"}
{"id": "2507.23773v1_chunk97", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "r of constraints specified. Do not include constraints about the number of passengers. If the constraint is a date, you can use relative dates (e.g., \"tomorrow\", \"next month\", \"after 8 PM\", etc.). Avoid using specific dates like \"December 25th\" to ensure the questions are relevant throughout the year. Your response should follow the JSON format below: 29Prompt for Generating Initial Constraints and Questions (Continued) Number of Constraints: <num_constraints> { \"num_constraints\": <num_constraints>, \"questions\": [ { \"constraints\": [<constraints>], \"question\": <question> }, ... ] } Below is a concrete example: Number of Constraints: 3 { \"num_constraints\": 3, \"questions\": [ { \"constraints\": [\"one-way\", \"New York to London\", \"departing next Friday\"], \"question\": \"Can you find a one-way flight from New York to London departing next Friday?\" }, ... ] } 30Prompt for Iteratively Expanding Constraints and Questions System : [Same as above] Instruction : [Same as above until “Your response shou"}
{"id": "2507.23773v1_chunk98", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": " from New York to London departing next Friday?\" }, ... ] } 30Prompt for Iteratively Expanding Constraints and Questions System : [Same as above] Instruction : [Same as above until “Your response should follow”] Your response should follow the JSON format below: Maximum number of constraints: <max_constraints> Starting constraints and questions: { \"num_constraints\": <num_constraints>, \"constraints\": [<constraints>], \"question\": <question> } Questions with increasing complexity: { \"questions\": [ { \"num_constraints\": <starting num_constraints + 1>, \"constraints\": [<previous constraints with 1 additional>], \"question\": <question> }, { \"num_constraints\": <starting num_constraints + 2>, \"constraints\": [<previous constraints with 2 additional>], \"question\": <question> }, ... (continue until reaching the maximum number of constraints) ] } Your Response: Maximum number of constraints: {max_num_constraints} Starting constraints and questions: {starting_constraint_questions} Questions with incre"}
{"id": "2507.23773v1_chunk99", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "hing the maximum number of constraints) ] } Your Response: Maximum number of constraints: {max_num_constraints} Starting constraints and questions: {starting_constraint_questions} Questions with increasing complexity: 31Prompt for Evaluation # Interaction Date and Time: {interaction_datetime} # Interaction History: [Concatenation of observations from all steps] Above are the webpages an assistant interacted with while trying to answer the user’s query. The user is looking for flights with the following constraints: {constraints} Here is the exact query provided by the user: {goal} Here is the assistant’s response: {message} Your task is to evaluate two aspects of the response: 1) Whether the assistant’s response is supported by the interaction history, and 2) Whether the assistant’s response satisfies the user constraints to the extent al- lowed by the results. Some Context: - To determine the seating class of a flight being returned, refer to the value of the \"Change seating class\" co"}
{"id": "2507.23773v1_chunk100", "paper_id": "2507.23773v1", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "text": "se satisfies the user constraints to the extent al- lowed by the results. Some Context: - To determine the seating class of a flight being returned, refer to the value of the \"Change seating class\" combobox. - It is not always possible to satisfy all the user constraints. In this case, examine whether the response is as close to the user constraints as possible. Answer in the following format: <think> Your thoughts and reasoning. </think> <grounding> Your assessment of whether the response is supported by the interaction history. Answer \"yes\" or \"no\" </grounding> <relevance> Your assessment of whether the response satisfies the user constraints to the extent allowed by the results. Answer \"yes\" or \"no\" </relevance> 32"}
{"id": "2507.23771v1_chunk0", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "Consensus-Driven Active Model Selection Justin Kay* MITGrant Van Horn UMass AmherstSubhransu Maji UMass AmherstDaniel Sheldon UMass AmherstSara Beery MIT Abstract The widespread availability of off-the-shelf machine learning models poses a challenge: which model, of the many available candidates, should be chosen for a given data analysis task? This question of model selection is traditionally answered by collecting and annotating a val- idation dataset—a costly and time-intensive process. We propose a method for active model selection, using predic- tions from candidate models to prioritize the labeling of test data points that efficiently differentiate the best can- didate. Our method, CODA , performs consensus- driven active model selection by modeling relationships between classifiers, categories, and data points within a probabilis- tic framework. The framework uses the consensus and dis- agreement between models in the candidate pool to guide the label acquisition process, and Ba"}
{"id": "2507.23771v1_chunk1", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": " categories, and data points within a probabilis- tic framework. The framework uses the consensus and dis- agreement between models in the candidate pool to guide the label acquisition process, and Bayesian inference to up- date beliefs about which model is best as more information is collected. We validate our approach by curating a col- lection of 26 benchmark tasks capturing a range of model selection scenarios. CODA outperforms existing methods for active model selection significantly, reducing the anno- tation effort required to discover the best model by upwards of 70% compared to the previous state-of-the-art. Code and data are available at: https://github.com/justinkay/coda. 1. Introduction The availability of off-the-shelf machine learning models is growing rapidly. As of this writing there are over 1.9M pre-trained models available for download from the Hug- gingFace Models repository [22], ranging from small spe- cialized models to large general-purpose foundation mod- els. "}
{"id": "2507.23771v1_chunk2", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ting there are over 1.9M pre-trained models available for download from the Hug- gingFace Models repository [22], ranging from small spe- cialized models to large general-purpose foundation mod- els. Application-specific model zoos are growing as well, curating sets of models for everything from wildlife moni- toring [18] to medicine [8, 46]. These zoos potentially en- able accurate data analysis without the need for custom ML development, but introduce a new challenge: which model, of the many available, performs best for a given set of data? Traditionally model selection decisions are made by collecting labels for a subset of the data in question and *Correspondence to: kayj@mit.edu 20 40 60 80 100 Number of labels queried0.00.20.40.60.81.0Fraction of benchmarks converged to < 1% regretModel Selection Label Efficiency Random Sampling Uncertainty Active TestingVMA ModelSelector CODA (Ours)Figure 1. We introduce CODA, a consensus-driven method for active model selection. This figure sh"}
{"id": "2507.23771v1_chunk3", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "odel Selection Label Efficiency Random Sampling Uncertainty Active TestingVMA ModelSelector CODA (Ours)Figure 1. We introduce CODA, a consensus-driven method for active model selection. This figure shows the number of labels needed to converge to the optimal or near-optimal (within 1% ac- curacy) model in a benchmark suite of 26 model selection tasks. CODA is significantly more label-efficient than prior work, iden- tifying a near-optimal model with fewer than 25 labeled examples over 50% of the time, and with fewer than 100 labeled examples over 80% of the time. evaluating the performance of each model on that subset. To ensure results are robust, these datasets need to be large, representing significant human effort for each new dataset. While reducing human effort during training has been well-studied [35, 59], efficient model selection at test time is relatively unexplored. Progress on this challenge will be beneficial for both users of pre-trained models and for researchers design"}
{"id": "2507.23771v1_chunk4", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "en well-studied [35, 59], efficient model selection at test time is relatively unexplored. Progress on this challenge will be beneficial for both users of pre-trained models and for researchers designing label-efficient algorithms. In partic- ular, the field of unsupervised domain adaptation (UDA) proposes to adapt algorithms to new data without any hu- man labels whatsoever, yet successful UDA methods are highly dependent on the use of human-labeled validation sets for model selection [15, 24, 28, 29, 42]. This contra- diction has motivated work in unsupervised model selec- tion [43, 56, 70], but so far these methods have proven to be unreliable, especially in challenging real-world condi- tions [15, 24, 28, 42]. Recently, methods for active model selection have been 1arXiv:2507.23771v1 [cs.LG] 31 Jul 2025proposed to identify an optimal model from a candidate set with fewer labels than required by traditional fully-labeled validation [25, 40, 45, 58]. Active methods use model pre- dic"}
{"id": "2507.23771v1_chunk5", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "cs.LG] 31 Jul 2025proposed to identify an optimal model from a candidate set with fewer labels than required by traditional fully-labeled validation [25, 40, 45, 58]. Active methods use model pre- dictions to guide the label acquisition process, iteratively querying a human expert for labels on specific data points that are expected to be most informative. Though promis- ing, prior work remains label-inefficient, often requiring several hundred to several thousand labels to reliably per- form model selection [40, 45, 58]. There are two key limi- tations that lead to this inefficiency: (1) models are largely treated independently of each other, both before and during the label collection process, ignoring valuable information captured by model agreement and disagreement; (2) cate- gories are also treated independently, ignoring correlations between data points that can be deduced from category- specific model errors. In this paper we propose a novel consensus- driven active model select"}
{"id": "2507.23771v1_chunk6", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "e also treated independently, ignoring correlations between data points that can be deduced from category- specific model errors. In this paper we propose a novel consensus- driven active model selection method, CODA , to address these limita- tions. Our approach models relationships between classi- fiers, categories, and data points in order to make more in- formed label queries. To do this we revisit classical prob- abilistic models of the classification data generating pro- cess. Specifically, we propose a framework inspired by the Dawid and Skene model of annotator agreement [14, 47], whereby each classifier is represented by a confusion matrix that captures its per-category performance characteristics. We adapt this framework for active model selection by con- structing a probabilistic estimate over which model is best that accounts for per-category classifier consensus and un- certainty. We then iteratively query for ground-truth labels on data points that are expected to provide"}
{"id": "2507.23771v1_chunk7", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "c estimate over which model is best that accounts for per-category classifier consensus and un- certainty. We then iteratively query for ground-truth labels on data points that are expected to provide maximal infor- mation about the probability that each model is the best. We validate our approach by curating a benchmarking suite of 26 model selection tasks representing a variety of real-world use cases across computer vision and natural lan- guage processing, which we publish alongside our method to support future model selection research. Our approach exceeds, often significantly, the performance of the previ- ous state-of-the-art on 18 out of these 26 tasks. In addition, our method is exceptionally label-efficient, often requiring fewer than 25 labeled examples to identify the best or near- best model (see Fig. 1). In summary, our main contributions are the following: 1. We introduce CODA, a novel method for active model selection. CODA leverages model consensuses and Bayesian infer"}
{"id": "2507.23771v1_chunk8", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ear- best model (see Fig. 1). In summary, our main contributions are the following: 1. We introduce CODA, a novel method for active model selection. CODA leverages model consensuses and Bayesian inference to identify the most informative la- bels for performing model selection at test time. 2. We curate a benchmarking suite of 26 active model se- lection tasks to validate our approach and compare with prior work. We release the data publicly to support fu- ture research in active model selection. 3. We demonstrate that CODA achieves state-of-the-art performance on 18 out of 26 tasks in our benchmark.Additionally, though not the main focus of our work, we show that CODA’s initialization routine allows us to match or exceed state-of-the-art unsupervised model se- lection results on 20 out of 26 tasks. 2. Related work Model selection in machine learning is typically per- formed using a held-out “validation” set to select between different candidate algorithms, hyperparameters, and/or trai"}
{"id": "2507.23771v1_chunk9", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": " 26 tasks. 2. Related work Model selection in machine learning is typically per- formed using a held-out “validation” set to select between different candidate algorithms, hyperparameters, and/or training checkpoints. Differences between training, valida- tion, and test distributions create challenges for model se- lection [3, 27, 30]. While in some cases out-of-distribution accuracy has been observed to be highly linearly correlated with in-distribution accuracy [41, 51, 52], in others it has been observed to be uncorrelated or even negatively corre- lated [41, 57, 64], indicating that reliable model selection for a dataset of interest cannot in general be performed us- ing a validation set from a different data distribution. This has implications both for pre-trained models sourced from model zoos, where little or nothing is known about the train- ing data distribution, as well as for models trained on one distribution and deployed on another. Our experiments and benchmarking suite e"}
{"id": "2507.23771v1_chunk10", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "model zoos, where little or nothing is known about the train- ing data distribution, as well as for models trained on one distribution and deployed on another. Our experiments and benchmarking suite evaluate both settings. Unsupervised model selection methods perform model se- lection without the use of test labels. These methods have largely been proposed in the context of unsupervised do- main adaptation , where unlabeled test-domain data is avail- able for training. Unsupervised validation methods in this setting typically compute a heuristic such as entropy based on model predictions on this unlabeled test-domain data, under the assumption that these measures are correlated with accuracy [43, 53, 56, 69]. Alternatively, methods may use the accuracy on labeled in-distribution examples, weighted by their “similarity” to out-of-distribution sam- ples, as a proxy [70]. Unfortunately, many of these methods have been shown to be poorly or even negatively correlated with test-domain accur"}
{"id": "2507.23771v1_chunk11", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "weighted by their “similarity” to out-of-distribution sam- ples, as a proxy [70]. Unfortunately, many of these methods have been shown to be poorly or even negatively correlated with test-domain accuracy [15, 24, 42]. One key limitation of this family of methods is that they consider the predictions of individual model checkpoints in isolation from any other models being evaluated; in con- trast, recent work has identified the possibility to utilize the predictions of all models concurrently to better estimate their individual performance [20, 60]. Our method also har- nesses this consensus information, but uses a probabilistic framework to aggregate and update our beliefs about the prediction set over time. Active learning and active testing methods intelligently select informative data points to reduce annotation effort for training and evaluating machine learning models [31, 32, 35, 44, 59]. Active testing is related to our setup but dif- fers in that the goal is to estimate the tes"}
{"id": "2507.23771v1_chunk12", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "oints to reduce annotation effort for training and evaluating machine learning models [31, 32, 35, 44, 59]. Active testing is related to our setup but dif- fers in that the goal is to estimate the test loss of one model, rather than select the best from a set of candidates. Exist- ing methods function by constructing unbiased importance 2sampling estimators of model performance [31]. While it is possible to perform active model selection by perform- ing active testing concurrently for all models and selecting the one with the lowest loss estimate, we will demonstrate this is significantly less label-efficient than specific targeted strategies for active model selection. Active model selection methods have focused predomi- nantly on the online setting, where data points are observed in a stream [25, 26, 37]. In contrast, we focus on the pool- based setting where a static collection of unlabeled data is available from the beginning. Early work in the pool-based setting resembles work in "}
{"id": "2507.23771v1_chunk13", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ream [25, 26, 37]. In contrast, we focus on the pool- based setting where a static collection of unlabeled data is available from the beginning. Early work in the pool-based setting resembles work in active testing, using importance- weighted loss estimates for each model [40, 58]. As pointed out by Kossen et al. [32], these importance-sampling-based approaches exhibit high variance early in the sampling pro- cess, since metrics are computed solely from the labels col- lected so far. Practically, this means model selection re- mains unreliable until a significant number of annotations have been collected. Recent work from Okanovic et al. [45] performs active model selection without importance sampling estimators by defining a simple single-parameter distribution over which model is best at any given time. At each time step, the pos- terior probability for each model being best is updated ac- cording to Posterior =1−ϵ ϵ×Prior if the model gets the label correct, where ϵis a hyperparamet"}
{"id": "2507.23771v1_chunk14", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": " any given time. At each time step, the pos- terior probability for each model being best is updated ac- cording to Posterior =1−ϵ ϵ×Prior if the model gets the label correct, where ϵis a hyperparameter determining the learning rate. This simple probabilistic approach has been shown to be more label-efficient than prior work, but still re- quires significant annotation effort to overcome both its un- informative priors and independence assumptions between data points. Our method addresses these limitations by con- structing informative unsupervised priors and by modeling correlated errors across the test data pool. Probabilistic models of agreement aggregate annotations created by a group of human annotators on a dataset. They do so by modeling a “data generating process” that de- scribes how annotations are created according to latent ran- dom variables like per-annotator accuracy. The Dawid- Skene model [14] (which we describe in more detail in Sec. 4.1) is an early example that rema"}
{"id": "2507.23771v1_chunk15", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "how annotations are created according to latent ran- dom variables like per-annotator accuracy. The Dawid- Skene model [14] (which we describe in more detail in Sec. 4.1) is an early example that remains popular today. Initially proposed for aggregating the predictions of doc- tors regarding patient outcomes, it has since found success in cleaning crowd-sourced annotations [50, 68] and merg- ing human- and AI-generated predictions [6, 63, 65]. Many extensions have been proposed that incorporate Bayesian inference [47, 48] or more complex data generating pro- cesses [7, 19, 68]. We extend the general latent variable framework for active model selection. We do not fit the model directly to predictions as in prior work; instead, we use the framework as a starting point and update it itera- tively to incorporate actively-collected information.3. Active model selection problem formulation Models and data We assume that we have a hypothesis setH={hk}|H| k=1consisting of candidate models from"}
{"id": "2507.23771v1_chunk16", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "vely to incorporate actively-collected information.3. Active model selection problem formulation Models and data We assume that we have a hypothesis setH={hk}|H| k=1consisting of candidate models from which we want to select. Each model has generated pre- dictions on some unlabeled test set D={xi}|D| i=1that we care about but cannot exhaustively annotate. We assume the predictive task is a C-way multi-class classification prob- lem with ˆck,i= arg max chk(xi), where ˆck,i∈ {1, . . . , C } andhk(xi)∈[0,1]Care the class prediction and the C- dimensional prediction vector of model hkon data point xi, respectively. Our setup is agnostic to whether hk(xi)is a soft score vector or one-hot labels. Active data point selection At every time step t, we query a human expert for a single ground truth label yi. The choice of which yito query for is up to the model selection algo- rithm. We partition Dinto disjoint unlabeled and labeled subsets: D=DU∪DL. Once a point has been queried for a ground-tr"}
{"id": "2507.23771v1_chunk17", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "label yi. The choice of which yito query for is up to the model selection algo- rithm. We partition Dinto disjoint unlabeled and labeled subsets: D=DU∪DL. Once a point has been queried for a ground-truth label it moves from DUtoDL. Model selection and evaluation At each time step t, a model selection algorithm returns its choice ˆh(t)of the model it currently believes is best. To evaluate these choices we assume that we know the form of the loss function Lthat we would use to evaluate each model in Hif we had test la- bels. Our true best model, h∗(the one we hope to select), is the one that minimizes this loss empirically over D: h∗= arg min h∈H1 |D||D|X i=1L(h(xi), yi), (1) where yiare the true labels corresponding to xi. In this pa- per we focus on accuracy-based loss functions; extending our framework to additional metrics is an interesting direc- tion for future work. We evaluate the efficacy of model selection algorithms based on the regret incurred at each time step, defined as t"}
{"id": "2507.23771v1_chunk18", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "g our framework to additional metrics is an interesting direc- tion for future work. We evaluate the efficacy of model selection algorithms based on the regret incurred at each time step, defined as the difference in loss between our chosen model ˆh(t)and the true best model h∗: Regrett=1 |D||D|X i=1h L\u0010 ˆh(t)(xi), yi\u0011 −L(h∗(xi), yi)i (2) We also measure cumulative regret at each time step, de- fined as the sum of all previous regrets: Cuml. Regrett=tX s=1Regrets (3) Note that we can only measure these values while bench- marking as it requires oracle access to ground truth labels. 4. Method 4.1. A Dawid-Skene model for classifier predictions The Dawid-Skene (DS) model is a probabilistic representa- tion of how human annotators generate predictions on a set 3Model 1 Predictionsx0yx1...xnxnModel 2 Predictionsx0yx1...xntt+1for each xi , for each possible class cj,compute hypothetical entropy of PB e s t at time t+1 if yi = cjestimate class confusion using consensus predictions as pseudo-"}
{"id": "2507.23771v1_chunk19", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "Model 2 Predictionsx0yx1...xntt+1for each xi , for each possible class cj,compute hypothetical entropy of PB e s t at time t+1 if yi = cjestimate class confusion using consensus predictions as pseudo-labelsx yConsensus of All ModelsOracle AnnotatorUpdated Confusion Matricesre-estimate PB e s t for time t+1 and repeatModel 1Model 2PB e s t at time tModel 2Model 1Initial Confusion MatricesModel 1Model 2estimate PB e s t with sum over classes and confusions weight by class priors for each xipartial update scaled by ηInitiali z e B eliefs S elect D ata Point to L abelS elect D ata Point to L abelL abel and Update Expe cte d Inf ormation Gain in PBestx0x1x2...xny 0E stimate PBest123Figure 2. CODA for active model selection. Simplified example with two models. At time step 0 we instantiate a Bayesian framework for tracking model performance over time (Sec. 4.1), using the consensus of all model predictions to instantiate per-model priors (Sec. 4.2). At each time step we perform three actions"}
{"id": "2507.23771v1_chunk20", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "esian framework for tracking model performance over time (Sec. 4.1), using the consensus of all model predictions to instantiate per-model priors (Sec. 4.2). At each time step we perform three actions: (1)We estimate PBest, the current probability that each model is best, by integrating over our current beliefs (Sec. 4.3); (2)We compute the expected information gain in the PBestdistribution that would result from labeling each point in our dataset, and select the argmax as the most informative point (Sec. 4.4); (3)We query for the ground truth label of our selected data point, evaluate whether each model correctly predicts the true label, and update our beliefs (Sec. 4.5). of data [14]. We adapt this model for the purpose of instead modeling the prediction process of machine learning mod- els. Unlike prior work, we do not fit the model directly to the set of predictions; rather, we iteratively fit parameters to actively-collected ground-truth labels over time. In particular, we base ou"}
{"id": "2507.23771v1_chunk21", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "els. Unlike prior work, we do not fit the model directly to the set of predictions; rather, we iteratively fit parameters to actively-collected ground-truth labels over time. In particular, we base our approach off of the Bayesian instantiation of the DS model introduced by Passonneau and Carpenter [47]. We model the prediction process of each classifier hkusing a confusion matrix Mkof size (C, C). Each row corresponds to a true class c∈ {1, . . . , C }and each column to a predicted class c′∈ {1, . . . , C }. Thus each cell in the matrix represents the conditional probability Mk, c, c′=P\u0000 ˆck,i=c′ yi=c\u0001 . (4) Our goal will be to perform statistical inference on the pa- rameters of this confusion matrix. As such, the components of the data generating process are represented by random variables with latent parameters. The data generating pro- cess proceeds as follows: 1. Each data point’s true class label yiis drawn randomly from per-data-point prior distributions over which class that d"}
{"id": "2507.23771v1_chunk22", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "les with latent parameters. The data generating pro- cess proceeds as follows: 1. Each data point’s true class label yiis drawn randomly from per-data-point prior distributions over which class that data point could be, yi∼Cat(π(xi)). 2. Each row of the classifier’s confusion matrix is drawn randomly from per-row distributions, Mk, c,·∼θk,c, where θk,cis the prior distribution over what the row of the confusion matrix could be. To accommodate Bayesian updates, we initialize each θk,cto be a Dirich- let prior. 3. The sampled true class indexes into the corresponding row of the classifier’s confusion matrix, Mk, yi. 4. The classifier’s prediction for that data point is sam- pled from the distribution over that row’s cells, ˆck,i∼Cat(Mk, yi). See the supplemental material Fig. 9 for an illustrative view.4.2. Constructing consensus priors (Fig. 2, Step 0) We begin by collecting each model’s prediction vectors over the unlabeled dataset D. We take advantage of the “wisdom of the crowd” (of "}
{"id": "2507.23771v1_chunk23", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "rative view.4.2. Constructing consensus priors (Fig. 2, Step 0) We begin by collecting each model’s prediction vectors over the unlabeled dataset D. We take advantage of the “wisdom of the crowd” (of classifiers) to form initial consensus labels by summing these probabilities across all models, si,c=HX k=1hk,c(xi)∀c= 1, . . . , C, and then define the consensus label c∗ i= arg max csi,c. For each model hk, we then compare its predictions hk(xi) against the consensus labels c∗ ifor all ito initialize empiri- cal confusion matrices: ˆMk,c,c′=|D|X i=1\u0000 1\u0002 c∗ i=c\u0003 ·hk,c′(xi)\u0001 (5) We then use these empirical estimates to create Dirichlet priors θover our beliefs in each row as: θk,c,c′= (βc,c′+αˆMk,c,c′)/ T, (6) βc,c′=  1, ifc′=c, 1 C−1,otherwise.(7) Where αis a hyperparameter controlling a blend between our empirical estimates and a static prior βrepresenting 50% macro-accuracy, and Tis a temperature parameter controlling the number of initial “pseudo-counts”. We use α= 0.1andT= 0.5by de"}
{"id": "2507.23771v1_chunk24", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "lend between our empirical estimates and a static prior βrepresenting 50% macro-accuracy, and Tis a temperature parameter controlling the number of initial “pseudo-counts”. We use α= 0.1andT= 0.5by default for all experiments. 4.3. Computing P Best(Fig. 2, Step 1) Throughout the active label collection process, we will cre- ate and update a probability distribution representing our belief in which model is best, PBest=\u0000 P(h=h∗)\u0001 h∈H(8) 4This distribution is the key component of our model selec- tion algorithm. At each time step, we return arg max hPBest as our choice of model, and (when benchmarking) evaluate our choice by computing the regret and cumulative regret of this choice at each time step. We focus on accuracy as our target metric. In this case a simple option would be to use the posterior means of each classifier’s overall accuracy, computed by summing each row’s diagonal probability weighted by an estimated marginal class prevalence ˆπ(c)derived from the consensus: Acc mean("}
{"id": "2507.23771v1_chunk25", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "posterior means of each classifier’s overall accuracy, computed by summing each row’s diagonal probability weighted by an estimated marginal class prevalence ˆπ(c)derived from the consensus: Acc mean(hk) =CX c=1ˆπ(c)Mk, c, c (9) ˆπ(c) =1 |D||H||D|X i=1|H|X k=1CX c′=1hk,c′(xi)Mk, c′, c (10) Eq. (9) has the benefit of being efficient to com- pute but ignores the probabilistic nature of our estimates, failing to account for differences in uncertainty between classes/classifiers. Remember that we have access to more than just point estimates of the confusion matrices: we model our beliefs in what these confusion matrix entries could be based on what we have observed so far. We do this with per-row Dirichlet distributions, allowing us to in- corporate additional uncertainty into accuracy estimates as follows. First, see that the marginal distribution for the cth class of the cth row’s Dirichlet distribution ( i.e. the diagonal entry) is a Beta distribution with parameters: αk,c=Mk,c,c, β k,"}
{"id": "2507.23771v1_chunk26", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "tes as follows. First, see that the marginal distribution for the cth class of the cth row’s Dirichlet distribution ( i.e. the diagonal entry) is a Beta distribution with parameters: αk,c=Mk,c,c, β k,c=CX c′̸=cMk,c,c′ (11) Then, to compute PBest, we can integrate over the mix- tures of all models’ per-row Beta distributions weighted by the class marginal ˆπ(c)as defined in Eq. (10). Supposing each classifier hk’s performance Xkis drawn independently from some distribution with PDF fk(x)and CDF Fk(x), the integral that computes the probability that hk’s draw ex- ceeds those of all others is: PBest(hk) =P\u0000 Xk= max lXl\u0001 (12) =Z1 0fk(x)Y l̸=kFl(x) dx, (13) Where the PDFs and CDFs are mixtures over the per-model per-row Beta distributions from Eq. (11): fk(x) =CX c=1ˆπ(c)fk,c(x) (14) Fl(x) =CX c=1ˆπ(c)Fl,c(x) (15)where fk,c(x)is the PDF of Beta (αk,c, βk,c)andFl,cis the CDF of Beta (αl,c, βl,c). Intuitively, to have Xkbe the largest draw, we can “fix” Xkat some value x(with probability dens"}
{"id": "2507.23771v1_chunk27", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "c=1ˆπ(c)Fl,c(x) (15)where fk,c(x)is the PDF of Beta (αk,c, βk,c)andFl,cis the CDF of Beta (αl,c, βl,c). Intuitively, to have Xkbe the largest draw, we can “fix” Xkat some value x(with probability density fk(x)), and then require that all Xlforl̸=klie at or below x(which happens with probability Fl(x)each). See supplemen- tal Fig. 10 for an illustrative view. In our implementa- tion, we discretize [0,1]and approximate the above inte- gral using a trapezoidal rule to integrate. Finally we obtain PBest=\u0000 PBest(h1), . . . , P Best(h|H|)\u0001 . 4.4. Selecting points to label (Fig. 2, Step 2) To decide which point to label next at each time step, we aim to pick the one that, on average, reduces our uncertainty overPBestthe most. We quantify our uncertainty using the Shannon entropy H(PBest). For a candidate point xi, let ˆπ(c|xi)be the probability that xibelongs to class cunder our current beliefs (as in Eq. (10), without marginalizing). For each hypothetical label c, we perform a virtual update"}
{"id": "2507.23771v1_chunk28", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "andidate point xi, let ˆπ(c|xi)be the probability that xibelongs to class cunder our current beliefs (as in Eq. (10), without marginalizing). For each hypothetical label c, we perform a virtual update of all confusion matrix rows according to the update procedure defined in the next section (Sec. 4.5, Eq. (18)). This yields a hypothetical distribution Pc Best. We measure the new entropy H(Pc Best)and then revert to our original state. Weighting by ˆπ(c|xi), the expected posterior entropy is CX c=1ˆπ\u0000 c|xi\u0001 H\u0000 Pc Best\u0001 . (16) Hence, the expected information gain (EIG) for point xiis EIG( xi) =H\u0000 PBest\u0001 −CX c=1ˆπ\u0000 c|xi\u0001 H\u0000 Pc Best\u0001 .(17) At each iteration, we compute EIG( xi)for all unlabeled points and query the label for the one with the highest EIG, then perform the realpartial update to our Dirichlet param- eters as in Eq. (18). 4.5. Updating beliefs (Fig. 2, Step 3) As true labels become available, we update our confusion matrix estimates to incorporate new information. Consider we "}
{"id": "2507.23771v1_chunk29", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "our Dirichlet param- eters as in Eq. (18). 4.5. Updating beliefs (Fig. 2, Step 3) As true labels become available, we update our confusion matrix estimates to incorporate new information. Consider we have just observed the label for xiisyi=c. Recall from Sec. 4.1 that we model each classifier’s class prediction for xias a random draw according to row cof its confusion matrix, ˆck,i∼Cat(Mk, c). We use the fact that our Dirichlet prior for this row is the conjugate prior for this categorical distribution to update the Dirichlet for the next time step as: θk, c,ˆck,i←θk, c,ˆck,i+η (18) Where ηis a learning rate hyperparameter allowing for par- tial updates. When η= 1 this reduces to the standard Dirchlet-categorical update rule. In practice, we find that partial updates with η < 1are useful for stability. We use η= 0.01by default for all experiments. 55. Datasets Models are not re-trained during model selection. Therefore amodel selection benchmark can be represented as a tuple (p,y), whe"}
{"id": "2507.23771v1_chunk30", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "or stability. We use η= 0.01by default for all experiments. 55. Datasets Models are not re-trained during model selection. Therefore amodel selection benchmark can be represented as a tuple (p,y), where p∈R|H|×|D|×Cis the set of predictions for each model h∈H, data point xi∈D, and class c∈C, and y={y1, . . . , y |D|}are the ground-truth labels for each data point in D. We perform model selection directly on the test set,i.e. there is no validation/test set split. This matches the pool-based setting of prior work in active testing and active model selection [31, 45]. We curate a suite of 26 diverse model selection bench- marking tasks from 3 different existing benchmarks along with over 3500 pre-trained models. This benchmark suite represents the largest empirical study of active model se- lection to date. In this section we describe the models in the candidate pool as well as how they were trained. In some cases, we train these models ourselves; in others, we source public pre-trained "}
{"id": "2507.23771v1_chunk31", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "lection to date. In this section we describe the models in the candidate pool as well as how they were trained. In some cases, we train these models ourselves; in others, we source public pre-trained models for which we do not have any information about the training process. We publish our benchmark suite, both the curated set of datasets and the pretrained models, to support future research. ModelSelector [45] is a benchmark suite focused on pre- trained models sourced from online repositories such as HuggingFace Models and PyTorch hub. We source the pre- diction files directly from the ModelSelector GitHub repos- itory. We curate tasks for which at least 100 test data points are available for easy comparison with other datasets in our benchmarking suite. In total we include ten image and text based classification tasks: CIFAR10 (low accuracy mod- els) [33], CIFAR10 (high accuracy models), PACS [34], and seven tasks from the GLUE language classification bench- mark [67]. We refer to t"}
{"id": "2507.23771v1_chunk32", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": " text based classification tasks: CIFAR10 (low accuracy mod- els) [33], CIFAR10 (high accuracy models), PACS [34], and seven tasks from the GLUE language classification bench- mark [67]. We refer to the vision tasks collectively as MSV (“ModelSelector Vision”). MSV and GLUE involve be- tween 9–114 models per task totaling 851 models. WILDS [30] is a benchmark of in-the-wild distribution shifts which provides an opportunity to study model se- lection in the domain generalization setting, where models have been specifically trained to generalize to new test data. We focus on all classification tasks in WILDS where per- forming standard model selection using the provided valida- tion sets results in a regret greater than 1%, i.e. where active model selection would be beneficial (experiments included in supplemental). These tasks are: iWildCam [4], which in- volves classifying wildlife in imagery; FMoW [10], which involves classification of land use in remote sensing im- agery; CivilCommen"}
{"id": "2507.23771v1_chunk33", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ncluded in supplemental). These tasks are: iWildCam [4], which in- volves classifying wildlife in imagery; FMoW [10], which involves classification of land use in remote sensing im- agery; CivilComments [5], which involves toxicity classifi- cation of text data; and Camelyon17 [2], which involves tu- mor classification in histopathology data. These tasks range from binary to 182-way classification. We train all base- line algorithms from Koh et al. [30] using their publicly- available codebase: empirical risk minimization with in- distribution validation (ERM) [66], CORAL [62], IRM [1], and GroupDRO [21]. We train each method for the defaultnumber of epochs used by WILDS, saving a checkpoint ev- ery epoch, resulting in between 20 and 240 models per task and 348 models total. DomainNet126 [49, 55] is an unsupervised domain adap- tation benchmark where the task is 126-way classification of objects in real and synthetic imagery. We follow Peng et al. [49] and construct 12 adaptation tasks"}
{"id": "2507.23771v1_chunk34", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "49, 55] is an unsupervised domain adap- tation benchmark where the task is 126-way classification of objects in real and synthetic imagery. We follow Peng et al. [49] and construct 12 adaptation tasks across 4 do- mains: real imagery, paintings, clipart, and sketches, and use the standard UDA training protocol outlined used in prior work [42, 49, 55]. We use the Powerful-Benchmarker codebase [43] to train 10 popular unsupervised domain al- gorithms on DomainNet126: ATDOC [36], BNM [11], BSP [9], CDAN [39], DANN [17], GVB [12], IM [61], MCC [23], MCD [54], and MMD [38]. We train each method for 40 epochs, saving a checkpoint every 2 epochs. In total, this gives us 10 algorithms ×20 checkpoints =200 models for each transfer task (2400 models overall). 6. Baselines We compare our method against five other active model se- lection methods, ranging from classic approaches to recent state-of-the-art methods. The methods are: Random sampling The simplest baseline samples a point uniformly at "}
{"id": "2507.23771v1_chunk35", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "st five other active model se- lection methods, ranging from classic approaches to recent state-of-the-art methods. The methods are: Random sampling The simplest baseline samples a point uniformly at random each time step and maintains an em- pirical risk estimate for each model over time. We perform model selection by returning the model with the lowest em- pirical risk estimate at every time step. Uncertainty sampling [13] We follow Okanovic et al. [45] to adapt classic committee-based uncertainty sampling techniques from active learning [13] to the active model se- lection scenario. At each time step, we greedily sample the point that most models in Hdisagree on, defined as the en- tropy of the mean prediction of all models. We perform model selection with empirical risk estimation. Active Testing [31] Active Testing aims to obtain label- efficient unbiased estimates of model performance through importance-weighted sampling. To do so, they use a surro- gate model , which is assumed "}
{"id": "2507.23771v1_chunk36", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ive Testing [31] Active Testing aims to obtain label- efficient unbiased estimates of model performance through importance-weighted sampling. To do so, they use a surro- gate model , which is assumed to be more accurate than any candidate models in H, to guide the data sampling process. Points are sampled stochastically in proportion to the esti- mated loss of the model of interest with respect to the sur- rogate’s predictions. We adapt the framework to the active model selection setting as follows: we instantiate the surro- gate model as the same ensemble we use to form our initial consensus estimates. We implement a naive extension of the Active Testing acquisition function that simply sums the ac- quisition probabilities from all models in the hypothesis set. We perform model selection with empirical risk estimation using unbiased risk estimators [16, 31]. VMA [40] VMA is an active model selection extension to the Active Testing framework. Their acquisition function is based on mini"}
{"id": "2507.23771v1_chunk37", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "with empirical risk estimation using unbiased risk estimators [16, 31]. VMA [40] VMA is an active model selection extension to the Active Testing framework. Their acquisition function is based on minimizing the pairwise variance of the difference between model loss estimates, where the loss estimates are the same importance-weighted estimates as [31]. Again we 6TaskRandomUncertaintyActiveVMAModel CODA Sampling Testing Selector (Ours)DomainNet126real→sketch 147.1 197.7 269.8 119.2 88.8 101.2 real→painting 143.6 167.9 118.9 139.9 92.1 87.2 real→clipart 237.7 217.6 152.1 206.9 153.2 231.7 sketch→real 280.4 252.7 236.4 273.4 268.4 11.9 sketch→painting 156.6 181.9 237.7 157.1 173.9 13.0 sketch→clipart 228.2 224.6 162.0 227.9 38.1 432.4 painting →real 364.8 224.0 215.6 358.9 293.4 2.4 painting →sketch 179.3 440.7 202.5 211.5 209.8 72.3 painting →clipart 222.7 296.6 251.4 271.8 73.2 43.1 clipart→real 322.1 177.1 159.1 306.4 72.7 25.3 clipart→sketch 247.2 924.8 282.6 291.3 532.7 51.3 clipart→p"}
{"id": "2507.23771v1_chunk38", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "sketch 179.3 440.7 202.5 211.5 209.8 72.3 painting →clipart 222.7 296.6 251.4 271.8 73.2 43.1 clipart→real 322.1 177.1 159.1 306.4 72.7 25.3 clipart→sketch 247.2 924.8 282.6 291.3 532.7 51.3 clipart→painting 147.0 162.9 222.6 167.9 131.7 122.2WILDSiwildcam 287.0 380.4 392.1 440.6 459.0 201.7 camelyon 175.2 311.6 206.1 160.1 198.3 288.7 fmow 189.7 191.9 153.0 189.2 211.7 70.0 civilcomments 140.9 13.3 76.7 50.1 125.3 318.6MSVcifar10-low 410.6 629.7 399.9 476.5 567.2 58.7 cifar10-high 346.2 281.7 154.9 383.7 90.9 74.1 pacs 216.6 6.9 101.8 116.5 97.9 57.9GLUEcola 368.1 169.5 239.8 317.8 207.8 2226.7 mnli 237.4 80.8 234.2 312.8 148.5 23.5 qnli 222.7 231.6 246.6 283.0 185.7 120.4 qqp 136.7 388.9 127.8 169.2 186.8 4.8 rte 375.7 390.3 424.4 674.7 243.6 283.8 sst2 219.9 89.6 174.9 373.6 202.0 51.7 mrpc 318.2 301.1 235.2 332.3 173.1 49.0 Table 1. Active model selection main results: cumulative re- gret at step 100. Best method per task in bold, second best under- lined (lower is better). CODA is"}
{"id": "2507.23771v1_chunk39", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "mrpc 318.2 301.1 235.2 332.3 173.1 49.0 Table 1. Active model selection main results: cumulative re- gret at step 100. Best method per task in bold, second best under- lined (lower is better). CODA is state-of-the-art on 18 out of 26 tasks, often significantly outperforming the next-best method, e.g. by90×on painting →real. Variances reported in supplemental. instantiate the surrogate model as the ensemble of all mod- els in Hand return the model with the lowest unbiased risk estimate every time step. ModelSelector [45] ModelSelector is currently the state- of-the-art method for active model selection. It utilizes a probability distribution similar to our PBest, but computes this independently of any per-model performance metrics. Their PBestis updated according to the following update rule: PBest,t+1(hk) =1−ϵ ϵ×PBest,t+1(hk)ifhkgets the label at time tcorrect. ϵis a learning rate hyperparameter that is set with a self-supervised protocol. We follow this protocol using their codebase t"}
{"id": "2507.23771v1_chunk40", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "e: PBest,t+1(hk) =1−ϵ ϵ×PBest,t+1(hk)ifhkgets the label at time tcorrect. ϵis a learning rate hyperparameter that is set with a self-supervised protocol. We follow this protocol using their codebase to find the optimal ϵfor any datasets that they do not benchmark in their paper. 7. Experiments Experimental settings All results are reported as the mean over five random seeds. We do not tune the hyperparame- ters of our method on each dataset; instead we select fixed values {T= 0.5, α= 0.1, η= 0.01}based on a limited set of initial experiments. Active model selection results Our main results for active model selection are shown in Tab. 1. We use accuracy as our loss function and cumulative regret at step 100 as our main point of comparison for all methods, as this provides a summary of overall performance in the few-label regime where active selection is most impactful. We report addi- tional metrics in the supplemental. For a more detailed look at performance within this time window, we"}
{"id": "2507.23771v1_chunk41", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ll performance in the few-label regime where active selection is most impactful. We report addi- tional metrics in the supplemental. For a more detailed look at performance within this time window, we visualize the re- 0 50 1000.02.55.07.5RegretMSV 7-10 class 0 50 100051015GLUE 2-3 class 0 50 100246WILDS Multiclass 62-182 class 0 50 100246WILDS Binary 2-class 0 50 1001234DomainNet 126-class 0 50 100 Steps0100200300400Cuml. regret 0 50 1000100200300 0 50 1000100200300 0 50 1000100200300 0 50 100050100150200Random Sampling UncertaintyActive Testing VMAModelSelector CODA (Ours)Figure 3. Active model selection average results. We visual- ize regret (top row) and cumulative regret (bottom row) from time steps 1 to 100, median value across all tasks within benchmarks. Lower is better. CODA is consistently the best performer over time for all settings except the binary classification tasks in WILDS. Full per-task results in supplemental. gret and cumulative regret over time in more detail in "}
{"id": "2507.23771v1_chunk42", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "nsistently the best performer over time for all settings except the binary classification tasks in WILDS. Full per-task results in supplemental. gret and cumulative regret over time in more detail in Fig. 3 and for all tasks separately in the supplemental. CODA outperforms all prior work on 18 out of the 26 datasets tested, often significantly, resulting in an over 80% reduction in regret compared to the next-best method on 5 datasets, greater than 50% reduction compared to the next- best on 11 datasets, and greater than 25% reduction from the next-best on 15 datasets. The next-best method is in- consistent across benchmarks. Of the eight datasets where ours is not the best method, uncertainty-based sampling and ModelSelector are best on three each while Active Testing and VMA are best on one each. Our method performs worst on CoLA and CivilComments, where we underperform ran- dom sampling by 6.1×and2.3×, respectively. We analyze these successes and failures in more detail in the remai"}
{"id": "2507.23771v1_chunk43", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "one each. Our method performs worst on CoLA and CivilComments, where we underperform ran- dom sampling by 6.1×and2.3×, respectively. We analyze these successes and failures in more detail in the remainder of this section. Ablation studies First we ablate the priors used by our method in Fig. 5. We see that the consensus-informed priors introduced in Eq. (5) are a key component of our good performance. Removing them increases regret signifi- cantly during the early parts of the label acquisition process. We also see that the diagonal-weighted prior introduced in Eq. (7) to regularize the consensus priors performs better than uniform on some, but not all, datasets regardless of the number of classes. Note that in binary classification, the uniform and diagonal settings are equivalent. In Fig. 6, we ablate our acquisition function (expected in- formation gain w.r.t. PBest, Eq. (17)). We compare with ran- dom sampling as well as uncertainty-based sampling [13] which greedily selects the da"}
{"id": "2507.23771v1_chunk44", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": ". 6, we ablate our acquisition function (expected in- formation gain w.r.t. PBest, Eq. (17)). We compare with ran- dom sampling as well as uncertainty-based sampling [13] which greedily selects the data point with the largest Shan- non entropy in the mean prediction of all models. We see that expected information gain results in the lowest cu- mulative regret in most cases. However we also see that uncertainty-based sampling also performs well in combi- nation with the CODA probabilistic framework, sometimes outperforming expected information gain. 70 1 Predicted label0 1True label0.98 0.021 0.46 0.54True best model 0 1 Class idx0.00.5Class proportionClass dist. True Est. 0 1 Predicted label0 1True label1 0 1 0Selected model, time 70 0 1 Class idx0.00.5Class proportionCivilComments: Failure Case CoLA: Failure Case Class dist. True Est.Figure 4. Failure analysis on CivilComments and CoLA. CODA may underestimate the performance of very biased classi- fiers in early steps (CivilComments, "}
{"id": "2507.23771v1_chunk45", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "e Case CoLA: Failure Case Class dist. True Est.Figure 4. Failure analysis on CivilComments and CoLA. CODA may underestimate the performance of very biased classi- fiers in early steps (CivilComments, left), but overestimate them in later steps (CoLA, right) when there is also data imbalance present (blue bars). More details in Sec. 7. Limitations and failure analysis We investigate CODA’s poor performance on CivilComments and CoLA in Fig. 4. We find that poor performance in these cases is caused by a combination of data imbalance and model bias. CODA uses the predicted class marginal in its updates (Eq. (17)), but regularizes this to counteract overconfident predictions (Eq. (7)). In CivilComments this causes us to upweight the contribution of minority class predictions early on, requir- ing a moderate amount of samples to identify the imbalance. In addition, the best model on CivilComments is extremely biased, with 98% accuracy on the majority class but only 54% accuracy on the minori"}
{"id": "2507.23771v1_chunk46", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ng a moderate amount of samples to identify the imbalance. In addition, the best model on CivilComments is extremely biased, with 98% accuracy on the majority class but only 54% accuracy on the minority class, exacerbating the prob- lem when selecting based on micro-accuracy. Similarly, in CoLA, CODA selects a very biased model—one that only predicts the negative class— later in the model selection process (step 70) again because the dataset is estimated to be more balanced than it actually is. We note that these fail- ures are exceptional cases, as CODA performs well on other datasets with significant imbalance ( e.g. iWildCam). This points to interesting directions for future work to address data imbalance, model bias, and use-case specific metrics. Additional unsupervised results Though not the main fo- cus of this paper, our method can also be used to perform unsupervised model selection by using only the consensus- informed priors to compute PBest. We show that this is re- markabl"}
{"id": "2507.23771v1_chunk47", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": " not the main fo- cus of this paper, our method can also be used to perform unsupervised model selection by using only the consensus- informed priors to compute PBest. We show that this is re- markably effective, matching or outperforming the previous state-of-the-art in unsupervised model selection in 20 of 26 benchmarks, in the supplemental material. 8. Conclusion and future work We have introduced a new method for active model se- lection that can identify the best model in a pool of can- didates with significantly fewer human labels than prior work. The ability to do so has implications for users of pre-trained machine learning models, who can use CODA to efficiently select the best model for their dataset, as well as for researchers in fields such as domain adaptation who face model selection challenges during model development. Our work opens up several exciting areas for future re- search. Within the active model selection framework there are several interesting directions: (1) "}
{"id": "2507.23771v1_chunk48", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "election challenges during model development. Our work opens up several exciting areas for future re- search. Within the active model selection framework there are several interesting directions: (1) How to better con- struct and utilize informative priors, whether from unsuper- 0 50 100051015RegretMSV 7-10 class 0 50 1000.02.55.07.510.0GLUE 2-3 class 0 50 100246WILDS Multiclass 62-182 class 0 50 100246WILDS Binary 2-class 0 50 1001234DomainNet 126-class 0 50 100 Steps0100200Cuml. regret 0 50 1000100200300 0 50 1000100200300 0 50 1000100200300 0 50 100050100150200Uniform Diagonal Unif+Consensus Diag+ConsensusFigure 5. Ablation of CODA prior design. We compare a uni- form prior on the confusion matrices (top row) with diagonal up- weighting (“Diag.” column; Eq. (7)) and consensus prior (“Cons.” column; Eq. (5)) we introduce. For binary classification tasks, uniform and diagonal weighting are equivalent. In most cases, both the consensus prior and diagonal upweighting provide bene- fits,"}
{"id": "2507.23771v1_chunk49", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ns.” column; Eq. (5)) we introduce. For binary classification tasks, uniform and diagonal weighting are equivalent. In most cases, both the consensus prior and diagonal upweighting provide bene- fits, and are complementary. 0 50 1000123RegretMSV 7-10 class 0 50 1000123GLUE 2-3 class 0 50 100123WILDS Multiclass 62-182 class 0 50 100246WILDS Binary 2-class 0 50 1000.50.60.7DomainNet 126-class 0 50 100 Steps050100150200Cuml. regret 0 50 100050100150 0 50 100050100150200 0 50 1000100200300 0 50 1000204060CODA (Random) CODA (Uncertainty) CODA (EIG) Figure 6. Ablation of CODA acquisition function. We use the CODA probabilistic framework and compare different data point acquisition functions: random sampling, uncertainty-based sampling, and expected information gain (EIG, the default). We see that EIG typically improves upon other sampling approaches, however uncertainty-based sampling is also a strong acquisition function in combination with the rest of the CODA framework. vised procedures o"}
{"id": "2507.23771v1_chunk50", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "t EIG typically improves upon other sampling approaches, however uncertainty-based sampling is also a strong acquisition function in combination with the rest of the CODA framework. vised procedures or by incorporating human-provided do- main knowledge; (2) Extending the active model selection framework beyond accuracy and classification to support more tasks and target metrics; (3) Exploring more sophis- ticated probabilistic models that better capture predictive quantities such as model confidence. More broadly, active model selection can be seen as one facet of a larger research goal of how to best utilize hu- man effort in the development and deployment of machine learning systems. We show that active model selection is a particularly effective use of annotation effort, but there are many things that could be done with collected labels, either in serial or in parallel. For example, an interesting research direction is how to perform active learning and/or active testing concurrentl"}
{"id": "2507.23771v1_chunk51", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "any things that could be done with collected labels, either in serial or in parallel. For example, an interesting research direction is how to perform active learning and/or active testing concurrently with active model selection, and how to efficiently allocate effort to the different tasks. We hope our work can provide a strong starting point for inves- tigating these questions. 8Acknowledgments We would like to thank Julia Chae, Mark Hamilton, Timm Haucke, Michael Hobley, Neha Hulkund, Rupa Kurinchi- Vendhan, Evan Shelhamer, and Edward Vendrow for feed- back on early drafts, and Serge Belongie, Emma Pier- son, Manish Raghavan, Shuvom Sadhuka, and Divya Shan- mugam for helpful discussions. This work was supported in part by NSF awards #2313998, #2330423, and #2329927, NSERC award #585136, and MIT J-WAFS seed grant #2040131. References [1] Martin Arjovsky, L ´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893 , 2019. 6 [2] P"}
{"id": "2507.23771v1_chunk52", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": ", and MIT J-WAFS seed grant #2040131. References [1] Martin Arjovsky, L ´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893 , 2019. 6 [2] Peter Bandi, Oscar Geessink, Quirine Manson, Mar- cory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the pa- tient level: the camelyon17 challenge. IEEE transactions on medical imaging , 38(2):550–560, 2018. 6 [3] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European confer- ence on computer vision (ECCV) , pages 456–473, 2018. 2 [4] Sara Beery, Grant Van Horn, Oisin Mac Aodha, and Pietro Perona. The iwildcam 2018 challenge dataset. arXiv preprint arXiv:1904.05986 , 2019. 6 [5] Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for me"}
{"id": "2507.23771v1_chunk53", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "Pietro Perona. The iwildcam 2018 challenge dataset. arXiv preprint arXiv:1904.05986 , 2019. 6 [5] Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measur- ing unintended bias with real data for text classification. In Companion proceedings of the 2019 world wide web confer- ence, pages 491–500, 2019. 6 [6] Steve Branson, Grant Van Horn, and Pietro Perona. Lean crowdsourcing: Combining humans and machines in an on- line system. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pages 7474–7483, 2017. 3 [7] Alexander Braylan, Madalyn Marabella, Omar Alonso, and Matthew Lease. A general model for aggregating annota- tions across simple, complex, and multi-object annotation tasks. Journal of Artificial Intelligence Research , 78:901– 973, 2023. 3 [8] M Jorge Cardoso, Wenqi Li, Richard Brown, Nic Ma, Eric Kerfoot, Yiheng Wang, Benjamin Murrey, Andriy Myro- nenko, Can Zhao, Dong Yang, et al. Monai: An open-s"}
{"id": "2507.23771v1_chunk54", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "gence Research , 78:901– 973, 2023. 3 [8] M Jorge Cardoso, Wenqi Li, Richard Brown, Nic Ma, Eric Kerfoot, Yiheng Wang, Benjamin Murrey, Andriy Myro- nenko, Can Zhao, Dong Yang, et al. Monai: An open-source framework for deep learning in healthcare. arXiv preprint arXiv:2211.02701 , 2022. 1 [9] Xinyang Chen, Sinan Wang, Mingsheng Long, and Jianmin Wang. Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation. In Interna- tional conference on machine learning , pages 1081–1090. PMLR, 2019. 6 [10] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition , pages 6172–6180, 2018. 6 [11] Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qing- ming Huang, and Qi Tian. Towards discriminability and diversity: Batch nuclear-norm maximization under label in- sufficient situations. In Proceedings of the IEEE/CVF con- ference on computer visi"}
{"id": "2507.23771v1_chunk55", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ng- ming Huang, and Qi Tian. Towards discriminability and diversity: Batch nuclear-norm maximization under label in- sufficient situations. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition , pages 3941–3950, 2020. 6, 13 [12] Shuhao Cui, Shuhui Wang, Junbao Zhuo, Chi Su, Qingming Huang, and Qi Tian. Gradually vanishing bridge for adver- sarial domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 12455–12464, 2020. 6 [13] Ido Dagan and Sean P Engelson. Committee-based sampling for training probabilistic classifiers. In Machine learning proceedings 1995 , pages 150–157. Elsevier, 1995. 6, 7 [14] Alexander Philip Dawid and Allan M Skene. Maximum like- lihood estimation of observer error-rates using the em algo- rithm. Journal of the Royal Statistical Society: Series C (Ap- plied Statistics) , 28(1):20–28, 1979. 2, 3, 4, 15 [15] Linus Ericsson, Da Li, and Timothy Hospedales. Better prac- tices f"}
{"id": "2507.23771v1_chunk56", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": " the em algo- rithm. Journal of the Royal Statistical Society: Series C (Ap- plied Statistics) , 28(1):20–28, 1979. 2, 3, 4, 15 [15] Linus Ericsson, Da Li, and Timothy Hospedales. Better prac- tices for domain adaptation. In International Conference on Automated Machine Learning , pages 4–1. PMLR, 2023. 1, 2 [16] Sebastian Farquhar, Yarin Gal, and Tom Rainforth. On sta- tistical bias in active learning: How and when to fix it. arXiv preprint arXiv:2101.11665 , 2021. 6 [17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of machine learning research , 17(59):1–35, 2016. 6 [18] Andres Hernandez, Zhongqi Miao, Luisa Vargas, Rahul Dodhia, Pablo Arbelaez, and Juan M Lavista Ferres. Pytorch-wildlife: A collaborative deep learning framework for conservation. arXiv preprint arXiv:2405.12930 , 2024. 1 [19] Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish"}
{"id": "2507.23771v1_chunk57", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "belaez, and Juan M Lavista Ferres. Pytorch-wildlife: A collaborative deep learning framework for conservation. arXiv preprint arXiv:2405.12930 , 2024. 1 [19] Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. Learning whom to trust with mace. In Pro- ceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 1120–1130, 2013. 3 [20] Dapeng Hu, Mi Luo, Jian Liang, and Chuan-Sheng Foo. Towards reliable model selection for unsupervised domain adaptation: An empirical study and a certified baseline. In The Thirty-eight Conference on Neural Information Process- ing Systems Datasets and Benchmarks Track , 2025. 2, 13 [21] Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning give robust classifiers? In International Conference on Machine Learn- ing, pages 2029–2037. PMLR, 2018. 6 [22] HuggingFace. HuggingFace Models, 2025. 1 [23] Ying Jin"}
{"id": "2507.23771v1_chunk58", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "onally robust supervised learning give robust classifiers? In International Conference on Machine Learn- ing, pages 2029–2037. PMLR, 2018. 6 [22] HuggingFace. HuggingFace Models, 2025. 1 [23] Ying Jin, Ximei Wang, Mingsheng Long, and Jianmin Wang. Minimum class confusion for versatile domain adaptation. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXI 16, pages 464–480. Springer, 2020. 6 [24] Tarun Kalluri, Sreyas Ravichandran, and Manmohan Chan- draker. Uda-bench: Revisiting common assumptions in un- 9supervised domain adaptation using a standardized frame- work. In European Conference on Computer Vision , pages 199–220. Springer, 2024. 1, 2 [25] Mohammad Reza Karimi, Nezihe Merve G ¨urel, Bojan Karla ˇs, Johannes Rausch, Ce Zhang, and Andreas Krause. Online active model selection for pre-trained classifiers. InInternational Conference on Artificial Intelligence and Statistics , pages 307–315. PMLR, 2021. 2, 3 [26] Parnian "}
{"id": "2507.23771v1_chunk59", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ang, and Andreas Krause. Online active model selection for pre-trained classifiers. InInternational Conference on Artificial Intelligence and Statistics , pages 307–315. PMLR, 2021. 2, 3 [26] Parnian Kassraie, Nicolas Emmenegger, Andreas Krause, and Aldo Pacchiano. Anytime model selection in linear ban- dits, 2023. 3 [27] Justin Kay, Peter Kulits, Suzanne Stathatos, Siqi Deng, Erik Young, Sara Beery, Grant Van Horn, and Pietro Perona. The caltech fish counting dataset: A benchmark for multiple- object tracking and counting. In European Conference on Computer Vision , pages 290–311. Springer, 2022. 2 [28] Justin Kay, Suzanne Stathatos, Siqi Deng, Erik Young, Pietro Perona, Sara Beery, and Grant Van Horn. Unsuper- vised domain adaptation in the real world: A case study in sonar video. In NeurIPS 2023 Computational Sustainability: Promises and Pitfalls from Theory to Deployment , 2023. 1 [29] Justin Kay, Timm Haucke, Suzanne Stathatos, Siqi Deng, Erik Young, Pietro Perona, Sara Beery, and"}
{"id": "2507.23771v1_chunk60", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "rIPS 2023 Computational Sustainability: Promises and Pitfalls from Theory to Deployment , 2023. 1 [29] Justin Kay, Timm Haucke, Suzanne Stathatos, Siqi Deng, Erik Young, Pietro Perona, Sara Beery, and Grant Van Horn. Align and distill: Unifying and improving domain adaptive object detection. arXiv preprint arXiv:2403.12029 , 2024. 1 [30] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubra- mani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the- wild distribution shifts. In International conference on machine learning , pages 5637–5664. PMLR, 2021. 2, 6, 16 [31] Jannik Kossen, Sebastian Farquhar, Yarin Gal, and Tom Rainforth. Active testing: Sample-efficient model evalu- ation. In International Conference on Machine Learning , pages 5753–5763. PMLR, 2021. 2, 3, 6 [32] Jannik Kossen, Sebastian Farquhar, Yarin Gal, and Thomas Rainforth. Active surrogate estimators: An active learning approach to"}
{"id": "2507.23771v1_chunk61", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "nference on Machine Learning , pages 5753–5763. PMLR, 2021. 2, 3, 6 [32] Jannik Kossen, Sebastian Farquhar, Yarin Gal, and Thomas Rainforth. Active surrogate estimators: An active learning approach to label-efficient model evaluation. Advances in Neural Information Processing Systems , 35:24557–24570, 2022. 2, 3 [33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6 [34] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generaliza- tion. In Proceedings of the IEEE international conference on computer vision , pages 5542–5550, 2017. 6 [35] Dongyuan Li, Zhen Wang, Yankai Chen, Renhe Jiang, Weip- ing Ding, and Manabu Okumura. A survey on deep active learning: Recent advances and new frontiers. IEEE Trans- actions on Neural Networks and Learning Systems , 2024. 1, 2 [36] Jian Liang, Dapeng Hu, and Jiashi Feng. Domain adaptation with auxiliary target domain-oriented classifier. In Proceed- ings"}
{"id": "2507.23771v1_chunk62", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "EE Trans- actions on Neural Networks and Learning Systems , 2024. 1, 2 [36] Jian Liang, Dapeng Hu, and Jiashi Feng. Domain adaptation with auxiliary target domain-oriented classifier. In Proceed- ings of the IEEE/CVF conference on computer vision and pattern recognition , pages 16632–16642, 2021. 6 [37] Xuefeng Liu, Fangfang Xia, Rick L Stevens, and Yuxin Chen. Contextual active online model selection with expertadvice. In ICML2022 Workshop on Adaptive Experimental Design and Active Learning in the Real World . ICML, 2022. 3 [38] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor- dan. Learning transferable features with deep adaptation net- works. In International conference on machine learning , pages 97–105. PMLR, 2015. 6 [39] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adapta- tion. Advances in neural information processing systems , 31, 2018. 6 [40] Mitsuru Matsuura and Satoshi Hara. Active model selection: A variance minimi"}
{"id": "2507.23771v1_chunk63", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "I Jordan. Conditional adversarial domain adapta- tion. Advances in neural information processing systems , 31, 2018. 6 [40] Mitsuru Matsuura and Satoshi Hara. Active model selection: A variance minimization approach. In NeurIPS 2023 Work- shop on Adaptive Experimental Design and Active Learning in the Real World . 2, 3, 6 [41] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in- distribution generalization. In International conference on machine learning , pages 7721–7735. PMLR, 2021. 2 [42] Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. Unsu- pervised domain adaptation: A reality check. arXiv preprint arXiv:2111.15672 , 2021. 1, 2, 6 [43] Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. Three new validators and a large-scale benchmark rank- ing for unsupervised domain adaptation. arXiv preprint arXiv:2208.07360 , 2"}
{"id": "2507.23771v1_chunk64", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": ", 2021. 1, 2, 6 [43] Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. Three new validators and a large-scale benchmark rank- ing for unsupervised domain adaptation. arXiv preprint arXiv:2208.07360 , 2022. 1, 2, 6, 13, 15 [44] Phuc Nguyen, Deva Ramanan, and Charless Fowlkes. Active testing: An efficient and robust framework for estimating ac- curacy. In International Conference on Machine Learning , pages 3759–3768. PMLR, 2018. 2 [45] Patrik Okanovic, Andreas Kirsch, Jannes Kasper, Torsten Hoefler, Andreas Krause, and Nezihe Merve G ¨urel. All mod- els are wrong, some are useful: Model selection with limited labels. arXiv preprint arXiv:2410.13609 , 2024. 2, 3, 6, 7, 13, 16 [46] Wei Ouyang, Fynn Beuttenmueller, Estibaliz G ´omez-de Mariscal, Constantin Pape, Tom Burke, Carlos Garcia- L´opez-de Haro, Craig Russell, Luc ´ıa Moya-Sans, Cristina De-La-Torre-Guti ´errez, Deborah Schmidt, et al. Bioimage model zoo: a community-driven resource for accessible deep learning in bioimage analysis."}
{"id": "2507.23771v1_chunk65", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "-de Haro, Craig Russell, Luc ´ıa Moya-Sans, Cristina De-La-Torre-Guti ´errez, Deborah Schmidt, et al. Bioimage model zoo: a community-driven resource for accessible deep learning in bioimage analysis. BioRxiv , pages 2022–06, 2022. 1 [47] Rebecca J Passonneau and Bob Carpenter. The benefits of a model of annotation. Transactions of the Association for Computational Linguistics , 2:311–326, 2014. 2, 3, 4 [48] Silviu Paun, Bob Carpenter, Jon Chamberlain, Dirk Hovy, Udo Kruschwitz, and Massimo Poesio. Comparing bayesian models of annotation. Transactions of the Association for Computational Linguistics , 6:571–585, 2018. 3 [49] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF inter- national conference on computer vision , pages 1406–1415, 2019. 6 10[50] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R ´e. Snorkel: Rapid train- ing data cre"}
{"id": "2507.23771v1_chunk66", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ational conference on computer vision , pages 1406–1415, 2019. 6 10[50] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R ´e. Snorkel: Rapid train- ing data creation with weak supervision. In Proceedings of the VLDB endowment. International conference on very large data bases , page 269, 2017. 3 [51] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classifiers generalize to cifar- 10?arXiv preprint arXiv:1806.00451 , 2018. 2 [52] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to im- agenet? In International conference on machine learning , pages 5389–5400. PMLR, 2019. 2 [53] Luca Robbiano, Muhammad Rameez Ur Rahman, Fabio Galasso, Barbara Caputo, and Fabio Maria Carlucci. Ad- versarial branch architecture search for unsupervised domain adaptation. In Proceedings of the IEEE/CVF winter confer- ence on applications of computer vision , pages 2918–2928, 2022"}
{"id": "2507.23771v1_chunk67", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ria Carlucci. Ad- versarial branch architecture search for unsupervised domain adaptation. In Proceedings of the IEEE/CVF winter confer- ence on applications of computer vision , pages 2918–2928, 2022. 2 [54] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In Proceedings of the IEEE con- ference on computer vision and pattern recognition , pages 3723–3732, 2018. 6 [55] Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Dar- rell, and Kate Saenko. Semi-supervised domain adaptation via minimax entropy. In Proceedings of the IEEE/CVF inter- national conference on computer vision , pages 8050–8058, 2019. 6 [56] Kuniaki Saito, Donghyun Kim, Piotr Teterwak, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Tune it the right way: Unsupervised validation of domain adaptation via soft neighborhood density. In Proceedings of the IEEE/CVF international conference on computer vision , pages 9184– 9193, 2021. "}
{"id": "2507.23771v1_chunk68", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "Tune it the right way: Unsupervised validation of domain adaptation via soft neighborhood density. In Proceedings of the IEEE/CVF international conference on computer vision , pages 9184– 9193, 2021. 1, 2 [57] Amartya Sanyal, Yaxi Hu, Yaodong Yu, Yian Ma, Yixin Wang, and Bernhard Sch ¨olkopf. Accuracy on the wrong line: On the pitfalls of noisy data for out-of-distribution generali- sation. arXiv preprint arXiv:2406.19049 , 2024. 2 [58] Christoph Sawade, Niels Landwehr, and Tobias Scheffer. Active comparison of prediction models. Advances in neural information processing systems , 25, 2012. 2, 3 [59] Burr Settles. Active learning literature survey. 2009. 1, 2 [60] Divya Shanmugam, Shuvom Sadhuka, Manish Raghavan, John Guttag, Bonnie Berger, and Emma Pierson. Evaluat- ing multiple models using labeled and unlabeled data. arXiv preprint arXiv:2501.11866 , 2025. 2 [61] Yuan Shi and Fei Sha. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. arX"}
{"id": "2507.23771v1_chunk69", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ing labeled and unlabeled data. arXiv preprint arXiv:2501.11866 , 2025. 2 [61] Yuan Shi and Fei Sha. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. arXiv preprint arXiv:1206.6438 , 2012. 6 [62] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Computer vision– ECCV 2016 workshops: Amsterdam, the Netherlands, Oc- tober 8-10 and 15-16, 2016, proceedings, part III 14 , pages 443–450. Springer, 2016. 6 [63] Takumi Tamura, Hiroyoshi Ito, Satoshi Oyama, and At- suyuki Morishima. Influence of ai’s uncertainty in the dawid- skene aggregation for human-ai crowdsourcing. In Interna-tional Conference on Information , pages 232–247. Springer, 2024. 3 [64] Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Ab- basnejad. Id and ood performance are sometimes inversely correlated on real-world datasets. Advances in Neural Infor- mation Processing Systems , 36:71703–71722, 2023. 2 [65] Grant Van Horn, Steve Brans"}
{"id": "2507.23771v1_chunk70", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "snejad. Id and ood performance are sometimes inversely correlated on real-world datasets. Advances in Neural Infor- mation Processing Systems , 36:71703–71722, 2023. 2 [65] Grant Van Horn, Steve Branson, Scott Loarie, Serge Be- longie, and Pietro Perona. Lean multiclass crowdsourcing. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 2714–2723, 2018. 3 [66] Vladimir N Vapnik. An overview of statistical learning the- ory. IEEE transactions on neural networks , 10(5):988–999, 1999. 6 [67] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language un- derstanding. arXiv preprint arXiv:1804.07461 , 2018. 6 [68] Peter Welinder, Steve Branson, Pietro Perona, and Serge Be- longie. The multidimensional wisdom of crowds. Advances in neural information processing systems , 23, 2010. 3 [69] Jianfei Yang, Hanjie Qian, Yuecong Xu, Kai Wang, and Li- hua Xie. Can "}
{"id": "2507.23771v1_chunk71", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "and Serge Be- longie. The multidimensional wisdom of crowds. Advances in neural information processing systems , 23, 2010. 3 [69] Jianfei Yang, Hanjie Qian, Yuecong Xu, Kai Wang, and Li- hua Xie. Can we evaluate domain adaptation models with- out target-domain labels? arXiv preprint arXiv:2305.18712 , 2023. 2 [70] Kaichao You, Ximei Wang, Mingsheng Long, and Michael Jordan. Towards accurate model selection in deep unsuper- vised domain adaptation. In International Conference on Machine Learning , pages 7124–7133. PMLR, 2019. 1, 2, 13 11Supplemental Material 9. Additional results 9.1. Alternate metrics Here we report tabular results for several alternate metrics, providing additional points of comparison to supplement our main results of cumulative regret at step 100 (Tab. 1). 9.1.1. Variance between seeds TaskRandomUncertaintyActiveVMAModel CODA Sampling Testing Selector (Ours)DomainNet126real→sketch 147.1 ±38.5 197.7±13.0 269.8±119.8 119.2±51.9 88.8±9.8 101.2±0.0 real→painting 143.6 ±"}
{"id": "2507.23771v1_chunk72", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ce between seeds TaskRandomUncertaintyActiveVMAModel CODA Sampling Testing Selector (Ours)DomainNet126real→sketch 147.1 ±38.5 197.7±13.0 269.8±119.8 119.2±51.9 88.8±9.8 101.2±0.0 real→painting 143.6 ±37.1 167.9±6.8 118.9±68.8 139.9±53.3 92.1±1.6 87.2±2.8 real→clipart 237.7 ±88.7 217.6±23.7 152.1±40.0 206.9±100.7 153.2±9.5 231.7±0.0 sketch→real 280.4 ±164.4 252.7±11.8 236.4±160.5 273.4±132.7 268.4±7.5 11.9±0.0 sketch→painting 156.6 ±72.6 181.9±7.3 237.7±151.7 157.1±60.9 173.9±7.1 13.0±0.0 sketch→clipart 228.2 ±74.5 224.6±43.2 162.0±83.6 227.9±85.6 38.1±9.2 432.4±1.9 painting →real 364.8 ±164.4 224.0±7.2 215.6±94.4 358.9±193.6 293.4±20.3 2.4±0.0 painting →sketch 179.3 ±53.5 440.7±32.9 202.5±79.7 211.5±73.3 209.8±5.2 72.3±0.2 painting →clipart 222.7 ±144.5 296.6±6.1 251.4±111.8 271.8±116.9 73.2±3.2 43.1±0.1 clipart→real 322.1 ±127.8 177.1±34.6 159.1±55.6 306.4±181.7 72.7±15.3 25.3±0.0 clipart→sketch 247.2 ±143.0 924.8±22.7 282.6±186.5 291.3±163.9 532.7±76.1 51.3±0.0 clipart→painting 147.0"}
{"id": "2507.23771v1_chunk73", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": ".2±3.2 43.1±0.1 clipart→real 322.1 ±127.8 177.1±34.6 159.1±55.6 306.4±181.7 72.7±15.3 25.3±0.0 clipart→sketch 247.2 ±143.0 924.8±22.7 282.6±186.5 291.3±163.9 532.7±76.1 51.3±0.0 clipart→painting 147.0 ±46.2 162.9±13.3 222.6±100.5 167.9±100.2 131.7±2.5 122.2±0.1WILDSiwildcam 287.0 ±65.1 380.4±7.1 392.1±194.1 440.6±103.6 459.0±56.0 201.7±0.0 camelyon 175.2 ±81.7 311.6±11.9 206.1±121.1 160.1±76.8 198.3±90.6 288.7±0.0 fmow 189.7 ±66.8 191.9±8.4 153.0±38.7 189.2±62.6 211.7±28.1 70.0±0.2 civilcomments 140.9 ±75.5 13.3±2.6 76.7±119.0 50.1±29.3 125.3±59.6 318.6±0.0MSVcifar10-low 410.6 ±231.1 629.7±5.9 399.9±207.5 476.5±114.4 567.2±23.9 58.7±0.0 cifar10-high 346.2 ±147.4 281.7±21.3 154.9±68.9 383.7±141.9 90.9±12.3 74.1±0.0 pacs 216.6 ±86.8 6.9±5.4 101.8±55.9 116.5±57.5 97.9±6.0 57.9±0.0GLUEcola 368.1 ±80.6 169.5±27.1 239.8±162.4 317.8±191.8 207.8±61.1 2226.7 ±0.0 mnli 237.4 ±132.2 80.8±27.9 234.2±105.2 312.8±111.5 148.5±114.4 23.5±0.0 qnli 222.7 ±68.5 231.6±76.2 246.6±112.4 283.0±78.8 185.7±89."}
{"id": "2507.23771v1_chunk74", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": ".1 ±80.6 169.5±27.1 239.8±162.4 317.8±191.8 207.8±61.1 2226.7 ±0.0 mnli 237.4 ±132.2 80.8±27.9 234.2±105.2 312.8±111.5 148.5±114.4 23.5±0.0 qnli 222.7 ±68.5 231.6±76.2 246.6±112.4 283.0±78.8 185.7±89.6 120.4±0.0 qqp 136.7 ±20.9 388.9±492.8 127.8±16.5 169.2±114.1 186.8±137.5 4.8±0.0 rte 375.7 ±184.6 390.3±68.4 424.4±244.4 674.7±186.7 243.6±73.2 283.8±0.0 sst2 219.9 ±108.1 89.6±39.6 174.9±54.3 373.6±128.6 202.0±131.4 51.7±0.0 mrpc 318.2 ±100.3 301.1±64.4 235.2±36.0 332.3±156.0 173.1±47.7 49.0±0.0 Table 2. Main results with variance between seeds: Cumula- tive regret at step 100. Same as Tab. 1 but ±one standard devi- ation between runs with different random seeds (5 random seeds used). Standard deviation of 0.0 (as in many CODA runs) indicates method is not stochastic for that task, e.g. because of asymmetric priors resulting in deterministic selection. 9.1.2. Instantaneous regret We report the average instantaneous regret of each method at steps 50 and 100 in Tab. 3 and Tab. 4, respecti"}
{"id": "2507.23771v1_chunk75", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ause of asymmetric priors resulting in deterministic selection. 9.1.2. Instantaneous regret We report the average instantaneous regret of each method at steps 50 and 100 in Tab. 3 and Tab. 4, respectively. These results are more influenced by stochasticity than cumula- tive regret, i.e. instantaneous regret may fluctuate widely between steps (see Fig. 8). 9.1.3. Success rate 0 50 1000.000.250.500.751.00Success rateMSV 7-10 class 0 50 1000.00.20.40.6GLUE 2-3 class 0 50 1000.050 0.025 0.0000.0250.050WILDS Multi 62-182 class 0 50 1000.00.10.2WILDS Binary 2-class 0 50 1000.000.050.100.15DomainNet 126-classRand Unc AT VMA MS CODA Figure 7. “Success rate” of each model in selecting the absolute best model at each time step. Mean over 5 random seeds. In all datasets, several methods have not yet selected the absolute best model by time step 100.TaskRandomUncertaintyActiveVMAModel CODA Sampling Testing Selector (Ours)DomainNet126real→sketch 1.3 ±0.6 2.0±0.2 0.8±0.6 1.3±1.2 0.6±0.0 1.0±0.0 real"}
{"id": "2507.23771v1_chunk76", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "selected the absolute best model by time step 100.TaskRandomUncertaintyActiveVMAModel CODA Sampling Testing Selector (Ours)DomainNet126real→sketch 1.3 ±0.6 2.0±0.2 0.8±0.6 1.3±1.2 0.6±0.0 1.0±0.0 real→painting 1.2 ±0.4 1.2±0.5 1.4±1.4 1.5±1.0 0.9±0.3 1.2±0.0 real→clipart 1.6 ±1.8 0.5±0.6 1.0±0.9 0.8±0.8 0.2±0.1 2.3±0.0 sketch→real 2.0 ±3.0 0.4±0.2 3.0±3.0 2.6±3.1 5.4±0.0 0.1±0.0 sketch→painting 1.1 ±1.3 0.9±1.1 2.2±2.3 0.9±0.6 5.5±0.0 0.1±0.0 sketch→clipart 1.8 ±1.4 0.2±0.2 1.6±1.6 2.1±0.8 0.0±0.0 4.8±0.0 painting →real 4.1 ±2.9 2.1±0.7 1.3±0.9 2.3±1.1 4.6±4.6 0.0±0.0 painting →sketch 1.5 ±1.8 4.3±0.0 2.3±1.5 1.9±1.5 1.8±0.7 0.9±0.0 painting →clipart 1.9 ±2.3 2.8±0.0 3.2±1.7 2.8±1.5 0.7±0.0 0.4±0.0 clipart→real 2.6 ±1.7 0.4±0.0 0.2±0.4 2.8±1.8 0.1±0.0 0.3±0.0 clipart→sketch 3.0 ±2.3 8.7±2.9 0.8±0.7 2.6±2.0 5.4±0.0 0.4±0.0 clipart→painting 1.6 ±0.9 1.1±0.0 2.6±1.9 1.3±1.5 1.0±0.1 1.2±0.0WILDSiwildcam 2.1 ±0.9 3.0±0.0 4.5±2.3 6.1±1.9 4.6±1.3 0.9±0.0 camelyon 0.5±0.2 3.5±0.0 2.1±1.7 0.9±1"}
{"id": "2507.23771v1_chunk77", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "9 0.8±0.7 2.6±2.0 5.4±0.0 0.4±0.0 clipart→painting 1.6 ±0.9 1.1±0.0 2.6±1.9 1.3±1.5 1.0±0.1 1.2±0.0WILDSiwildcam 2.1 ±0.9 3.0±0.0 4.5±2.3 6.1±1.9 4.6±1.3 0.9±0.0 camelyon 0.5±0.2 3.5±0.0 2.1±1.7 0.9±1.0 1.5±0.8 0.6±0.0 fmow 1.4 ±1.2 0.6±0.0 1.2±0.7 1.6±1.0 1.0±0.0 0.8±0.0 civilcomments 0.9 ±1.1 0.0±0.0 0.8±1.5 0.3±0.3 1.6±2.7 3.3±0.0MSVcifar10-low 5.5 ±4.1 7.7±0.0 5.3±3.4 4.7±2.6 7.5±0.4 0.0±0.0 cifar10-high 3.1 ±2.3 3.2±0.0 0.2±0.2 2.6±1.7 0.0±0.0 0.0±0.0 pacs 2.2 ±1.9 0.0±0.0 0.6±0.8 0.8±0.7 1.4±0.0 0.0±0.0GLUEcola 3.6 ±1.3 2.0±0.9 2.3±1.9 2.5±1.9 1.4±1.0 0.4±0.0 mnli 2.6 ±3.0 0.1±0.0 1.0±1.7 2.6±1.6 0.1±0.1 0.1±0.0 qnli 1.3 ±1.3 0.9±1.2 1.0±1.4 1.7±1.5 0.1±0.2 0.0±0.0 qqp 1.4 ±0.2 1.3±2.8 1.1±0.1 1.0±0.6 0.3±0.4 0.0±0.0 rte 3.2 ±4.4 0.0±0.0 3.7±3.6 5.9±3.9 0.0±0.0 0.0±0.0 sst2 1.9 ±2.5 0.2±0.0 0.1±0.1 3.1±3.5 0.0±0.0 0.0±0.0 mrpc 2.0 ±2.4 1.6±1.9 1.6±1.2 2.8±2.4 0.8±0.5 0.5±0.0 Table 3. Average instantaneous regret at step 50. Mean and standard deviation reported over 5 random seeds"}
{"id": "2507.23771v1_chunk78", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "0.2±0.0 0.1±0.1 3.1±3.5 0.0±0.0 0.0±0.0 mrpc 2.0 ±2.4 1.6±1.9 1.6±1.2 2.8±2.4 0.8±0.5 0.5±0.0 Table 3. Average instantaneous regret at step 50. Mean and standard deviation reported over 5 random seeds. TaskRandomUncertaintyActiveVMAModel CODA Sampling Testing Selector (Ours)DomainNet126real→sketch 1.2 ±1.1 0.0±0.0 2.2±2.2 1.0±0.6 0.8±0.5 1.0±0.0 real→painting 1.1 ±0.6 0.6±0.0 0.8±0.2 0.9±0.6 1.3±0.3 0.2±0.0 real→clipart 0.6 ±0.4 1.0±0.0 0.6±0.3 1.5±2.0 0.2±0.0 2.3±0.0 sketch→real 1.7 ±2.8 0.5±0.0 0.7±1.1 0.2±0.1 0.1±0.0 0.1±0.0 sketch→painting 0.7 ±0.7 0.0±0.0 0.8±0.8 0.8±0.9 0.1±0.1 0.1±0.0 sketch→clipart 0.9 ±0.8 0.2±0.3 0.9±0.5 2.1±2.0 0.0±0.0 0.5±0.0 painting →real 3.3 ±2.4 1.2±0.0 1.1±0.9 1.4±0.8 1.2±0.0 0.0±0.0 painting →sketch 0.6 ±0.5 0.7±0.0 0.7±0.7 1.3±1.9 1.7±0.0 0.5±0.0 painting →clipart 2.0 ±1.7 0.5±0.2 2.3±1.7 1.7±1.6 0.3±0.0 0.4±0.0 clipart→real 1.4 ±1.6 0.6±0.0 0.7±0.8 0.7±0.6 0.1±0.0 0.3±0.0 clipart→sketch 1.6 ±2.0 5.3±2.1 1.0±0.7 0.6±0.4 0.6±0.0 0.6±0.0 clipart→painti"}
{"id": "2507.23771v1_chunk79", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "g →clipart 2.0 ±1.7 0.5±0.2 2.3±1.7 1.7±1.6 0.3±0.0 0.4±0.0 clipart→real 1.4 ±1.6 0.6±0.0 0.7±0.8 0.7±0.6 0.1±0.0 0.3±0.0 clipart→sketch 1.6 ±2.0 5.3±2.1 1.0±0.7 0.6±0.4 0.6±0.0 0.6±0.0 clipart→painting 0.6±0.5 1.1±0.0 1.4±0.7 1.1±0.3 1.0±0.1 1.1±0.0WILDSiwildcam 1.4 ±1.7 3.0±0.0 3.2±3.0 2.8±2.5 4.1±1.6 0.9±0.0 camelyon 0.5±0.9 3.3±0.7 0.9±1.5 1.4±1.5 1.2±0.9 0.6±0.0 fmow 1.4 ±0.9 1.0±0.8 0.7±0.3 1.2±0.5 1.5±1.4 0.4±0.0 civilcomments 0.7 ±1.2 0.0±0.0 0.3±0.4 0.2±0.4 0.3±0.2 2.8±0.0MSVcifar10-low 1.2 ±1.3 5.7±0.0 2.8±2.9 3.2±1.2 6.3±1.9 0.0±0.0 cifar10-high 1.0 ±1.4 3.2±0.0 0.3±0.2 1.9±1.6 0.0±0.0 0.0±0.0 pacs 1.1 ±0.7 0.0±0.0 0.4±0.6 0.6±0.5 0.8±0.5 0.0±0.0GLUEcola 2.9 ±1.8 0.2±0.5 1.4±1.2 2.8±2.7 0.4±0.0 56.0±0.0 mnli 0.3 ±0.3 0.1±0.0 0.4±0.2 0.7±0.8 0.2±0.1 0.2±0.0 qnli 0.7 ±1.1 0.0±0.0 1.6±2.1 1.5±1.9 0.1±0.2 0.4±0.0 qqp 1.0 ±0.6 0.2±0.3 1.0±1.1 0.9±0.7 0.7±0.9 0.0±0.0 rte 1.2 ±2.6 0.0±0.0 0.8±1.8 4.3±4.2 0.0±0.0 0.0±0.0 sst2 0.8 ±0.7 0.1±0.0 1.0±0.9 2.0±2.9 0.0±0.0 0.0±0.0 mrpc 0.2"}
{"id": "2507.23771v1_chunk80", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "1.6±2.1 1.5±1.9 0.1±0.2 0.4±0.0 qqp 1.0 ±0.6 0.2±0.3 1.0±1.1 0.9±0.7 0.7±0.9 0.0±0.0 rte 1.2 ±2.6 0.0±0.0 0.8±1.8 4.3±4.2 0.0±0.0 0.0±0.0 sst2 0.8 ±0.7 0.1±0.0 1.0±0.9 2.0±2.9 0.0±0.0 0.0±0.0 mrpc 0.2 ±0.2 0.2±0.1 0.4±0.5 2.6±2.9 0.0±0.0 0.5±0.0 Table 4. Average instantaneous regret at step 100. Mean and standard deviation over 5 random seeds. 9.2. Unsupervised model selection results Our method defines a prior over model performance that creates a strong starting point for active model selection. This can be used in isolation, without active label collec- tion, to perform unsupervised model selection. We com- pare against five existing methods for model selection in unsupervised domain adaptation: Source validation Model selection is performed using val- idation accuracy on “source” data, i.e. using a validation set 12Task Source val DEV Entropy BNM EnsV CODADomainNet126real→sketch 0.5 4.6 3.1 3.1 0.4 1.0 real→clip 4.5 49.3 0.3 6.5 2.8 2.3 real→paint 1.3 34.7 2.3 2.0 0.2 1.2 sketch →r"}
{"id": "2507.23771v1_chunk81", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": " i.e. using a validation set 12Task Source val DEV Entropy BNM EnsV CODADomainNet126real→sketch 0.5 4.6 3.1 3.1 0.4 1.0 real→clip 4.5 49.3 0.3 6.5 2.8 2.3 real→paint 1.3 34.7 2.3 2.0 0.2 1.2 sketch →real 4.7 6.3 9.1 8.5 4.5 0.1 sketch →clip 2.4 13.4 5.9 6.4 3.0 4.8 sketch →paint 3.8 3.5 3.2 3.2 1.7 0.1 clip→real 1.5 6.3 5.6 5.6 0.5 0.3 clip→sketch 2.8 4.9 5.1 4.9 2.2 0.4 clip→paint 3.1 7.8 4.2 4.2 4.2 1.2 paint→real 0.0 36.6 3.3 3.3 0.1 0.1 paint→sketch 0.7 26.1 4.3 4.3 3.3 0.9 paint→clip 2.9 16.2 6.3 6.3 1.1 0.4WILDSiwildcam 9.8 - - - 0.9 6.1 camelyon 4.3 - - - 13.1 11.3 fmow 0.8 - - - 0.9 0.8 civilcomments - - - - 3.8 4.7MSVcifar10-low - - - - 2.3 2.3 cifar10-high - - - - 4.9 4.9 pacs - - - - 0.4 0.4GLUEcola - - - - 5.3 5.0 mnli - - - - 3.0 3.0 qnli - - - - 3.3 3.3 qqp - - - - 1.1 1.1 rte - - - - 14.8 14.8 sst2 - - - - 3.6 3.6 mrcp - - - - 1.0 1.0 Table 5. Unsupervised model selection results. We report re- gret at step 0 (lower is better) for all methods on all tasks. Best method fo"}
{"id": "2507.23771v1_chunk82", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "e - - - - 14.8 14.8 sst2 - - - - 3.6 3.6 mrcp - - - - 1.0 1.0 Table 5. Unsupervised model selection results. We report re- gret at step 0 (lower is better) for all methods on all tasks. Best method for each task is in bold. CODA matches or exceeds state- of-the-art performance on 20 out of 26 tasks. Note that because models/predictions for MSV and GLUE are black-box/one-hot (as in ModelSelector [45]), the only comparison we are able to make is to EnsV . from the same distribution as the training set. Target entropy [43] Shannon entropy is computed on pre- diction scores on the test set. Model selection is performed by selecting the model with the lowest entopy, i.e. the model that is most confident in its test predictions. Deep embedded validation [70] Computes a classification loss for each source validation sample, and weights each loss based on a computed probability that the sample “be- longs to” the test domain. This probability comes from a separate domain classifier trained on s"}
{"id": "2507.23771v1_chunk83", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ch source validation sample, and weights each loss based on a computed probability that the sample “be- longs to” the test domain. This probability comes from a separate domain classifier trained on source and test data. Batch nuclear norm [11] Performs singular value decom- position on the prediction matrix. Model selection is per- formed by selecting the model with the minmium nuclear norm ( i.e. minimum sum of singular values). EnsV [20] All models in the hypothesis set are ensembled. To perform model selection, accuracy is estimated for each model with respect to the ensemble’s predictions. 9.3. Unaggregated results on all tasks In Fig. 8 we visualize regret and cumulative regret at every step for all baselines on all tasks. 130 50 1000.02.55.07.510.0Regretpainting real 0 50 1002.55.07.510.0painting sketch 0 50 10024painting clipart 0 50 100024sketch painting 0 50 1000510sketch real 0 50 1000246sketch clipart 0 50 100 Steps0100200300Cuml. regret 0 50 1000100200300400 0 50 100010020"}
{"id": "2507.23771v1_chunk84", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": ".510.0painting sketch 0 50 10024painting clipart 0 50 100024sketch painting 0 50 1000510sketch real 0 50 1000246sketch clipart 0 50 100 Steps0100200300Cuml. regret 0 50 1000100200300400 0 50 1000100200300 0 50 1000100200 0 50 1000100200 0 50 1000100200300400Random Sampling UncertaintyActive Testing VMAModelSelector CODA (Ours) 0 50 10002468Regretclipart real 0 50 100051015clipart sketch 0 50 10012345clipart painting 0 50 100024real painting 0 50 100246real sketch 0 50 100024real clipart 0 50 100 Steps0100200300Cuml. regret 0 50 1000250500750 0 50 100050100150200 0 50 100050100150 0 50 1000100200 0 50 1000100200 0 50 1002468Regretiwildcam 0 50 10024fmow 0 50 100024civilcomments 0 50 1002.55.07.510.0camelyon 0 50 10002468cifar10 4070 0 50 1000510cifar10 5592 0 50 100024pacs 0 50 100 Steps0100200300400Cuml. regret 0 50 100050100150200 0 50 1000100200300 0 50 1000100200300 0 50 1000200400600 0 50 1000100200300400 0 50 100050100150200 0 50 10002040Regretglue/cola 0 50 10001020glue/mnli 0 50"}
{"id": "2507.23771v1_chunk85", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "0200300400Cuml. regret 0 50 100050100150200 0 50 1000100200300 0 50 1000100200300 0 50 1000200400600 0 50 1000100200300400 0 50 100050100150200 0 50 10002040Regretglue/cola 0 50 10001020glue/mnli 0 50 10005101520glue/qnli 0 50 10005101520glue/qqp 0 50 10001020glue/rte 0 50 10005101520glue/sst2 0 50 100051015glue/mrpc 0 50 100 Steps0500100015002000Cuml. regret 0 50 1000100200300 0 50 1000100200 0 50 1000200400 0 50 1000200400600 0 50 1000100200300 0 50 1000100200300Figure 8. Results on all benchmarks. 1410. Implementation details 10.1. Dawid-Skene data generating process cyicic1.true class dra wn from item c ategoric al prior2. c onfusion ro ws dra wn from dirichlets3. class dra w inde x es c onfusion matrix4. pre diction dra wn from ro w c ategoric al~~~~~ x₃0 . 00 .20 .40 . 60 . 81 . 00 . 00 .20 .40 . 60 . 81 . 00 . 00 .20 .40 . 60 . 81 . 0 x₂ x₃x₁ 0 . 00 .20 .40 . 60 . 81 . 00 . 00 .20 .40 . 60 . 81 . 00 . 00 .20 .40 . 60 . 81 . 0 x₂ x₃x₁ 0 . 00 .20 .40 . 60 . 81 . 00 . 00 .20 .40 . "}
{"id": "2507.23771v1_chunk86", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": " 00 .20 .40 . 60 . 81 . 00 . 00 .20 .40 . 60 . 81 . 0 x₂ x₃x₁ 0 . 00 .20 .40 . 60 . 81 . 00 . 00 .20 .40 . 60 . 81 . 00 . 00 .20 .40 . 60 . 81 . 0 x₂ x₃x₁ 0 . 00 .20 .40 . 60 . 81 . 00 . 00 .20 .40 . 60 . 81 . 00 . 00 .20 .40 . 60 . 81 . 0 Figure 9. A visual depiction of the Dawid-Skene [14] data gener- ating process that we adapt to active model selection. See Sec. 4.1 of the main paper for more details. We visualize the data generating process of the Bayesian implementation of the Dawid-Skene model in Fig. 9, as de- scribed in Sec. 4.1. We repeat the text here for easy refer- ence. The data generating process proceeds as follows: 1. Each data point’s true class label yiis drawn randomly from per-data-point prior distributions over which class that data point could be, yi∼Cat(π(xi)). 2. Each row of the classifier’s confusion matrix is drawn randomly from per-row distributions, Mk, c,·∼θk,c, where θk,cis the prior distribution over what the row of the confusion matrix could be. To acco"}
{"id": "2507.23771v1_chunk87", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ach row of the classifier’s confusion matrix is drawn randomly from per-row distributions, Mk, c,·∼θk,c, where θk,cis the prior distribution over what the row of the confusion matrix could be. To accommodate Bayesian updates, we initialize each θk,cto be a Dirich- let prior. 3. The sampled true class indexes into the corresponding row of the classifier’s confusion matrix, Mk, yi. 4. The classifier’s prediction for that data point is sam- pled from the distribution over that row’s cells, ˆck,i∼Cat(Mk, yi). 10.2. Computing PBest We illustrate visually the computation of PBestfrom Sec. 4.3 in Fig. 10 in the simplified case of two models. To compute the probability that Model 1 is best, we integrate over all possible accuracy values that Model 1 could have. For every possible accuracy, we compute the probability that Model 1 has that accuracy (defined by its PDF f), multiplied that the probability Model 2 has accuracy lessthan that value (defined by its CDF F). 11. Data and model details W"}
{"id": "2507.23771v1_chunk88", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "ute the probability that Model 1 has that accuracy (defined by its PDF f), multiplied that the probability Model 2 has accuracy lessthan that value (defined by its CDF F). 11. Data and model details We provide more details about the datasets and models in our benchmarking suite in Tab. 6, Tab. 7, and Tab. 8. M odel 1 Prob ability DensityM odel 2 Cumulativ e DensityAc curac yP(M odel 1 best)fmodel1(p)Fmodel2(p)integratepFigure 10. A visual depiction of the integration technique used to construct our distribution over which model is best, PBest. See Sec. 10.2 of the supplemental and Sec. 4.3 of the main paper for more details. DomainNet126 Task Num. Classes Num. Checkpoints Test set size sketch painting 12610 algs= 200 308620 epochs sketch real 12610 algs= 200 2093920 epochs sketch clipart 12610 algs= 200 561120 epochs clipart painting 12610 algs= 200 308620 epochs clipart sketch 12610 algs= 200 731320 epochs clipart real 12610 algs= 200 2093920 epochs painting sketch 12610 algs= 200 731"}
{"id": "2507.23771v1_chunk89", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "2610 algs= 200 561120 epochs clipart painting 12610 algs= 200 308620 epochs clipart sketch 12610 algs= 200 731320 epochs clipart real 12610 algs= 200 2093920 epochs painting sketch 12610 algs= 200 731320 epochs painting real 12610 algs= 200 2093920 epochs painting clipart 12610 algs= 200 561120 epochs realpainting 12610 algs= 200 308620 epochs realsketch 12610 algs= 200 731320 epochs realclipart 12610 algs= 200 561120 epochs Table 6. Dataset details: DomainNet126. For each transfer task, we first train a “source-only” model on the source domain. We then train 10 UDA models for each transfer task (source domain →target domain) using the Powerful Benchmarker codebase [43]. 15WILDS Task Num. Classes Num. Checkpoints Test set size Regret w/ val. RxRx1 11394 algs= 72 34,4320.1 18 epochs Amazon 54 algs= 12 100,0500.1 3 epochs CivilComments 24 algs= 20 133,782N/A 5 epochs fMoW 624 algs= 240 22,1080.8 60 epochs iWildCam 1824 algs= 48 42,7919.8 12 epochs Camelyon17 24 algs= 40 85,0544.3 10 epoc"}
{"id": "2507.23771v1_chunk90", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "54 algs= 12 100,0500.1 3 epochs CivilComments 24 algs= 20 133,782N/A 5 epochs fMoW 624 algs= 240 22,1080.8 60 epochs iWildCam 1824 algs= 48 42,7919.8 12 epochs Camelyon17 24 algs= 40 85,0544.3 10 epochs Table 7. Dataset details: WILDS. We show metrics for all classi- fication datasets in WILDS where we could perform model selec- tion. In our main experiments, we only use the benchmarks where near-perfect model selection can be performed trivially using the default in-distribution validation set. We train all models ourselves using the public code from Koh et al. [30]. MSV Dataset Num. Classes Num. Checkpoints Test set size CIFAR10-High 10 80 10000 CIFAR10-Low 10 80 10000 PACS 7 30 9991 GLUE Dataset Num. Classes Num. Checkpoints Test set size CoLA 2 109 1043 MNLI 3 82 9815 QNLI 2 90 5463 QQP 2 101 40430 RTE 2 87 277 SST2 2 97 872 MRPC 2 95 408 Table 8. Dataset details: MSV and GLUE. All model check- points sourced directly from Okanovic et al. [45]. 16"}
{"id": "2507.23771v1_chunk91", "paper_id": "2507.23771v1", "title": "Consensus-Driven Active Model Selection", "text": "P 2 101 40430 RTE 2 87 277 SST2 2 97 872 MRPC 2 95 408 Table 8. Dataset details: MSV and GLUE. All model check- points sourced directly from Okanovic et al. [45]. 16"}
{"id": "2507.23751v1_chunk0", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "COT-S ELF-INSTRUCT : BUILDING HIGH -QUALITY SYNTHETIC PROMPTS FOR REASONING AND NON -REASONING TASKS Ping Yu1, Jack Lanchantin1, Tianlu Wang1, Weizhe Yuan1,2, Olga Golovneva1 Ilia Kulikov1, Sainbayar Sukhbaatar1, Jason Weston1,2, Jing Xu1 1FAIR at Meta,2NYU ABSTRACT We propose CoT-Self-Instruct , a synthetic data generation method that instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the given seed tasks, and then to generate a new synthetic prompt of similar quality and complexity for use in LLM training, followed by filtering for high-quality data with automatic metrics. In verifiable reasoning, our synthetic data significantly outperforms existing training datasets, such as s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For non-verifiable instruction-following tasks, our method surpasses the performance of human or standard self-instruct prompts on both AlpacaEval 2.0 and Arena-Hard. 1 I NTRODUCTION The transformative rise of La"}
{"id": "2507.23751v1_chunk1", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "fiable instruction-following tasks, our method surpasses the performance of human or standard self-instruct prompts on both AlpacaEval 2.0 and Arena-Hard. 1 I NTRODUCTION The transformative rise of Large Language Models (LLMs) has initiated a substantial paradigm shift in the domain of deep learning (Zhang et al., 2023; Guo et al., 2023; Long et al., 2024). The development of such models emphasizes scale, and relies heavily on large volumes of high-quality data (Gandhi et al., 2024; Abdin et al., 2024). However, acquiring such data from human sources can often be challenging or even impractical due to factors such as high costs, data scarcity, and privacy concerns (Kurakin et al., 2023). Furthermore, several studies (Hosking et al., 2023; Singh et al., 2023; Gilardi et al., 2023) have pointed out that human-generated data, being inherently prone to biases and errors, may not always be ideal for model training or evaluation. In this context, synthetic data emerges as a viable alternativ"}
{"id": "2507.23751v1_chunk2", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ted out that human-generated data, being inherently prone to biases and errors, may not always be ideal for model training or evaluation. In this context, synthetic data emerges as a viable alternative for obtaining high-quality datasets. Synthetic data is artificially generated to replicate the characteristics and patterns of real-world data. One innovative approach to creating such data is the Self-Instruct method (Wang et al., 2022a), which utilizes LLMs themselves to generate instruction-following examples. This method begins by selecting a small set of seed instruction-following samples, which are then used to prompt LLMs to produce additional demonstrations in a similar format. Since then a number of variants have been introduced that increase the complexity of queries (Liu et al., 2023; Zeng et al., 2024), maintain semantic diversity (Ding et al., 2023), scale the synthetic data (Yuan et al., 2023), and use these methods in self-improvement loops (Yuan et al., 2024). However, a "}
{"id": "2507.23751v1_chunk3", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "023; Zeng et al., 2024), maintain semantic diversity (Ding et al., 2023), scale the synthetic data (Yuan et al., 2023), and use these methods in self-improvement loops (Yuan et al., 2024). However, a significant challenge with these approaches is to ensure the quality and effectiveness of the generated data for language model train- ing. Overall, generating high-quality synthetic data and optimizing its use for both reasoning and non-reasoning tasks still remains insufficiently understood. In this paper, we present Chain-of-Thought(CoT)-Self-Instruct, a method that both (i) uses reason- ing to help create high quality synthetic data; and (ii) self-filters the created data to only keep the highest quality ones, see Figure 1. We show the efficacy of this approach for creating both verifiable reasoning data and non-verifiable instruction following tasks, where in both cases using Chain-of- Thought (CoT) to help generate the examples outperforms those generated without CoT. To curate high "}
{"id": "2507.23751v1_chunk4", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " reasoning data and non-verifiable instruction following tasks, where in both cases using Chain-of- Thought (CoT) to help generate the examples outperforms those generated without CoT. To curate high quality verifiable data we introduce Answer-Consistency, which discards examples where the CoT-Self-Instruct-generated answer does not match the majority vote solution of the LLM, with the assumption that those examples are either incorrectly labeled or too difficult. For non-verifiable data we use the recent Rejecting Instruction Preferences (RIP) (Yu et al., 2025) method which measures the quality of prompts based on the distribution of reward model scores from LLM solutions. In both 1arXiv:2507.23751v1 [cs.AI] 31 Jul 2025<think> Okay, let's tackle this. The user wants me to generate a new reasoning question based on the given seed questions. Let me first understand the seed questions to ensure my new question is inspired but original. ...... Let me try to think of a concrete example ..."}
{"id": "2507.23751v1_chunk5", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "easoning question based on the given seed questions. Let me first understand the seed questions to ensure my new question is inspired but original. ...... Let me try to think of a concrete example ...... But that's similar to Seed Question 2. Maybe modify it. Alternatively, maybe use different functions. For example ...... But that seems too simple . Maybe the user wants a more complex problem. Let me think again. ...... Alternatively, think of a problem involving ...... So the answer is 1. This is a good candidate . It's a novel question, not directly copying the seeds, and the answer is a single scalar …. Let me check if it meets the requirements....... Yes, this seems to fit. The question is self-contained, not copying the seeds, and the answer is a single scalar. So this should be the new question.</think> [New Question Begin] Find the value of $ k $ such that the system of equations $ kx + y = 1 $ and $ x + ky = 1 $ has infinitely many solutions .[New Question End] [Final Answer t"}
{"id": "2507.23751v1_chunk6", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "new question.</think> [New Question Begin] Find the value of $ k $ such that the system of equations $ kx + y = 1 $ and $ x + ky = 1 $ has infinitely many solutions .[New Question End] [Final Answer to New Question Begin]\\\\boxed{1}[Final Answer to New Question End] Analyze Seed Instructions Brainstorm candidates & self evaluate Final self evaluation - Step 1 #Common Elements List#: …… - Step 2 #Plan#: The plan is to create a synthetic prompt that combines the elements of …. This prompt will explore a scenario where …. - Step 3 #Synthetic Prompt#: Develop a message for the administrators of the Meiji Shrine in Tokyo, suggesting that they incorporate more digital and interactive elements into their traditional New Year (Oshogatsu) festivities, such as an app that allows visitors to explore the shrine's history and cultural significance, or an augmented reality feature that highlights the shrine's traditional architecture and decorations. ……Find Common Elements Generate a plan Seed reason"}
{"id": "2507.23751v1_chunk7", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "he shrine's history and cultural significance, or an augmented reality feature that highlights the shrine's traditional architecture and decorations. ……Find Common Elements Generate a plan Seed reasoning Prompts Seed non-verif iable Prompts RIP Filter Answer-Consistency Filter Step1. Synthetic Instruction Creation via Chain-of-Thought (CoT) Step 2. Synthetic Instruction Curation Prompt x Answer y0 Answer y1 Majority Voted Answer y* y* match y0 ? Accept x ✅ consistent inconsistent Answer yn…… Reject x 🚫 Figure 1: CoT-Self-Instruct. Our method first prompts LLMs to reason and generate new instruc- tions given seed prompts, followed by automatically curating high-quality data using either Answer- Consistency for verifiable reasoning tasks, or RIP (Yu et al., 2025) for non-verifiable tasks. cases, filtering provides further gains. For reasoning tasks, CoT-Self-Instruct generated training data outperforms Self-Instruct and existing datasets such as s1k (Muennighoff et al., 2025) and Open- M"}
{"id": "2507.23751v1_chunk8", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ses, filtering provides further gains. For reasoning tasks, CoT-Self-Instruct generated training data outperforms Self-Instruct and existing datasets such as s1k (Muennighoff et al., 2025) and Open- MathReasoning (Moshkov et al., 2025) across MATH500, AMC23, AIME24 and GPQA-Diamond. For non-reasoning tasks, it outperforms human data from WildChat (Zhao et al., 2024) and Self- Instruct, whether filtered or not, on both AlpacaEval 2 and ArenaHard. Overall, models trained with CoT-Self-Instruct generated training data provided the best results of the methods we tested. 2 R ELATED WORK Synthetic Data Generation Synthetic data is produced using algorithms (Saxton et al., 2019), generative models (Borisov et al., 2022; Meng et al., 2022), or simulations (Vezhnevets et al., 2023), rather than being directly created by humans (Liu et al., 2024). It presents a promising solution for training models, particularly in scenarios where real-world data is scarce, expensive, or difficult to obtain. Se"}
{"id": "2507.23751v1_chunk9", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ng directly created by humans (Liu et al., 2024). It presents a promising solution for training models, particularly in scenarios where real-world data is scarce, expensive, or difficult to obtain. Self-Instruct (Wang et al., 2022a) first proposed a framework that lets a language model be prompted with seed data as few-shot examples in order to generate new synthetic data. Such data has been used to then self-train language models, e.g. in the Self-Rewarding framework (Yuan et al., 2024). Evol Instruct (Zeng et al., 2024) proposed to increase prompt complexity by letting the language model re-write the original prompts with increasing complexity. Other specific methods of creating complex synthetic data have been proposed, for example multi-hop question answering (Lupidi et al., 2024) or difficult reasoning questions (Yuan et al., 2025) both grounded on documents. Synthetic data has also been used to help train agents (Zhao et al., 2025; Zhou et al., 2025) and tool- use models (Mekala "}
{"id": "2507.23751v1_chunk10", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ifficult reasoning questions (Yuan et al., 2025) both grounded on documents. Synthetic data has also been used to help train agents (Zhao et al., 2025; Zhou et al., 2025) and tool- use models (Mekala et al., 2024), and for rewriting pre-training data (Maini et al., 2024; Nguyen et al., 2025). Synthetic Data Selection Data selection is a critical component for post-training with synthetic data (and for data in general). Previously, LLM training was regarded as largely dependent on the size of available training data (Mishra et al., 2021; Wei et al., 2021; Wang et al., 2022b). More recent work has revealed that training on a smaller yet higher-quality curated set of prompts tends to be more effective in improving models’ both instruction following and reasoning capabilities (Zhou et al., 2024; Chen et al., 2024; Muennighoff et al., 2025; Ye et al., 2025). In addition to preprocessing 2techniques such as deduplication of similar prompts using similarity metrics such as ROUGE-L similarity "}
{"id": "2507.23751v1_chunk11", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "4; Chen et al., 2024; Muennighoff et al., 2025; Ye et al., 2025). In addition to preprocessing 2techniques such as deduplication of similar prompts using similarity metrics such as ROUGE-L similarity score (Wang et al., 2022a) or clustering (Chen et al., 2023), as language models become more powerful, prompt curation can also be facilitated by using LLMs themselves as a quality judge. Recent work studies employing powerful language models to measure the complexity, diversity and quality of instructions (Lu et al., 2023; Chen et al., 2024; Touvron et al., 2023; Dubey et al., 2024; Li et al., 2023a). The success of RLHF for post-training (Stiennon et al., 2020; Rafailov et al., 2024) has attracted more attention to collecting large scale and high quality preference data. Most work involving preference optimization employs existing methods derived from pretraining and instruction-tuning (Touvron et al., 2023; Muennighoff et al., 2025), such as deduplication, clustering, quality classifier"}
{"id": "2507.23751v1_chunk12", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "preference optimization employs existing methods derived from pretraining and instruction-tuning (Touvron et al., 2023; Muennighoff et al., 2025), such as deduplication, clustering, quality classifiers or filtering heuristics. Rejecting Instruction Preferences (RIP) (Yu et al., 2025) is a recent method that gives strong performance by leveraging reward models based on the LLM responses to filter prompts. For verifiable reasoning tasks, Self-Consistency filtering (Prasad et al., 2024) has also been shown to be a high quality curation method by rejecting prompts where LLM solutions show no agreement, as the task is either incorrectly labeled or too difficult. 3 C HAIN -OF-THOUGHT (COT)-S ELF-INSTRUCT (CoT)-Self-Instruct is an approach to generate high quality synthetic data for training using rea- soning. We first assume access to a language model, and a small amount of high quality human- annotated seed data. We consider both verifiable reasoning domains, and non-verifiable general inst"}
{"id": "2507.23751v1_chunk13", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "g rea- soning. We first assume access to a language model, and a small amount of high quality human- annotated seed data. We consider both verifiable reasoning domains, and non-verifiable general instruction following. Our approach involves two stages: 1. Synthetic Instruction Creation with Chain-of-Thought (CoT): given sample human- annotated seed instructions, we instruct the LLM to reason step by step to come up with instructions of similar complexity and domain. 2. Synthetic Instruction Curation: we curate the generated synthetic data to keep only high- quality instructions for self-training. We then train LLMs using the generated high quality synthetic instructions. We describe each stage in turn. 3.1 S YNTHETIC INSTRUCTION CREATION VIA COT The process of CoT-Self-Instruct data creation starts with a small set of seed instructions as the instruction pool. Multiple instructions are sampled at random from the instruction pool, and then used to few-shot prompt a language model to gen"}
{"id": "2507.23751v1_chunk14", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "on starts with a small set of seed instructions as the instruction pool. Multiple instructions are sampled at random from the instruction pool, and then used to few-shot prompt a language model to generate a series of intermediate reasoning steps, followed by a new instruction. Unlike standard Self-Instruct (Wang et al., 2022a) which directly prompts the model to write new instructions given a list of seed instructions, each time we show the LLM an N-shot set of sample instructions, we first ask it to carefully analyze the given instructions, such as domain, complexity and purpose. After analyzing the seed instructions, reflecting on what makes them high quality prompts, the LLM is prompted to reason step by step to come up with a plan to generate a new self-contained instruction that is of similar quality and complexity as the given seed instructions, and ultimately to output the final synthetic instruction satisfying these requirements in a strict answer format. Verifiable reasoning "}
{"id": "2507.23751v1_chunk15", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "f similar quality and complexity as the given seed instructions, and ultimately to output the final synthetic instruction satisfying these requirements in a strict answer format. Verifiable reasoning tasks For reasoning tasks where there is a deterministic answer which we can compare against to generate verifiable rewards during training, we instruct the LLM to use reasoning to generate both an instruction and the verifiable target. The prompt we used for CoT- Self-Instruct on reasoning tasks is given in Figure 2. General instruction following tasks For tasks involving general instruction-following with open- ended responses, we direct the LLM to use reasoning to generate only the instruction, not the re- sponse itself. In these instances, later during training on this synthetic data we utilize a reward model to assess the responses, eliminating the need for a reference answer. The prompt we used for CoT-Self-Instruct on general instruction following tasks is given in Figure 3. Seed pr"}
{"id": "2507.23751v1_chunk16", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ilize a reward model to assess the responses, eliminating the need for a reference answer. The prompt we used for CoT-Self-Instruct on general instruction following tasks is given in Figure 3. Seed prompt pools for instruction-following typically include various different domains. When selecting few-shot prompts, for example combining prompts from storytelling and coding could result in unnatural synthetic 3Figure 2: CoT-Self-Instruct prompt generation template for verifiable reasoning tasks. You are a reasoning question generator assistant . Your goal is to create a novel, and challenging reasoning question. You are provided the following seed questions: Seed Question 1: {INSTRUCTION 1 } Seed Question 2: {INSTRUCTION 2 } Your task is to: 1. Write a brand-new, self-contained reasoning question that meets the following requirements: (a) The question draws inspiration from the seed question without copying it verbatim, remaining novel and of comparable difficulty. (b) The question’s fina"}
{"id": "2507.23751v1_chunk17", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "tion that meets the following requirements: (a) The question draws inspiration from the seed question without copying it verbatim, remaining novel and of comparable difficulty. (b) The question’s final answer should be a single, unambiguous scalar value (e.g., an integer, reduced fraction, exact radical), or another answer type that can be verified in one step (e.g., ‘yes/no,’ a choice from A to D). 2. Then reason step by step, solve the new question and format your output as follows: [New Question Begin] {your generated question }[New Question End] [Final Answer to New Question Begin] \\boxed{your final answer }[Final Answer to New Question End] Figure 3: CoT-Self-Instruct prompt generation template for general instruction following tasks. You are a prompt generator assistant . Your goal is to create diverse and creative synthetic prompts. Please follow the steps below to create synthetic prompts. Step 1: Carefully read #Prompt 1# and #Prompt 2#. Identify and list all the common elemen"}
{"id": "2507.23751v1_chunk18", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "s to create diverse and creative synthetic prompts. Please follow the steps below to create synthetic prompts. Step 1: Carefully read #Prompt 1# and #Prompt 2#. Identify and list all the common elements between these two prompts. If no common elements are found, list the main elements from each prompt. Step 2: Develop a comprehensive plan based on the #Common Elements List# or #Main Ele- ments List# from Step 1. This plan will guide the generation of new synthetic prompts that are similar to the original prompts. Step 3: Execute the plan step by step and provide one #Synthetic Prompt#. Please reply strictly in the following format: - Step 1 #Common Elements List# or #Main Elements List#: - Step 2 #Plan#: - Step 3 #Synthetic Prompt#: #Prompt 1#: {INSTRUCTION 1 } #Prompt 2#: {INSTRUCTION 2 } prompts. To address this, we propose to first label seed prompts into categories, and then sample from the categories first. 3.2 S YNTHETIC INSTRUCTION CURATION Even with the strongest language model"}
{"id": "2507.23751v1_chunk19", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " prompts. To address this, we propose to first label seed prompts into categories, and then sample from the categories first. 3.2 S YNTHETIC INSTRUCTION CURATION Even with the strongest language models, not all generated synthetic instructions are well-defined and answerable, or are effective in base model self-training. We therefore apply a curation step to select higher quality synthetic instructions from the pool of generated data for final post-training with RL. 4Verifiable reasoning tasks We propose Answer-Consistency to filter and retain only high quality data. Given the task instruction, we first instruct the LLM to generate Kresponses and take the majority response. We then reject the data example and remove it from the training pool if the majority response does not match the target answer in the synthetic data example generated by CoT- Self-Instruct (i.e., via Figure 2). Because the target answer from CoT-Self-Instruct is generated with extensive reasoning steps during instru"}
{"id": "2507.23751v1_chunk20", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "et answer in the synthetic data example generated by CoT- Self-Instruct (i.e., via Figure 2). Because the target answer from CoT-Self-Instruct is generated with extensive reasoning steps during instruction writing and question-answering (Figure 2) this differs from the way a response is generated at inference time and might be more accurate. Hence, comparing if the two labels match gives an extra layer of filtering. We confirm in our experiments that Answer-Consistency is superior to standard Self-Consistency filtering (Prasad et al., 2024). General instruction following tasks For non-verifiable tasks, the Answer-Consistency method is not applicable. Instead, we employ the Rejecting Instruction Preferences ( RIP) method as proposed by Yu et al. (2025). In this method, for a given task instruction, Kresponses are generated, and each response is evaluated using a reward model (RM), resulting in a score for each response. The filtering process is then based on the distribution of these sc"}
{"id": "2507.23751v1_chunk21", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ruction, Kresponses are generated, and each response is evaluated using a reward model (RM), resulting in a score for each response. The filtering process is then based on the distribution of these scores. In our setting we use the lowest score among these Kresponses to represent the score of the synthetic prompt. We then filter the data by selecting only those prompts with the higher scores. Notably, this approach can also be applied to verifiable tasks, and we conduct experiments in that context as well. 3.3 S ELF-TRAINING WITH SYNTHETIC DATA After generating the synthetic training data, we can conduct RL training on the set of generated instructions. We compare the performance of self-trained LLMs with models trained on human- annotated data and on seed instructions in reasoning and non-reasoning domains respectively. For verifiable reasoning tasks, we use GRPO (Shao et al., 2024), and for general instruction following we consider both offline DPO (Rafailov et al., 2024) and online "}
{"id": "2507.23751v1_chunk22", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "-reasoning domains respectively. For verifiable reasoning tasks, we use GRPO (Shao et al., 2024), and for general instruction following we consider both offline DPO (Rafailov et al., 2024) and online DPO, which can perform much better, see e.g. Lanchantin et al. (2025). 4 E XPERIMENTAL SETUP We study the effectiveness of our synthetic prompt generation approach for reasoning and non- reasoning domains along the following two axes: synthetic prompt generation, and prompt curation. 4.1 R EASONING Seed Instructions We use s1k (Muennighoff et al., 2025) reasoning instructions as our seed rea- soning tasks. The s1k dataset consists of 1000 high-quality, diverse and difficult reasoning prompts. To conduct self-training with verifiable rewards we select a subset of s1k consisting of 893 verifi- able reasoning instructions by filtering out theorem-proving questions and only keeping those that yield a scalar, single-valued, or simple closed-form answers that can be easily verified (such as 1,A,"}
{"id": "2507.23751v1_chunk23", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ble reasoning instructions by filtering out theorem-proving questions and only keeping those that yield a scalar, single-valued, or simple closed-form answers that can be easily verified (such as 1,A, False ,2(n−1)(n−2) n(n+1)). We then use this subset as the seed instruction pool to generate more verifiable reasoning instructions. Prompt Generation The CoT-Self-Instruct template is given in Figure 2, and the baseline Self- Instruct method is given in Appendix Figure 6. To evaluate how CoT-Self-Instruct compares to base- lines for generating verifiable reasoning tasks, we apply these methods to Qwen3-4B-Base models, Qwen3-4B model with Think mode and Qwen3-4B model with NoThink mode (Yang et al., 2025). We use temperature = 0.7 and top-p=0.8 for Qwen3-4B-Base and Qwen3-4B (NoThink mode), and temperature = 0.6 and top-p=0.95 for Qwen3-4B (Think mode). RLVR Training All our reasoning experiments use GRPO training initialized from Qwen3-4B- Base with reinforcement learning from rule-based"}
{"id": "2507.23751v1_chunk24", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "nd temperature = 0.6 and top-p=0.95 for Qwen3-4B (Think mode). RLVR Training All our reasoning experiments use GRPO training initialized from Qwen3-4B- Base with reinforcement learning from rule-based verifiable rewards (RLVR). For hyperparameters, we use a cosine learning rate scheduler with a peak value of 1e−6and adopt the AdamW optimizer for the policy model. We set the number of training epochs to 40 with a batch size of 128. For rollouts, we sample 16 rollouts for each prompt with temperature = 0.6 and top-p=0.95, with a 5maximum length of 4096 tokens. All GRPO experiments are conducted with VeRL(Sheng et al., 2024) and Math-Verify1as verifier. Baselines & Variations We construct targets for Self-Instruct and CoT-Self-Instruct when build- ing the generated instructions. As a baseline, we also train on the original s1k prompts rather than synthetic data, in that case we use the public available DeepSeek R1 (Guo et al., 2025) thinking solution from simplescaling/s1K-1.1 to build ta"}
{"id": "2507.23751v1_chunk25", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": ", we also train on the original s1k prompts rather than synthetic data, in that case we use the public available DeepSeek R1 (Guo et al., 2025) thinking solution from simplescaling/s1K-1.1 to build targets. We also compare to training on OpenMath- Reasoning which consists of 10k prompts (Moshkov et al., 2025) with publicly available solutions by DeepSeek-R1 and QwQ-32B. We also explore some alternative ways of filtering data or con- structing targets, explored as variations on our main experiments: • Self-Consistency filtering: generating responses Ktimes with random seeds and then se- lecting the majority-voted answer as the target or rejecting the example if the majority answer receives fewer votes than a given threshold (50% in our main experiments). • RIP filtering: we use the infly/INF-ORM-Llama3.1-70B (Minghao Yang, 2024) RM. • Best-of-K targets: constructing targets by selecting the highest scored answer out of K responses using INF-ORM-Llama3.1-70B RM. Evaluation We evaluate us"}
{"id": "2507.23751v1_chunk26", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "F-ORM-Llama3.1-70B (Minghao Yang, 2024) RM. • Best-of-K targets: constructing targets by selecting the highest scored answer out of K responses using INF-ORM-Llama3.1-70B RM. Evaluation We evaluate using Math500 (Hendrycks et al., 2021; Lightman et al., 2023), AIME 2024, AMC 23, and GPQA Diamond (Rein et al., 2024). We use temperature 0.6 and top-p 0.95 to generate predictions. For each problem we generate N = 16 solutions and report average accuracy. 4.2 N ON-VERIFIABLE INSTRUCTION FOLLOWING Seed Instructions We use the Wildchat-RIP-Filtered-by-8b-Llama dataset2which includes 4k high-quality prompts filtered from 20k raw wildchat prompts as our seed prompts. Unlike in some reasoning tasks, this data includes a variety of different domains. Combining, for instance, prompts from storytelling and coding as few-shot examples could result in generating unnatural synthetic prompts. To address this, we categorized all seed data into 8 distinct categories. During sampling, we select 2 seed pr"}
{"id": "2507.23751v1_chunk27", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "g and coding as few-shot examples could result in generating unnatural synthetic prompts. To address this, we categorized all seed data into 8 distinct categories. During sampling, we select 2 seed prompts from the same category to serve as few-shot prompts. Our seed data spans 8 categories: Writing & Storytelling, Technical & Programming, Creative & Design, Data & Analy- sis, Education & Research, Communication & Support, Business & Marketing, and Miscellaneous. Prompt Generation We evaluate how CoT-Self-Instruct compares to baselines for generating non-verifiable instruction-following tasks by applying these methods using LLama 3.1-8B-Instruct. The CoT-Self-Instruct template is given in Figure 3, and the baseline Self-Instruct method is given in Figure 4. We also experiment with a prompt that lies between the two methods by generating a short rather than long CoT, given in Figure 5. For RIP filtering, we use the Athene-RM-8B model3 reward model over 32 responses. DPO Training We trai"}
{"id": "2507.23751v1_chunk28", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " that lies between the two methods by generating a short rather than long CoT, given in Figure 5. For RIP filtering, we use the Athene-RM-8B model3 reward model over 32 responses. DPO Training We train via DPO starting from LLama 3.1-8B-Instruct, leveraging the fairseq2 library (Balioglu, 2023). We use a batch size of 64and learning rate of 1e−6with dropout rate of 0.0 and a βvalue of 0.1 throughout the experiments. For each prompt, we generate 64 responses. These responses are then annotated with Athene-RM-8B to select pairs. Compared to human prompts, our synthetic prompts tend to be more complex, resulting in longer average response lengths, which can lead to length explosion. During DPO training, the evaluation judge often favors longer responses, potentially causing response lengths to increase over time (Yuan et al., 2024). To mitigate this issue, we adopted the approach outlined by Wu et al. (2024), which involves combining the reward score with length information to determine t"}
{"id": "2507.23751v1_chunk29", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "o increase over time (Yuan et al., 2024). To mitigate this issue, we adopted the approach outlined by Wu et al. (2024), which involves combining the reward score with length information to determine the preferred response. This method ensures that shorter responses are selected when scores are similar. We applied a length normalization coefficient of 0.2 for the length-normalized reward. This is applied for all methods, in each case sampling 5k DPO pairs. 1https://github.com/huggingface/Math-Verify 2https://huggingface.co/datasets/facebook/Wildchat-RIP-Filtered-by-8b-Llama 3https://huggingface.co/Nexusflow/Athene-RM-8B . 6Table 1: CoT-Self-Instruct results on reasoning tasks , comparing to baselines, fine-tuning Qwen3- 4B-Base with GRPO. For Self-Instruct and CoT-Self-Instruct the synthetic data (including targets) is constructed with Qwen3-4B. We report pass@1 averaged over 16 seeds. CoT-Self-Instruct gen- erates synthetic data that outperforms existing prompt training sets and the Se"}
{"id": "2507.23751v1_chunk30", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "c data (including targets) is constructed with Qwen3-4B. We report pass@1 averaged over 16 seeds. CoT-Self-Instruct gen- erates synthetic data that outperforms existing prompt training sets and the Self-Instruct method, particularly when applying our data filtering methods. MATH AIME AMC GPQA # Train 500 24 23 Diamond Avg. ↑ Qwen3-4B-Base (Zero-Shot) - 67.4 10.6 42.0 24.2 36.1 s1kPrompts + (R1) Gold Label 893 68.6 18.5 51.3 40.1 44.6 OpenMathReasoning Prompts + Gold Label 10,000 79.0 13.3 62.5 35.4 47.5 Self-Instruct 5000 81.1 16.3 58.1 42.5 49.5 + Self-Consistency Filter 3467 83.6 18.5 68.5 44.1 53.6 + RIP Filter 2254 84.5 21.2 65.9 45.5 54.5 CoT-Self-Instruct 5000 84.9 20.4 62.2 44.4 53.0 + Self-Consistency Filter 4034 85.2 22.5 67.8 44.9 55.1 + RIP Filter 2419 85.7 24.4 70.5 44.4 56.2 + Answer-Consistency Filter 2926 86.5 24.6 72.3 45.5 57.2 + Answer-Consistency Filter (more data) 10,000 86.7 26.7 73.8 47.4 58.7 Online DPO training We also assess online DPO by following the training"}
{"id": "2507.23751v1_chunk31", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " Answer-Consistency Filter 2926 86.5 24.6 72.3 45.5 57.2 + Answer-Consistency Filter (more data) 10,000 86.7 26.7 73.8 47.4 58.7 Online DPO training We also assess online DPO by following the training settings described by Lanchantin et al. (2025). We use the default sampling parameters (temperature= 1.0, top-p= 1.0) to generate exploration rollouts. We train models using the fairseq2 library (Balioglu, 2023), where model inference is performed with the vllm library (Kwon et al., 2023). Evaluation To evaluate the helpfulness and quality of responses, we employ AlpacaEval 2.0 (Li et al., 2023b; Dubois et al., 2024) and Arena-Hard (Li et al., 2024b;a). These are robust instruction- following benchmarks that show a strong correlation with user preferences. Originally, AlpacaEval used GPT-4 Preview (11/06) as the judge, while Arena-Hard utilized GPT-4.1 for its leaderboard. However, since we do not have access to these specific OpenAI API versions, we conduct our tests using two alternativ"}
{"id": "2507.23751v1_chunk32", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "eview (11/06) as the judge, while Arena-Hard utilized GPT-4.1 for its leaderboard. However, since we do not have access to these specific OpenAI API versions, we conduct our tests using two alternative (and newer) judges: GPT-4-turbo and GPT-4o. For generating predictions, we set the decoding temperature to 0.6 and the top-p to 0.9, aligning with the commonly used values of the seed model in our study. Our validation set used for checkpoint selection is based on a held-out set of 470 examples, comprising 253 validation examples from Li et al. (2023a) and 218 Evol-Test set examples from Xu et al. (2023). 5 E XPERIMENTAL RESULTS Our main results are given in Table 1 for reasoning tasks and Table 2 for non-reasoning tasks. Various other variations and ablations are given in the Appendix. 5.1 R EASONING TASKS Synthetic instructions generated by CoT-Self-Instruct outperform Self-Instruct In Table 1 where Qwen3-4B-Base models are GRPO trained on Qwen3-4B generated prompts and responses, CoT-"}
{"id": "2507.23751v1_chunk33", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "R EASONING TASKS Synthetic instructions generated by CoT-Self-Instruct outperform Self-Instruct In Table 1 where Qwen3-4B-Base models are GRPO trained on Qwen3-4B generated prompts and responses, CoT-Self-Instruct achieves an average accuracy of 53.0%, outperforming Self-Instruct which yields 49.5% (both without data filtering). As shown in Appendix Table 8 similar trends are observed when training on Qwen3-4B-base, rather than Qwen3-4B, generated targets. Filtered CoT-Self-Instruct outperforms filtered Self-Instruct Applying filtering methods to both CoT-Self-Instruct and Self-Instruct improves both methods, despite the overall amount of train- ing data decreasing, see Table 1. That is, it is better to have high quality synthetic data than more data that is lower quality. However, we find that CoT-Self-Instruct still maintains its advantage over Self-Instruct, whichever filtering method is used. E.g. with Self-Consistency Filtering Self-Instruct improves from 49.5% →53.6%, while CoT-S"}
{"id": "2507.23751v1_chunk34", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " that CoT-Self-Instruct still maintains its advantage over Self-Instruct, whichever filtering method is used. E.g. with Self-Consistency Filtering Self-Instruct improves from 49.5% →53.6%, while CoT-Self-Instruct improves from 53.0% →55.1%. Similar findings are observed with RIP filtering as well. We find empirically that the optimal filtering criteria is to filter out prompts with lower than 50% RIP-Scores. 7Table 2: CoT-Self-Instruct results on general instruction following tasks , comparing to base- lines, fine-tuning LLama 3.1-8B-Instruct with offline and online DPO. CoT-Self-Instruct generates synthetic data that outperforms human-written prompt training sets and the Self-Instruct method, particularly when applying our data filtering methods. Both AlpacaEval 2 and ArenaHard are eval- uated with two kinds of judge: GPT-4 Turbo and GPT-4o, with similar conclusions. Training AlpacaEval LC Winrate ArenaHard Score Method GPT-4 Turbo GPT-4o GPT-4 Turbo GPT-4oAvg.↑ LLama 3.1-8B-Instruct "}
{"id": "2507.23751v1_chunk35", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "uated with two kinds of judge: GPT-4 Turbo and GPT-4o, with similar conclusions. Training AlpacaEval LC Winrate ArenaHard Score Method GPT-4 Turbo GPT-4o GPT-4 Turbo GPT-4oAvg.↑ LLama 3.1-8B-Instruct DPO 27.3 21.3 32.0 27.8 27.1 Human prompts (WildChat) DPO 49.1 43.0 52.7 42.6 46.8 + RIP Filter DPO 57.6 44.5 59.1 41.7 50.7 Self-Instruct DPO 52.9 46.0 51.8 39.2 47.4 + RIP Filter DPO 55.2 46.1 55.6 39.5 49.1 CoT-Self-Instruct DPO 58.5 48.6 62.0 46.7 53.9 + RIP Filter DPO 63.2 49.4 60.2 45.8 54.7 Human prompts (Wildchat) Online DPO 80.1 62.7 64.4 45.5 63.1 CoT-Self-Instruct + RIP Online DPO 83.2 68.7 67.3 49.3 67.1 High quality synthetic prompts generated by CoT-Self-Instruct significantly outperform seed instructions and other publicly available reasoning prompts CoT-Self-Instructions outperform s1k, see Table 1, where models trained on 2926 filtered CoT-Self-Instructions achieve 57.2%. This is much higher than the 44.6% achieved with s1k prompts using R1 labels (and s1k results are even"}
{"id": "2507.23751v1_chunk36", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "form s1k, see Table 1, where models trained on 2926 filtered CoT-Self-Instructions achieve 57.2%. This is much higher than the 44.6% achieved with s1k prompts using R1 labels (and s1k results are even lower, 43.8%, with Qwen3-4B labels, see Appendix Table 5). Filtering CoT-Self-Instruction to the same training size as s1k yields 54.2%, still significantly higher, see Appendix Table 3. These results also outperform using 10K OpenMath-Reasoning instructions with gold labels, which gives 47.5%. Increasing the CoT-Self-Instruction with Answer-Consistency filtering data to 10k improves results further with an average of 58.7%. Overall, CoT-Self-Instruct with Answer-Consistency filter gives the best performance of all existing datasets or synthetic data construction methods tested. 5.2 N ON-VERIFIABLE INSTRUCTION FOLLOWING TASKS Synthetic instructions generated by CoT-Self-Instruct outperform Self-Instruct For non- reasoning tasks, allowing the model to create a plan beforehand with CoT-Self"}
{"id": "2507.23751v1_chunk37", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "FIABLE INSTRUCTION FOLLOWING TASKS Synthetic instructions generated by CoT-Self-Instruct outperform Self-Instruct For non- reasoning tasks, allowing the model to create a plan beforehand with CoT-Self-Instruct also sig- nificantly enhances the quality of synthetic data, see Table 2. Averaged over AlpacaEval 2 and ArenaHard, CoT-Self-Instruct achieves an average of 53.9 vs. Self-Instruct’s 47.4, both without fil- tering and training with DPO. We also observe that asking for longer CoT reasoning chains provides more gains that shorter CoTs, see Appendix Table 11, further emphasizing the need for reasoning when producing synthetic data. RIP Filtering improves CoT-Self-Instruct results further Applying the RIP filter to each method we find it to be effective across all types of synthetic generation methods tested. This boosts the CoT-Self-Instruct results from 53.9 →54.7. RIP also improves Self-Instruct as well, from 47.4 →49.1, but still underperforming CoT-Self-Instruct. We can also appl"}
{"id": "2507.23751v1_chunk38", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ation methods tested. This boosts the CoT-Self-Instruct results from 53.9 →54.7. RIP also improves Self-Instruct as well, from 47.4 →49.1, but still underperforming CoT-Self-Instruct. We can also apply RIP filtering to human prompts from WildChat in a similar manner. In this case we actually see a larger boost, from 46.8→50.7. We attribute this to human data being relatively noisy compared to synthetic data, which can make filtering more important. High quality synthetic prompts generated by CoT-Instruct significantly outperform human prompts Our best performing DPO-trained model is achieved by using CoT-Self-Instruct with RIP data filtering, yielding 54.7. This outperforms LLama 3.1-8B-Instruct (27.1) or training on hu- man prompts from WildChat with (46.8) or without RIP data filtering (50.7). We also performed experiments with online DPO, which improved results further. In that setting human prompts from WildChat obtain 63.1 while CoT-Self-Instruct+RIP obtains 67.1. Overall, we find"}
{"id": "2507.23751v1_chunk39", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "50.7). We also performed experiments with online DPO, which improved results further. In that setting human prompts from WildChat obtain 63.1 while CoT-Self-Instruct+RIP obtains 67.1. Overall, we find CoT-Self-Instruct with RIP filtering to yield the best performance over all existing datasets or synthetic data construc- tion methods tested. 86 C ONCLUSION In this paper, we propose CoT-Self-Instruct, a synthetic data creation and curation pipeline that in- structs LLMs to plan and reason to come up with new synthetic prompts given seed instructions, and then filters them for quality, either using Answer-Consistency for verifiable tasks or RIP fil- tering when they are not verifiable. We show that applying our method improves models’ abilities in both the reasoning and non-reasoning domains by creating high quality synthetic instructions for RL training, surpassing existing seed human-annotated instructions and public training sets on challenging benchmarks. REFERENCES Marah Abdin, Jyot"}
{"id": "2507.23751v1_chunk40", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " by creating high quality synthetic instructions for RL training, surpassing existing seed human-annotated instructions and public training sets on challenging benchmarks. REFERENCES Marah Abdin, Jyoti Aneja, Harkirat Behl, S ´ebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 techni- cal report. arXiv preprint arXiv:2412.08905 , 2024. Can Balioglu. fairseq2, 2023. URL http://github.com/facebookresearch/ fairseq2 . Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models are realistic tabular data generators. arXiv preprint arXiv:2210.06280 , 2022. Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. Instructzero: Efficient instruction optimization for black-box large language models. arXiv preprint arXiv:2306.03082 , 2023. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng"}
{"id": "2507.23751v1_chunk41", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " for black-box large language models. arXiv preprint arXiv:2306.03082 , 2023. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. AlpaGasus: Training a better alpaca with fewer data. InThe Twelfth International Conference on Learning Representations , 2024. URL https: //openreview.net/forum?id=FdVXgSJhvz . Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233 , 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 , 2024. Yann Dubois, Bal ´azs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled al- pacaeval: A simple way to debias automatic evaluators."}
{"id": "2507.23751v1_chunk42", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "of models. arXiv preprint arXiv:2407.21783 , 2024. Yann Dubois, Bal ´azs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled al- pacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475 , 2024. Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, and Graham Neubig. Better syn- thetic data by retrieving and transforming existing datasets. arXiv preprint arXiv:2404.14361 , 2024. Fabrizio Gilardi, Meysam Alizadeh, and Ma ¨el Kubli. Chatgpt outperforms crowd workers for text- annotation tasks. Proceedings of the National Academy of Sciences , 120(30):e2305016120, 2023. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yu- peng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597 , 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incent"}
{"id": "2507.23751v1_chunk43", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "and detection. arXiv preprint arXiv:2301.07597 , 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 , 2021. Tom Hosking, Phil Blunsom, and Max Bartolo. Human feedback is not gold standard. arXiv preprint arXiv:2309.16349 , 2023. 9Alexey Kurakin, Natalia Ponomareva, Umar Syed, Liam MacDermed, and Andreas Terzis. Harness- ing large-language models to generate private synthetic text. arXiv preprint arXiv:2306.01684 , 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for l"}
{"id": "2507.23751v1_chunk44", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "arXiv preprint arXiv:2306.01684 , 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles , 2023. Jack Lanchantin, Angelica Chen, Janice Lan, Xian Li, Swarnadeep Saha, Tianlu Wang, Jing Xu, Ping Yu, Weizhe Yuan, Jason E Weston, et al. Bridging offline and online reinforcement learning for llms. arXiv preprint arXiv:2506.21495 , 2025. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gon- zalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939 , 2024a. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024b."}
{"id": "2507.23751v1_chunk45", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "406.11939 , 2024a. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024b. URL https://lmsys.org/blog/2024-04-19-arena-hard/ . Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259 , 2023a. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval , 5 2023b. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth International Conference on Learning Representations , 2023. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Z"}
{"id": "2507.23751v1_chunk46", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth International Conference on Learning Representations , 2023. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et al. Best practices and lessons learned on synthetic data. arXiv preprint arXiv:2404.07503 , 2024. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for align- ment? a comprehensive study of automatic data selection in instruction tuning. arXiv preprint arXiv:2312.15685 , 2023. Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. On llms-driven synthetic data generation, curation, and evaluation: A survey. arXiv preprint arXiv:2406.15126 , 2024. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. # instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In The Twelfth Intern"}
{"id": "2507.23751v1_chunk47", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. # instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In The Twelfth International Conference on Learning Representations , 2023. Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foer- ster, Roberta Raileanu, and Maria Lomeli. Source2synth: Synthetic data generation and curation grounded in real data sources. arXiv preprint arXiv:2409.08239 , 2024. Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. Rephras- ing the web: A recipe for compute and data-efficient language modeling. arXiv preprint arXiv:2401.16380 , 2024. Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria Lomeli, Jingbo Shang, and Jane Dwivedi-Yu. Toolverifier: Generalization to new tools via self-verification. arXiv preprint arXiv:2402.14158 , 2024. Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Generating tr"}
{"id": "2507.23751v1_chunk48", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ngbo Shang, and Jane Dwivedi-Yu. Toolverifier: Generalization to new tools via self-verification. arXiv preprint arXiv:2402.14158 , 2024. Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Generating training data with language models: Towards zero-shot language understanding. Advances in Neural Information Processing Systems , 35:462–477, 2022. 10Xiaoyu Tan Minghao Yang, Chao Qu. Inf-orm-llama3.1-70b, 2024. URL [https:// huggingface.co/infly/INF-ORM-Llama3.1-70B](https://huggingface. co/infly/INF-ORM-Llama3.1-70B) . Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773 , 2021. Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art math- ematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891 , 2025. Niklas Muen"}
{"id": "2507.23751v1_chunk49", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art math- ematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891 , 2025. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand `es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393 , 2025. Thao Nguyen, Yang Li, Olga Golovneva, Luke Zettlemoyer, Sewoong Oh, Ludwig Schmidt, and Xian Li. Recycling the web: A method to enhance pre-training data quality and quantity for language models. arXiv preprint arXiv:2506.04689 , 2025. Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, and Jane Yu. Self-consistency preference opti- mization. arXiv preprint arXiv:2411.04109 , 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direc"}
{"id": "2507.23751v1_chunk50", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ane Yu. Self-consistency preference opti- mization. arXiv preprint arXiv:2411.04109 , 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems , 36, 2024. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Di- rani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a bench- mark. In First Conference on Language Modeling , 2024. David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical rea- soning abilities of neural models. arXiv preprint arXiv:1904.01557 , 2019. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024. Guangm"}
{"id": "2507.23751v1_chunk51", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "iao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 , 2024. Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585 , 2023. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems , 33:3008–3021, 2020. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmi"}
{"id": "2507.23751v1_chunk52", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "stiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems , 33:3008–3021, 2020. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. Alexander Sasha Vezhnevets, John P Agapiou, Avia Aharon, Ron Ziv, Jayd Matyas, Edgar A Du´e˜nez-Guzm ´an, William A Cunningham, Simon Osindero, Danny Karmon, and Joel Z Leibo. Generative agent-based modeling with actions grounded in physical, social, or digital space using concordia. arXiv preprint arXiv:2312.03664 , 2023. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560 , 2022a. 11Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh K"}
{"id": "2507.23751v1_chunk53", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "annaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560 , 2022a. 11Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, An- jana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705 , 2022b. Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 , 2021. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594 , 2024. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. W"}
{"id": "2507.23751v1_chunk54", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594 , 2024. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 , 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 , 2025. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387 , 2025. Ping Yu, Weizhe Yuan, Olga Golovneva, Tianhao Wu, Sainbayar Sukhbaatar, Jason Weston, and Jing Xu. Rip: Better models by survival of the fittest prompts. arXiv preprint arXiv:2501.18578 , 2025. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2"}
{"id": "2507.23751v1_chunk55", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ompts. arXiv preprint arXiv:2501.18578 , 2025. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020 , 3, 2024. Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Ilia Kulikov, Kyunghyun Cho, Dong Wang, Yuandong Tian, Jason E Weston, et al. Naturalreasoning: Reasoning in the wild with 2.8 m challenging questions. arXiv preprint arXiv:2502.13124 , 2025. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825 , 2023. Weihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, and Weizhu Chen. Automatic instruction evolving for large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , pp. 6998–7018, 2024. Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, C"}
{"id": "2507.23751v1_chunk56", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ng for large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , pp. 6998–7018, 2024. Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam, Chu Myaet Thwal, Ye Lin Tun, Le Luang Huy, et al. A complete sur- vey on generative ai (aigc): Is chatgpt from gpt-4 to gpt-5 all you need? arXiv preprint arXiv:2303.11717 , 2023. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335 , 2025. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470 , 2024. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neura"}
{"id": "2507.23751v1_chunk57", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "rXiv:2405.01470 , 2024. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems , 36, 2024. Yifei Zhou, Sergey Levine, Jason Weston, Xian Li, and Sainbayar Sukhbaatar. Self-challenging language model agents. arXiv preprint arXiv:2506.01716 , 2025. 12Figure 4: Self-Instruct prompt generation template for non-verifiable instruction following tasks. Below are sample tasks from user. 1.<begin>{INSTRUCTION 1 }</end> 2.<begin>{INSTRUCTION 2 }</end> Come up with one new task, wrapped with <begin>and</end> Figure 5: Short CoT prompt generation template for non-verifiable instruction following tasks. Below are sample tasks from user. 1.<begin>{INSTRUCTION 1 }</end> 2.<begin>{INSTRUCTION 2 }</end> Come up with one new task, wrapped with <begin>and</end>. Please provide your Chain- of-Thought first and then provide the new generated task. 7 A PPE"}
{"id": "2507.23751v1_chunk58", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "RUCTION 1 }</end> 2.<begin>{INSTRUCTION 2 }</end> Come up with one new task, wrapped with <begin>and</end>. Please provide your Chain- of-Thought first and then provide the new generated task. 7 A PPENDIX We report results when matching the training size to 893the same as our seed tasks in Table 3. Table 3: CoT-Self-Instruct results on reasoning tasks with same size training sets , comparing to baselines, fine-tuning Qwen3-4B-Base with GRPO. For Self-Instruct and CoT-Self-Instruct the synthetic data (including targets) is constructed with Qwen3-4B. We report pass@1 averaged over 16 seeds. # TrainMATH AIME AMC GPQA 500 24 23 Diamond Avg. ↑ Qwen3-4B-Base (Zero-Shot) - 67.4 10.6 42.0 24.2 36.1 s1kPrompts + (R1) Gold Label 893 68.6 18.5 51.3 40.1 44.6 Self-Instruct 893 80.5 17.2 57.3 41.3 49.1 + Self-Consistency Filter 893 81.9 20.0 62.8 41.5 51.5 + RIP Filter 893 82.7 21.5 61.4 43.1 52.2 CoT-Self-Instruct 893 82.4 19.8 60.0 41.3 50.9 + Self-Consistency Filter 893 83.2 22.7 65.1 41.6 53.1 "}
{"id": "2507.23751v1_chunk59", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " + Self-Consistency Filter 893 81.9 20.0 62.8 41.5 51.5 + RIP Filter 893 82.7 21.5 61.4 43.1 52.2 CoT-Self-Instruct 893 82.4 19.8 60.0 41.3 50.9 + Self-Consistency Filter 893 83.2 22.7 65.1 41.6 53.1 + RIP Filter 893 83.0 21.0 63.9 42.9 52.7 + Answer-Consistency Filter 893 83.7 23.1 66.1 44.1 54.2 We further compare CoT-Self-Instruct with other templates on reasoning tasks: • Self-Instruct-Then-Solve (NoCoT): prompting LLMs to first generate a question then an answer to its own generated question, without any thinking or CoT, see Figure 8. • CoT-Self-Instruct (NoSolve): prompting LLMs to reason step-by-step to generate a ques- tion, without giving the “reference” answer, see Figure 7. We report additional results with varying prompt templates below. 13Figure 6: Self-Instruct (standard, without CoT) prompt generation template for verifiable reasoning tasks. You are a reasoning question generator assistant . Your goal is to create a novel, and challenging reasoning question. You are prov"}
{"id": "2507.23751v1_chunk60", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "hout CoT) prompt generation template for verifiable reasoning tasks. You are a reasoning question generator assistant . Your goal is to create a novel, and challenging reasoning question. You are provided the following seed questions: Seed Question 1: {INSTRUCTION 1 } Seed Question 2: {INSTRUCTION 2 } Your task is to write a brand-new, self-contained reasoning question that meets the following require- ments: 1. The question draws inspiration from the seed question without copying it verbatim, remaining novel and of comparable difficulty. 2. The question’s final answer should be a single, unambiguous scalar value (e.g., an integer, reduced fraction, exact radical), or another answer type that can be verified in one step (e.g., ‘yes/no,’ a choice from A to D). 3. Do not include any solution, hint, or answer-—only the question statement itself. Please put your generated problem strictly in the format of [New Question Begin] {your generated question }[New Question End] Figure 7: CoT-Self-"}
{"id": "2507.23751v1_chunk61", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": ", hint, or answer-—only the question statement itself. Please put your generated problem strictly in the format of [New Question Begin] {your generated question }[New Question End] Figure 7: CoT-Self-Instruct (No-Solve) prompt generation template for verifiable reasoning tasks without answering (i.e., generate a question only). You are a reasoning question generator assistant . Your goal is to create a novel, and challenging reasoning question. You are provided the following seed questions: Seed Question 1: {INSTRUCTION 1 } Seed Question 2: {INSTRUCTION 2 } Your task is to write a brand-new, self-contained reasoning question that meets the following require- ments: 1. The question draws inspiration from the seed question without copying it verbatim, remaining novel and of comparable difficulty. 2. The question’s final answer should be a single, unambiguous scalar value (e.g., an integer, reduced fraction, exact radical), or another answer type that can be verified in one step (e.g., ‘y"}
{"id": "2507.23751v1_chunk62", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "ulty. 2. The question’s final answer should be a single, unambiguous scalar value (e.g., an integer, reduced fraction, exact radical), or another answer type that can be verified in one step (e.g., ‘yes/no,’ a choice from A to D). 3. Do not include any solution, hint, or answer-—only the question statement itself. Please reason step by step and put your generated problem strictly in the format of [New Question Begin] {your generated question }[New Question End] 14Figure 8: Self-Instruct-Then-Solve (i.e. No CoT) prompt generation template for verifiable reason- ing tasks. You are a reasoning question generator assistant . Your goal is to create a novel, and challenging reasoning question. You are provided the following seed questions: Seed Question 1: {INSTRUCTION 1 } Seed Question 2: {INSTRUCTION 2 } Your task is to: 1. Write a brand-new, self-contained reasoning question that meets the following requirements: (a) The question draws inspiration from the seed question without copying it"}
{"id": "2507.23751v1_chunk63", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "RUCTION 2 } Your task is to: 1. Write a brand-new, self-contained reasoning question that meets the following requirements: (a) The question draws inspiration from the seed question without copying it verbatim, remaining novel and of comparable difficulty. (b) The question’s final answer should be a single, unambiguous scalar value (e.g., an integer, reduced fraction, exact radical), or another answer type that can be verified in one step (e.g., ‘yes/no,’ a choice from A to D). 2. Then solve the new question and format your output as follows: [New Question Begin] {your generated question }[New Question End] [Final Answer to New Question Begin] \\boxed{your final answer }[Final Answer to New Question End] Table 4: Results of CoT-Self-Instruct, comparing to baselines, for reasoning tasks on targets sampled from Qwen3-4B .We conduct GRPO-training using Qwen3-4B-Base model on synthetic prompts generated by different templates, with targets sampled from Qwen3-4B. We report pass@1 averaged ov"}
{"id": "2507.23751v1_chunk64", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "argets sampled from Qwen3-4B .We conduct GRPO-training using Qwen3-4B-Base model on synthetic prompts generated by different templates, with targets sampled from Qwen3-4B. We report pass@1 averaged over 16 seeds. Two filter thresholds are used: SC = Self-Consistency Rate (i.e. the ratio majority votes over total votes) and RSc = RIP score (i.e. the quantile of minimum response score.) Filter MATH AIME AMC GPQA # Train Thres. 500 24 23 Diamond Avg. Self-Instruct 5000 - 81.1 16.2 58.1 42.5 49.5 + Self-Consistency Filter 3467 SC≥0.5 83.6 18.5 68.5 44.1 53.6 + RIP Filter 2254 RSc≥0.5 84.5 21.2 65.9 45.5 54.5 Self-Instruct-Then-Solve (NoCoT) 5000 - 74.5 9.8 47.7 39.0 42.7 + Answer-Consistency Filter 646 - 75.6 12.9 53.9 38.1 45.1 + Self-Consistency Filter 3369 SC≥0.5 74.8 10.8 49.8 37.5 43.2 + RIP Filter 2162 RSc≥0.5 75.0 11.0 52.3 38.0 44.1 CoT-Self-Instruct (NoSolve) 5000 - 84.3 20.2 65.5 43.7 53.4 + Self-Consistency Filter 3972 SC≥0.5 84.7 24.8 67.5 44.9 55.5 + RIP Filter 2431 RSc≥0.5 84"}
{"id": "2507.23751v1_chunk65", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "RIP Filter 2162 RSc≥0.5 75.0 11.0 52.3 38.0 44.1 CoT-Self-Instruct (NoSolve) 5000 - 84.3 20.2 65.5 43.7 53.4 + Self-Consistency Filter 3972 SC≥0.5 84.7 24.8 67.5 44.9 55.5 + RIP Filter 2431 RSc≥0.5 84.9 24.2 72.3 44.6 56.5 CoT-Self-Instruct 5000 - 84.9 20.4 62.2 44.4 53.0 + Answer-Consistency Filter 2926 - 86.5 24.6 72.3 45.5 57.2 + Self-Consistency filter 4034 SC≥0.5 85.2 22.5 67.8 44.9 55.1 + RIP filter 2491 RSc≥0.5 85.7 24.4 70.5 44.4 56.2 15Table 5: 893-train-size-matching results of CoT-Self-Instruct, comparing to baselines, for rea- soning tasks on targets sampled from Qwen3-4B : We conduct GRPO-training using Qwen3-4B- Base on selected s1k verifiable prompts and 893 synthetic prompts generated by different templates, with targets sampled from Qwen3-4B. We report pass@1 averaged over 16 seeds on MATH500, AMC23, AMIE24, GPQA-Diamond. Two filter thresholds are used: SC = Self-Consistency Rate (i.e. the ratio majority votes over total votes) and RSc = RIP score (i.e. the quantile of"}
{"id": "2507.23751v1_chunk66", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " 16 seeds on MATH500, AMC23, AMIE24, GPQA-Diamond. Two filter thresholds are used: SC = Self-Consistency Rate (i.e. the ratio majority votes over total votes) and RSc = RIP score (i.e. the quantile of minimum response score.) Filter MATH AIME AMC GPQA # Train Thres. 500 24 23 Diamond Avg. s1kPrompts + Qwen3-4B Target 893 - 71.3 13.7 51.5 38.7 43.8 Self-Instruct 893 - 80.5 17.2 57.3 41.3 49.1 + Self-Consistency Filter 893 SC≥0.5 81.9 20.0 62.8 41.5 51.5 + RIP Filter 893 RSc≥0.5 82.7 21.5 61.4 43.1 52.2 CoT-Self-Instruct (NoSolve) 893 - 82.5 20.2 61.7 41.4 51.4 + Self-Consistency Filter 893 SC≥0.5 83.6 20.6 61.7 43.0 52.2 + RIP Filter 893 RSc≥0.5 83.4 24.8 64.1 42.8 53.8 CoT-Self-Instruct 893 - 82.4 19.8 60.0 41.3 50.9 + Answer-Consistency Filter 893 - 83.7 23.1 66.1 44.1 54.2 + RIP Filter 893 RSc≥0.5 83.2 22.7 65.1 41.6 53.1 + Self-Consistency Filter 893 SC≥0.5 83.0 21.0 63.9 42.9 52.7 Table 6: Results of CoT-Self-Instruct, comparing to baselines, for reasoning tasks on majority-voted t"}
{"id": "2507.23751v1_chunk67", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "93 RSc≥0.5 83.2 22.7 65.1 41.6 53.1 + Self-Consistency Filter 893 SC≥0.5 83.0 21.0 63.9 42.9 52.7 Table 6: Results of CoT-Self-Instruct, comparing to baselines, for reasoning tasks on majority-voted targets sampled from Qwen3-4B model : We conduct GRPO-training using Qwen3-4B-Base on synthetic prompts generated by different templates, with majority-voted tar- gets sampled from Qwen3-4B. We report pass@1 averaged over 16 seeds. Different from Table 4 we use majority voted answers by Qwen3-4B model instead of single sampled responses. The con- clusions are similar to Table 4. MATH AIME AMC GPQA Majority-V oted Qwen3-4B Target # Train 500 24 23 Diamond Avg. Self-Instruct 5000 80.8 15.6 57.2 43.7 49.3 + Self-Consistency Filter 3467 80.9 17.7 63.9 46.3 52.2 CoT-Self-Instruct (NoSolve) 5000 82.9 21.9 65.3 44.4 53.6 + Self-Consistency Filter 3972 83.7 21.3 68.8 44.2 54.5 Table 7: Results of CoT-Self-Instruct, comparing to baselines, for reasoning tasks on Best-of-K targets sampled from Qwen3-"}
{"id": "2507.23751v1_chunk68", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " 21.9 65.3 44.4 53.6 + Self-Consistency Filter 3972 83.7 21.3 68.8 44.2 54.5 Table 7: Results of CoT-Self-Instruct, comparing to baselines, for reasoning tasks on Best-of-K targets sampled from Qwen3-4B model using the reward model infly/INF-ORM-Llama3.1-70B (Minghao Yang, 2024): We conduct GRPO-training using Qwen3-4B-Base on selected s1k veri- fiable prompts and synthetic prompts generated by different templates with targets sampled from Qwen3-4B. We report pass@1 averaged over 16 seeds on MATH500, AMC23, AMIE24, GPQA- Diamond. MATH AIME AMC GPQA Best-of-K Qwen3-4B Targets # Train 500 24 23 Diamond Avg. Self-Instruct 5000 83.8 18.8 62.0 44.4 52.2 + RIP Filter 2254 84.1 20.8 68.4 46.6 55.0 CoT-Self-Instruct (NoSolve) 5000 82.9 22.5 64.8 42.7 53.2 + RIP Filter 3651 85.2 24.4 71.1 46.8 56.9 16Table 8: Results of CoT-Self-Instruct, comparing to baselines, for reasoning tasks on targets sampled from Qwen3-4B-Base model responses : We conduct GRPO-training using Qwen3-4B- Base and report p"}
{"id": "2507.23751v1_chunk69", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " 16Table 8: Results of CoT-Self-Instruct, comparing to baselines, for reasoning tasks on targets sampled from Qwen3-4B-Base model responses : We conduct GRPO-training using Qwen3-4B- Base and report pass@1 averaged over 16 seeds on 4 benchmarks. Different from Table 4 we use answers sampled from the Qwen3-4B-Base model. #TrainMATH AIME AMC GPQA 500 24 23 Diamond Avg. Self-Instruct (Qwen3-4B-Base NoCoT) 5000 75.7 13.1 51.4 28.0 42.1 + Self-Consistency Filter 2815 75.9 11.5 54.8 29.5 42.9 + RIP Filter 3492 75.4 12.5 51.2 28.2 41.8 Self-Instruct (Qwen3-4B NoThink) 5000 75.3 11.0 55.4 27.1 42.2 + Self-Consistency Filter 1757 75.1 11.9 52.2 27.0 41.5 + RIP Filter 2263 75.8 13.8 51.1 30.6 42.8 CoT-Self-Instruct (Qwen3-4B NoSolve) 5000 75.5 11.0 52.2 31.4 42.5 + Self-Consistency Filter 1672 77.0 15.4 50.5 35.4 44.6 + RIP Filter 2456 76.2 14.6 53.3 30.4 43.6 Table 9: 893-train-size-matching results of CoT-Self-Instruct, comparing to baselines, for rea- soning tasks on targets sampled from Qwen"}
{"id": "2507.23751v1_chunk70", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "15.4 50.5 35.4 44.6 + RIP Filter 2456 76.2 14.6 53.3 30.4 43.6 Table 9: 893-train-size-matching results of CoT-Self-Instruct, comparing to baselines, for rea- soning tasks on targets sampled from Qwen3-4B-Base model responses . These experiments are the train-size-matching variants of Table 8. MATH AIME AMC GPQA # Train 500 24 23 Diamond Avg. Qwen3-4B-Base (Zero-Shot) - 67.4 10.6 42.0 24.2 36.1 s1k Prmpt + Qwen3-4B-Base Label 893 75.1 10.4 47.3 28.7 40.4 Self-Instruct (Qwen3-4B-Base NoCoT) 893 75.3 10.0 51.7 27.1 41.0 + Self-Consistency Filter 893 75.7 11.7 51.3 28.2 41.7 + RIP Filter 893 76.2 12.5 50.5 29.2 42.1 Self-Instruct (Qwen3-4B NoThink) 893 75.3 10.0 51.7 27.1 41.0 + Self-Consistency Filter 893 76.2 10.2 53.3 26.6 41.6 + RIP Filter 893 76.0 11.9 52.2 31.3 42.8 CoT-Self-Instruct (Qwen3-4B NoSolve) 893 75.9 10.2 51.6 30.1 41.9 + Self-Consistency Filter 893 76.2 11.5 54.1 34.0 43.9 + RIP Filter 893 77.1 13.1 50.0 33.9 43.5 Table 10: Results of CoT-Self-Instruct and other prompt t"}
{"id": "2507.23751v1_chunk71", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "wen3-4B NoSolve) 893 75.9 10.2 51.6 30.1 41.9 + Self-Consistency Filter 893 76.2 11.5 54.1 34.0 43.9 + RIP Filter 893 77.1 13.1 50.0 33.9 43.5 Table 10: Results of CoT-Self-Instruct and other prompt templates for reasoning tasks on majority-voted targets from Qwen3-4B-Base model : We conduct GRPO-training using Qwen3- 4B-Base and report pass@1 averaged over 16 seeds on 4 benchmarks. Different from Table 8 we use majority-voted targets sampled from the Qwen3-4B-Base model. MATH AIME AMC GPQA Majority-V oted Qwen3-4B-Base Target # Train 500 24 23 Diamond Avg. Self-Instruct (Qwen3-4B-Base) 5000 76.2 11.7 51.7 30.5 42.5 + Self-Consistency Filter 2815 77.5 13.1 54.5 29.0 43.6 CoT-Self-Instruct (Qwen3-4B-Base, No Solve) 5000 76.3 13.1 49.7 30.2 42.3 CoT-Self-Instruct (Qwen3-4B NoSolve) 5000 76.1 12.3 54.5 31.3 43.5 + Self-Consistency Filter 1672 77.0 13.5 55.3 31.4 44.3 17Table 11: Additional comparisons for non-verifiable instruction following tasks using differ- ent synthetic generation pr"}
{"id": "2507.23751v1_chunk72", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": " 12.3 54.5 31.3 43.5 + Self-Consistency Filter 1672 77.0 13.5 55.3 31.4 44.3 17Table 11: Additional comparisons for non-verifiable instruction following tasks using differ- ent synthetic generation prompts . CoT-Self-Instruct with long CoT generates synthetic data that outperforms short CoT and standard Self-Instruct templates. Both AlpacaEval 2 and ArenaHard are evaluated with two kinds of judge: GPT-4 Turbo and GPT-4o, with similar conclusions. Training AlpacaEval LC Winrate ArenaHard Score Method GPT-4 Turbo GPT-4o GPT-4 Turbo GPT-4oAvg. Self-Instruct (No CoT) DPO 52.9 46.0 51.8 39.2 47.4 + RIP Filter DPO 55.2 46.1 55.6 39.5 49.1 CoT-Self-Instruct (Short CoT) DPO 56.5 44.3 51.6 34.1 46.6 + RIP Filter DPO 59.0 37.7 54.3 37.5 47.1 CoT-Self-Instruct DPO 58.5 48.6 62.0 46.7 53.9 + RIP Filter DPO 63.2 49.4 60.2 45.8 54.7 18"}
{"id": "2507.23751v1_chunk73", "paper_id": "2507.23751v1", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "text": "r DPO 63.2 49.4 60.2 45.8 54.7 18"}
{"id": "2507.23740v1_chunk0", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs Nasim Shirvani-Mahdavi[0009−0006−2733−2242], Devin Wingfield, Amin Ghasemi, and Chengkai Li[0000−0002−1724−8278] University of Texas at Arlington, 701 S Nedderman Dr, Arlington, TX, 76019, USA {nasim.shirvanimahdavi2,dtw8917,mxg6185}@mavs.uta.edu , cli@uta.edu Abstract. Knowledgegraphs(KGs)oftencontainsufficientinformation to support the inference of new facts. Identifying logical rules not only improves the completeness of a knowledge graph but also enables the de- tection of potential errors, reveals subtle data patterns, and enhances the overall capacity for reasoning and interpretation. However, the complex- ity of such rules, combined with the unique labeling conventions of each KG, can make them difficult for humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the A"}
{"id": "2507.23740v1_chunk1", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "r humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery algorithm from the bench- mark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, includ- ing zero- and few-shot prompting, including variable entity types, and chain-of-thought reasoning. We conduct a comprehensive human eval- uation of the generated explanations based on correctness, clarity, and hallucination, and also assess the use of large language models as auto- matic judges. Our results demonstrate promising performance in terms ofexplanationcorrectnessandclarity,althoughseveralchallengesremain for future research. All scripts and data used in this study are publicly available at https://github.com/idirlab/KGRule2NL. Keywords: Knowledge graphs, Logical rules, Natural language expla- nation, L"}
{"id": "2507.23740v1_chunk2", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "or future research. All scripts and data used in this study are publicly available at https://github.com/idirlab/KGRule2NL. Keywords: Knowledge graphs, Logical rules, Natural language expla- nation, Large language models, Interpretability 1 Introduction Knowledge graphs (KGs) encode factual information as triples of the form (sub- ject s, predicate p, object o). They are integral to a wide range of artificial intelligence tasks and applications [9,11]. Although large-scale KGs (e.g., Free- base [3] and Wikidata [19]) contain a vast number of triples, they are often incomplete, which adversely affects their usefulness in downstream applications. However, KGs often hold sufficient information to infer new facts [7,17]. For example, if a KG indicates that a certain woman is the mother of a child, it is quite likely that her husband is the child’s father. Identifying such rules can help infer highly probable missing facts which can be further verified by human data workers or experts. In a"}
{"id": "2507.23740v1_chunk3", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "ild, it is quite likely that her husband is the child’s father. Identifying such rules can help infer highly probable missing facts which can be further verified by human data workers or experts. In addition to enhancing the completeness of KGs, such rules can also aid in detecting potential errors, deepening our understanding of thearXiv:2507.23740v1 [cs.CL] 31 Jul 20252 N. Shirvani-Mahdavi et al. data’s inherent patterns, and facilitating reasoning and interpretability [13,7]. Rule learning systems, such as AMIE [8,2] and AnyBURL [12], derive Horn rules for symbolic reasoning and link prediction in KGs. These rules can serve as explanations for specific predictions; for instance, such rules can assist domain scientists in uncovering underlying missing relationships within their data. However, rules are often challenging to comprehend for humans, espe- cially for non-experts. The difficulty arises from the abstract logical struc- ture and the complexity of the rules; the number of log"}
{"id": "2507.23740v1_chunk4", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "ever, rules are often challenging to comprehend for humans, espe- cially for non-experts. The difficulty arises from the abstract logical struc- ture and the complexity of the rules; the number of logical components, re- ferred to as atoms, as well as the nuanced nature of entity and relation la- bels within each KG. For instance, as explained in [18], label of predicates in the Freebase dataset follow the format /[domain]/[type]/[label] (e.g., /ameri- can_football/player_rushing_statistics/team ).Withoutproperbackgroundknowl- edge about such differences in KG labels, evaluating logical rules can become cumbersome. One way to address this challenge is by providing natural language explana- tions for logical rules, which enhance accessibility and usability, aid KG man- agement in cross-disciplinary contexts, and improve transparency for researchers andpractitioners.Pre-definedtemplatescangeneratesuchexplanations,butthis approach is not scalable, as it is impractical to manually extract "}
{"id": "2507.23740v1_chunk5", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "iplinary contexts, and improve transparency for researchers andpractitioners.Pre-definedtemplatescangeneratesuchexplanations,butthis approach is not scalable, as it is impractical to manually extract all logical rules from a large KG and define templates for each. To handle unseen rules, solutions leveraging large language models (LLMs) are promising due to LLMs’ genera- tive abilities and generalization capability. Related work has focused on natural language generation from logical forms [21,5], natural language generation from KGs [16], encoding and translating natural rules [6,1], and rule-based reasoning with LLMs [15,22]. To the best of our knowledge, this is the first work to examine the effec- tiveness of LLMs in generating natural language explanations for logical rules. We mined the rules by the AMIE 3.5.1 algorithm, the latest version released in 2024, using the widely used cross-domain benchmark dataset FB15k-237 [4] and two properly preprocessed large-scale variants of the"}
{"id": "2507.23740v1_chunk6", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "the rules by the AMIE 3.5.1 algorithm, the latest version released in 2024, using the widely used cross-domain benchmark dataset FB15k-237 [4] and two properly preprocessed large-scale variants of the Freebase dataset, FB-CVT- REV and FB+CVT-REV [18] (Section 2). We investigated a range of prompting strategies, such as zero and few-shot prompting [10], incorporating an instance of the rule, including variable entity types and Chain-of-Thought (CoT) rea- soning [20] (Section 3). To evaluate the quality of the generated explanations, we conducted detailed human evaluations based on criteria such as correctness, clarity, and hallucination. Additionally, we explored the potential of LLM-as-a- judge [23] for this task (Section 4). Our findings indicate that combining CoT prompting with variable type information yields the most accurate and readable explanations. Overall, our findings highlight a promising direction for this task. We conclude the work and outline potential avenues for future"}
{"id": "2507.23740v1_chunk7", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": " type information yields the most accurate and readable explanations. Overall, our findings highlight a promising direction for this task. We conclude the work and outline potential avenues for future research in Sec- tion 5. All the scripts and data produced from this work are available from our GitHub repository at https://github.com/idirlab/KGRule2NL. 2 Rule Selection from Knowledge Graphs 2.1 Rule Mining Algorithm We employed AMIE 3.5.1, a well-established rule learning system in its latest version, due to its comprehensive metrics for rule evaluation as well as its provenRule2Text: NL Explanation of Logical Rules in KGs 3 compatibility with our chosen benchmark datasets. In AMIE, a rule has a body (antecedent) and a head (consequent), represented as B1∧B2∧. . .∧Bn⇒H , or in simplified form− →B⇒H. The body consists of multiple atoms B1,. . ., Bnand the head Hitself is also an atom. In an atom r(h,t), which is another representation of a triple ( h,r,t), the subject and/or the objec"}
{"id": "2507.23740v1_chunk8", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": " form− →B⇒H. The body consists of multiple atoms B1,. . ., Bnand the head Hitself is also an atom. In an atom r(h,t), which is another representation of a triple ( h,r,t), the subject and/or the object are variables to be instantiated. The prediction of the head can be carried out when all the body atoms can be instantiated in the KG. In AMIE, the concept of support quantifies the amount of evidence (i.e., correct predictions) for each rule in the data. It is defined as the number of distinct (subject, object) pairs in the head of all valid instantiations of the rule in the KG. The concept of head coverage , a proportional version of support, is the fraction of supportover the number of facts in relation r, where ris the relation in the head atom. The standard confidence of a rule is the fraction of supportover the number of instantiations of the rule body. To mine the rules, we used the default settings of AMIE for optimized performance, with minimum thresholds of 0.1 for head coverag"}
{"id": "2507.23740v1_chunk9", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "raction of supportover the number of instantiations of the rule body. To mine the rules, we used the default settings of AMIE for optimized performance, with minimum thresholds of 0.1 for head coverage and standard confidence , and a maximum threshold of 3 for the number of atoms. 2.2 Datasets For our experiments, we leveraged three datasets. FB15k-237, a small subset of the Freebase dataset, was selected as it is a widely used benchmark for KG com- pletion, recognized for avoiding the data leakage issues of FB15k [4]. Its multi- domain coverage makes it well-suited for extracting logical rules with diverse relations. FB-CVT-REV and FB+CVT-REV [18] datasets (Statistics shown in Table 1) are large-scale variants of the Freebase dataset designed to eliminate the data leakage issue previously identified in FB15k. FB+CVT-REV includes me- diator entities (i.e., Compound Value Type nodes) originally present in Freebase to represent n-ary relations. In contrast, FB-CVT-REV converts n-ary rela"}
{"id": "2507.23740v1_chunk10", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": " identified in FB15k. FB+CVT-REV includes me- diator entities (i.e., Compound Value Type nodes) originally present in Freebase to represent n-ary relations. In contrast, FB-CVT-REV converts n-ary relation- ships centered on a CVT node into binary relations by concatenating the edges that connect entities through the CVT node, a method also used in FB15k-237. As shown in Table 1, the conversion process has resulted in a higher number of rules in these two datasets compared to those in FB+CVT-REV. Including these datasets facilitates the analysis of large-scale data and the effects of me- diator nodes and concatenated relationships on the derived rules and generated explanations. The label of a concatenated relation is formed by merging the labels of two underlying relations. As a result, the label becomes lengthy, taking the format ofdomain1/type1/label1./domain2/type2/label2 . Notably, the domains and even types can sometimes be identical in concatenated labels, but label1 and label2 a"}
{"id": "2507.23740v1_chunk11", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "e label becomes lengthy, taking the format ofdomain1/type1/label1./domain2/type2/label2 . Notably, the domains and even types can sometimes be identical in concatenated labels, but label1 and label2 are always distinct. This format differs from the simpler structure of standard relations, which follow the format of domain/type/label . Thus, this added com- plexity can pose a greater challenge for LLMs in generating natural language explanations. 3 Methodology 3.1 Prompting Strategies To generate natural language explanations for logical rules, we conducted our experiments in three phases. All scripts and prompts, rules, generated explana-4 N. Shirvani-Mahdavi et al. tions, and annotated data are available at our GitHub repository. In all prompts usedinourexperiments,weprovidedbackgroundknowledgetothemodelstoen- hance their understanding of the syntax and labels of the datasets. This content includes the format of predicates in the datasets as mentioned in Section 2. This background kno"}
{"id": "2507.23740v1_chunk12", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "wledgetothemodelstoen- hance their understanding of the syntax and labels of the datasets. This content includes the format of predicates in the datasets as mentioned in Section 2. This background knowledge is particularly important because, in rules that involve concatenated relations, the resulting lengthy labels with multiple components can easily confuse the model. Phase 1: Zero-Shot vs. Few-Shot Prompting In the first phase, we compared zero-shot and few-shot prompting strategies using rules from a small subset of the Freebase dataset, specifically FB15k-237. The objective was to assess the impact of in-context examples on explanation quality and establish a baseline.Thefew-shotpromptincludestwo(rule,explanation)pairsasexamples. In this phase and phase 2, we employed OpenAI’s GPT-3.5 Turbo model for its balance of performance, efficiency, and cost-effectiveness. A total of 100 rules with the highest head coverage were selected for human evaluation, covering a broad range of domain"}
{"id": "2507.23740v1_chunk13", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "Turbo model for its balance of performance, efficiency, and cost-effectiveness. A total of 100 rules with the highest head coverage were selected for human evaluation, covering a broad range of domains, from music and media to medicine and space. To ensure the quality of the annotations and mitigate potential subjectivity, we tasked three individuals with annotating the data. For each rule, annotators were shown both the rule and a concrete instantiation to aid understanding, along with two generated explanations, one from zero-shot prompting and one from few-shot. Their task was to identify which explanation better captured the semantics of the rule. In cases of comparable semantic accuracy, the more naturally worded explanation was preferred. After selecting the better explanation, annotators rated it using the evaluation metrics described in Section 3.2. As discussed in Section4,thefew-shotpromptingstrategydidnotyieldsignificantimprovements over the zero-shot baseline. Phase 2: Util"}
{"id": "2507.23740v1_chunk14", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "ators rated it using the evaluation metrics described in Section 3.2. As discussed in Section4,thefew-shotpromptingstrategydidnotyieldsignificantimprovements over the zero-shot baseline. Phase 2: Utilizing Variable Entity Types in The Prompt This phase initially incorporated rule instantiations into the prompt design. However, analysis of the generated explanations revealed persistent limita- tions in the model’s ability to identify variable entity types, leading us to adopt integration of these types in the prompt. For instance, in the rule ?b/time/event/instance_of_recurring_event World Series => World Series /sports/sports_championship/events ?b,World Series is a constant entity and ?b is a variable entity. In Freebase datasets, entities can belong to multiple types. Consequently, each variable entity is associated with a list of types. Given an edge type and its edge instances, there is almosta function that maps from the edge type to a type that all subjects in the edge instances "}
{"id": "2507.23740v1_chunk15", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "variable entity is associated with a list of types. Given an edge type and its edge instances, there is almosta function that maps from the edge type to a type that all subjects in the edge instances belong to, and simi- larly,almostsuch a function for objects[18]. For the example above, the variable ?b’stypesareeither /time/event or/sports/sports_championship_event .Forthis phase, three annotators annotated 100 rules, rules with the highest head cov- erage, 50 top rules from FB-CVT-REV, and 50 from FB+CVT-REV. Unlike the previous phase, the annotators were asked to complete metric evaluations for explanations from both prompts, the zero-shot prompt as our baseline, and the prompt incorporating variable type. As discussed in Section 4, our findings show that providing variable type information significantly improved the model’s performance in generating accurate explanations.Rule2Text: NL Explanation of Logical Rules in KGs 5 Phase 3: Comparing Models & Chain-of-Thought Prompting Build"}
{"id": "2507.23740v1_chunk16", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "tion significantly improved the model’s performance in generating accurate explanations.Rule2Text: NL Explanation of Logical Rules in KGs 5 Phase 3: Comparing Models & Chain-of-Thought Prompting Building on the strong impact of incorporating variable entity types into the prompts,wefurtherleveragedthereasoningcapabilitiesenabledbyCoTprompt- ing. This prompt guides the model through five reasoning steps. First, it parses the rule and identifies its components, including constant entities, variable en- tities, and relations. Second, for each variable entity, it determines the most contextually relevant type. Third, it interprets each atom in the rule using the selected types. Fourth, it synthesizes the information to infer the rule’s overall implication. Finally, it generates a concise natural language explanation. The prompt also includes two illustrative examples with CoT reasoning to support the model’s understanding. In this phase, we expanded our evaluation to include two additional"}
{"id": "2507.23740v1_chunk17", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "al language explanation. The prompt also includes two illustrative examples with CoT reasoning to support the model’s understanding. In this phase, we expanded our evaluation to include two additional models, GPT-4o Mini and Gemini 2.0 Flash, alongside GPT-3.5 Turbo. These models were selected to provide a balanced comparison in terms of performance, efficiency, and cost-effectiveness. Three annotators evaluated new explanations, generated via CoT prompting by the three models, for the same set of rules used in phase 2. As discussed in Section 4, GPT-3.5 Turbo shows improved performance compared to phase 2, while Gemini 2.0 Flash achieves the highest overall performance, followed by GPT-4o Mini. 3.2 Evaluation Metrics for Generated Explanations To evaluate the generated explanations, we used the following metrics for human and automatic evaluation. Correctness :Evaluationoftheexplanation’saccuracyonascalefrom1(com- pletely incorrect) to 5 (fully correct). Correctness refers to the expl"}
{"id": "2507.23740v1_chunk18", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "ed the following metrics for human and automatic evaluation. Correctness :Evaluationoftheexplanation’saccuracyonascalefrom1(com- pletely incorrect) to 5 (fully correct). Correctness refers to the explanation’s in- clusionofallcomponentsoftherule,presentedintheexactlogicalorderspecified by the rule. Clarity: Evaluation of the explanation on a scale from 1 (very unclear) to 5 (very clear). Clarity refers to the ease with which the explanation can be understood and how naturally it reads. This metric exclusively assesses the ex- planation, independent of the correctness of the underlying rule. # of missed entities :Thenumberofentitiespresentintherulebutnotstated in the explanation. # of missed relations : The number of relations (i.e., predicates) present in the rule but not stated in the explanation. # of hallucinated entities : The number of entities absent from the rule but incorrectly stated in the explanation. # of hallucinated relations : The number of relations absent from the rule"}
{"id": "2507.23740v1_chunk19", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "lanation. # of hallucinated entities : The number of entities absent from the rule but incorrectly stated in the explanation. # of hallucinated relations : The number of relations absent from the rule but incorrectly stated in the explanation. Rule logicalness : Although the meaningfulness of a rule is not directly related to the generation of explanations, we asked the annotators to rate the rules on a scale from 1 (not logically sound) to 2 (moderately logical), and 3 (logically sound). This metric exclusively evaluates the rule itself, without considering the explanation. Perplexity : Given the absence of reference sentences for comparison with the explanations, as our automatic evaluation metric, we computed perplexity using GPT-2. While it is a useful measure of the model’s fluency and coherence, it is not an indication of the correctness of the explanations.6 N. Shirvani-Mahdavi et al. 4 Experiments & Results Phase 1 The annotated data, available on our GitHub repository, represe"}
{"id": "2507.23740v1_chunk20", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "d coherence, it is not an indication of the correctness of the explanations.6 N. Shirvani-Mahdavi et al. 4 Experiments & Results Phase 1 The annotated data, available on our GitHub repository, represents an aggregation of input from three annotators. For each rule, we select the ex- planation receiving the majority vote and calculate the average of the measures for that chosen explanation only. For instance, if annotators 1 and 2 selected the explanation generated from the zero-shot prompt for a particular rule, while the third annotator chose the explanation from the few-shot prompt, we only averaged the measures provided by annotators 1 and 2 for that rule. Table 2 presents the average of all measures for all annotated rules (denoted as all), as well as for explanations generated from zero and few-shot prompts separately (denoted as prompt 1 and prompt 2, respectively). Specifically, for rules where the explanation generated from prompt 1 was favored by the major- ity of annotators, "}
{"id": "2507.23740v1_chunk21", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "ro and few-shot prompts separately (denoted as prompt 1 and prompt 2, respectively). Specifically, for rules where the explanation generated from prompt 1 was favored by the major- ity of annotators, the table reports the average measures for those explanations. The same approach is applied to rules where explanations from prompt 2 were preferred. Additionally, we provide these measures for the subset of rules where annotators unanimously selected the same explanation (denoted as unanimous), comparing them to the remaining rules with majority voting (denoted as major- ity).Themeasures#missedentities,#missedrelations,#hallucinatedentities, # hallucinated relations, correctness, clarity, logicalness, and perplexity are de- noted as m ent, m rel, h ent, h rel, correctness, clarity, logical, and perplexity in Table 2 (and 3) respectively. These results demonstrate the model, overall, generates relatively accurate and clear explanations with low perplexity. Among the 100 sentences selected "}
{"id": "2507.23740v1_chunk22", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "nd perplexity in Table 2 (and 3) respectively. These results demonstrate the model, overall, generates relatively accurate and clear explanations with low perplexity. Among the 100 sentences selected for human annotation, 49 were assigned to explanation 2, derived from the few-shot prompt, while the remaining sentences were assigned to explanation 1. Notably, annotators reached unanimous agreement on 48% of the rules. Furthermore, the number of missed or hallucinated elements is negligible. Our observations in- dicate that most hallucinations stem from the labels of relations, particularly concatenated relations. The model tends to generate additional entities or rela- tions to explain the complex labels associated with concatenated relations. Table 1. Statistics of the datasets dataset # of triples # of rules FB15k-237 310,116 6,320 FB-CVT-REV 125,124,274 14,355 FB+CVT-REV 134,213,735 2,965Table 2. Evaluation results on the annotated data in phase 1 m ent↓m rel↓h ent↓h rel↓correctness"}
{"id": "2507.23740v1_chunk23", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "of triples # of rules FB15k-237 310,116 6,320 FB-CVT-REV 125,124,274 14,355 FB+CVT-REV 134,213,735 2,965Table 2. Evaluation results on the annotated data in phase 1 m ent↓m rel↓h ent↓h rel↓correctness↑clarity↑logical↑perplexity↓ all 0.10 0.04 0.29 0.10 4.36 4.67 2.36 36.14 prompt 1 0.14 0.05 0.25 0.07 4.40 4.69 2.29 37.85 prompt 2 0.06 0.03 0.34 0.12 4.32 4.64 2.44 34.36 unanimous 0.13 0.03 0.35 0.12 4.34 4.68 2.29 33.80 majority 0.08 0.05 0.24 0.07 4.37 4.66 2.43 38.30 Phase 2 Table 3 presents the results for this phase, averaged across all annotators. Explanation 2, generated using the variable type prompt, consis- tently shows higher correctness and clarity across all categories, highlighting the importance of type information for model comprehension. Both explanation types have minimal missing entities and relations. However, Explanation 2 also shows slightly higher hallucination rates and increased perplexity. Rules with three atoms and those involving concatenated relations gener"}
{"id": "2507.23740v1_chunk24", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": " missing entities and relations. However, Explanation 2 also shows slightly higher hallucination rates and increased perplexity. Rules with three atoms and those involving concatenated relations generally receive lower correctness and clarity scores, likely due to their increased complexity. Inter-Rule2Text: NL Explanation of Logical Rules in KGs 7 estingly, despite these lower scores, annotators rated the rules from these two categories as more logically coherent. Table 3. Evaluation results on the annotated data in phase 2 explanation from zero-shot prompt explanation from variable type prompt logical↑m ent↓m rel↓h ent↓h rel↓correct↑clarity↑perplexity↓m ent↓m rel↓h ent↓h rel↓correct↑clarity↑perplexity↓ all 2.58 0.06 0.10 0.22 0.09 3.94 4.12 29.05 0.05 0.07 0.21 0.13 4.21 4.19 33.07 2 atoms 2.50 0.03 0.04 0.08 0.05 4.22 4.35 34.10 0.31 0.41 0.15 0.16 4.25 4.30 38.59 3 atoms 2.62 0.08 0.13 0.31 0.12 3.78 3.99 26.21 0.07 0.08 0.24 0.11 4.18 4.12 29.97 binary 2.59 0.08 0.10 0.18 0.08 4.0"}
{"id": "2507.23740v1_chunk25", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "ms 2.50 0.03 0.04 0.08 0.05 4.22 4.35 34.10 0.31 0.41 0.15 0.16 4.25 4.30 38.59 3 atoms 2.62 0.08 0.13 0.31 0.12 3.78 3.99 26.21 0.07 0.08 0.24 0.11 4.18 4.12 29.97 binary 2.59 0.08 0.10 0.18 0.08 4.04 4.22 31.02 0.06 0.03 0.20 0.11 4.32 4.28 34.11 mediator 2.51 0.08 0.13 0.16 0.06 4.15 4.13 24.22 0.01 0.11 0.16 0.06 4.36 4.2 28.65 concatenated 2.60 0.02 0.08 0.35 0.15 3.63 3.91 27.63 0.05 0.11 0.25 0.20 3.88 3.99 33.33 Phase 3 Given the negligible number of hallucinated and missing entities and relations, we evaluated the explanations in phase 3 using only correctness, clarity, and perplexity. Table 4 presents the results. Overall, the models exhibit trends similar to those observed in Phase 2. For example, all models perform bet- ter on shorter rules, particularly those with only two atoms, and achieve higher performanceonrulesinvolvingonlybinaryrelationscomparedtothosewithcon- catenated ones. GPT-3.5 Turbo shows improved performance with CoT prompt- ing compared to its performance u"}
{"id": "2507.23740v1_chunk26", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "ms, and achieve higher performanceonrulesinvolvingonlybinaryrelationscomparedtothosewithcon- catenated ones. GPT-3.5 Turbo shows improved performance with CoT prompt- ing compared to its performance using only variable entities. This improvement is consistent across all categories except for rules that include mediator nodes. GPT-4o Mini is the second-best performing model and demonstrates relatively strong performance on rules containing at least one concatenated relation. Gem- ini 2.0 Flash demonstrates the best overall performance. Its explanations are the most concise, though in rare instances, it includes remarks such as, “Note: This rule is likely flawed.” Notably, the lowest clarity scores across all models are observed for rules involving mediator nodes. Additionally, most models ex- hibit their highest perplexity on rules with only two atoms, which is somewhat unexpected given the simplicity of these rules. Table 4. Evaluation results on the annotated data in phase 3 GPT-3.5 T"}
{"id": "2507.23740v1_chunk27", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "x- hibit their highest perplexity on rules with only two atoms, which is somewhat unexpected given the simplicity of these rules. Table 4. Evaluation results on the annotated data in phase 3 GPT-3.5 Turbo GPT-4o mini Gemini 2.0 Flash correct↑clarity↑perplexity↓correct↑clarity↑perplexity↓correct↑clarity↑perplexity↓ all 4.28 4.26 32.40 4.45 4.53 31.57 4.67 4.70 27.19 2 atoms 4.38 4.43 34.08 4.52 4.62 40.96 4.80 4.76 29.98 3 atoms 4.22 4.17 31.46 4.42 4.51 26.26 4.61 4.68 25.62 binary 4.40 4.42 34.58 4.50 4.58 33.52 4.70 4.71 27.77 mediator 4.13 4.07 26.26 4.24 4.49 26.82 4.69 4.63 26.92 concatenated 4.10 4.07 31.57 4.50 4.51 30.38 4.63 4.75 26.19 LLM-as-a-Judge One of the limitations of this work is the absence of ground truth data, which restricts our ability to fine-tune models effectively. A potential solution lies in leveraging the LLM-as-a-judge approach. If a reliably fair and consistent judge model can be designed, it becomes possible to use a strong model, such as Gemini 2.0 Flas"}
{"id": "2507.23740v1_chunk28", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": ". A potential solution lies in leveraging the LLM-as-a-judge approach. If a reliably fair and consistent judge model can be designed, it becomes possible to use a strong model, such as Gemini 2.0 Flash, to generate (rule, explanation) pairs. The judge can then evaluate these pairs, and those receiving high scores can be treated as pseudo-ground truth for fine-tuning smaller open-source models. Additionally, low-scoring examples can be analyzed to identify patterns and better understand the types of explanations or rules that pose challenges for the model. To explore this direction, we developed an LLM-as-a-judge prompt. LLM- based judges often exhibit bias toward models from their family [14], for ex- ample, GPT models tend to favor responses generated by other GPT models.8 N. Shirvani-Mahdavi et al. To account for this potential bias, we evaluated the performance of the two best models, GPT-4o Mini and Gemini 2.0 Flash, using both GPT-4o Mini and Gem- ini 2.0 Flash as judges. This res"}
{"id": "2507.23740v1_chunk29", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "davi et al. To account for this potential bias, we evaluated the performance of the two best models, GPT-4o Mini and Gemini 2.0 Flash, using both GPT-4o Mini and Gem- ini 2.0 Flash as judges. This resulted in a total of four evaluation settings for a more balanced comparison. Since clarity can be a highly subjective metric, we focusedouranalysisoncorrectness.TheinformationprovidedtotheLLMjudges was identical to that given to human annotators: the rule, an instance of the rule, the list of variable types, and the explanation. Table 5 presents the correlation between LLM judges and human annotators. Because annotator scores are aver- aged across multiple individuals, they are represented as floating-point numbers, whereas LLM judge scores are integers. To ensure a fair comparison, we rounded the annotator scores to the nearest whole number before computing correlation coefficients. We used Spearman correlation to measure rank-order agreement, as- sessing how similarly the judges and anno"}
{"id": "2507.23740v1_chunk30", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": " the annotator scores to the nearest whole number before computing correlation coefficients. We used Spearman correlation to measure rank-order agreement, as- sessing how similarly the judges and annotators rank the explanations. Pearson correlation was used to evaluate the strength of the linear relationship between their actual scores. Both LLM judges show moderate agreement with annota- tors on the correctness of explanations generated by GPT-4o Mini. Gemini 2.0 Flash also aligned reasonably well with annotators when evaluating its own out- puts, whereas GPT-4o Mini showed weak agreement in that setting. Although these results are not ideal, they point to a promising direction for future work in leveraging LLMs for scalable evaluation and dataset generation. Table 5. Correlation between LLM judges and annotators for correctness Judge Explanation generated by Spearman Pearson GPT-4o mini GPT-4o mini 0.528 0.595 Gemini 2.0 Flash GPT-4o mini 0.498 0.603 GPT-4o mini Gemini 2.0 Flash 0.2"}
{"id": "2507.23740v1_chunk31", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": " LLM judges and annotators for correctness Judge Explanation generated by Spearman Pearson GPT-4o mini GPT-4o mini 0.528 0.595 Gemini 2.0 Flash GPT-4o mini 0.498 0.603 GPT-4o mini Gemini 2.0 Flash 0.221 0.208 Gemini 2.0 Flash Gemini 2.0 Flash 0.429 0.527 5 Conclusion & Future Work We employed three LLMs with multiple prompting strategies to generate natural language explanations for logical rules extracted by the AMIE algorithm from three datasets at varying scales. Human evaluation indicated encouraging results regarding accuracy and clarity, although rule complexity presents challenges for future research. Our findings indicate that the combination of Chain-of-Thought prompting and variable type information yields the most accurate and readable explanations. Future research can extend this work by evaluating LLM perfor- mance on more complex rules beyond AMIE’s extraction capabilities, exploring additional knowledge bases such as Wikidata, which encode facts differently, and construc"}
{"id": "2507.23740v1_chunk32", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "is work by evaluating LLM perfor- mance on more complex rules beyond AMIE’s extraction capabilities, exploring additional knowledge bases such as Wikidata, which encode facts differently, and constructing reference explanations to fine-tune LLMs for improved generation quality. Acknowledgments. This material is based upon work supported by the National Science Foundation under Grants TIP-2333834. We also extend our gratitude to the Texas Advanced Computing Center (TACC) for providing computing resources for this work’s experimentation. References 1. Æsøy, K., Ozaki, A.: Rule Learning as Machine Translation using the Atomic Knowledge Bank. arXiv:2311.02765 (2023)Rule2Text: NL Explanation of Logical Rules in KGs 9 2. Betz, P., Galárraga, L., Ott, S., Meilicke, C., Suchanek, F., Stuckenschmidt, H.: PyClause-Simple and efficient rule handling for knowledge graphs. In: IJCAI. pp. 8610–8613 (2023) 3. Bollacker, K., Evans, C., Paritosh, P., Sturge, T., Taylor, J.: Freebase: a collabo- rativel"}
{"id": "2507.23740v1_chunk33", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "t, H.: PyClause-Simple and efficient rule handling for knowledge graphs. In: IJCAI. pp. 8610–8613 (2023) 3. Bollacker, K., Evans, C., Paritosh, P., Sturge, T., Taylor, J.: Freebase: a collabo- ratively created graph database for structuring human knowledge. In: SIGMOD. pp. 1247–1250 (2008) 4. Bordes, A., Usunier, N., Garcia-Durán, A., Weston, J., Yakhnenko, O.: Translating embeddings for modeling multi-relational data. In: NeurIPS. pp. 2787–2795 (2013) 5. Chen, Z., Chen, W., Zha, H., Zhou, X., Zhang, Y., Sundaresan, S., Wang, W.Y.: Logic2Text: High-fidelity natural language generation from logical forms. arXiv:2004.14579 (2020) 6. Clark, P., Tafjord, O., Richardson, K.: Transformers as soft reasoners over lan- guage. arXiv preprint arXiv:2002.05867 (2020) 7. Galárraga, L., Teflioudi, C., Hose, K., Suchanek, F.M.: Fast Rule Mining in Onto- logical Knowledge Bases with AMIE ++. VLDB 24(6), 707–730 (Dec 2015) 8. Galárraga, L.A., Teflioudi, C., Hose, K., Suchanek, F.: AMIE: association rul"}
{"id": "2507.23740v1_chunk34", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "ose, K., Suchanek, F.M.: Fast Rule Mining in Onto- logical Knowledge Bases with AMIE ++. VLDB 24(6), 707–730 (Dec 2015) 8. Galárraga, L.A., Teflioudi, C., Hose, K., Suchanek, F.: AMIE: association rule mining under incomplete evidence in ontological knowledge bases. In: WWW. pp. 413–422 (2013) 9. Ji, S., Pan, S., Cambria, E., Marttinen, P., Philip, S.Y.: A survey on kgs: Repre- sentation, acquisition, and applications. TNNLS 33(2), 494–514 (2021) 10. Kermani, A., Perez-Rosas, V., Metsis, V.: A systematic evaluation of llm strategies for mental health text analysis: Fine-tuning vs. prompt engineering vs. rag. In: The 10th Workshop on CLPsych. p. 172 (2025) 11. Kiafar, B., Ravva, P.U., Joy, A.A., Daher, S., Barmaki, R.L.: Mena: Multi- modal epistemic network analysis for visualizing competencies and emotions. arXiv preprint arXiv:2504.02794 (2025) 12. Meilicke, C., Chekol, M.W., Betz, P., Fink, M., Stuckeschmidt, H.: Anytime bottom-up rule learning for large-scale KGC. VLDB 33(1), 131–16"}
{"id": "2507.23740v1_chunk35", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "cies and emotions. arXiv preprint arXiv:2504.02794 (2025) 12. Meilicke, C., Chekol, M.W., Betz, P., Fink, M., Stuckeschmidt, H.: Anytime bottom-up rule learning for large-scale KGC. VLDB 33(1), 131–161 (2024) 13. Nakashole, N., Sozio, M., Suchanek, F.M., Theobald, M.: Query-time reasoning in uncertain RDF KBs with soft and hard rules. VLDS 884(6), 15–20 (2012) 14. Panickssery, A., Bowman, S., Feng, S.: LLM evaluators recognize and favor their own generations. NeurIPs 37, 68772–68802 (2024) 15. Servantez, S., Barrow, J., Hammond, K., Jain, R.: Chain of Logic: Rule-Based Reasoning with Large Language Models. arXiv:2402.10400 (2024) 16. Shi,X.,Zhu,Z.,Zhang,Z.,Li,C.:Hallucinationmitigationinnaturallanguagegen- eration from large-scale open-domain knowledge graphs. In: EMNLP. pp. 12506– 12521 (2023) 17. Shirvani-Mahdavi, N., Akrami, F., Li, C.: On Large-scale Evaluation of Embedding Models for Knowledge Graph Completion. arXiv:2504.08970 (2025) 18. Shirvani-Mahdavi, N., Akrami, F., Saeef, M"}
{"id": "2507.23740v1_chunk36", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "2023) 17. Shirvani-Mahdavi, N., Akrami, F., Li, C.: On Large-scale Evaluation of Embedding Models for Knowledge Graph Completion. arXiv:2504.08970 (2025) 18. Shirvani-Mahdavi, N., Akrami, F., Saeef, M.S., Shi, X., Li, C.: Comprehensive analysis of Freebase and dataset creation for robust evaluation of knowledge graph link prediction models. In: ISWC. pp. 113–133. Springer (2023) 19. Vrandečić,D.,Krötzsch,M.:Wikidata:afreecollaborativeknowledgebase.CACM 57(10), 78–85 (2014) 20. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. NeurIPS 35, 24824–24837 (2022) 21. Wu, X., Cai, Y., Lian, Z., Leung, H.f., Wang, T.: Generating natural language from logic expressions with structural representation. TASLP 31, 1499–1510 (2023) 22. Yang, Z., Du, X., Mao, R., Ni, J., Cambria, E.: Logical reasoning over natural language as knowledge representation: A survey. arXiv:2303.12023 (2023)10 N. "}
{"id": "2507.23740v1_chunk37", "paper_id": "2507.23740v1", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "text": "resentation. TASLP 31, 1499–1510 (2023) 22. Yang, Z., Du, X., Mao, R., Ni, J., Cambria, E.: Logical reasoning over natural language as knowledge representation: A survey. arXiv:2303.12023 (2023)10 N. Shirvani-Mahdavi et al. 23. Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al.: Judging llm-as-a-judge with mt-bench and chatbot arena. NeurIPS 36, 46595–46623 (2023)"}
{"id": "2507.23735v1_chunk0", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "1 Distributed AI Agents for Cognitive Underwater Robot Autonomy Markus Buchholz, Ignacio Carlucho, Michele Grimaldi, and Yvan R. Petillot Abstract —Achieving robust cognitive autonomy in robots navi- gating complex, unpredictable environments remains a fundamen- tal challenge in robotics. This paper presents Underwater Robot Self-Organizing Autonomy (UROSA), a groundbreaking architec- ture leveraging distributed Large Language Model AI agents inte- grated within the Robot Operating System 2 (ROS 2) framework to enable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA decentralises cognition into specialised AI agents responsible for multimodal perception, adaptive reasoning, dynamic mission planning, and real-time decision-making. Central innovations include flexible agents dynamically adapting their roles, retrieval-augmented generation utilising vector databases for efficient knowledge management, reinforcement learning- driven behavioural optimisation, and aut"}
{"id": "2507.23735v1_chunk1", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " agents dynamically adapting their roles, retrieval-augmented generation utilising vector databases for efficient knowledge management, reinforcement learning- driven behavioural optimisation, and autonomous on-the-fly ROS 2 node generation for runtime functional extensibility. Extensive empirical validation demonstrates UROSA’s promising adaptability and reliability through realistic underwater missions in simulation and real-world deployments, showing significant advantages over traditional rule-based architectures in handling unforeseen scenarios, environmental uncertainties, and novel mission objectives. This work not only advances underwater autonomy but also establishes a scalable, safe, and versatile cognitive robotics framework capable of generalising to a diverse array of real-world applications.1 I. I NTRODUCTION The pursuit of truly autonomous robots capable of effec- tively navigating and interacting within complex, unstructured environments remains a grand challenge in the"}
{"id": "2507.23735v1_chunk2", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ld applications.1 I. I NTRODUCTION The pursuit of truly autonomous robots capable of effec- tively navigating and interacting within complex, unstructured environments remains a grand challenge in the field of robotics [1]. For decades, the dominant paradigm has been rooted in traditional robotic systems, which rely on modular architectures and meticulously pre-defined, rule-based algorithms [2]. While these systems have demonstrated notable success in controlled and predictable settings such as factory automation, they increasingly reveal their limitations when confronted with the inherent dynamism and uncertainty of real-world scenarios [3]. Legacy approaches often struggle with novel or unforeseen situations, demanding extensive manual reprogramming for even minor environmental changes and fundamentally lacking the adaptability needed for open-ended tasks [4]. This inherent inflexibility prevents them from truly operating in a self-playing manner, where the system autonomously adapt"}
{"id": "2507.23735v1_chunk3", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "and fundamentally lacking the adaptability needed for open-ended tasks [4]. This inherent inflexibility prevents them from truly operating in a self-playing manner, where the system autonomously adapts and solves problems without continuous human intervention. Within this evolving landscape, the rapid advancement of Large Language Models (LLMs) and Vision Language Models (VLMs) offers a compelling pathway toward more flexible, adaptive, and robust robot autonomy. Unlike conventional M. Buchholz, I. Carlucho, M. Grimaldi, and Y .R. Petillot are with the School of Engineering & Physical Sciences, Heriot-Watt University, Edinburgh, UK. e-mail: m.buchholz@hw.ac.uk. 1UROSA project webpage: https://markusbuchholz.github.io/urosa.htmlsoftware, which executes a rigid set of programmed instructions, AI agents are designed to understand complex information, reason about it, and generate a sequence of actions to perform specific tasks [5], [6]. This approach fundamentally shifts the development p"}
{"id": "2507.23735v1_chunk4", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": ", AI agents are designed to understand complex information, reason about it, and generate a sequence of actions to perform specific tasks [5], [6]. This approach fundamentally shifts the development paradigm from imperative programming, where every step of a task is explicitly coded (e.g., move forward 1 meter, turn left 90 degrees ), to declarative goal setting, allowing the agent to autonomously determine how to fulfill its mission (e.g., inspect the valve ). By leveraging vast pre-trained knowledge repositories, AI agents exhibit emergent reasoning capabilities that can adapt to new tasks and conditions without requiring exhaustive manual reprogramming [7], [8]. However, the integration of these agents in robotic applications introduces new challenges, such as ensuring robustness and verifiability of the decision made and protecting against possible hallucinations [9], [10]. The increasing capabilities of advanced AI systems also bring to the forefront critical considerations relate"}
{"id": "2507.23735v1_chunk5", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rifiability of the decision made and protecting against possible hallucinations [9], [10]. The increasing capabilities of advanced AI systems also bring to the forefront critical considerations related to AI safety and verification, encompassing complex issues such as alignment of the mental models of the operators and the AI system [11]–[13]. UROSA is inspired by ROSA [11], which is used as a parser from high level natural language to generate actions through ROS messages. Unlike ROSA, our work integrates agentic AI at multiple levels of the autonomy architecture, able to make decisions as well as communicate through ROS. This is a novel and highly capable autonomy framework that fundamentally replaces the traditional paradigm of a human-governed main program with a distributed network of specialised and dynamically adaptable AI agents . This distributed architecture represents a radical shift towards true autonomy, where once a mission is initiated with high-level commands, the syste"}
{"id": "2507.23735v1_chunk6", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " specialised and dynamically adaptable AI agents . This distributed architecture represents a radical shift towards true autonomy, where once a mission is initiated with high-level commands, the system operates with minimal human interaction and oversight. The distributed AI agents communicate and solve problems together, without needing human intervention or the development of coding or extra programmatic loops. This allows for emergent system-level intelligence, where human interaction primarily involves providing high-level commands through natural language. Each agent is responsible for a specific aspect of the robot’s operational workflow, ranging from multimodal perception (e.g., vision, depth, sonar) to high- level strategic planning. Crucially, the AI agents within UROSA are not merely generative; they are designed as agentic AI entities , capable of autonomous decision-making and action- taking within their environment without requiring continuous human intervention in the loo"}
{"id": "2507.23735v1_chunk7", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rely generative; they are designed as agentic AI entities , capable of autonomous decision-making and action- taking within their environment without requiring continuous human intervention in the loop [14], [15]. The realisation of this distributed cognitive architecture is founded upon the following key innovations that form the core of our UROSA framework:arXiv:2507.23735v1 [cs.RO] 31 Jul 20252 •Decoupled Reasoning and Environmental Adaptabil- ity: The framework achieves architectural flexibility by replacing traditional code-based logic with pretrained AI agents that handle system functions. Domain experts, for example a Remotely Operated Vehicle (ROV) pilot, can build systems using these existing agents without extensive reengineering. The system is adaptable in near-real time to a large variety of changes in the environment and internal states, and can access a much larger set of data through its ability to understand descriptions in natural language accessible from a variety of "}
{"id": "2507.23735v1_chunk8", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rge variety of changes in the environment and internal states, and can access a much larger set of data through its ability to understand descriptions in natural language accessible from a variety of sources, e.g., live meteorological ocean data, vehicle design. •Behaviour adaptation and lifelong learning : The system is also able to learn and adapt in real-time to improve existing agentic behaviour or generate new ones, based on prompts from operators or other agents. It can also improve over time by using previous experiences and operator feedback. This flexibility is supported by a Vector Database (VDB) that stores and retrieves past experiences, observational data, simulator outcomes and external knowledge. This comprehensive data forms the foundation for Retrieval-Augmented Generation (RAG), facilitating context-driven and informed decision-making. •Autonomous On-the-Fly Function Extension : The sys- tem can generate code on the fly to deal with unforeseen circumstances to extend "}
{"id": "2507.23735v1_chunk9", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " (RAG), facilitating context-driven and informed decision-making. •Autonomous On-the-Fly Function Extension : The sys- tem can generate code on the fly to deal with unforeseen circumstances to extend its functionality dynamically based on real-time requirements identified autonomously by the Planning Agent. This means that if a new functional component is needed for the mission, the Planning Agent can request its generation, testing, and integration without human code intervention. These capabilities can be further refined through online, agent-driven instructional tuning, where agents learn optimal response strategies through iterative, interactive feedback. The new code can be validated in simulation before being deployed on the real platform. This enables unprecedented adaptability in response to evolving mission demands and environmental contingencies. •Dynamic, Predictive System Diagnostics : The system is able to perform diagnostics of its internal state and environmental conditi"}
{"id": "2507.23735v1_chunk10", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "esponse to evolving mission demands and environmental contingencies. •Dynamic, Predictive System Diagnostics : The system is able to perform diagnostics of its internal state and environmental conditions, without the requirement of a predefined static fault tree or a fixed set of unit tests. •Inherent Safety and Control : UROSA explicitly ad- dresses aspects of AI safety and control. Through the introduction of several robust ROS 2 mechanisms and architectural constraints, such as a dedicated safety parser and a layered design, we introduce fine-grained control over the agents’ outputs and behaviours. This framework aims to mitigate hallucinations and increase the likelihood of actions aligning with predefined safety protocols, thereby contributing to the critical goal of aligning AI with human values in autonomous operations. While UROSA leverages contemporary LLMs and VLMs, its foundational architecture is inherently model-agnostic and designed with a forward-looking perspective. Rec"}
{"id": "2507.23735v1_chunk11", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "human values in autonomous operations. While UROSA leverages contemporary LLMs and VLMs, its foundational architecture is inherently model-agnostic and designed with a forward-looking perspective. Recognising the rapid advancements in artificial intelligence its modular design allows for the seamless integration of future, more advancedAI paradigms as they emerge. We aim to propose a general concept for distributed cognitive autonomy that can evolve and benefit from future breakthroughs in AI, ensuring its continued relevance and adaptability. We validate each of the key innovations previously outlined through a series of demanding use-case scenarios. These empirical tests provide concrete evidence of the system’s performance in areas such as real-time adaptation, autonomous code generation, continuous learning, and advanced diagnostics. To further illustrate these capabilities, this paper is accompanied by a supplementary video2demonstrating our key experiments in both simulation and "}
{"id": "2507.23735v1_chunk12", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "n, continuous learning, and advanced diagnostics. To further illustrate these capabilities, this paper is accompanied by a supplementary video2demonstrating our key experiments in both simulation and real-world deployments. Finally, we analyse these results to address the critical challenges of AI safety and verifiability, and discuss the future trajectory of distributed cognitive architectures like UROSA in an era of rapidly advancing artificial intelligence. II. L ITERATURE OVERVIEW : THEASCENT OF COGNITIVE AUTONOMY IN ROBOTICS The pursuit of cognitive autonomy has historically navigated a path between symbolic reasoning and reactive, embodied in- telligence. Early endeavours, influenced by symbolic AI, sought to replicate cognition through formal logic and planning [2], [16], [17], but these top-down systems often proved unstable when faced with the uncertainty and dynamism of the real world [3], [18], [19]. In response, a bottom-up approach emerged with behaviour-based robotics and"}
{"id": "2507.23735v1_chunk13", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "these top-down systems often proved unstable when faced with the uncertainty and dynamism of the real world [3], [18], [19]. In response, a bottom-up approach emerged with behaviour-based robotics and the principle of embodied, situated cognition, which emphasised that intelligence arises from the direct sensorimotor interaction between an agent and its environment [20]–[23]. While architectures like subsumption produced remarkably robust reactive behaviours [4], they lacked the capacity for abstract reasoning and complex planning. Cognitive architectures such as SOAR [7] and ACT-R [8], [24] attempted to bridge this gap by integrating symbolic processing with more plausible cognitive mechanisms, yet often struggled with the knowledge acquisition bottleneck and extensive hand- engineering required for novel domains [25]. The recent advent of LLMs has been a paradigm shift, offer- ing powerful new avenues for high-level robotic cognition [26]. Trained on vast datasets, models like GPT-4 "}
{"id": "2507.23735v1_chunk14", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "equired for novel domains [25]. The recent advent of LLMs has been a paradigm shift, offer- ing powerful new avenues for high-level robotic cognition [26]. Trained on vast datasets, models like GPT-4 have demonstrated remarkable emergent capabilities in reasoning and natural language understanding [5], [6], [27], [28], making them potent tools for robotics [29]. Research has rapidly demonstrated their efficacy in translating high-level natural language commands into actionable plans, as seen in works like SayCan [30], [31]. This has been extended by grounding language in rich perceptual data using VLMs [32]–[36], generating executable robot code from text [37], [38], and leveraging interactive reasoning paradigms such as ReAct [39] for complex, embodied problem-solving. However, integrating LLMs effectively and safely into robotic systems introduces a new set of research challenges, moving the frontier from single-model planning to robust, multi-agent architectures. The concept of a si"}
{"id": "2507.23735v1_chunk15", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "g LLMs effectively and safely into robotic systems introduces a new set of research challenges, moving the frontier from single-model planning to robust, multi-agent architectures. The concept of a single, monolithic robot brain gives way to distributed cognitive architectures, which promise greater modularity, fault tolerance, and emergent 2The video is available at: https://markusbuchholz.github.io/urosa.html3 intelligence from the coordination of multiple specialised agents [40]–[43]. Realising this vision of true cognitive autonomy [44]– [46] requires overcoming significant hurdles. These include ensuring robustness against model hallucinations [9], [10], [47], achieving real-time performance on resource-constrained hardware [48], and addressing the critical challenges of AI safety, verification, and ethical alignment in safety-critical applications [49]–[52]. This paper introduces UROSA, a novel distributed AI agent architecture designed to directly address this new frontier. Our "}
{"id": "2507.23735v1_chunk16", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rification, and ethical alignment in safety-critical applications [49]–[52]. This paper introduces UROSA, a novel distributed AI agent architecture designed to directly address this new frontier. Our work moves beyond using a single LLM for planning and instead proposes a collaborative ecosystem of specialised agentic nodes integrated deeply within the ROS 2 framework. By decentralising cognition, UROSA enhances robustness and scalability. It explicitly tackles the challenges of reliability and safety through mechanisms such as RAG for factual grounding, per-agent safety parsers to validate outputs, and dynamic, on-the-fly function generation to adapt to unforeseen circumstances. Through the design and extensive empirical validation of this framework, we demonstrate a concrete and scalable pathway toward more capable, adaptable, and reliable cognitive autonomy in real-world robotic systems. III. A RCHITECTURE The UROSA framework fundamentally reimagines robotic autonomy by replacing a "}
{"id": "2507.23735v1_chunk17", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " pathway toward more capable, adaptable, and reliable cognitive autonomy in real-world robotic systems. III. A RCHITECTURE The UROSA framework fundamentally reimagines robotic autonomy by replacing a single, monolithic control program with a distributed cognitive architecture, as depicted in the overview in Fig. 1. This design is rooted in the principles of embodied and situated cognition [21]–[23], where intelligence emerges from the dynamic interaction between multiple spe- cialised agents and their environment. At its core, UROSA de- centralizes high-level reasoning across a network of AI agents, each functioning as an intelligent, collaborative unit. These agents leverage the robust communication backbone of ROS 2 to coordinate their actions and share knowledge. This hybrid architecture follows a clear separation: deterministic, low-level controllers handle time-critical tasks like stabilization, while the distributed AI agents perform the high-level cognitive functions of reasonin"}
{"id": "2507.23735v1_chunk18", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "e follows a clear separation: deterministic, low-level controllers handle time-critical tasks like stabilization, while the distributed AI agents perform the high-level cognitive functions of reasoning, planning, and adapting to novel situations. The fundamental building block of this architecture is the Agentic ROS 2 Node . As detailed at the top right of Fig. 1, this is not a standard ROS 2 node but a composite entity. It fuses the high-level reasoning of an AI Agent with the robust communication of its ROS 2 Node Implementation. Each of these nodes encapsulates the core AI Reasoner , a critical Safety Parser to validate all outputs, and standard Publishers/Subscribers for system-wide communication. This modular design is a core innovation of UROSA, allowing specialised intelligence to be embedded directly within the robotics framework. A. The Set of Cognitive Intelligent Agents The cognitive core of UROSA is not a single brain but a distributed team of agents with distinct roles, al"}
{"id": "2507.23735v1_chunk19", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "e embedded directly within the robotics framework. A. The Set of Cognitive Intelligent Agents The cognitive core of UROSA is not a single brain but a distributed team of agents with distinct roles, all communicating over ROS 2.a) The Commander AI Agent: This agent serves as the central cognitive orchestrator, akin to a vessel’s captain. Upon receiving a high-level mission description, it utilises techniques inspired by Chain-of-Thought prompting to decompose the goal into a coherent sequence of sub-tasks for the specialist agents [53]. Its primary role is to maintain cognitive consistency by employing a reflection and revision cycle, similar to the self-correction processes described in state-of-the-art agent architectures [54]. b) The Autonomous Crew (Specialist Agents): A suite of specialist agents, analogous to an expert crew, handles specific functional domains. The agents shown in Fig. 1 are key examples, including the Perception & Scene Reasoning Agent , theMotion Planning Agent "}
{"id": "2507.23735v1_chunk20", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "list agents, analogous to an expert crew, handles specific functional domains. The agents shown in Fig. 1 are key examples, including the Perception & Scene Reasoning Agent , theMotion Planning Agent , the Autonomous Code Synthesis Agent , the Predictive Diagnostics Agent , the Capability Assessment Agent , and the Digital Twin Curator Agent . This set is not fixed; the modularity of the framework allows for the seamless integration of new agents as mission complexity demands. c) Shared Resources (ROS 2 & VDB): All agents are connected by the ROS 2 framework, which serves as the distributed communication backbone for the entire system. Furthermore, the VDB serves as a distributed, long-term memory accessible by all agents, enabling them to learn and improve their performance over time through RAG. B. Operational Flow: Digital Twin and Real-World Interaction A typical mission unfolds in two interconnected loops: 1. Cognitive Control Loop: TheCommander Agent receives a mission and direct"}
{"id": "2507.23735v1_chunk21", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ugh RAG. B. Operational Flow: Digital Twin and Real-World Interaction A typical mission unfolds in two interconnected loops: 1. Cognitive Control Loop: TheCommander Agent receives a mission and directs the specialist agents to generate plans and actions. For instance, the Motion Planning Agent generates a trajectory, which is passed to the low-level Controls & Interfaces to be executed by the robot in the Real World . 2. Digital Twin Loop: Concurrently, the robot’s Sensors provide real-time Meta data to the Digital Twin Curator Agent . This agent reasons about the real-world state and sends Fidelity Injections -updates and corrections-to the Simulator . The simulator’s Virtual State , now synchronised with reality, is used to create a high-fidelity Digital Twin for predictive analysis and proactive planning. IV. M ECHANISMS This section provides a detailed description of UROSA’s main mechanisms, which were initially introduced in Section I. Section V will then present different use cas"}
{"id": "2507.23735v1_chunk22", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "proactive planning. IV. M ECHANISMS This section provides a detailed description of UROSA’s main mechanisms, which were initially introduced in Section I. Section V will then present different use cases that demonstrate the performance of each mechanism. A. Decoupled Reasoning and Environmental Adaptability A primary innovation of UROSA is its ability to decouple high-level mission goals from low-level code implementation. This is achieved not just by distributing agents, but by fundamentally replacing traditional programmatic logic with a structured, pre-configured reasoning process embedded within each AI agent. The other innovation is the adaptability that these agents allow. The main engine behind UROSA’s adaptability is the4 Fig. 1: The UROSA Cognitive Architecture. The system is comprised of three main parts: (left) the real-world vehicle and its low-level controls; (top) the Digital Twin environment, which is continuously updated by a dedicated curator agent; and (center) the co"}
{"id": "2507.23735v1_chunk23", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rised of three main parts: (left) the real-world vehicle and its low-level controls; (top) the Digital Twin environment, which is continuously updated by a dedicated curator agent; and (center) the cognitive core of the framework, a distributed set of intelligent AI agents. The Commander AI Agent receives the high-level mission description and acts as a central orchestrator, ensuring cognitive consistency. It directs a crew of Specialist Agents , each responsible for a specific domain like planning, perception, or diagnostics. All agents communicate over the ROS 2 bus and access a shared, distributed VDB for long-term memory and experiential learning. The Digital Twin Curator Agent observes real-world sensor data Meta data and injects updates Fidelity Injections into the Simulator , whose Virtual State is fused with real-world data to form the comprehensive Digital Twin. The inset (top right) details the composition of each Agentic Node , which fuses the high-level AI Agent with its RO"}
{"id": "2507.23735v1_chunk24", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "Virtual State is fused with real-world data to form the comprehensive Digital Twin. The inset (top right) details the composition of each Agentic Node , which fuses the high-level AI Agent with its ROS 2 Node Implementation , encapsulating the AI Reasoner , a Safety Parser , and communication interfaces. pre-deployment tuning of each agent for its specific role. This is achieved not by changing model weights, but by engineering a detailed set of instructions within the SYSTEM prompt of itsModelfile . This prompt acts as the agent’s core cognitive and behavioural model, containing: •A Core Directive: Defining its primary function and role within the multi-agent system (e.g., You are a motion planner orYou are a follower agent that must coordinate with a master agent ). •Domain Knowledge and Physical Models: Textual descriptions of vehicle dynamics, environmental forces, and physical constraints like tether properties. •Reasoning Guidelines: Instructions on how to approach a problem, cov"}
{"id": "2507.23735v1_chunk25", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " Physical Models: Textual descriptions of vehicle dynamics, environmental forces, and physical constraints like tether properties. •Reasoning Guidelines: Instructions on how to approach a problem, covering both individual and cooperative logic (e.g., You must compute a collision-free path that yields to the master agent’s trajectory ). •A Strict Output Format: A rigid template for its response, as detailed in our safety strategy (Section V-D) In operation, the agent’s ROS 2 node subscribes to relevant topics and passes this multi-modal information to the tuned AI core. The AI agent then executes its embedded reasoning process to formulate a solution. This entire thinking process , guided by the initial SYSTEM prompt , replaces thousandsof lines of explicit code, allowing for unprecedented flexibility. B. Behaviour adaptation and lifelong learning UROSA is designed for continuous learning, where agents adapt their behaviour based on new information and past experiences. We enable this c"}
{"id": "2507.23735v1_chunk26", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "flexibility. B. Behaviour adaptation and lifelong learning UROSA is designed for continuous learning, where agents adapt their behaviour based on new information and past experiences. We enable this capability through two distinct, complementary mechanisms: (1) learning using a VDB with RAG for knowledge-grounding, and (2) online policy refine- ment using a novel Teacher-Student model for behavioural shaping. The primary mechanism for grounding and learning from past data is the VDB. At the start of a mission, a Flexible AI Agent is instantiated with a specific behaviour defined in itsSYSTEM prompt . This initial prompt instructs the agent on its core task and, crucially, how to utilise information from the VDB. During operation, the VDB is dynamically updated with valuable, curated data. In each operational cycle, the agent queries the VDB to retrieve contextually relevant past experiences. This RAG process grounds the agent’s reasoning, dramatically improving decision quality and acc"}
{"id": "2507.23735v1_chunk27", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "In each operational cycle, the agent queries the VDB to retrieve contextually relevant past experiences. This RAG process grounds the agent’s reasoning, dramatically improving decision quality and accelerating performance by leveraging a growing repository of relevant experience.5 To facilitate more direct online behavioural refinement, we introduce a novel Teacher-Student Instructional Tuning mechanism. This approach enables autonomous adaptation of a Student agent’s core policy through direct interaction with a Teacher agent. The process unfolds over episodes: the Student performs an action (e.g., generates a description). The Teacher agent analyses the response and its corrective action is to generate a new, modified SYSTEM prompt for the Student. This prompt acts as a rich, instructive policy guidance signal. This new prompt is then used to create a new, re-tuned instance of the Student agent for the next episode, creating a powerful meta-learning loop where the Teacher actively sh"}
{"id": "2507.23735v1_chunk28", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " policy guidance signal. This new prompt is then used to create a new, re-tuned instance of the Student agent for the next episode, creating a powerful meta-learning loop where the Teacher actively shapes the Student’s core reasoning policy from one episode to the next. C. Autonomous On-the-Fly Function Extensibility A key innovation of UROSA is its ability to autonomously generate, test, and integrate new software components at runtime, extending its own functionality without direct human intervention. This capacity for self-extension allows the system to adapt its software architecture to unforeseen challenges or mission requirements, effectively enabling runtime self-repair . This mechanism is enabled by the Autonomous Code Synthesis AI Agent , operating under the direction of the Commander AI Agent . The process begins when the Comman- derautonomously identifies a functional gap in the system, for instance, determining that a specific data filter or a novel planning algorithm is re"}
{"id": "2507.23735v1_chunk29", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "nder AI Agent . The process begins when the Comman- derautonomously identifies a functional gap in the system, for instance, determining that a specific data filter or a novel planning algorithm is required to handle the current situation. It then formulates a set of high-level requirements and sends them as a natural language request to the Code Synthesis agent. It is crucial to note that the AI’s role is strictly that of a powerful code synthesiser; it does not perform the requested task itself. Upon receiving the requirements, the agent: (1) Synthesises Code: Generates the source code for a complete ROS 2 node in a suitable language like Python; (2) Generates Tests: Simultaneously creates a suite of unit tests to validate the new code’s functional correctness and safety; (3) Saves and Deploys: If the automated tests pass, the generated source code is saved as a new executable file. This new node is then seamlessly launched and integrated into the live ROS 2 computation graph. This e"}
{"id": "2507.23735v1_chunk30", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "eploys: If the automated tests pass, the generated source code is saved as a new executable file. This new node is then seamlessly launched and integrated into the live ROS 2 computation graph. This entire workflow (from the identification of need to the deployment of a validated new capability) is performed on-the-fly. D. Dynamic, Predictive System Diagnostics UROSA features an advanced diagnostics capability that moves beyond static fault trees, using an AI agent to reason about the system’s health based on live data. This allows for the detection of complex, emergent failures that are not defined by simple error codes. This capability is enabled by a dedicated Diagnostic AI Agent whose behaviour is defined by a highly structured, task-specific reasoning process embedded within its core SYSTEM prompt . This transforms the generic LLM into a specialised diagnostic expert. The prompt instructs the agent to:(1)Perform a multi-step time-series analysis over a sliding window of 10 consecu"}
{"id": "2507.23735v1_chunk31", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "SYSTEM prompt . This transforms the generic LLM into a specialised diagnostic expert. The prompt instructs the agent to:(1)Perform a multi-step time-series analysis over a sliding window of 10 consecutive JSON status messages from thevehicle. (2)Utilise an embedded domain-specific physical model-the vehicle’s thruster allocation matrix-to determine the expected thruster behaviour for any commanded movement. (3)Compare the expected PWM values against the observed data for each thruster in the time window, using defined logic to classify distinct fault types (e.g., dead orout-of-range ). (4)Aggregate these findings to identify drifts, asymmetries, or other anomalies. (5)Report its findings in a rigid, three- line format: a) Issue Identification, c) Status Summary, and b) Suggested Action. This entire diagnostic procedure, governed by the initial prompt, allows the agent to use its holistic understanding of the vehicle’s expected behaviour to diagnose issues without relying on a predefine"}
{"id": "2507.23735v1_chunk32", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "is entire diagnostic procedure, governed by the initial prompt, allows the agent to use its holistic understanding of the vehicle’s expected behaviour to diagnose issues without relying on a predefined list of faults. E. Inherent Safety and Control Mechanisms Finally, UROSA explicitly addresses AI safety and control through its architecture, aiming to ensure that the agents’ actions are verifiable and aligned with human intent. This is achieved through a multi-layered safety strategy, implemented from the agent’s creation to its final output. Safety in UROSA is not a single component but an integrated, three-tiered approach: 1. Proactive Behavioural Scaffolding: The first and most critical layer of control is established during each agent’s creation via its Modelfile . This involves embedding a com- plete, task-specific reasoning process and strict output format directly into the agent’s core SYSTEM prompt . By providing detailed instructions, domain knowledge, physical models, and rea"}
{"id": "2507.23735v1_chunk33", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ing a com- plete, task-specific reasoning process and strict output format directly into the agent’s core SYSTEM prompt . By providing detailed instructions, domain knowledge, physical models, and reasoning guidelines, we transform the generic LLM into a highly specialised and predictable, task-oriented engine. The detailed implementation of this principle for the Diagnostic Agent , as described in Section IV-D , is a prime example of this technique. This same scaffolding approach is applied to all agents in the UROSA framework to ensure reliable and verifiable behaviour. 2. Dynamic Contextual Grounding: The second layer is provided by the heavy reliance on RAG, orchestrated by the Brain Agent . While the SYSTEM prompt defines how an agent should reason, RAG ensures that what it reasons about is grounded in a verified knowledge base and real-world data from the VDB. This provides the necessary context to make its structured output factually relevant to the current mission, further miti"}
{"id": "2507.23735v1_chunk34", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "out is grounded in a verified knowledge base and real-world data from the VDB. This provides the necessary context to make its structured output factually relevant to the current mission, further mitigating the risk of ungrounded decisions. 3. Reactive Output Validation: As a final, reactive backstop, every agentic node contains a Safety Parser . This component validates all LLM outputs before they are published as ROS 2 messages. It acts as a critical final checkpoint, checking for correct syntax and ensuring the command adheres to predefined operational safety rules (e.g., maximum speed or depth). Any command that fails this validation is blocked. Furthermore, the distributed architecture itself provides inherent fault tolerance, as the failure of one specialist agent does not necessarily compromise the entire system. V. R ESULTS To substantiate the claims made in this paper, we conducted a comprehensive evaluation strategy designed to validate each6 TABLE I: Dynamic Avoidance: AI Ag"}
{"id": "2507.23735v1_chunk35", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "y compromise the entire system. V. R ESULTS To substantiate the claims made in this paper, we conducted a comprehensive evaluation strategy designed to validate each6 TABLE I: Dynamic Avoidance: AI Agent vs. Enhanced A* Mission ID Error Delta (AI - A*) (m) AI Agent Success (%) 1 2.56 80% 2 3.78 80% 3 4.23 80% 4 6.37 60% 5 4.68 60% of UROSA’s core innovations. The experiments were both in simulation, using in a high-fidelity simulation environment, and in real-time robotic platforms. We utilise an Autonomous Surface Vehicle (ASV) and an Autonomous Underwater Vehicle (AUV) [55], in a water tank (3.5 m x 3.0 m x 2.5 m) available in our laboratories. The following subsections are structured to address each innovation introduced in Section I and described in detail in Section IV , presenting empirical validation through targeted use-case scenarios. A. Decoupled Reasoning and Environmental Adaptability We validated this through several complex scenarios, cover- ing both multi-agent coordinat"}
{"id": "2507.23735v1_chunk36", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "pirical validation through targeted use-case scenarios. A. Decoupled Reasoning and Environmental Adaptability We validated this through several complex scenarios, cover- ing both multi-agent coordination and single-robot manipula- tion, with quantitative results presented for each to demonstrate performance and reliability. 1) Constrained Multi-Robot Coordination With Obstacles: In this simulation, a tethered ASV-AUV system (with a 10 m tether) was tasked with achieving a position goal while avoiding an obstacle, as shown in Fig. 2. This required the AI agent to reason about the complex system dynamics, including the tether’s catenary shape, to maintain a valid formation throughout the maneuver. To benchmark UROSA’s performance, we compared it against a traditional motion planner based on an enhanced A* algorithm [56], [57]. A critical distinction in this evaluation is the source of obstacle information: the traditional A* planner was provided with the ground-truth position of the obst"}
{"id": "2507.23735v1_chunk37", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "an enhanced A* algorithm [56], [57]. A critical distinction in this evaluation is the source of obstacle information: the traditional A* planner was provided with the ground-truth position of the obstacle, whereas the UROSA framework had to rely on a Perception & Scene Reasoning Agent to detect and track the obstacle from a live camera feed. The validation was performed across 5 distinct mission configurations, each featuring a unique map with different start positions, goal locations, and obstacle layouts to test the system’s adaptability. For each of these configurations, we conducted 5 trials to account for any stochasticity in the perception and planning process. The key comparative metrics- the additional positioning error introduced by the AI agent compared to the A* baseline, and the agent’s task success rate-are presented in Table I. The results show an expected trade-off: the AI agent’s positioning error is higher because it operates on a live perception feed, unlike the A* pl"}
{"id": "2507.23735v1_chunk38", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "e agent’s task success rate-are presented in Table I. The results show an expected trade-off: the AI agent’s positioning error is higher because it operates on a live perception feed, unlike the A* planner, which used perfect information. Nevertheless, achieving a success rate of up to 80% on this difficult perception-in-the-loop task is a significant result, demonstrating that the framework can close the loop from raw perception to complex multi-robot coordination using only emergent reasoning. 2) Map-Based Path Planning for Multi-Robot Systems: To test the ability to interpret abstract data, we tasked the AI system with planning a path for the tethered ASV-AUV system using only a 2D map image and a text-based goal (Fig. 2). We again compared the AI-driven approach to the enhanced A* Fig. 2: Illustration of UROSA’s flexible, multi-modal planning architecture. The system’s AI agents act as a cognitive hub, integrating pre-defined mission data or dynamic, real-time perceptions to enable"}
{"id": "2507.23735v1_chunk39", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " 2: Illustration of UROSA’s flexible, multi-modal planning architecture. The system’s AI agents act as a cognitive hub, integrating pre-defined mission data or dynamic, real-time perceptions to enable seamless switching between goal-driven and perception-driven behaviors. In the map-based planning scenario shown, the agent must generate a collision-free path from the blue circle (start position) to the red circle (goal position), treating the yellow circles as obstacles. TABLE II: Map-Based Planning: AI Agent vs. Enhanced A* Map ID Error Delta (AI - A*) (m) AI Agent Success (%) 1 3.34 80% 2 2.98 80% 3 7.81 60% 4 8.13 60% 5 6.26 40% planner [56], [57]. The averaged results over 5 trials for each map layout are compared in Table II. The results in Table II demonstrate the trade-offs of an end-to-end reasoning system. The higher positioning error and variable success rates are an expected consequence of the task’s complexity; the AI agent must first interpret the raw map image, identify o"}
{"id": "2507.23735v1_chunk40", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "d-to-end reasoning system. The higher positioning error and variable success rates are an expected consequence of the task’s complexity; the AI agent must first interpret the raw map image, identify obstacles, and then plan a path, a process with more inherent uncertainty than the A* planner’s, which operates on pre-defined maps. The key achievement is that the system can succeed in this complex task at all, reaching success rates of up to 80% on. This is a significant leap in autonomy because the UROSA framework reasons directly from raw images and text to create a plan, bypassing the time-consuming manual setup required by traditional planners. 3) Flexible Motion Planning for a UVMS: We further validated this principle on an Underwater Vehicle Manipulator System (UVMS) [58], [59], focusing on autonomous motion planning driven by textual commands (Fig. 4). The experiment, conducted in a real-world tank, was designed to test the Commander AI Agent ’s ability to interpret nuanced natura"}
{"id": "2507.23735v1_chunk41", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ng on autonomous motion planning driven by textual commands (Fig. 4). The experiment, conducted in a real-world tank, was designed to test the Commander AI Agent ’s ability to interpret nuanced natural language instructions. To simplify the test’s scope and focus purely on command interpretation and planning, the coordinates for key locations like goal 1 andgoal 2 were predefined within the agent’s SYSTEM prompt . To provide a quantitative benchmark, we compared its7 TABLE III: Manipulator Planning & Interpretation Performance Metric AI Agent Naive A*+PRM Planning Time (s) 1.3 0.05 Interpretation Success (%) 90% N/A Planning Success (%) 100% 100% performance against a baseline traditional planner using a naive A* search on a Probabilistic Roadmap (PRM) [60]. To test the reliability of the AI’s language understanding, we conducted 10 trials using prompts with varied phrasing and intent. These included explicit instructions for avoidance (e.g., Go to goal 1 and check the obstacles on the"}
{"id": "2507.23735v1_chunk42", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "AI’s language understanding, we conducted 10 trials using prompts with varied phrasing and intent. These included explicit instructions for avoidance (e.g., Go to goal 1 and check the obstacles on the way ) as well as more ambiguous phrasing (e.g., Inspect goal 2 and check for any boxes on your way ). Crucially, the test also included direct commands that omitted any mention of obstacle avoidance, such as Inspect goal 2 . In these cases, to test literal command adherence, the Commander AI Agent intentionally disregarded perceptual data about obstacles and commanded a direct path, leading to a planned collision. The averaged results of these trials are summarized in Table III. The results highlight the unique capabilities of the UROSA approach. The system achieved a 90% Interpretation Success rate, correctly understanding the user’s intent in 9 out of the 10 trials. Crucially, for every prompt that was correctly understood, the subsequent Planning Success rate for generating a valid tra"}
{"id": "2507.23735v1_chunk43", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ss rate, correctly understanding the user’s intent in 9 out of the 10 trials. Crucially, for every prompt that was correctly understood, the subsequent Planning Success rate for generating a valid trajectory was 100%. This clearly distinguishes between the agent’s highly robust (but not infallible) natural language understanding and its near-perfect planning capability once the goal is known—a distinction the traditional method (N/A) cannot address. This showcases the framework’s power in translating high-level, human-like instructions directly into complex robot actions. B. Behaviour Adaptation and Lifelong Learning We evaluated these adaptive learning capabilities through targeted experiments demonstrating both experiential learning via RAG and online policy refinement. 1) Visual Positioning with Experiential Learning: To vali- date how the framework’s use of experience improves resilience against external disturbances, we conducted simulation tests focused on visual positioning. In "}
{"id": "2507.23735v1_chunk44", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ning with Experiential Learning: To vali- date how the framework’s use of experience improves resilience against external disturbances, we conducted simulation tests focused on visual positioning. In this scenario, an AUV was tasked with maintaining a stable position and orientation relative to a static feature on the seabed (a pipeline segment), as shown in the simulated views in Fig. 5. This capability is enabled by a Flexible AI Agent that uses a VDB as a short-term visual-temporal memory. This allows the agent to reason not just about its current state, but about the dynamics of recent events. To isolate the impact of this experience buffer, we bench- marked the agent’s performance in two conditions: (1) with VDB access disabled, and (2) with full VDB access. The test involved letting the AUV follow a pipeline and then applying a simulated external force to induce a specific lateral deviation. To evaluate the robustness of the agent’s response, we tested three magnitudes of absolut"}
{"id": "2507.23735v1_chunk45", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "the AUV follow a pipeline and then applying a simulated external force to induce a specific lateral deviation. To evaluate the robustness of the agent’s response, we tested three magnitudes of absolute deviation (1.0 m, 1.5 m, and 2.5 m), applying the force to induce errors to both the left and the right. We then measured the Recovery Time —the time takenTABLE IV: Disturbance Recovery with Experiential Learning DeviationRecovery Time (w/o VDB, s)Recovery Time (w/ VDB, s) 1.0 m 5.9 2.6 1.5 m 9.0 3.2 2.5 m 12.6 4.4 TABLE V: Averaged Online Policy Refinement via Teacher- Student Loop Episode Avg. Response Length (Words) Avg. Info. Relevance (%) 1 45 5% 3 18 65% 6 5 100% for the agent to guide the AUV back to the pipe centerline. Each of these six conditions was run 3 times, and the results, averaged across both left and right directions for each deviation magnitude, are presented in Table IV. The results demonstrate the significant value of experiential learning. The agent with VDB access"}
{"id": "2507.23735v1_chunk46", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "veraged across both left and right directions for each deviation magnitude, are presented in Table IV. The results demonstrate the significant value of experiential learning. The agent with VDB access recovered much faster because it leveraged the visual history to infer the disturbance dynamics and issue a proactive correction command. In contrast, the baseline agent could only react to its instantaneous error, proving less effective. The key hardware-independent finding is that the VDB provides the temporal context required for more resilient control. 2) Online Behavioural Tuning via Teacher-Student Interac- tion: To demonstrate more direct online behavioural adaptation, we validated our novel Teacher-Student Instructional Tuning mechanism . In this paradigm, a Teacher agent guides a Student agent’s policy by generating a new, more restrictive SYSTEM prompt as a form of instructive feedback, creating a powerful meta-learning loop. We conducted a series of real- world experiments in a"}
{"id": "2507.23735v1_chunk47", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "dent agent’s policy by generating a new, more restrictive SYSTEM prompt as a form of instructive feedback, creating a powerful meta-learning loop. We conducted a series of real- world experiments in a cluttered tank environment (Fig. 4) where the Commander AI Agent acted as the Teacher and the Perception & Scene Reasoning AI Agent was the Student . The Teacher’s objective was to guide the Student’s policy from providing verbose descriptions of the whole scene to reporting only the presence and location of a specific target. We ran a total of 20 trials, comprising 5 trials for each of four distinct target types: red balls, pink buoys, a fishing net, and other submerged obstacles. At each episode in a trial, theStudent generated a description, and the Teacher provided corrective feedback by generating a new SYSTEM prompt to re-tune the Student’s behaviour for the next episode. The learning process was quantified over three episodes, and Table V shows the averaged performance across all 2"}
{"id": "2507.23735v1_chunk48", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "generating a new SYSTEM prompt to re-tune the Student’s behaviour for the next episode. The learning process was quantified over three episodes, and Table V shows the averaged performance across all 20 trials. The results demonstrate a consistent and rapid convergence on the desired behaviour across all target types. On average, the Student’s policy quickly shifted from general scene description to specific, targeted reporting, confirming the mechanism’s ability to perform targeted, online behavioural shaping for a variety of perceptual goals using structured, linguistic guidance. 3) Validation: We evaluated this mechanism’s capacity for runtime software adaptation and self-repair using the controlled test setup shown in Fig. 6. For this evaluation, we simulated an autonomous request from the Commander AI Agent by manually publishing a ROS 2 topic containing the high-level8 Fig. 3: The AI agent workflow for the UVMS manipulation task. A high-level textual command from the User terminal"}
{"id": "2507.23735v1_chunk49", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "the Commander AI Agent by manually publishing a ROS 2 topic containing the high-level8 Fig. 3: The AI agent workflow for the UVMS manipulation task. A high-level textual command from the User terminal is interpreted by the Commander AI Agent , which then directs the Motion Planning andPerception agents to execute the task. Fig. 4: A direct Before and After comparison of the Teacher-Student instructional tuning mechanism. (Left) Without the Teacher enabled, the Student agent observers the scene (1) and defaults to a verbose scene description as seen in (2). (Right) With the Teacher enabled, the Teacher gives precise feedback (3) to the Student over several episodes, resulting in a converged output that is concise (4) and provides only the information requested by the Teacher Fig. 5: The control loop for visually-guided disturbance rejection. The AUV’s camera feeds a continuous stream of images into a VDB, which provides visual-temporal memory. ThePerception & Scene Reasoning AI Agent us"}
{"id": "2507.23735v1_chunk50", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "trol loop for visually-guided disturbance rejection. The AUV’s camera feeds a continuous stream of images into a VDB, which provides visual-temporal memory. ThePerception & Scene Reasoning AI Agent uses this historical context to calculate a precise positional error, which the AUV’s control system then uses to issue corrective motion commands. natural language requirements for a new node. A trial was considered a success only if the agent completed the full end- to-end process: correctly interpreting the request, generating valid Python code, creating and passing its own unit tests, and integrating the new node into the live system. A failure was defined as any case where the generated code did not compile, failed its unit tests, or was functionally incorrect. We conducted 10 trials for each of the following field- relevant scenarios: The field-relevant scenarios included gen- erating: (1)a stateful noise-reduction filter to average sensor values over a 10-sample window; (2)a standard "}
{"id": "2507.23735v1_chunk51", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ch of the following field- relevant scenarios: The field-relevant scenarios included gen- erating: (1)a stateful noise-reduction filter to average sensor values over a 10-sample window; (2)a standard sensor fusionTABLE VI: Performance of Autonomous Node Generation Generated Node Avg. Gen. Time (s) Success Rate Stateful Averaging Filter 1.5 80% Kalman Filter (2x Odom) 5.8 80% Kalman Filter (DVL+Compass) 5.9 70% node implementing a Kalman filter for two odometry topics; and(3)an emergency navigation repair node. This primary evaluation scenario simulated a critical Inertial Navigation System (INS) failure, requiring the system to synthesize a new Kalman filter to fuse compass and Doppler Velocity Log (DVL) data to mitigate navigational drift. The performance of the autonomous generation process is summarized in Table VI. Most significantly, in the navigation failure scenario, the dynamically generated Kalman filter node, once integrated, successfully fused the compass and DVL data, reduc"}
{"id": "2507.23735v1_chunk52", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ess is summarized in Table VI. Most significantly, in the navigation failure scenario, the dynamically generated Kalman filter node, once integrated, successfully fused the compass and DVL data, reducing the navigational drift rate by an estimated 70% compared to dead-reckoning with DVL alone. This provides powerful evidence of the system’s ability to perform runtime self-repair of a critical capability. Most significantly, in the navigation failure scenario, the dynamically generated Kalman filter node, once integrated, successfully fused the compass and DVL data, reducing the navigational drift rate by an estimated 70% compared to dead- reckoning with DVL alone. C. Dynamic, Predictive System Diagnostics This capability was evaluated through a series of proof- of-concept experiments where we simulated various hardware failure modes. For each test case, we programmatically disabled9 Fig. 6: The evaluation workflow for autonomous node gen- eration. 1) A user gives a query. 2) The Comman"}
{"id": "2507.23735v1_chunk53", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " we simulated various hardware failure modes. For each test case, we programmatically disabled9 Fig. 6: The evaluation workflow for autonomous node gen- eration. 1) A user gives a query. 2) The Commander AI Agent generates the requirements and passes them over to theAutonomous Code Synthesis Agent . 3) The Autonomous Code Synthesis Agent generates, tests, and deploys the new ROS 2 node. 4) For this evaluation, feedback on the new node’s performance was routed to the user terminal. It is important to note that in a fully autonomous mission, the Commander AI Agent can make the starting request and the feedback would be sent back to it to inform subsequent decisions. Fig. 7: Excerpt of the code generated. For full details please check the website and videos. specific thrusters via the vehicle’s control interface. The ArduPilot system [61], observing the lack of response, would then generate an updated vehicle status JSON reflecting the fault. The Diagnostic AI Agent’s task was to monitor "}
{"id": "2507.23735v1_chunk54", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "’s control interface. The ArduPilot system [61], observing the lack of response, would then generate an updated vehicle status JSON reflecting the fault. The Diagnostic AI Agent’s task was to monitor this stream and correctly identify which thrusters were malfunctioning. We ran 5 trials for each fault configuration, and the agent correctly diagnosed the system state with 100% accuracy across all tests, as summarised in Table VII. To further demonstrate the agent’s ability to track a dynamically changing system state, weTABLE VII: Summary of Diagnostic Test Cases and Results Test Case Simulated Fault Diagnosis Accuracy 1 All Thrusters OK 100% (5/5) 2 Thruster 2 Disabled 100% (5/5) 3 Thruster 6 Disabled 100% (5/5) 4 Thrusters 2 & 3 Disabled 100% (5/5) 5 Thrusters 2, 3, 6, 7 Disabled 100% (5/5) Fig. 8: Validation of the Predictive Diagnostics Agent during a simulated two-thruster failure. The plots on the right show the raw PWM signals from the ROS 2 interface, clearly indicating that thr"}
{"id": "2507.23735v1_chunk55", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "5) Fig. 8: Validation of the Predictive Diagnostics Agent during a simulated two-thruster failure. The plots on the right show the raw PWM signals from the ROS 2 interface, clearly indicating that thrusters 2 and 3 are unresponsive (flatlined) while the others are active. The Diagnostic Agent continuously analyzes this time-series data, reasons about the discrepancy between commanded and actual behavior, and autonomously generates the correct high-level diagnosis shown in the terminal output (top left). This demonstrates the agent’s ability to translate low-level signal anomalies into a precise, human-readable fault report without relying on pre-programmed error codes. conducted a specific transition test. First, we disabled thrusters 2 and 3 to induce a fault. After the agent correctly diagnosed this failure, we re-enabled the thrusters in the interface. The agent’s subsequent report immediately and accurately reflected the system’s return to a healthy state. Figure 8 shows the agent’"}
{"id": "2507.23735v1_chunk56", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "agnosed this failure, we re-enabled the thrusters in the interface. The agent’s subsequent report immediately and accurately reflected the system’s return to a healthy state. Figure 8 shows the agent’s real-time terminal output, capturing these precise system faults. D. Inherent Safety and Control Mechanisms To validate the safety of the agent, we conduct a complex evaluation of multi-agent behaviour. Two AUVs, each con- trolled by an independent AI agent, navigated a maze-like environment toward a shared goal (Fig. 9). The agents must then coordinate to avoid each other, but must do so via emergent coordination, without preplanning. The key to this capability was embedding not only a negotiation protocol but also the vehicle’s physical dimensions into each agent’s behavioural constitution ( SYSTEM prompt ). This allowed each agent to reason spatially about its own footprint and the other’s intended path. The evaluation involved 8 test runs with increasingly complex trajectory conflict"}
{"id": "2507.23735v1_chunk57", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " ( SYSTEM prompt ). This allowed each agent to reason spatially about its own footprint and the other’s intended path. The evaluation involved 8 test runs with increasingly complex trajectory conflicts, where paths crossed or required agents to pass each other in tight corridors. Each test case was run 5 times to ensure repeatability. During operation, the agents continuously share their intended trajectories via ROS 2. Each10 Fig. 9: Iustration of the decentralised collision avoidance scenario, where two AI agents negotiate a solution via ROS 2. Agent A, marked as Yellow, and Agent B, marked with green. Each has a starting position and a goal point, with conflicting goals. TABLE VIII: Performance of Decentralised Negotiation (Avg. over 5 runs) Test ID Negotiation Time (s) Min. Safe Distance (m) 1 0.05 1.2 2 0.05 1.4 3 0.06 2.1 4 0.12 0.21 5 0.13 0.22 6 0.06 2.32 7 0.14 0.32 8 0.13 0.76 agent then predicts the future states along both paths, calculat- ing the minimum distance between t"}
{"id": "2507.23735v1_chunk58", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ce (m) 1 0.05 1.2 2 0.05 1.4 3 0.06 2.1 4 0.12 0.21 5 0.13 0.22 6 0.06 2.32 7 0.14 0.32 8 0.13 0.76 agent then predicts the future states along both paths, calculat- ing the minimum distance between their vehicle’s bounding volumes. If this predicted distance fell below a predefined safety threshold, a collision was considered imminent, triggering a human-independent negotiation dialogue. Based on its pre- configured reasoning process, one agent would identify the conflict and autonomously compute a new path yielding to the other. Table VIII summarises the averaged performance, focusing on the efficiency of the negotiation and the safety of the resulting manoeuvre. Across all 40 trials, the agent pair successfully negotiated a collision-free path every time. The data in Table VIII shows that the negotiation was exceptionally efficient, with a resolution consistently found in well under 0.2 seconds. It should be noted that this time is presented as a reference, as it is dependent on the"}
{"id": "2507.23735v1_chunk59", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "at the negotiation was exceptionally efficient, with a resolution consistently found in well under 0.2 seconds. It should be noted that this time is presented as a reference, as it is dependent on the specific AI model and GPU hardware used. Critically, even in the most constrained scenarios where the vehicles had to pass with very tight clearances (e.g., a minimum distance of just 0.21 m in Test ID 4), a positive safe distance was always maintained. This consistent success in difficult configurations confirms that an effective, emergent deconfliction strategy canbe derived directly from high-level textual instructions about vehicle geometry. VI. C ONCLUSION This paper presented UROSA, a distributed AI agent architecture that replaces monolithic programmatic control with a collaborative ecosystem of agentic entities. By embedding a structured reasoning process into each agent via behavioural constitutions, UROSA decouples high-level mission goals from low-level code implementation, ena"}
{"id": "2507.23735v1_chunk60", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "osystem of agentic entities. By embedding a structured reasoning process into each agent via behavioural constitutions, UROSA decouples high-level mission goals from low-level code implementation, enabling a new level of cognitive flexibility in the underwater domain. Through empirical validation, we demonstrated that this framework effectively handles multi-robot coordination from abstract inputs, improves performance through experiential learning via RAG, autonomously synthesises new software modules at runtime, and performs predictive diagnostics without static fault trees. These capabilities, protected by a multi-layered safety strategy, proved both reliable and effective across demanding scenarios. While these findings are significant, challenges in deploying such cognitive systems remain. The engineering of effective behavioural constitutions is a complex new skill, and the formal verification and real-time performance of every AI-driven decision require further research. Future "}
{"id": "2507.23735v1_chunk61", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "main. The engineering of effective behavioural constitutions is a complex new skill, and the formal verification and real-time performance of every AI-driven decision require further research. Future work will address these challenges by exploring two frontiers: automating the generation of agent constitutions, and enabling runtime self- reconfiguration. This latter capability would allow an agent, upon receiving a diagnosis of hardware failure, to autonomously rewrite and deploy a new control model—such as a modified thruster allocation matrix—to ensure the system continues its mission with gracefully degraded performance. This research provides a scalable and safe framework for developing more adaptable and resilient autonomous systems, moving beyond pre-programmed tools towards reasoning partners capable of overcoming both environmental and internal, unforeseen challenges. REFERENCES [1]S. J. Russell and P. Norvig, Artificial intelligence: A modern approach . Pearson Education, 2020"}
{"id": "2507.23735v1_chunk62", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "g partners capable of overcoming both environmental and internal, unforeseen challenges. REFERENCES [1]S. J. Russell and P. Norvig, Artificial intelligence: A modern approach . Pearson Education, 2020. [2]N. J. Nilsson, “Shakey the robot,” SRI International Menlo Park CA Artificial Intelligence Center, Tech. Rep., 1984. [3]R. A. Brooks, “Intelligence without representation,” Artificial Intelligence , vol. 47, no. 1-3, pp. 139–159, 1991. [4] R. C. Arkin, Behavior-based robotics . MIT Press, 1998. [5]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, others, and D. Amodei, “Language models are few-shot learners,” in Advances in Neural Information Processing Systems , vol. 33, 2020, pp. 1877–1901. [6]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, others, and I. Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems , 2017, pp. 5998–6008. [7] J. E. Laird, The Soar cognitive architecture . MIT Press, 2012. [8]J"}
{"id": "2507.23735v1_chunk63", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rs, and I. Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems , 2017, pp. 5998–6008. [7] J. E. Laird, The Soar cognitive architecture . MIT Press, 2012. [8]J. R. Anderson, E. H. Bothell, M. D. Byrne, S. Douglass, C. Lebiere, and Y . Qin, “An integrated theory of the mind,” Psychological Review , vol. 111, no. 4, p. 1036, 2004. [9]Z. Ji, N. Lee, R. Fries, T. Yu, D. Su, Y . Xu, others, and A. Madotto, “Survey of hallucination in natural language generation,” ACM Computing Surveys , 2023. [10] L. Huang, D. Y . Zhou, H. Edwards, and W. Zhou, “A survey on hallucination in large language models,” arXiv preprint arXiv:2305.13565, 2023.11 [11] R. Royce, M. Kaufmann, J. Becktor, S. Moon, K. Carpenter, K. Pak, A. Towler, R. Thakker, and S. Khattak, “Enabling novel mission operations and interactions with rosa: The robot operating system agent,” NASA/JPL-Caltech Technical Report, 2025. [12] Anthropic, “Alignment research,” https://www.anthropic.com/resea"}
{"id": "2507.23735v1_chunk64", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "g novel mission operations and interactions with rosa: The robot operating system agent,” NASA/JPL-Caltech Technical Report, 2025. [12] Anthropic, “Alignment research,” https://www.anthropic.com/research/ alignment, 2024. [13] OpenAI, “Alignment and safety,” https://openai.com/safety, 2024. [14] EY , “The future of autonomous systems: agentic ai in consumer products,” EY Insights , March 2025. [15] Endava, “Agentic ai: Definition, types, applications,” 2025. [16] J. McCarthy, “Programs with common sense,” in Mechanisation of Thought Processes , vol. 1, 1959, pp. 77–84. [17] R. E. Fikes and N. J. Nilsson, “Strips: A new approach to theorem proving applied to problem solving,” Artificial Intelligence , vol. 2, no. 3-4, pp. 189–208, 1971. [18] J. McCarthy and P. J. Hayes, “Some philosophical problems from the standpoint of artificial intelligence,” Machine Intelligence , vol. 4, pp. 463–502, 1969. [19] P. E. Agre and D. Chapman, “Pengi: An implementation of a theory of activity,” in AAAI "}
{"id": "2507.23735v1_chunk65", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "problems from the standpoint of artificial intelligence,” Machine Intelligence , vol. 4, pp. 463–502, 1969. [19] P. E. Agre and D. Chapman, “Pengi: An implementation of a theory of activity,” in AAAI , 1987, pp. 268–272. [20] R. A. Brooks, “A robust layered control system for a mobile robot,” IEEE Journal of Robotics and Automation , vol. 2, no. 1, pp. 14–23, 1986. [21] R. Pfeifer and C. Scheier, Understanding intelligence . MIT Press, 1999. [22] A. Clark, Being there: Putting brain, body, and world together again . MIT Press, 1997. [23] M. Wilson, “Six views of embodied cognition,” Psychonomic Bulletin & Review , vol. 9, no. 4, pp. 625–636, 2002. [24] J. R. Anderson, How can the mind occur in the body? Oxford University Press, 2007. [25] R. J. Brachman and H. J. Levesque, “The tractability of subsumption in frame-based description languages,” in AAAI , 1984, pp. 34–37. [26] P. Kumar, “Large language models (LLMs): survey, technical frameworks, and future challenges,” Artif. Intell. Re"}
{"id": "2507.23735v1_chunk66", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "y of subsumption in frame-based description languages,” in AAAI , 1984, pp. 34–37. [26] P. Kumar, “Large language models (LLMs): survey, technical frameworks, and future challenges,” Artif. Intell. Rev. , vol. 57, no. 10, pp. 1–51, Aug. 2024. [27] OpenAI, “Gpt-4 technical report,” arXiv preprint arXiv:2303.08774, 2023. [28] S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, F. Kamani, others, and Y . Zhang, “Sparks of artificial general intelligence: Early experiments with gpt-4,” arXiv preprint arXiv:2303.12712, 2023. [29] N. Mirchev, S. Jiang, S. Shah, and A. Garg, “Large language models as general-purpose policies for robots,” arXiv preprint arXiv:2305.05042, 2023. [30] M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, G. Danihelka, others, and R. Tanwani, “Do as i can, not as i say: Grounding language in robotic affordances,” arXiv preprint arXiv:2204.01691, 2022. [31] A. Brohan, N. Brown, W. L. Brohan, Y . C. Chen, Y . Chebotar, J. Castro, others, and S. Levine, “"}
{"id": "2507.23735v1_chunk67", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": " not as i say: Grounding language in robotic affordances,” arXiv preprint arXiv:2204.01691, 2022. [31] A. Brohan, N. Brown, W. L. Brohan, Y . C. Chen, Y . Chebotar, J. Castro, others, and S. Levine, “Rt-2: Vision-language-action models transfer web knowledge to robotic control,” arXiv preprint arXiv:2207.05736, 2023. [32] A. Radford, J. W. Kim, C. Xu, G. Xu, G. Chen, G. Brockman, others, and I. Sutskever, “Learning transferable visual models from natural language supervision,” in International Conference on Machine Learning . PMLR, 2021, pp. 8748–8763. [33] A. K. Gupta, A. Yan, J. Guo, Y . Cheng, L. Yang, L. H. Chen, others, and K. Keutzer, “Visual grounding for language-guided navigation,” in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , 2023, pp. 3562–3572. [34] D. Driess, A. Zeng, S. Cabi, M. S. Sajjadi, D. Allen, C. Lynch, others, and M. Toussaint, “Palm-e: An embodiment-aware language model for instruction following with visual and tactile feed"}
{"id": "2507.23735v1_chunk68", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "–3572. [34] D. Driess, A. Zeng, S. Cabi, M. S. Sajjadi, D. Allen, C. Lynch, others, and M. Toussaint, “Palm-e: An embodiment-aware language model for instruction following with visual and tactile feedback,” arXiv preprint arXiv:2203.16939, 2023. [35] C. Jia, Y . T. Chen, Z. Lu, S. Tunyasuvunakool, N. De Freitas, and D. Tarlow, “Scaling up visual and vision-language representation learning with noisy text supervision,” in International Conference on Machine Learning . PMLR, 2021, pp. 4615–4625. [36] K. Zhu, C. Gan, L. Wang, Y . Fang, X. Dai, and S. Han, “Vision-language models are zero-shot reward function approximators,” arXiv preprint arXiv:2303.02896, 2023. [37] J. Liang, W. Zeng, G. Mu, S. Yang, T. L. Griffiths, Y . Zhu, others, and D. Song, “Code as policies: Language model-based discrete action policies for embodied ai,” arXiv preprint arXiv:2209.07753, 2023. [38] A. Zeng, S. Song, C. Lee, N. Rodriguez-Ruiz, T. Van-Hove, R. S. Fearing, others, and C. Lynch, “Socratic models: Compo"}
{"id": "2507.23735v1_chunk69", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ete action policies for embodied ai,” arXiv preprint arXiv:2209.07753, 2023. [38] A. Zeng, S. Song, C. Lee, N. Rodriguez-Ruiz, T. Van-Hove, R. S. Fearing, others, and C. Lynch, “Socratic models: Composing zero-shot multimodality with language,” in International Conference on Machine Learning . PMLR, 2023, pp. 26 944–26 967. [39] S. Yao, W. Zhao, J. Wang, Y . Cao, S. Narasimhan, and D. Zhao, “React: Synergizing reasoning and acting in language models for task solving,” arXiv preprint arXiv:2210.03629, 2023.[40] Y . U. Cao, A. S. Fukunaga, and A. B. Kahng, “Cooperative mobile robotics: Antecedents and directions,” Autonomous Robots , vol. 4, no. 1, pp. 7–27, 1997. [41] P. Stone, Ed., Multiagent systems: A modern approach to distributed artificial intelligence . MIT Press, 2000. [42] M. Wooldridge, An introduction to multiagent systems . John Wiley & Sons, 2009. [43] R. Olfati-Saber, J. A. Fax, and R. M. Murray, “Consensus and cooperation in networked multi-agent systems,” Proceedings of "}
{"id": "2507.23735v1_chunk70", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "ridge, An introduction to multiagent systems . John Wiley & Sons, 2009. [43] R. Olfati-Saber, J. A. Fax, and R. M. Murray, “Consensus and cooperation in networked multi-agent systems,” Proceedings of the IEEE , vol. 95, no. 1, pp. 215–233, 2007. [44] J. M. Bradshaw, R. R. Hoffman, M. Johnson, and P. J. Feltovich, “Beyond human-centered autonomy: Collaboration and reciprocal adaptation in human-agent teams,” IEEE Intelligent Systems , vol. 32, no. 3, pp. 70–78, 2017. [45] M. Longo, L. Rathenau, and J. Weber, Ethical autonomy in intelligent agents: Embedding ethics into artificial intelligence . Springer Nature, 2023. [46] R. D. Beer, M. Randall, and P. Fitch, “Evolving dynamical neural networks for adaptive behavior,” Adaptive Behavior , vol. 22, no. 1, pp. 3–27, 2014. [47] J. Maynez, S. Narayan, L. Lokhande, and R. Reddy, “On faithfulness and hallucination in abstractive summarization,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , 2020, p"}
{"id": "2507.23735v1_chunk71", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "rayan, L. Lokhande, and R. Reddy, “On faithfulness and hallucination in abstractive summarization,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , 2020, pp. 1883–1896. [48] M. Verhelst and E. Moons, “What edge computing can do for deep learning,” in 2017 IEEE International Conference on Computer Commu- nications Workshops (INFOCOM WKSHPS) . IEEE, 2017, pp. 1–6. [49] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and I. Sutskever, “Concrete ai safety problems,” arXiv preprint arXiv:1606.06565, 2016. [50] P. Koopman and M. Wagner, “Challenges in autonomous vehicle verification and validation,” SAE International Journal of Transportation Safety , vol. 5, no. 1, pp. 19–27, 2017. [51] P. Lin, K. Abney, and R. Jenkins, “Robot ethics: Mapping the issues for computer scientists,” AI Magazine , vol. 32, no. 1, p. 15, 2011. [52] W. Wallach and C. Allen, Moral machines: Teaching robots right from wrong . Oxford University Press, 2008. "}
{"id": "2507.23735v1_chunk72", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "Mapping the issues for computer scientists,” AI Magazine , vol. 32, no. 1, p. 15, 2011. [52] W. Wallach and C. Allen, Moral machines: Teaching robots right from wrong . Oxford University Press, 2008. [53] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning in large language models,” arXiv preprint arXiv:2201.11903, 2022. [54] A. Madaan, N. Tandon, P. Gupta, K. Hallinan, L. Gao, S. Wiegreffe, U. Alon, P. Cair, Y . Yang, A. Cohan et al. , “Self-refine: Iterative refinement with self-feedback,” arXiv preprint arXiv:2303.17651, 2023. [55] J. S. Willners, I. Carlucho, S. Katagiri, C. Lemoine, J. Roe, D. Stephens, T.Łuczy ´nski, S. Xu, Y . Carreno, `E. Pairet et al. , “From market-ready rovs to low-cost auvs,” in OCEANS 2021: San Diego–Porto . IEEE, 2021, pp. 1–7. [56] M. Buchholz, I. Carlucho, Z. Huang, M. Grimaldi, P. Nicolay, S. Tun c ¸ay, and Y . R. Petillot, “Framework for robust motion planning of tether"}
{"id": "2507.23735v1_chunk73", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "OCEANS 2021: San Diego–Porto . IEEE, 2021, pp. 1–7. [56] M. Buchholz, I. Carlucho, Z. Huang, M. Grimaldi, P. Nicolay, S. Tun c ¸ay, and Y . R. Petillot, “Framework for robust motion planning of tethered multi-robot systems in marine environments,” in Proceedings of the IEEE/MTS OCEANS Conference . Brest, France: IEEE, May 2025. [57] M. Buchholz, I. Carlucho, M. Grimaldi, and Y . R. Petillot, “Tethered multi-robot systems in marine environments,” in Proceedings of the ICRA 2025 Workshop on Marine Robotics , New Orleans, USA, 2025. [58] M. Buchholz, I. Carlucho, M. Grimaldi, M. Koskinopoulou, and Y . R. Petillot, “Context-aware behavior learning with heuristic motion memory for underwater manipulation,” 2025. [Online]. Available: https://arxiv.org/abs/2507.14099 [59] E. Morgan, I. Carlucho, W. Ard, and C. Barbalata, “Autonomous underwater manipulation: Current trends in dynamics, control, planning, perception, and future directions,” Current Robotics Reports , vol. 3, no. 4, pp. 187–198,"}
{"id": "2507.23735v1_chunk74", "paper_id": "2507.23735v1", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "text": "W. Ard, and C. Barbalata, “Autonomous underwater manipulation: Current trends in dynamics, control, planning, perception, and future directions,” Current Robotics Reports , vol. 3, no. 4, pp. 187–198, 2022. [60] M. Buchholz, I. Carlucho, M. Grimaldi, and Y . R. Petillot, “Context- aware behavior learning with heuristic motion memory for underwater manipulation,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) . New Orleans, USA: IEEE, 2025. [61] The ArduPilot Development Team, “ArduPilot: Open source autopilot software suite,” https://ardupilot.org, 2025, accessed: 5-July-2025."}
{"id": "2507.23726v1_chunk0", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving ByteDance Seed AI4Math Abstract LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain- specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose Seed-Prover , a lemma- style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves 78.1%of formalized past IMO problems, saturates MiniF2F, and achieves over 50% on PutnamBench, outperforming the previous state-of-the-"}
{"id": "2507.23726v1_chunk1", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "es that enable both deep and broad reasoning. Seed-Prover proves 78.1%of formalized past IMO problems, saturates MiniF2F, and achieves over 50% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry , which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning. Project Page: https://github.com/ByteDance-Seed/Seed-Prover 2021 2022 2023 2024 2025 Publication Date30405060708090100Pass Rate on miniF2F-test (%) Proof Artifact Co-trainingCurriculum LearningHypertree Proof Search ReProverCOPRADeepSeek-Prover-V1 InternLM2-Math-Plus TheoremLlamaLean-ST aRInternLM2-StepProverDeepSeek-Prover-V1.5 ABEL AlchemyInternLM2."}
{"id": "2507.23726v1_chunk2", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": " Artifact Co-trainingCurriculum LearningHypertree Proof Search ReProverCOPRADeepSeek-Prover-V1 InternLM2-Math-Plus TheoremLlamaLean-ST aRInternLM2-StepProverDeepSeek-Prover-V1.5 ABEL AlchemyInternLM2.5-StepProver 3D-ProverHunyuanProver STPBFS-Prover Goedel-ProverLeanabell-ProverKimina-Prover-PreviewDeepSeek-Prover-V2 DSP+Kimina-ProverGoedel-Prover-V2 Delta-ProverSeed-Prover Figure 1 Growth in MiniF2F-Test performance over time. 1arXiv:2507.23726v1 [cs.AI] 31 Jul 20251 Introduction Recent advances in large language models (LLMs) have shown extending reasoning lengths through natural language can significantly boost performance on math benchmarks such as MATH and AIME [ 4,11]. Training suchmodelsrequiresreinforcementlearning(RL)onverifiableanswers. Itisextremelydifficulttoautomatically, or even manually, verify a proof in natural language since each step must be carefully checked for correctness [13]. This poses significant challenges for applying reinforcement learning to the training o"}
{"id": "2507.23726v1_chunk3", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ven manually, verify a proof in natural language since each step must be carefully checked for correctness [13]. This poses significant challenges for applying reinforcement learning to the training of large language models to prove mathematical statements. Unlike natural language, formal languages such as Lean can provide a clear and automatic signal on the correctness of a formalized proof. A noteworthy work from AlphaProof [3] uses Lean to successfully solve 3 problems from the 2024 International Mathematical Olympaid (IMO). AlphaProof demonstrates that LLMs using formal language are capable of proving very challenging problems that LLMs using natural language fail to prove. There are two types of LLM formal provers, step-level provers [ 3,14,22,23,25] and whole-proof generation provers[5,24]. Step-level provers incrementally generate Lean code line-by-line. While this enables close interaction with the Lean environment, it requires special scaffolding to generate a complete Lean pr"}
{"id": "2507.23726v1_chunk4", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "[5,24]. Step-level provers incrementally generate Lean code line-by-line. While this enables close interaction with the Lean environment, it requires special scaffolding to generate a complete Lean proof, and the interaction is often too granular to allow high-level reasoning. In contrast, whole-proof models generate an entire Lean proof at once, but typically lack interaction with the Lean compiler. Recent work has shown that combining whole-proof models with long chain-of-thought reasoning [ 9,15,21] substantially outperforms step-level provers. In this work, we propose Seed-Prover , a whole-proof model with following features: •Lemma-Style Proving : Seed-Prover tries to generate useful intermediate lemmas before proving the main statement. These lemmas serve as shared knowledge across different inference paths. •Iterative Proof Refinement : Seed-Prover iteratively refines its proof based on Lean compiler feedback, previous proved lemmas, and self-summarization. •Test-Time Scaling : "}
{"id": "2507.23726v1_chunk5", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ifferent inference paths. •Iterative Proof Refinement : Seed-Prover iteratively refines its proof based on Lean compiler feedback, previous proved lemmas, and self-summarization. •Test-Time Scaling : We implement a three-tiered inference strategy that enables Seed-Prover to think both deeply and broadly—allocating thinking budget to fine details while exploring interesting properties. •SOTA Performance : Seed-Prover proves 5 out of 6 problems in IMO 2025, saturates MiniF2F [ 31] (shown in Figure 1), and outperforms prior work by up to 3×on multiple formal benchmarks. DuetothelackofsufficientgeometrysupportinLean, Seed-Proverincorporatesadedicatedgeometryreasoning engine Seed-Geometry . Similar to existing line of efforts in AlphaGeometry [2, 19] and TongGeometry [30], Seed-Geometry follows the forward-chaining design in the reasoning engine implementation, where the system derives all known facts by checking applicable rules until closure is reached. By backward-tracing fact dependenci"}
{"id": "2507.23726v1_chunk6", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": " the forward-chaining design in the reasoning engine implementation, where the system derives all known facts by checking applicable rules until closure is reached. By backward-tracing fact dependencies, Seed-Geometry identifies the minimum dependency relations in a geometry problem’s configuration, seperating the problem context from the auxiliary constructions necessary to prove a problem. Using statistics derived from more than past 20 years of math olympiad competitions, Seed-Geometry performs extensive search in the geometry space defined by its dedicated domain-specific language and establishes a repository of 230 million unique geometry problems requiring auxiliaries. A Seed model trained on such dedicated geometry data becomes an exceptionally effective neuro-symbolic geometry prover, where it fills in the missing auxiliary geometry elements and the geometry reasoning engine performs step-by-step forward- chaining, completing the final proof of a problem. In experiments, Seed-G"}
{"id": "2507.23726v1_chunk7", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "er, where it fills in the missing auxiliary geometry elements and the geometry reasoning engine performs step-by-step forward- chaining, completing the final proof of a problem. In experiments, Seed-Geometry solves 43 of the IMO-AG-50 (vs.42 by AlphaGeometry 2), a benchmark that curates geometry problems of IMO from 2000 to 2024. It also sets a new state-of-the-art on the IMO shortlist geometry problems from 2000 to 2022, and notably solves the geometry problem of IMO 2025 under just 2 seconds. 2 Approach Here we introduce the two systems we used in IMO 2025, Seed-Geometry and Seed-Prover. 2.1 Seed-Geometry Seed-Geometry builds on the success of TongGeometry [ 30] and performs a major redesign. From a global perspective, both systems leverage trained neural models to complete missing auxiliary constructions and 2specialized reasoning engines to forward-chain derivation. However, Seed-Geometry is a substantial upgrade over TongGeometry in the following aspects. 2.1.1 Extended Domain-Spe"}
{"id": "2507.23726v1_chunk8", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "iary constructions and 2specialized reasoning engines to forward-chain derivation. However, Seed-Geometry is a substantial upgrade over TongGeometry in the following aspects. 2.1.1 Extended Domain-Specific Language Seed-Geometry constructs geometric diagrams in the principle of ruler-and-compass construction. However, plain ruler-and-compass construction steps can be long and cumbersome, making the language representation of the constructions overly verbose, introducing unnecessary burden on both the neural processing of Transformer- based large language model and the symbolic derivation of the backend engine. To mitigate these issues, Seed-Geometry groups particular action sequences into specific actions, making the representation of the problem concise enough. Of particular note, Seed-Geometry has several such composite actions: isogonal conjugate with respect to a triangle and a point, exsimilitude center of two circles, insimilitude center of two circles. All three actions can be r"}
{"id": "2507.23726v1_chunk9", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ometry has several such composite actions: isogonal conjugate with respect to a triangle and a point, exsimilitude center of two circles, insimilitude center of two circles. All three actions can be represented with primitive ruler-and-compass actions; yet the construction sequence in itself is non-trivial and unnecessarily clumsy. 2.1.2 Extremely Fast Reasoning Engine Seed-Geometry improved its reasoning engine’s performance by rewriting its backend in C++ and making it accessible to Python users through Pybind11. This change led to roughly 100-fold speed increase compared to the Python implementation in TongGeometry. The C++ implementation handles memory more efficiently and benefits from compiler optimizations, allowing for much faster deepsearches within the reasoning engine. This is particularly crucial because the engine’s forward-chaining design typically slows down considerably when the search tree expands widely at deeper levels. 2.1.3 Exceptional Large Language Model Seed-Geo"}
{"id": "2507.23726v1_chunk10", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "articularly crucial because the engine’s forward-chaining design typically slows down considerably when the search tree expands widely at deeper levels. 2.1.3 Exceptional Large Language Model Seed-Geometry utilizes a high-performing large language model from the Seed family [ 17]. This particular Seed model has undergone extensive pre-training on vast datasets of coding and mathematics, granting it a wide array of specialized skills. The specific model size was chosen considering the number of data tokens. We considered training two models in an actor–critic setup initially: the policy model that proposed possible next auxiliary element to construct and a value model that predicted the number of steps to go from the state. However, we note from preliminary experiments that a single Seed model serving as the policy would suffice, contradicting the design of both a policy model and a value model in existing work [ 30]. We also note that a policy model unspecialized to the specific goal m"}
{"id": "2507.23726v1_chunk11", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "del serving as the policy would suffice, contradicting the design of both a policy model and a value model in existing work [ 30]. We also note that a policy model unspecialized to the specific goal makes both training and solving more manageable and therefore, we only trained the model on pairs of problem context and auxiliaries, without the target fact goal in the prompt. 2.1.4 Extensive Search When presented with a new problem, Seed-Geometry first transforms the representation into a canonical form. If the reasoning backend successfully finds the goal fact to prove in the reasoning process, the problem is considered immediately solved. Otherwise, Seed-Geometry initiates a search process. In particular, Seed- Geometry employs beam search, with the policy model generating proposals for each beam in the buffer. With the extremely fast reasoning engine, Seed-Geometry supports running each new generated proposal in time. If any one of the proposal leads to the proof, the problem is consi"}
{"id": "2507.23726v1_chunk12", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ch beam in the buffer. With the extremely fast reasoning engine, Seed-Geometry supports running each new generated proposal in time. If any one of the proposal leads to the proof, the problem is considered proven; otherwise we select the top few proposals for the next step of expansion in the search tree based on each proposal’s cumulative negative log likelihood. The search process terminates until a fixed number of steps have been consumed. Seed-Geometry’s search process is made efficient and scalable. Compared to TongGeometry, Seed-Geometry’s solving process supports a distributed setup where each GPU process communicates with each other and blocked only at the beam selection point. Each GPU process is also equipped with a CPU thread pool that asynchronously executes the reasoning step during language model inference such that the reasoning cost can be overlapped with language model inference. 3theorem eg : 1+ 2= 3:= by haveh0 : 1+ 1= 2:= by ring haveh1 : 1+ (1+ 1) = 3:= by ring lin"}
{"id": "2507.23726v1_chunk13", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "during language model inference such that the reasoning cost can be overlapped with language model inference. 3theorem eg : 1+ 2= 3:= by haveh0 : 1+ 1= 2:= by ring haveh1 : 1+ (1+ 1) = 3:= by ring linarith [h0, h1] #printaxiomseglemmaround1_h0 : 1+ 1= 2:= by ring lemmaround1_h1 : 1+ (1+ 1) = 3:= by ring theorem eg : 1+ 2= 3:= by linarith [round1_h0, round1_h1] #printaxiomsegWhole proof Lemma -styleFigure 2 An example of whole proof and lemma-style proof in Lean 4. 2.2 Seed-Prover Seed-Prover is a large language model specialized in formal reasoning in Lean 4. Its most significant distinction from prior work lies in its adoption of lemma-style proving as the proof paradigm, which places lemmas at the center of the reasoning process. This approach offers several key advantages: it enables clear identification of lemmas that have and have not been proved, indicating the progress made in solving the main problem; lemmas can be processed independently and combined freely; lemmas from differ"}
{"id": "2507.23726v1_chunk14", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ear identification of lemmas that have and have not been proved, indicating the progress made in solving the main problem; lemmas can be processed independently and combined freely; lemmas from different inference trajectories can be combined to address more challenging problems. Both the training and inference procedures of Seed-Prover are designed around lemmas. 2.2.1 Lemma-Style Proving Previous works [ 9,15,21] trained the model to generate whole proofs starting with the keyword theorem. In contrast, we first require the model to generate some useful lemmas—each introduced by the keyword lemma—before generating the main proof using theorem by applying the generated lemmas. An example is shown in Figure 2. This lemma-style proof provides following merits. First, it allows clear identification of the lemmas that have been successfully proved, and those that need further refinement. Second, lemmas are modular—they can be compiled independently, stored independently, and combined freel"}
{"id": "2507.23726v1_chunk15", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "cation of the lemmas that have been successfully proved, and those that need further refinement. Second, lemmas are modular—they can be compiled independently, stored independently, and combined freely. Additionally, proofs of lemmas may provide inspiration to the model for proving unproved lemmas and the main problem. To enable this workflow, we establish a lemma pool for each difficult problem, which stores comprehensive data from all our inference runs, including lemma statements, lemma names, complete proofs, proof difficulties, and dependency relations. The lemma pool is typically used to (1) retrieve the most relevant lemmas by name or formal statement; (2) sample the most difficult lemmas according to their proof difficulty. 2.2.2 Conjecture Proposing When tackling challenging contest-level math problems, human contestants often identify interesting properties of the problem and use them to guide their reasoning. Seed-Prover is trained to propose such potentially useful properti"}
{"id": "2507.23726v1_chunk16", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "st-level math problems, human contestants often identify interesting properties of the problem and use them to guide their reasoning. Seed-Prover is trained to propose such potentially useful properties by chain-of-thought reasoning. Notably, this process differs from chain-of-thought reasoning to solve the problem directly; rather, it emphasizes broad exploration of the problem space without committing to a particular approach. Take functional equations as an example: one might conjecture that the function is injective,surjective ,bijective,monotonic , orperiodic—all without engaging in deep, targeted reasoning about the problem itself. In a sense, many useful properties of a problem may be enumerated before full problem resolution. The proposer module accepts an unsolved problem and, optionally, some already proved lemmas as input, and generates 10–50 candidate conjectures about properties of the problem. Proposing multiple conjectures in parallel significantly increases diversity an"}
{"id": "2507.23726v1_chunk17", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ionally, some already proved lemmas as input, and generates 10–50 candidate conjectures about properties of the problem. Proposing multiple conjectures in parallel significantly increases diversity and the likelihood of covering the valuable properties. For each problem, we may repeat this process multiple times to create a large conjecture pool. This approach differs from Draft, Sketch, Prove [ 7], which presumes the ability to fully solve the problem upfront. In contrast, our method performs broad exploratory searches over the problem space enabling 4ProblemNLLemma 1Lemma 2Lemma 3Lemma ..Lemma nWhole ProofGenerationProblemWhole ProofLean Compiler ❌Self summaryWhole ProofLean Compiler ❌Self summaryWhole ProofLean Compiler ❌LightInference SettingProblemWhole ProofLean Compiler ❌Fail lemma 1Fail lemma 2Fail lemma nLight InferenceLight InferenceLight InferenceInner RefinementProblem with New ContextWhole ProofLean Compiler ❌Fail lemma 2Fail lemma 4Fail lemma nLight InferenceLight Inferen"}
{"id": "2507.23726v1_chunk18", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "1Fail lemma 2Fail lemma nLight InferenceLight InferenceLight InferenceInner RefinementProblem with New ContextWhole ProofLean Compiler ❌Fail lemma 2Fail lemma 4Fail lemma nLight InferenceLight InferenceLight InferenceProblem with New ContextWhole ProofLean Compiler ✅Self summaryWhole ProofLean Compiler ✅MediumInference SettingLean Compiler ✅TheoremOuter RefinementFigure 3 The workflows of single-pass whole proof generation, light, and medium inference settings. discovery of useful properties, which makes it possible to solve problems that the model cannot solve directly in natural language. 2.2.3 Training To enable seamless interaction between Seed-Prover and Lean, we adopt multi-stage, multi-task reinforcement learning (RL) based on VAPO [ 29]. The RL reward is 1 if the formal statement is successfully proven, and 0 otherwise. Additionally, a formatting penalty is applied to encourage the model to generate lemmas before attempting the main theorem. As training progresses, problem diff"}
{"id": "2507.23726v1_chunk19", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "s successfully proven, and 0 otherwise. Additionally, a formatting penalty is applied to encourage the model to generate lemmas before attempting the main theorem. As training progresses, problem diffculty, problem quality, and maximum output length are progressively increased. The training dataset comprises a combination of open-source datasets [ 1,8,12,27,28] and in-house formalized problems. For problems that are too difficult for single-pass generation, we use our proposer to generate easier problem variants and put these into the training dataset. We also exclude problems which are too easy (i.e. proof rate above 1/4) from RL training. Unlike prior work [ 15,21] that only utilize formal statements as prompts for RL training, our approach randomly incorporates natural language hints, natural language proofs, similar lemmas, proved lemmas, failed lemmas, failed attempts, summaries of previous attempts, and Lean compiler feedback into the prompt. This diverse prompting strategy enhan"}
{"id": "2507.23726v1_chunk20", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "atural language proofs, similar lemmas, proved lemmas, failed lemmas, failed attempts, summaries of previous attempts, and Lean compiler feedback into the prompt. This diverse prompting strategy enhances the model’s adaptability within our inference pipeline by enabling it to understand and utilize various types of input. 2.2.4 Test-Time Scaling Here, we introduce our approach to test-time scaling of Seed-Prover. Depending on available inference budgets and problem difficulties, we developed three levels of strategies, which are illustrated in Figure 3 and Figure 4. LightPrevious work evaluated LLM theorem provers by Pass@ k. We find that iteratively refining proofs using Lean compiler feedback [ 32] in conjunction with self-summarization can surpass the limits of single-pass inference token budgets and improve proof ability significantly. In the light setting, each proof attempt is refined up to 8–16 times and evaluated under Pass@8–16. We denote the sample budget of Pass@ nand up tom"}
{"id": "2507.23726v1_chunk21", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "en budgets and improve proof ability significantly. In the light setting, each proof attempt is refined up to 8–16 times and evaluated under Pass@8–16. We denote the sample budget of Pass@ nand up tomrefinements as n×m, so the sample budget of the light setting is equivalent to generating the whole proof at Pass@64–256. This setting completes in 1–2 hours. Under this setting, Seed-Prover proves IMO 2022 P2 (MOHS = 151), whereas without refinement, the same problem can only be proved in Pass@8192. We 1MOHSisMathOlympiadHardnessScale: https://web.evanchen.cc/upload/MOHS-hardness.pdf . Wenotethatthehardness scale for human contestants may not be well-aligned with the difficulty of proving it in Lean using an LLM. 5ProblemConjecture 0Conjecture 1Conjecture 2Conjecture 3Conjecture ..Conjecture nLight InferenceLight InferenceLight InferenceLight InferenceLight InferenceLight InferenceLemma iLemma jLemma kMedium InferenceProposerSelect based on proof difficultyConjecture 10Conjecture 11Conjec"}
{"id": "2507.23726v1_chunk22", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "nLight InferenceLight InferenceLight InferenceLight InferenceLight InferenceLight InferenceLemma iLemma jLemma kMedium InferenceProposerSelect based on proof difficultyConjecture 10Conjecture 11Conjecture 12Conjecture 13Conjecture ..Conjecture 1nLight InferenceLight InferenceLight InferenceLight InferenceLight InferenceLight InferenceProposerProblem Lemma 1iLemma 1jLemma 1kMedium InferenceSelect based on proof difficultyProblemHeavy Inference SettingFigure 4 The workflow of heavy inference setting. observed two dominant behaviors of Seed-Prover in the light inference setting. First, it fixes Lean syntax errors in response to Lean compiler feedback. Second, it refines initial proof sketches, a process that might entirely alter the reasoning trajectory. Medium Proofs for challenging competition problems are often lengthy and structurally complex. The medium test-time setting introduces both outer and inner refinement processes. The outer refinement process mirrors the light setting which"}
{"id": "2507.23726v1_chunk23", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ion problems are often lengthy and structurally complex. The medium test-time setting introduces both outer and inner refinement processes. The outer refinement process mirrors the light setting which refines the proof of the original main problem. The inner refinement process targets difficult lemmas that the outer refinement process generates but fails to prove, using a light setting with an 8×8budget to handle finer details. If any inner refinement successfully proves at least one lemma (indicating meaningful progress), the outer refinement process will update this information into the prompt and continue its refinement. Under this setting, Seed-Prover solves harder problems like IMO 2003 P6 (MOHS = 35), IMO 2020 P5 ( MOHS = 20), IMO 2024 P1 ( MOHS = 5) and IMO 2025 P5 ( MOHS = 15), and final proofs potentially exceeding 1000 lines of code. HeavyWhile the medium setting encourages reasoning in depthover proof details, it lacks the breadth needed to explore diverse properties of the "}
{"id": "2507.23726v1_chunk24", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": " final proofs potentially exceeding 1000 lines of code. HeavyWhile the medium setting encourages reasoning in depthover proof details, it lacks the breadth needed to explore diverse properties of the given problem. Under the heavy inference setting, Seed-Prover begins with a conjecture pool and an empty lemma pool for the given problem. Initially, the proposer generates thousands of conjectures (by default 5000) to populate the conjecture pool. During inference, Seed-Prover tries to prove or disprove every conjecture in the conjecture pool using the light setting. Successfully proved conjectures are moved into the lemma pool. Seed-Prover leverages the lemma pool and Lean compiler feedback to refine the proof attempts. Additional conjectures are proposed based on proved lemmas. After days of thinking, the lemma pool accumulates several thousand nontrivial math facts. Each lemma is scored based on its proof rate, semantic relevance, and proof length. The proof rate serves as a strong ind"}
{"id": "2507.23726v1_chunk25", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "f thinking, the lemma pool accumulates several thousand nontrivial math facts. Each lemma is scored based on its proof rate, semantic relevance, and proof length. The proof rate serves as a strong indicator of lemma value; empirically, lemmas with low proof rate are often crucial to the final proof. Semantic relevance is assessed by an LLM judge. Unrelated lemmas or lemmas with short proof lengths are removed. We take hundreds of the top-ranked lemmas to help Seed-Prover finish the proof of the main problem using the medium inference setting. Seed-Prover has been trained to select and integrate these lemmas into a complete proof during RL. IMO 2017 P2 ( MOHS = 40), IMO 2025 P3 ( MOHS = 25) and IMO 2025 P4 ( MOHS = 15) are proved under the heavy inference setting. 3 Evaluation 63.1 Seed-Geometry Using Seed-Geometry’s backend, we performed large-scale problem generation. In particular, we took a similar approach to TongGeometry [ 30] by collecting data statistics of geometry problems ove"}
{"id": "2507.23726v1_chunk26", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "metry Using Seed-Geometry’s backend, we performed large-scale problem generation. In particular, we took a similar approach to TongGeometry [ 30] by collecting data statistics of geometry problems over more than the past 20 years, and ran the problem generation program on the trees. In more than 7 days, the problem generation program found over 230 million unique problems, reaching 8x the search efficiency compared to the Python implementation. After necessary preprocessing using a pretrained byte-pair encoding [ 18], the dataset totaled 38B tokens. We trained both a policy model to complete the auxiliary objects given the context and a value model to estimate the number of steps remaining under the current state. The policy model was initialized from a pretrained Seed model and the value model was initialized with the trained policy model. However, in the experiments, we found that under extensive search, the value model could harm the general performance due to large errors in value "}
{"id": "2507.23726v1_chunk27", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "model was initialized with the trained policy model. However, in the experiments, we found that under extensive search, the value model could harm the general performance due to large errors in value estimation. We also compared generating auxiliary actions step-by-step with beam search and generating the whole auxiliary sequence in one go. Results from the latter were significantly inferior to the former. Therefore, in the final evaluation, we used step-by-step generation with beam search in a distributed setup, with each GPU process hosting a policy model for proposal generation. Table 1 lists the performance of Seed-Geometry in IMO geometry problems from 2000 to 2024. Using the accounting method in IMO-AG-50, Seed-Geometry achieves 43 problem solves compared to AlphaGeometry 2, reaching 1 more solution. The problems that Seed-Geometry cannot solve but AlphaGeometry 2 does are not proof-based problems but rather computation-based problems, which AlphaGeometry 2 could potentially addr"}
{"id": "2507.23726v1_chunk28", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": " 1 more solution. The problems that Seed-Geometry cannot solve but AlphaGeometry 2 does are not proof-based problems but rather computation-based problems, which AlphaGeometry 2 could potentially address using its algebraic engine. Table 2 lists the performance of our model in the much harder IMO shortlist problems from 2000 to 2022. The original benchmark of IMOSL-AG-30 was claimed to be established from the problems in the benchmark, but omitted many of the hard problems. Here, we include all hard-level problems in the shortlist and present the full results. As shown in the tables, AlphaGeometry 2 solved 19 of the 39 problems and our Seed-Geometry solved 22 of them. In addition, Seed-Geometry solved IMO 2025 P2 in 2 seconds after it received the human-provided problem formulation. Based on the results provided, Seed-Geometry has established a new state-of-the-art in automated geometry problem solving, surpassing the performance of its predecessor, AlphaGeometry 2. In summary, by solv"}
{"id": "2507.23726v1_chunk29", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "on the results provided, Seed-Geometry has established a new state-of-the-art in automated geometry problem solving, surpassing the performance of its predecessor, AlphaGeometry 2. In summary, by solving more problems in both the standard IMO and the more difficult IMO shortlist benchmarks, Seed-Geometry demonstrates a superior capability in handling complex, proof-based geometry challenges, setting a new performance standard in the field. Table 1 Performance comparison of AlphaGeometry 2 (AG2) and Seed-Geometry (SG) in IMO geometry problems from 2000 to 2024. Note that in IMO-AG-50, the 2002 P2, 2003 P4, 2004 P5, 2008 P1, and 2009 P4 are separated into two, whereas we merge each of them into one. ID AG2 SG 2000 P1 ✓ ✓ 2000 P6 ✓ ✓ 2001 P1 X X 2001 P5 ✓ X 2002 P2 ✓ ✓ 2002 P6 X X 2003 P3 X X 2003 P4 ✓ ✓ 2004 P1 ✓ ✓ 2004 P5 ✓ ✓ 2005 P1 ✓ ✓ 2005 P5 ✓ ✓ 2006 P1 X✓ 2006 P6 X X 2007 P2 ✓ ✓ID AG2 SG 2007 P4 ✓ ✓ 2008 P1 ✓ ✓ 2008 P6 ✓ ✓ 2009 P2 ✓ ✓ 2009 P4 ✓ X 2010 P2 ✓ ✓ 2010 P4 ✓ ✓ 2011 P6 ✓ ✓"}
{"id": "2507.23726v1_chunk30", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": " 2003 P4 ✓ ✓ 2004 P1 ✓ ✓ 2004 P5 ✓ ✓ 2005 P1 ✓ ✓ 2005 P5 ✓ ✓ 2006 P1 X✓ 2006 P6 X X 2007 P2 ✓ ✓ID AG2 SG 2007 P4 ✓ ✓ 2008 P1 ✓ ✓ 2008 P6 ✓ ✓ 2009 P2 ✓ ✓ 2009 P4 ✓ X 2010 P2 ✓ ✓ 2010 P4 ✓ ✓ 2011 P6 ✓ ✓ 2012 P1 ✓ ✓ 2012 P5 ✓ ✓ 2013 P3 ✓ ✓ 2013 P4 ✓ ✓ 2014 P3 ✓ ✓ 2014 P4 ✓ ✓ 2015 P3 ✓ ✓ID AG2 SG 2015 P4 ✓ ✓ 2016 P1 ✓ ✓ 2017 P4 ✓ ✓ 2018 P1 ✓ ✓ 2018 P6 X✓ 2019 P2 ✓ ✓ 2019 P6 ✓ ✓ 2020 P1 ✓ ✓ 2020 P6 X X 2021 P3 ✓ ✓ 2021 P4 ✓ ✓ 2022 P4 ✓ ✓ 2023 P2 ✓ ✓ 2023 P6 X✓ 2024 P4 ✓ ✓ 7Table 2 Performance comparison of AlphaGeometry 2 (AG2) and Seed-Geometry (SG) in IMO shortlist geometry problems from 2000 to 2022. Note that in IMOSL-AG-30, many geometry problems are ignored and here we fill in those missing geometry problems for comparison, with 2016 G7 merged into one. ID AG2 SG 2002 G7 ✓ ✓ 2002 G8 ✓ ✓ 2003 G5 ✓ ✓ 2004 G7 X✓ 2004 G8 X✓ 2005 G5 ✓ ✓ 2005 G6 X✓ 2006 G9 ✓ ✓ 2007 G8 X X 2009 G6 ✓ ✓ 2009 G7 ✓ X 2009 G8 ✓ X 2010 G5 ✓ ✓ID AG2 SG 2011 G3 X X 2011 G4 X✓ 2011 G5 X✓ 2011 G6 ✓ X 2011 G7 ✓ X 2012 "}
{"id": "2507.23726v1_chunk31", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "003 G5 ✓ ✓ 2004 G7 X✓ 2004 G8 X✓ 2005 G5 ✓ ✓ 2005 G6 X✓ 2006 G9 ✓ ✓ 2007 G8 X X 2009 G6 ✓ ✓ 2009 G7 ✓ X 2009 G8 ✓ X 2010 G5 ✓ ✓ID AG2 SG 2011 G3 X X 2011 G4 X✓ 2011 G5 X✓ 2011 G6 ✓ X 2011 G7 ✓ X 2012 G6 X✓ 2012 G7 X X 2012 G8 X✓ 2014 G7 ✓ ✓ 2015 G5 ✓ ✓ 2015 G7 X X 2015 G8 X X 2016 G5 ✓ ✓ID AG2 SG 2016 G6 ✓ ✓ 2016 G7 ✓ ✓ 2016 G8 X X 2017 G7 X X 2017 G8 X X 2018 G7 ✓ ✓ 2019 G6 ✓ ✓ 2019 G8 X X 2020 G8 ✓ ✓ 2021 G8 X X 2022 G6 X X 2022 G7 X X 2022 G8 X X Table 3 Performance comparison of Seed-Prover against previous systems across formal math tasks. The performance on PutnamBench is using the number of proved statements instead of percentage following previous works [20]. Metric Seed-Prover Previous SOTA IMO 2025 4/6 (Heavy, 5/6 post-competition) 5/6 (Natural language, Gemini) Past IMO 78.1% (Heavy) — MiniF2F-valid 100.0% (Medium1) 90.6% (DeepSeek-Prover-V2 [15]) MiniF2F-test 99.6% (Medium2) 92.2% (Kimina-Prover [21]) PutnamBench 331/657 (Medium) 86/657 (Goedel-Prover-V2 [9]) CombiBench 30."}
{"id": "2507.23726v1_chunk32", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "eavy) — MiniF2F-valid 100.0% (Medium1) 90.6% (DeepSeek-Prover-V2 [15]) MiniF2F-test 99.6% (Medium2) 92.2% (Kimina-Prover [21]) PutnamBench 331/657 (Medium) 86/657 (Goedel-Prover-V2 [9]) CombiBench 30.0% (Medium) 10.0% (Deepseek-Prover-V2 [15]) MiniCTX-v2 81.8% (Light) 44.3% (o4-mini [16]) 1One problem used Heavy.2One problem failed under Heavy. 3.2 Seed-Prover To evaluate the performance of Seed-Prover, we tested it on IMO 20252, past IMO problems, MiniF2F [ 31], PutnamBench [ 20], CombiBench [ 10], and MiniCTX-v2 [ 6] covering a wide range of mathematical domains. For PutnamBench and CombiBench, we first evaluate using the light setting and use the medium setting for unsolved problems. For IMO problems and MiniF2F, we also use the heavy setting for the remaining unsolved problems. The results are shown in Table 3. Unless otherwise specified, we use Lean v4.14.0 with its corresponding Mathlib version. IMO 2025 During the IMO 2025 contest, all problems were translated into formal statem"}
{"id": "2507.23726v1_chunk33", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ults are shown in Table 3. Unless otherwise specified, we use Lean v4.14.0 with its corresponding Mathlib version. IMO 2025 During the IMO 2025 contest, all problems were translated into formal statements by human experts. For fill-in-the-blank problems (“determine xsuch that ...”), initial solution candidates were generated by Seed1.6-Thinking before translation. We conducted searches for IMO 2025 Problems 1, 3, 4, and 5 using both the medium and heavy inference settings in parallel. As required by the IMO committee, all proof submissions were due by July 18th. Seed-Geometry solved Problem 2 instantly, and Seed-Prover derived the proof for Problem 5 under the medium inference setting, while proofs for the other three problems required the heavy inference setting. Notably, the proof for Problem 1 was finished after the deadline. 2ByteDance was officially invited to participate in IMO 2025. 8Past IMO To evaluate our system’s performance on past IMO problems, we curated a dataset consist"}
{"id": "2507.23726v1_chunk34", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "oblem 1 was finished after the deadline. 2ByteDance was officially invited to participate in IMO 2025. 8Past IMO To evaluate our system’s performance on past IMO problems, we curated a dataset consisting of 155 past IMO problems. Most problems were adapted from Compfiles3and MiniF2F [ 31]. Additionally, a subset of problems was manually added or corrected by human experts. For problems prior to 2017, we used light and medium settings. For problems after 2017, we use the heavy inference setting if the medium inference setting failed. Seed-Prover successfully proves 121/155 problems, achieving an overall success rate of 78.1%. By difficulty, Seed-Prover proves 47/55 easy problems (P1 or P4), 47/56 medium problems (P2 or P5), and 27/44 hard problems (P3 or P6). By subject area, it proves 72/85 algebra problems, 42/55 number theory problems, and 7/14 combinatorics problems. This demonstrates that the Seed-Prover’s performance at IMO 2025 reflects consistent capability on IMO problems acros"}
{"id": "2507.23726v1_chunk35", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "5 algebra problems, 42/55 number theory problems, and 7/14 combinatorics problems. This demonstrates that the Seed-Prover’s performance at IMO 2025 reflects consistent capability on IMO problems across all years. MiniF2F [ 31]Under the medium setting, we prove 99.6%problems on both MiniF2F-valid and MiniF2F-test. We used the heavy inference setting to tackle the last problem in both splits (i.e. IMO 1990 P3 and IMOSL 2007 Algebra P6). Seed-Prover successfully proved IMO 1990 P3 and failed on IMOSL 2007 Algebra P6. Interestingly, among the most challenging problems solved in MiniF2F are ones such as AMC12A 2021 P12, AMC12A 2021 P25, and AMC12A 2020 P9, which are relatively straightforward to reason about in natural language, but pose significant challenges when formalized in Lean. This difficulty arises primarily from obstacles in applying Vieta’s formulas or the non-triviality of counting roots. PutnamBench [ 20]We evaluated Seed-Prover on PutnamBench using the light and medium inferen"}
{"id": "2507.23726v1_chunk36", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "iculty arises primarily from obstacles in applying Vieta’s formulas or the non-triviality of counting roots. PutnamBench [ 20]We evaluated Seed-Prover on PutnamBench using the light and medium inference settings. Under the light inference setting only, Seed-Prover proved 201/657problems from PutnamBench. Using the medium inference setting improved this performance to 331/657problems. This result shows a significant performance jump compared to previous works on undergraduate math problems. CombiBench [ 10]Previous benchmarks have primarily focused on number theory and algebra problems. CombiBench is a benchmark specifically centered on combinatorial problems, where the problems often involve newly-defined concepts. Here, we evaluate Seed-Prover on CombiBench using the medium inference setting. Our model proves 30 out of 100 problems from CombiBench, outperforming previous work. Nonetheless, relative to other benchmarks, our model still struggles with proving combinatorics problems. Min"}
{"id": "2507.23726v1_chunk37", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ing. Our model proves 30 out of 100 problems from CombiBench, outperforming previous work. Nonetheless, relative to other benchmarks, our model still struggles with proving combinatorics problems. MiniCTX-v2 [6] To test our system on a broader range of mathematical subjects from real-world formalization projects—including the ability to understand new definitions, notations, and lemmas—we evaluated Seed- Prover on MiniCTX-v2. This dataset includes context-rich problems from formalization repositories in subjects such as axiomatic systems, high-energy physics, analysis, and research-level number theory, all of which were written after Nov. 2024 to prevent data contamination. For evaluation purposes, we used the light inference setting under Lean v4.16.0. Our system successfully achieved 81.8%of MiniCTX-v2, which demonstrates its strong potential in real-world automated theorem proving, generalizing beyond standalone competition problems. For comparison, the baseline o4-mini solved 44.3%"}
{"id": "2507.23726v1_chunk38", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "8%of MiniCTX-v2, which demonstrates its strong potential in real-world automated theorem proving, generalizing beyond standalone competition problems. For comparison, the baseline o4-mini solved 44.3%statements at Pass@8 [16]. 4 Conclusion In this work, we presented Seed-Geometry and Seed-Prover—two formal reasoning frameworks that integrate the capabilities of large language models. Both systems substantially outperform previous formal reasoning frameworks. Seed-Geometry accelerates verification and scales the search mechanism. Seed-Prover leverages iterative refinement and a three-tiered test-time inference strategy to achieve state-of-the-art. Notably, we successfully proved 5 out of 6 problems in IMO 2025, demonstrating the efficacy of these formal systems. Formal languages like Lean offer rapid proof verification, making them far more cost-effective than human experts and more reliable than LLM judges. Our future work will focus on combining formal systems with large language mode"}
{"id": "2507.23726v1_chunk39", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ffer rapid proof verification, making them far more cost-effective than human experts and more reliable than LLM judges. Our future work will focus on combining formal systems with large language models to tackle open conjectures. 3https://github.com/dwrensha/compfiles 9References [1]Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, and Nick Haber. Big-math: A large-scale, high-quality math dataset for reinforcement learning in language models, 2025. URL https://arxiv.org/abs/2502.17387 . [2]Yuri Chervonyi, Trieu H Trinh, Miroslav Olšák, Xiaomeng Yang, Hoang Nguyen, Marcelo Menegali, Junehyuk Jung, Vikas Verma, Quoc V Le, and Thang Luong. Gold-medalist performance in solving olympiad geometry with alphageometry2. arXivpreprint arXiv:2502.03544, 2025. [3]DeepMind. Ai solves imo problems at silver medal level. https://deepmind.google/discover/blog/ ai-solves-imo-problems-at-silver-medal-level"}
{"id": "2507.23726v1_chunk40", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ith alphageometry2. arXivpreprint arXiv:2502.03544, 2025. [3]DeepMind. Ai solves imo problems at silver medal level. https://deepmind.google/discover/blog/ ai-solves-imo-problems-at-silver-medal-level/ , 2024. [4]DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 . [5]Kefan Dong and Tengyu Ma. Stp: Self-play llm theorem provers with iterative conjecturing and proving, 2025. URL https://arxiv.org/abs/2502.00212 . [6]Jiewen Hu, Thomas Zhu, and Sean Welleck. miniCTX: Neural theorem proving with (long-)contexts. In The Thirteenth International Conference onLearning Representations , 2025. URL https://openreview.net/forum? id=KIgaAqEFHW . [7]Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li, Mateja Jamnik, Guillaume Lample, and Yuhuai Wu. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. In TheEleventh International Conference onLear"}
{"id": "2507.23726v1_chunk41", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "x, Jiacheng Liu, Wenda Li, Mateja Jamnik, Guillaume Lample, and Yuhuai Wu. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. In TheEleventh International Conference onLearning Representations, 2023. [8]Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://github.com/project-numina/aimo-progress-prize](https://github. com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf) , 2024. [9]Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, et al. Goedel-prover: A frontier model for open-source automated theorem proving. arXivpreprint arXiv:2502.07640, 2025. [10]Junqi Liu, Xiaohan Lin, Jonas Bayer, Yael Dillies, Weijie Jiang, Xiaodan Liang, Roman Soletskyi, Haiming Wang, Yunzhou Xie, Beibe"}
{"id": "2507.23726v1_chunk42", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "e automated theorem proving. arXivpreprint arXiv:2502.07640, 2025. [10]Junqi Liu, Xiaohan Lin, Jonas Bayer, Yael Dillies, Weijie Jiang, Xiaodan Liang, Roman Soletskyi, Haiming Wang, Yunzhou Xie, Beibei Xiong, et al. Combibench: Benchmarking llm capability for combinatorial mathematics. arXivpreprint arXiv:2505.03171, 2025. [11] OpenAI. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720 . [12]Zhongyuan Peng, Yifan Yao, Kaijing Ma, Shuyue Guo, Yizhe Li, Yichi Zhang, Chenchen Zhang, Yifan Zhang, Zhouliang Yu, Luming Li, et al. Criticlean: Critic-guided reinforcement learning for mathematical formalization. arXivpreprint arXiv:2507.06181, 2025. [13]Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunović, Nikola Jovanović, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad, 2025. URL https://arxiv.org/abs/2503.21934 . [14]Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and"}
{"id": "2507.23726v1_chunk43", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad, 2025. URL https://arxiv.org/abs/2503.21934 . [14]Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. arXivpreprint arXiv:2202.01344, 2022. [15]ZZ Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, et al. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXivpreprint arXiv:2504.21801, 2025. [16]Tate Rowney, Jiewen Hu, Thomas Zhu, and Sean Welleck. miniCTX leaderboard. https://cmu-l3.github.io/ minictx/leaderboard.html , 2025. Accessed July 28, 2025. [17]ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning. arXivpreprint arXiv:2504."}
{"id": "2507.23726v1_chunk44", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "n Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning. arXivpreprint arXiv:2504.13914, 2025. [18]Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units.arXivpreprint arXiv:1508.07909, 2015. 10[19]Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476–482, 2024. [20]GeorgeTsoukalas, JasperLee, JohnJennings, JimmyXin, MichelleDing, MichaelJennings, AmitayushThakur, and Swarat Chaudhuri. Putnambench: Evaluating neural theorem-provers on the putnam mathematical competition. InTheThirty-eight Conference onNeuralInformation Processing Systems Datasets andBenchmarks Track. [21]Haiming Wang, Mert Unsal, Xiaohan Lin, Mantas Baksys, Junqi Liu, Marco Dos Santos, Flood Sung, Marina Vinyes, Zhenzhe Ying, Zekai Zhu, Jianqiao Lu, Hugues de Saxcé,"}
{"id": "2507.23726v1_chunk45", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "s Datasets andBenchmarks Track. [21]Haiming Wang, Mert Unsal, Xiaohan Lin, Mantas Baksys, Junqi Liu, Marco Dos Santos, Flood Sung, Marina Vinyes, Zhenzhe Ying, Zekai Zhu, Jianqiao Lu, Hugues de Saxcé, Bolton Bailey, Chendong Song, Chenjun Xiao, Dehao Zhang, Ebony Zhang, Frederick Pu, Han Zhu, Jiawei Liu, Jonas Bayer, Julien Michel, Longhui Yu, Léo Dreyfus-Schmidt, Lewis Tunstall, Luigi Pagani, Moreira Machado, Pauline Bourigault, Ran Wang, Stanislas Polu, Thibaut Barroyer, Wen-Ding Li, Yazhe Niu, Yann Fleureau, Yangyang Hu, Zhouliang Yu, Zihan Wang, Zhilin Yang, Zhengying Liu, and Jia Li. Kimina-prover preview: Towards large formal reasoning models with reinforcement learning. 2025. URL http://arxiv.org/abs/2504.11354 . [22]Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, and Kai Chen. Internlm2.5- stepprover: Advancing automated theorem proving via expert iteration on large-scale lean problems, 2024. URL https://arxiv.org/abs/2410.15700 . [23]Zijian Wu, Jia"}
{"id": "2507.23726v1_chunk46", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "Dahua Lin, and Kai Chen. Internlm2.5- stepprover: Advancing automated theorem proving via expert iteration on large-scale lean problems, 2024. URL https://arxiv.org/abs/2410.15700 . [23]Zijian Wu, Jiayu Wang, Dahua Lin, and Kai Chen. Lean-github: Compiling github lean repositories for a versatile lean prover, 2024. URL https://arxiv.org/abs/2407.17227 . [24]Huajian Xin, Z.Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Haowei Zhang, Qihao Zhu, Dejian Yang, Zhibin Gou, Z.F. Wu, Fuli Luo, and Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. In TheThirteenth International Conference onLearning Representations , 2025. URL https://openreview.net/forum?id=I4YAIwrsXa . [25]Ran Xin, Chenguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, and Kai Shen. Bfs-prover: Scalable best-first tree search for llm-based automatic theorem pro"}
{"id": "2507.23726v1_chunk47", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "?id=I4YAIwrsXa . [25]Ran Xin, Chenguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, and Kai Shen. Bfs-prover: Scalable best-first tree search for llm-based automatic theorem proving, 2025. URL https://arxiv. org/abs/2502.03438 . [26]Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. Advances inNeuralInformation Processing Systems, 36:21573–21612, 2023. [27]Huaiyuan Ying, Zijian Wu, Yihan Geng, JIayu Wang, Dahua Lin, and Kai Chen. Lean workbook: A large-scale lean problem set formalized from natural language math problems. In TheThirty-eight Conference onNeural Information Processing Systems Datasets andBenchmarks Track. [28]Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. a"}
{"id": "2507.23726v1_chunk48", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "[28]Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXivpreprint arXiv:2503.14476, 2025. [29]Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025. URL https://arxiv.org/abs/2504.05118 . [30]Chi Zhang, Jiajun Song, Siyu Li, Yitao Liang, Yuxi Ma, Wei Wang, Yixin Zhu, and Song-Chun Zhu. Proposing and solving olympiad geometry with guided tree search. arXivpreprint arXiv:2412.10673, 2024. [31]Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. minif2f: a cross-system benchmark for formal oly"}
{"id": "2507.23726v1_chunk49", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "g and solving olympiad geometry with guided tree search. arXivpreprint arXiv:2412.10673, 2024. [31]Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. minif2f: a cross-system benchmark for formal olympiad-level mathematics. In International Conference onLearning Representations. [32]Yichi Zhou, Jianqiu Zhao, Yongxin Zhang, Bohan Wang, Siran Wang, Luoxin Chen, Jiahui Wang, Haowei Chen, Allan Jie, Xinbo Zhang, et al. Solving formal math problems by decomposition and iterative reflection. arXiv preprint arXiv:2507.15225, 2025. 11Appendix A Contributors The names are sorted alphabetically. An asterisk * indicates a member who left Seed. Algorithm Luoxin Chen, Liankai Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Wenlei Shi, Jiahui Wang, Siran Wang, Chenrui Wei, Shufa Wei, Yonghui Wu, Huajian Xin, Fan Yang, Hongyi Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang*, Yichi Zhou, Thomas Hanwen Zhu DataJinming Gu, Wenhao Huang, Zhicheng Jiang, Xiaoran Jin, Kaijing "}
{"id": "2507.23726v1_chunk50", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "Wei, Yonghui Wu, Huajian Xin, Fan Yang, Hongyi Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang*, Yichi Zhou, Thomas Hanwen Zhu DataJinming Gu, Wenhao Huang, Zhicheng Jiang, Xiaoran Jin, Kaijing Ma, Jiawei Shen, Tong Sun, Chenrui Wei, Shufa Wei, Yuchen Wu, Yihang Xia, Huaiyuan Ying*, Zheng Yuan, Ge Zhang InfraCheng Ren, He Sun, Zhihong Wang, Tianyun Zhao*, Jianqiu Zhao, Thomas Hanwen Zhu B LooKeng: An Easy-to-Use and Effective Python Interface for Lean Interacting with Lean poses significant challenges that limit the flexibility of Lean-based workflows. The most popular interface, LeanDojo [ 26], only supports earlier versions of Lean 4, restricting users from accessing Lean’s newest updates. Furthermore, LeanDojo requires creating a Lean repository for interaction, which makes it impractical to use considering the massive scale of Lean interaction during model development and inference. To address these issues, we introduce LooKeng, a REPL4-based Python interface designed to simp"}
{"id": "2507.23726v1_chunk51", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ractical to use considering the massive scale of Lean interaction during model development and inference. To address these issues, we introduce LooKeng, a REPL4-based Python interface designed to simplify and accelerate the interaction process. LooKeng offers powerful features for developers while providing a user-friendly interface for end-users. The core functionality of LooKeng includes ‘init_state’, ‘run_tac’, and ‘verify_proof’. One can use LooKeng to interact with Lean step-by-step or verify an entire proof directly. The key features of LooKeng are summarized as follows: •Stateless Design : A Lean state can be simultaneously processed using different LooKeng instances, enabling effortless scaling and sharing. •Complex Tactics : Complex tactics such as apply?andall_goals are fully supported, with enhanced infotree integration to prevent false positive proofs. •Version-Free : The LooKeng CLI allows users to manage and switch between different Lean versions. •Memory Control : Users "}
{"id": "2507.23726v1_chunk52", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "ported, with enhanced infotree integration to prevent false positive proofs. •Version-Free : The LooKeng CLI allows users to manage and switch between different Lean versions. •Memory Control : Users can easily track the memory consumption of the Lean backend, set custom thresholds, and automatically terminate processes when memory usage exceeds the limit. •Proof Verification : LooKeng provides a straightforward method, ‘verify_proof’, to rigorously verify the final proof using the native Lean interface, ensuring correctness and reliability. •Proof Simplification : LooKeng can remove useless tactics and hypothesis in the proof to obtain a simpler proof. •Statement Negation : LooKeng is able to generate the negated statement of a statement. •Multi-Concurrency Support : LooKeng can run as a service, handling thousands of concurrent requests via async architecture and resource isolation. 4https://github.com/leanprover-community/repl 12"}
{"id": "2507.23726v1_chunk53", "paper_id": "2507.23726v1", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "text": "service, handling thousands of concurrent requests via async architecture and resource isolation. 4https://github.com/leanprover-community/repl 12"}
{"id": "2507.23704v1_chunk0", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ENHANCED VELOCITY FIELD MODELING FOR GAUSSIAN VIDEO RECONSTRUCTION Zhenyang Li1∗, Xiaoyang Bai1∗, Tongchen Zhang2, Pengfei Shen1, Weiwei Xu2, Yifan Peng1† 1The University of Hong Kong2State Key Laboratory of Computer-aided Design & Computer Graphics, Zhejiang University ABSTRACT High-fidelity 3D video reconstruction is essential for enabling real-time rendering of dynamic scenes with realistic motion in virtual and augmented reality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has achieved near-photorealistic results in video reconstruction due to the great representation capability of deep deformation networks. However, in videos with complex motion and significant scale variations, deformation networks often overfit to irregular Gaussian trajectories, leading to suboptimal visual quality. Moreover, the gradient-based densification strategy designed for static scene reconstruction proves inadequate to address the absence of dynamic content. In light of these challe"}
{"id": "2507.23704v1_chunk1", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "timal visual quality. Moreover, the gradient-based densification strategy designed for static scene reconstruction proves inadequate to address the absence of dynamic content. In light of these challenges, we propose a flow-empowered velocity field modeling scheme tailored for Gaussian video reconstruction, dubbed FlowGaussian-VR . It consists of two core components: a velocity field rendering (VFR) pipeline which enables optical flow-based optimization, and a flow-assisted adaptive densification (FAD) strategy that adjusts the number and size of Gaussians in dynamic regions. We also explore a temporal velocity refinement (TVR) post-processing algorithm to further estimate and correct noise in Gaussian trajectories via extended Kalman filtering. We validate our model’s effectiveness on multi-view dynamic reconstruction and novel view synthesis with multiple real-world datasets containing challenging motion scenarios, demonstrating not only notable visual improvements (over 2.5 dB gain "}
{"id": "2507.23704v1_chunk2", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ti-view dynamic reconstruction and novel view synthesis with multiple real-world datasets containing challenging motion scenarios, demonstrating not only notable visual improvements (over 2.5 dB gain in PSNR) and less blurry artifacts in dynamic textures, but also regularized and trackable per-Gaussian trajectories. Keywords Dynamic Scene Reconstruction ·Gaussian Video ·Velocity Field. 1 Introduction 3D scene reconstruction is a crucial step for crafting immersive and realistic experiences in a wide range of augmented and virtual reality (AR/VR) applications [ 1,2], including cinema-level virtual modeling [ 3,4], autonomous driving [ 5], robotics [ 6,7], and medical surgery [ 8], as it accurately represents the spatial layouts of 3D environments and readily enables users to interact with and even manipulate scene contents. In this domain, 3D Gaussian Splatting (3DGS) [ 9] has recently demonstrated remarkable progress in both reconstruction quality and rendering efficiency, outperformin"}
{"id": "2507.23704v1_chunk3", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "nd even manipulate scene contents. In this domain, 3D Gaussian Splatting (3DGS) [ 9] has recently demonstrated remarkable progress in both reconstruction quality and rendering efficiency, outperforming established neural radiance field (NeRF) variants [ 10]. Noteworthy progress has also been observed in the more challenging dynamic scene reconstruction task [ 11,12,13], where Gaussian Splatting (GS) is also being increasingly adopted to modeling dynamic scenes. Existing dynamic GS frameworks mainly follow two schemes: deformation-based methods learn a deformation field [ 14] to represent 3D motions, while trajectory-based methods explicitly model the geometry transformation between consecutive frames [ 15,16]. The former scheme, leveraging the exceptional representation capability of neural networks, has generally demonstrated better reconstruction quality across diverse datasets. We attribute the superior rendering quality of deformation-based methods to two key factors: minimal motio"}
{"id": "2507.23704v1_chunk4", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "l networks, has generally demonstrated better reconstruction quality across diverse datasets. We attribute the superior rendering quality of deformation-based methods to two key factors: minimal motion between frames which facilitates model convergence, and large Gaussian population which yields sufficient optimizable parameters during training. However, as shown in Fig. 1, Gaussian trajectories predicted by the deformation network often lack local consistency and struggle to align with the true dynamics of moving objects, leading to suboptimal ∗These authors contributed equally. †e-mail: evanpeng @hku.hkarXiv:2507.23704v1 [cs.CV] 31 Jul 2025Enhanced Velocity Field Modeling for Gaussian Video Reconstruction Image Rendering Ours (33.21 dB)Baseline (25.92 dB)4DGSOursGT Velocity Modeling Velocity RenderingFAD Figure 1: Left: Deformation-based 4DGS encounters difficulties in reconstructing scenes and rendering novel views under challenging conditions, such as significant motion and other c"}
{"id": "2507.23704v1_chunk5", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "city RenderingFAD Figure 1: Left: Deformation-based 4DGS encounters difficulties in reconstructing scenes and rendering novel views under challenging conditions, such as significant motion and other complex dynamics. Our FlowGaussian-VR exhibits commendable performance on given scenes (e.g., “cut-roasted-beef”). Middle : We compare the ground-truth optical flow, the deformation network of baseline (4DGS), and the velocity field rendered by our method. Right : We render the velocity field for scene Gaussians, constrain it with flow-based losses, and employ the FAD strategy to add Gaussians for dynamic objects in the canonical space. video reconstruction quality and restricting the application of these methods to high-quality video datasets with fewer camera fluctuations, small-scale motion, and clear content. Stable and physically accurate Gaussian trajectories ensure both rendering and semantic consistency in dynamic regions during novel view synthesis, resulting in photorealistic outc"}
{"id": "2507.23704v1_chunk6", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": " and clear content. Stable and physically accurate Gaussian trajectories ensure both rendering and semantic consistency in dynamic regions during novel view synthesis, resulting in photorealistic outcomes. Therefore, their prospects in fields such as autonomous driving and robotics, where critical applications in semantic segmentation and object tracking demand accurate reconstruction of object trajectories [17], appear limited. As optical flow [ 18] has been proven effective in enhancing the camera pose estimation consistency in monocular video reconstruction, we adopt a similar approach to supervising the motion of 3D Gaussians. Nonetheless, we observe that such a methodology cannot be trivially applied to Gaussian representations for two primary reasons. Firstly, the centers of 3D Gaussians do not align with object surfaces. This misalignment may cause conflicts between velocity field-based learning and photometric supervision, leading to misguided optimization. Secondly, the defaul"}
{"id": "2507.23704v1_chunk7", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ians do not align with object surfaces. This misalignment may cause conflicts between velocity field-based learning and photometric supervision, leading to misguided optimization. Secondly, the default densification strategy of dynamic GS, inherited from 3DGS, relies on the gradient of photometric losses [ 11]. We find experimentally that this strategy struggles to capture regions with substantial motion and does not effectively align with the dynamic information present in the scene. Therefore, how to overcome these problems and incorporate motion cues into GS frameworks becomes a critical challenge. To this purpose, we propose FlowGaussian-VR , a velocity field modeling pipeline that incorporates optical flow as the ground truth for 3D Gaussian dynamics to enhance their temporal controllability and interpretability. We render 2D velocity fields with differentiable rasterization for each Gaussian and supervise them using outputs from models such as RAFT [ 19]. Consequently, we introdu"}
{"id": "2507.23704v1_chunk8", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ability and interpretability. We render 2D velocity fields with differentiable rasterization for each Gaussian and supervise them using outputs from models such as RAFT [ 19]. Consequently, we introduce three losses based on the rendered velocity fields to facilitate the alignment of Gaussian attributes with real-world dynamics captured by estimated optical flows. The windowed velocity error (Lwin)employs a temporal sliding window to optimize rendered velocity fields across multiple frames. The flow warping error ( Lwarp)computes the discrepancy between rendered images and warped ground truths from the next frame. Additionally, we leverage SAM-v2 [ 20] to segment dynamic regions for each video and compute the dynamic rendering loss ( Ldyn)to further refine the model’s representation of dynamic scene components. While velocity field-based optimization effectively controls the trajectories of Gaussian centers, the combination of flow-based losses and the original photometric loss may sti"}
{"id": "2507.23704v1_chunk9", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "amic scene components. While velocity field-based optimization effectively controls the trajectories of Gaussian centers, the combination of flow-based losses and the original photometric loss may still fall behind object motion when the scene undergoes significant changes between frames. To mitigate this issue, we introduce a flow-assisted adaptive densification (FAD) strategy that adds Gaussians by identifying challenging dynamic regions on the 2D frame instead of cloning and splitting existing ones. Compared to the conventional densification algorithm, this design helps to complete scene components that are missing from initialization based on a single video frame. Finally, we explore the possibility of leveraging the 2Enhanced Velocity Field Modeling for Gaussian Video Reconstruction 𝑡! Canonical Point Cloud (𝒢)[…,𝑡−1,𝑡,𝑡+1,…]TimestampsCOLMAP …… Velocity Field Rendering (V.R.) 𝐼!\"#𝐼!\"$𝐼!𝐼!%$𝐼!%#(𝛥!\"#|!\"%(𝛥!\"#(𝛥!|!\"#(𝛥!&#|!(𝛥!&%|!&#(𝛥!(𝛥!&#(𝛥!&% *Δ'Corrected deformation*Δ'|'\"(Estima"}
{"id": "2507.23704v1_chunk10", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "! Canonical Point Cloud (𝒢)[…,𝑡−1,𝑡,𝑡+1,…]TimestampsCOLMAP …… Velocity Field Rendering (V.R.) 𝐼!\"#𝐼!\"$𝐼!𝐼!%$𝐼!%#(𝛥!\"#|!\"%(𝛥!\"#(𝛥!|!\"#(𝛥!&#|!(𝛥!&%|!&#(𝛥!(𝛥!&#(𝛥!&% *Δ'Corrected deformation*Δ'|'\"(Estimated deformationCorrection4DGS’ GaussiansOurs’ GaussiansInitialization Temporal Velocity Refinement (Trialing) ⃗𝑣, ⃗𝑣#! ⃗𝑣## ⃗𝑣#% …… 𝐵$ ⃗𝑣. ⃗𝑣%# ⃗𝑣%% ⃗𝑣%& ……𝐵#Rasterize DepthFlow-assisted Adaptive Densification (FAD) 𝐼!𝐼!&# Pixel-wise Flow Warping ℒ&'() … …… Video ClipWindow size𝑉!\"#→!𝑉!→!%#𝑉!\"#→!\"%𝐹!\"#→!ℒ&'(!𝐹!→!%#𝐹!%#→!%(++ Background Depth Spatial consistency interpolation𝒢)*+𝒢#𝒢(𝒢, Flow loss & gradient selected pixel𝒢*-./! Flow Warping Error ( ) & Windowed Velocity Error ( )ℒ&'()ℒ&,- ℒ&'(\"ℒ&'(# Figure 2: Overview of our FlowGaussian-VR pipeline . The velocity attribute is introduced into each Gaussian within the dynamic 3D GS pipeline to ensure that the motion of Gaussians closely mimics real-world physical motion. To achieve this, we impose several constraints on the rendered velocity:"}
{"id": "2507.23704v1_chunk11", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ch Gaussian within the dynamic 3D GS pipeline to ensure that the motion of Gaussians closely mimics real-world physical motion. To achieve this, we impose several constraints on the rendered velocity: Lwarp ensures temporal color consistency by aligning the predicted velocity with sequential frame data; Lwinmaximizes the supervision effect of ground truth optical flow on velocity predictions; Ldynenhances rendering quality and velocity accuracy for dynamic regions. Finally, to globally refine the predicted velocity across the temporal domain, we explore a TVR post-processing strategy, which integrates the traditional Extended Kalman Filter (EKF). Extended Kalman Filter (EKF) [ 21] as a post-processing temporal velocity refinement (TVR) technique for Gaussian motion estimation. We conduct a comprehensive evaluation of our model on challenging datasets: Nvidia-long [ 22], and Neu3D [ 23]. Experimental results indicate that our FlowGaussian-VR pipeline, when integrated with the 4DGS [ 14]"}
{"id": "2507.23704v1_chunk12", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": " comprehensive evaluation of our model on challenging datasets: Nvidia-long [ 22], and Neu3D [ 23]. Experimental results indicate that our FlowGaussian-VR pipeline, when integrated with the 4DGS [ 14] baseline, improves the overall PSNR of novel view rendering by approximately 2.5 dB across challenging scenes, with a gain of over 2.0 dB on dynamic regions. Additionally, we compare our method with alternative approaches following the same scheme, namely 4D-GS [ 24] and SC-GS [ 25], yielding superior performance. We further demonstrate that the TVR strategy effectively smoothens Gaussian trajectories and aligns them more closely with ground truth optical flows, substantiating the potential of a multi-stage learning framework for dynamic GS. In summary, our contributions are as follows: •We propose a velocity field modeling pipeline, FlowGaussian-VR , which facilitates optical flow-based optimization via velocity field rendering in dynamic Gaussian splatting models to enhance the temporal"}
{"id": "2507.23704v1_chunk13", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ropose a velocity field modeling pipeline, FlowGaussian-VR , which facilitates optical flow-based optimization via velocity field rendering in dynamic Gaussian splatting models to enhance the temporal behavior of Gaussian instances. •We introduce the FAD strategy to address challenges of optimizing Gaussians in dynamic scenes with abrupt changes. By integrating flow-based supervision into its pipeline, FAD enhances the robustness of dynamic GS video processing. •We select challenging datasets comprising real-world captured videos to experimentally demonstrate the superior performance of FlowGaussian-VR on complex dynamic scenes. 2 Related Work 2.1 Video Reconstruction Neural video reconstruction methods [ 26,27] have significantly advanced the modeling and rendering of dynamic scenes [ 28,29], with progress in monocular videos [ 30,31], time-synchronized videos [ 32], sparse camera views [ 33], and stereo cameras [ 34]. In dynamic settings, NeRF-based approaches have made notable strid"}
{"id": "2507.23704v1_chunk14", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "29], with progress in monocular videos [ 30,31], time-synchronized videos [ 32], sparse camera views [ 33], and stereo cameras [ 34]. In dynamic settings, NeRF-based approaches have made notable strides, particularly for video reconstruction. Some models represent dynamic scenes as time-conditional radiance fields [ 35,36,37], or use 3Enhanced Velocity Field Modeling for Gaussian Video Reconstruction deformation networks separate from the canonical radiance field [ 38]. Others, such as DyNeRF [ 23], Nerfies [ 39], and HyperNeRF [ 40], encode temporal information into latent representations. For trajectory learning, approaches like DynamicNeRF [ 41] and NSFF [ 42] regress optical flow from spacetime coordinates, while DynIBaR [ 43] models trajectory fields using DCT bases. However, NeRF representations are computationally intensive, posing challenges for reconstructing complex dynamic scenes in videos. 2.2 Dynamic 3D Gaussian Splatting The emergence of 3DGS [ 9,44,45] has significantly "}
{"id": "2507.23704v1_chunk15", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "resentations are computationally intensive, posing challenges for reconstructing complex dynamic scenes in videos. 2.2 Dynamic 3D Gaussian Splatting The emergence of 3DGS [ 9,44,45] has significantly improved training speeds and reduced memory usage in video reconstruction models. Dynamic 3DGS variants primarily fall into two categories: those with time-dependent Gaussian parameters [ 11,12] and those employing deformation fields [ 46,13]. The latter offers a lightweight network, an initial point cloud, and fast training. These advantages make them well-suited for 3D video streaming, compression, and web-based demonstrations, while directly applying the previous type of dynamic 3DGS in these applications may struggle to handle the substantial storage concern. However, many 3DGS methods rely on datasets with minimal motion blur, slow object motion, and without flickers. When interference factors are present, dynamic 3DGS [ 11] performs multiview 3D optimization on the point clouds of ea"}
{"id": "2507.23704v1_chunk16", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "y on datasets with minimal motion blur, slow object motion, and without flickers. When interference factors are present, dynamic 3DGS [ 11] performs multiview 3D optimization on the point clouds of each frame, but the high number of iterations significantly increases training time. A more efficient approach applies deformation to the initial frame instead. Yet, optimizing the deformation field solely based on point cloud and temporal features [ 47] lacks explicit constraints [ 48], reducing its controllability and adherence to physical motion laws. Without well-defined motion dynamics, the model struggles to accurately capture per-instance motion within the point cloud. 2.3 Flow-assisted Gaussians Several studies [ 49,50,51,52] have explored integrating dynamic flows, such as velocity fields, with dynamic GS. MODGS [ 53] leverages off-the-shelf depth [ 54] and flow [ 19] estimators to compute the 3D flow between frames, initializing the deformation network for point cloud prediction. H"}
{"id": "2507.23704v1_chunk17", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ds, with dynamic GS. MODGS [ 53] leverages off-the-shelf depth [ 54] and flow [ 19] estimators to compute the 3D flow between frames, initializing the deformation network for point cloud prediction. However, it is constrained by the limitations of depth estimation and struggles to adapt to Gaussian changes during densification and pruning, reducing the effectiveness of flow supervision. Gaussian-Flow [ 15] and Shape of Motion [ 16] improve temporal motion modeling by introducing motion bases into the deformation network but avoid directly supervising Gaussian parameters or trajectories. These methods also overlook the impact of deformation updates on the rasterizer’s rendering, as Gaussian attributes are indirectly modified through geometric reshaping [ 55] rather than being directly controlled. Allowing the rasterizer to render velocity fields provides direct supervision of Gaussian properties, enhancing both deformation controllability and temporal motion representation. More relevan"}
{"id": "2507.23704v1_chunk18", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "olled. Allowing the rasterizer to render velocity fields provides direct supervision of Gaussian properties, enhancing both deformation controllability and temporal motion representation. More relevant to our research is MotionGS [ 50], which decouples foreground Gaussians from background ones and supervises object motion with pseudo 2D optical flow. However, we observe in practice that these Gaussians still undergo large, erratic displacements during optimization, causing numerous background Gaussians to drift into the foreground and corrupt its rendering. Furthermore, MotionGS only controls the center position of each Gaussian, while overlooking and constraining the optimization space of other attributes. Such a design prevents it from reaching globally optimal solution, especially in fast-moving scenes. Our method overcomes this drawback by directly rasterizing velocity fields from dynamic scenes. 3 Gaussian Video with Velocity Field Modeling As evidenced in Fig. 1, although deforma"}
{"id": "2507.23704v1_chunk19", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "st-moving scenes. Our method overcomes this drawback by directly rasterizing velocity fields from dynamic scenes. 3 Gaussian Video with Velocity Field Modeling As evidenced in Fig. 1, although deformation-based dynamic GS methods such as 4DGS [ 14] can generate visually appealing rendering results, their per-Gaussian motion estimation lacks constraints and local coherence. In light of this, we introduce FlowGaussian-VR, whose pipeline is illustrated in Fig. 2. It comprises a dynamic GS backbone (Sec. 3.1), multiple flow-based constraints empowered by velocity field rendering (Sec. 3.2 & 3.3), and the flow-assisted adaptive densification strategy (Sec. 3.4). The proposed pipeline centers around the allocation of 2D velocity attributes (e.g., 2D velocity vector) to each Gaussian point and the subsequent modeling of motion dynamics with this enhanced mapping. We name this mechanism velocity field modeling throughout the manuscript. In addition, we discuss our exploration on the Extended K"}
{"id": "2507.23704v1_chunk20", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "e subsequent modeling of motion dynamics with this enhanced mapping. We name this mechanism velocity field modeling throughout the manuscript. In addition, we discuss our exploration on the Extended Kalman Filter (EKF)-aided temporal velocity refinement post-processing in Sec. 5 and the supplementary material. 3.1 Preliminary: 4D Gaussian Splatting (4DGS) In 4DGS [ 14], a dynamic 3D scene is represented by a set of Gaussians G. Each Gaussian in the scene is defined as Gi= (µi,Σi, ci, σi), with µi∈R3being the center coordinates, Σi∈R3×3being the covariance matrix controlling the spread and orientation, ci∈R3being the color representation, and σi∈[0,1]being the opacity value. For simplicity of presentation, we omit the Gaussian index ihereafter. To capture temporal changes, 4DGS applies a deformation fieldD(x, t) :R3×R→R3to each Gaussian center µ0in the canonical space, predicting its position at time t 4Enhanced Velocity Field Modeling for Gaussian Video Reconstruction Figure 3: Velocit"}
{"id": "2507.23704v1_chunk21", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "formation fieldD(x, t) :R3×R→R3to each Gaussian center µ0in the canonical space, predicting its position at time t 4Enhanced Velocity Field Modeling for Gaussian Video Reconstruction Figure 3: Velocity Rendering in 3D Gaussian Rasterization. Gaussian velocities are alpha-blended along each projection ray. Through multi-view optimization, these Gaussian velocities progressively converge toward the actual motion of the object. asµt=µ0+D(µ0, t). The deformation model Dis trained end-to-end with the Gaussian parameters using image losses. During rendering, µtis projected onto the 2D plane as µ′ tand the intensity at pixel pis computed following the formulation in 3DGS: I(p) =NX i=1αiciiY j=1(1−αj), where αi=σi·exp\u0014 −1 2(p−µ′ i)TΣ−1 i(p−µ′ i)\u0015 .(1) Inspired by the alpha blending technique in Eq. 1, we propose the velocity field rendering algorithm below to obtain 2D representations of scene motion through differentiable rasterization. 3.2 Velocity Field Rendering Given the point cloud P0rec"}
{"id": "2507.23704v1_chunk22", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": ". 1, we propose the velocity field rendering algorithm below to obtain 2D representations of scene motion through differentiable rasterization. 3.2 Velocity Field Rendering Given the point cloud P0reconstructed by COLMAP [ 56] and two adjacent timestamps {t, t+δt}, we obtain the velocity field in a manner similar to the pixel color rendering in Eq. (1) by projecting the deformed point clouds, Ptand Pt+δt, onto the pixel plane, and calculating their displacement over δt. Specifically, we first transform a 3D point cloud Pfrom world coordinates to camera coordinates by applying the camera’s extrinsic parameters, i.e. the rotation matrix Rand translation vector T. Then, we project the 3D camera coordinates onto the 2D image plane by utilizing the camera’s intrinsic matrix K, which depends on the focal lengths and optical center of the camera. Ultimately, the 2D pixel coordinates pis obtained by normalizing the homogeneous coordinates. In such a way, we enable the rendering of 2D velocity "}
{"id": "2507.23704v1_chunk23", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "the focal lengths and optical center of the camera. Ultimately, the 2D pixel coordinates pis obtained by normalizing the homogeneous coordinates. In such a way, we enable the rendering of 2D velocity fields from the velocity attributes of Gaussians and the subsequent optimization of Gaussian parameters to capture motion dynamics effectively, completing the velocity field modeling pipeline. Velocity Field Rasterization. Each Gaussian in the 3D point cloud is assigned a velocity vector that represents its movement between adjacent frames (Fig. 3). The projected 2D velocity vector allows us to rasterize the temporal motion information at each pixel, akin to color rasterization in the original 3DGS formulation, and to enable a more explicit control of Gaussian attributes by the deformation network. Analogous to Eq. (1), the velocity field V(p)rendering process is formulated as: V(p) =NX i=1αiviiY j=1(1−αj). (2) 5Enhanced Velocity Field Modeling for Gaussian Video Reconstruction By definiti"}
{"id": "2507.23704v1_chunk24", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "work. Analogous to Eq. (1), the velocity field V(p)rendering process is formulated as: V(p) =NX i=1αiviiY j=1(1−αj). (2) 5Enhanced Velocity Field Modeling for Gaussian Video Reconstruction By definition, the projected velocity serves as a reasonable estimation of the rendered optical flow, assuming that a pixel is predominantly influenced by Gaussians that contribute to its color, and its radiance is carried away by the motion of those Gaussians. Therefore, the velocity field on the pixel is a sum of the velocity of dominant Gaussians, weighed by their densities and visibilities. Alternatively, we may render two consecutive frames and apply optical flow estimation on the outcome directly. However, both experimental findings and theoretical analysis suggest that it is not as effective as our splatting-based approach. Additional details are provided in the supplementary materials. The newly introduced velocity attributes of Gaussians can be optimized via backpropagation through the raste"}
{"id": "2507.23704v1_chunk25", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "r splatting-based approach. Additional details are provided in the supplementary materials. The newly introduced velocity attributes of Gaussians can be optimized via backpropagation through the rasterization process. To utilize this property and optimize the velocity on Gaussians, we design specific losses for velocity field rendering to complement the conventional photometric loss. 3.3 Losses on Rendered Velocity Windowed Velocity Error ( Lwin).To refine the rendered velocity field at each iteration, we employ a temporal sliding window to sample multiple video frames during training. This approach ensures sufficient supervision and improves the model’s consistency in predicting point cloud deformations across different timestamps. This sliding window operation is applied to all losses in this subsection. Specifically, given the rendered velocity field image ˆViat timestamp i, we perform τ+ 1rendering processes within the same iteration, which generates τadditional rendered velocity i"}
{"id": "2507.23704v1_chunk26", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": " this subsection. Specifically, given the rendered velocity field image ˆViat timestamp i, we perform τ+ 1rendering processes within the same iteration, which generates τadditional rendered velocity images, denoted as ˆVi(τ) ={Vi+k}τ k=1, that span ktimestamps after i. The windowed velocity error ( Lwin) is calculated between ˆVi(τ)and ground truth optical flows eVi(τ)at those timestamps. Formally: Lwin=∥ˆVi(τ)−eVi(τ)∥1. (3) This loss ensures that the predicted velocity aligns with the actual scene dynamics across multiple consecutive frames. Flow Warping Error ( Lwarp).During training, the rendered velocity field registers the correspondence between RGB values of adjacent frames. To enhance this temporal correspondence, we propose the flow warping error ( Lwarp), which supervises frame-wise alignment by warping rendered RGB values in the next frame to the current frame using the rendered velocity field and minimizing its discrepancy from ground truth, as defined by: Lwarp=∥Wt+δt→t(ˆIt"}
{"id": "2507.23704v1_chunk27", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "se alignment by warping rendered RGB values in the next frame to the current frame using the rendered velocity field and minimizing its discrepancy from ground truth, as defined by: Lwarp=∥Wt+δt→t(ˆIt+δt,ˆVt+δt→t)−˜It∥1, (4) where ˆIand˜Irepresent rendered and ground truth images respectively, and Wt+δt→tdenotes the warping operation from timestamp t+δttot.Lwarp provides another pathway of aligning the predicted velocity field with the actual motion dynamics observed in the scene, and helps to rectify any inaccuracies in the predicted motion, thereby improving the consistency of the deformation field and enhancing the overall fidelity of reconstruction. Dynamic Rendering Loss ( Ldyn).To improve the rendering accuracy of dynamic objects in the scene, we perform dynamic foreground segmentation to isolate moving objects from the background and compute masked image error: Ldyn=∥ ˆI⊙Mdyn,˜I⊙Mdyn∥1, (5) where Mdynrepresents the dynamic foreground segmentation mask, and ⊙denotes element-wise "}
{"id": "2507.23704v1_chunk28", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "on to isolate moving objects from the background and compute masked image error: Ldyn=∥ ˆI⊙Mdyn,˜I⊙Mdyn∥1, (5) where Mdynrepresents the dynamic foreground segmentation mask, and ⊙denotes element-wise multiplication. This loss emphasizes dynamic regions in the scene and focuses the optimization process on the motion and appearance of moving objects. 3.4 Flow-assisted Adaptive Densification (FAD) Examining the experimental results of the baseline reveals its inability to handle challenging scenarios with significant object dynamics, where Gaussians are often absent in such regions. While the rendering and supervision of the velocity field effectively influence the positions and parameters of existing Gaussians, regions without Gaussians remain unoptimized. To address this limitation, we retain the 3DGS densification strategy while aiding it with the FAD strategy (Fig. 4). Unlike conventional densification that clones and splits existing Gaussians, FAD leverages flow-based losses and thei"}
{"id": "2507.23704v1_chunk29", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "in the 3DGS densification strategy while aiding it with the FAD strategy (Fig. 4). Unlike conventional densification that clones and splits existing Gaussians, FAD leverages flow-based losses and their gradients to directly add Gaussians to dynamic regions in the canonical space. It aims to compensate for inadequate Gaussians in dynamic objects by aligning their positions with underlying motion dynamics, thereby achieving more accurate and temporally consistent representations. Specifically, it lifts prominent pixels in the velocity field, which are correlated to dynamic scene elements, into the 3D space and constructs 3D Gaussians of dynamic objects correspondingly, ultimately enhancing the model’s representation and alignment with scene motion. Flow-assisted Pixel Selection. We use the loss map Lwinand the gradient map ∇Lwinfrom Section 3.2 to identify pixel positions where additional Gaussians are needed. We define a threshold ϵto select pixel locations that exhibit sufficiently lar"}
{"id": "2507.23704v1_chunk30", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "oss map Lwinand the gradient map ∇Lwinfrom Section 3.2 to identify pixel positions where additional Gaussians are needed. We define a threshold ϵto select pixel locations that exhibit sufficiently large losses and gradients: P={p:Lwin(p)> ϵand∇Lwin(p)> ϵ}. Since the loss and gradient maps 6Enhanced Velocity Field Modeling for Gaussian Video Reconstruction Figure 4: Flow-assisted adaptive densification (FAD) incorporates flow gradients and loss maps to identify pixel locations requiring densification. These pixels are lifted to the current frame’s 3D space using rendered depth and then transformed back into the canonical space via deformation-induced displacement. The attributes of these new Gaussians are calculated through kNN-weighted interpolation. may contain non-zero pixels in the background, we employ an off-the-shelf segmentation model SAM-v2 [ 20] to extract and exclude background pixels from P. Lifting to 3D Space. We lift the filtered pixels to 3D world coordinates P′using the"}
{"id": "2507.23704v1_chunk31", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ground, we employ an off-the-shelf segmentation model SAM-v2 [ 20] to extract and exclude background pixels from P. Lifting to 3D Space. We lift the filtered pixels to 3D world coordinates P′using the rendered depth map Z, camera intrinsics K, and camera extrinsics [ R,T], inverting the procedure described in Sec. 3.2. To lift a point (x, y)from 2D space to 3D space, we use the intrinsic and extrinsic camera parameters, as well as the projection model. The general relationship between the 2D image coordinates and the 3D world coordinates can be expressed as: P′= (x3D, y3D, z3D)T=K[R,T]·(x2D, y2D,1)T·Z, (6) where (x2D, y2D)are the 2D pixel coordinates in the image plane, (x3D, y3D, z3D)are the corresponding 3D coordi- nates in the world space, Kis the camera intrinsic matrix, which includes parameters such as focal length and principal point, Zis the depth value corresponding to the point in the 3D world. This equation assumes that we have the depth value Zfor the 3D point, which can be"}
{"id": "2507.23704v1_chunk32", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": " parameters such as focal length and principal point, Zis the depth value corresponding to the point in the 3D world. This equation assumes that we have the depth value Zfor the 3D point, which can be obtained through depth sensing or estimated through stereo vision techniques. Once the 3D coordinates are determined, we can perform operations such as motion tracking and dynamic scene reconstruction in 3D space. Farthest Point Sampling (FPS). To keep the number of added points manageable, we employ the established FPS [ 57,58] to downsample the candidates with a ratio r. FPS works by iteratively selecting points that are farthest from those already selected in a point cloud. This process ensures that the selected subset of candidates, denoted as P′′, spreads evenly across the space. Spatial Consistency Interpolation. After downsampling, to ensure consistency of the new Gaussians’ attributes in the canonical space, we use the K-nearest neighbors (kNN) algorithm to find the set of existin"}
{"id": "2507.23704v1_chunk33", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": " Consistency Interpolation. After downsampling, to ensure consistency of the new Gaussians’ attributes in the canonical space, we use the K-nearest neighbors (kNN) algorithm to find the set of existing Gaussians surrounding each candidate in P′′. To mitigate the spatial discontinuity of Gaussians parameters, we further obtain the radius rc on the pixel plane for each new Gaussian using differentiable rasterization and lift it to 3D to determine whether the neighboring points found by kNN lie within the candidate Gaussian’s spatial range. Points outside the radius indicate that the attributes between two Gaussians may not be continuous and thus cannot be used for attribute interpolation. To be more specific, we know that the FAD process operates on the frame at the current timestamp during training. The deformation network Dis achieved by transforming the point cloud P0in the canonical space to obtain the current point cloud Pt. Pt=P0+D(P0, t). (7) Given that the pixels selected by Lwar"}
{"id": "2507.23704v1_chunk34", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "training. The deformation network Dis achieved by transforming the point cloud P0in the canonical space to obtain the current point cloud Pt. Pt=P0+D(P0, t). (7) Given that the pixels selected by Lwarp andLwinare projected into the 3D space through depth to obtain new Gaussians Gf, we use kNN to find the top-k nearest neighbors from the original Gaussians surrounding Gf. The process can be described as follows: Gc=kNN(Pt,Gf, k),if|Pt− Gf|< rc, (8) 7Enhanced Velocity Field Modeling for Gaussian Video Reconstruction where rcrefers to the 3D range of each Gaussian in Gfand we finally obtain Gcas newly densified Gaussians. Therefore, the new Gaussians Gcadded at the current moment need to be mapped back to the canonical space Gcano cby reversing the offset calculated through the deformation: Gcano c=Gc−D(P0, t). (9) Table 1: Quantitative evaluation on the Nvidia dataset [22]. The resolution and video length of each scene vary. “Face.” and “Pg.” refer to dynamicFace and Playground. [PSNR (D"}
{"id": "2507.23704v1_chunk35", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "cano c=Gc−D(P0, t). (9) Table 1: Quantitative evaluation on the Nvidia dataset [22]. The resolution and video length of each scene vary. “Face.” and “Pg.” refer to dynamicFace and Playground. [PSNR (DPSNR) ↑, SSIM ↑, LPIPS ↓] are reported. Same applies to Table 2. Scene 4DGS 4D-GS SC-GS MotionGS Ours Balloon1 20.51 (21.24) / 0.619 / 0.317 23.06 (21.78) / 0.740 / 0.210 18.17 (16.94) / 0.666 / 0.440 20.72 (19.97) / 0.611 / 0.472 24.50 (24.54) / 0.757 / 0.290 Skating 26.80 (16.35) / 0.885 / 0.203 28.06 (18.32) / 0.872 / 0.131 18.09 (7.86) / 0.529 / 0.535 23.21 (12.94) / 0.834 / 0.370 29.91 (19.56) / 0.916 / 0.180 Face. 19.47 (24.58) / 0.783 / 0.185 18.61 (22.40) / 0.801 / 0.153 12.38 (9.25) / 0.332 / 0.546 15.15 (20.67) / 0.560 / 0.446 21.82 (27.20) / 0.854 / 0.042 Jumping 24.33 (17.75) / 0.847 / 0.246 23.80 (18.25) / 0.803 / 0.199 13.37 (7.96) / 0.368 / 0.639 20.53 (14.31) / 0.771 / 0.403 27.89 (21.07) / 0.887 / 0.216 Truck 25.18 (22.19) / 0.805 / 0.279 24.11 (20.67) / 0.791 / 0.260 16.8"}
{"id": "2507.23704v1_chunk36", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "47 / 0.246 23.80 (18.25) / 0.803 / 0.199 13.37 (7.96) / 0.368 / 0.639 20.53 (14.31) / 0.771 / 0.403 27.89 (21.07) / 0.887 / 0.216 Truck 25.18 (22.19) / 0.805 / 0.279 24.11 (20.67) / 0.791 / 0.260 16.87 (12.95) / 0.486 / 0.612 20.47 (18.68) / 0.698 / 0.498 26.22 (23.96) / 0.835 / 0.255 Pg. 19.98 (16.65) / 0.602 / 0.319 21.66 (16.92) / 0.757 / 0.179 12.27 (13.12) / 0.197 / 0.613 18.07 (15.62) / 0.448 / 0.533 22.18 (18.15) / 0.695 / 0.281 Umbrella 22.85 (21.24) / 0.527 / 0.416 24.01 (21.84) / 0.574 / 0.346 6.76 (4.65) / 0.000 / 0.719 20.80 (19.19) / 0.471 / 0.625 24.11 (22.32) / 0.599 / 0.366 Avg. 22.73 (20.00) / 0.724 / 0.280 23.33 (20.03) / 0.763 / 0.211 13.99 (10.39) / 0.368 / 0.586 19.85 (17.34) / 0.628 / 0.478 25.23 (22.40) / 0.792 / 0.232 Table 2: Quantitative evaluation on the Neu3D dataset [23] with a resolution of 1,352 ×1,014. Scene 4DGS 4D-GS SC-GS MotionGS Ours martini 22.47 (20.97) / 0.810 / 0.240 23.85 (21.22) / 0.861 / 0.219 13.33 (9.24) / 0.306 / 0.615 21.05 (20.17) / 0.80"}
{"id": "2507.23704v1_chunk37", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "taset [23] with a resolution of 1,352 ×1,014. Scene 4DGS 4D-GS SC-GS MotionGS Ours martini 22.47 (20.97) / 0.810 / 0.240 23.85 (21.22) / 0.861 / 0.219 13.33 (9.24) / 0.306 / 0.615 21.05 (20.17) / 0.805 / 0.405 24.19 (23.69) / 0.856 / 0.206 spinach 24.92 (22.88) / 0.831 / 0.222 26.81 (26.76) / 0.833 / 0.261 17.11 (15.89) / 0.432 / 0.531 25.94 (23.20) / 0.864 / 0.333 27.06 (25.40) / 0.881 / 0.186 beef 26.61 (25.08) / 0.881 / 0.192 28.48 (28.79) / 0.916 / 0.163 16.42 (15.74) / 0.414 / 0.552 21.55 (20.37) / 0.832 / 0.367 29.46 (28.58) / 0.924 / 0.166 salmon 22.95 (21.45) / 0.844 / 0.209 24.79 (23.24) / 0.896 / 0.171 8.23 (6.79) / 0.186 / 0.650 22.49 (21.62) / 0.813 / 0.388 25.67 (24.79) / 0.883 / 0.180 flame 25.70 (23.49) / 0.851 / 0.218 29.35 (26.37) / 0.951 / 0.143 6.71 (5.98) / 0.054 / 0.654 25.92 (23.04) / 0.878 / 0.323 28.86 ( 26.59 ) / 0.927 / 0.157 sear 26.46 (26.12) / 0.868 / 0.199 26.94 (29.36) / 0.914 / 0.163 6.72 (6.39) / 0.052 / 0.660 24.96 (26.13) / 0.865 / 0.347 28.54 (29.49)"}
{"id": "2507.23704v1_chunk38", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "4 25.92 (23.04) / 0.878 / 0.323 28.86 ( 26.59 ) / 0.927 / 0.157 sear 26.46 (26.12) / 0.868 / 0.199 26.94 (29.36) / 0.914 / 0.163 6.72 (6.39) / 0.052 / 0.660 24.96 (26.13) / 0.865 / 0.347 28.54 (29.49) / 0.922 / 0.156 Avg. 24.85 (23.33) / 0.847 / 0.213 26.70 (25.96) / 0.895 / 0.187 11.42 (10.01) / 0.241 / 0.610 23.65 (22.42) / 0.982 / 0.361 27.30 (26.47) / 0.899 / 0.175 4 Temporal Velocity Refinement (TVR) with Extended Kalman Filtering (EKF) While deformation network-based methods can establish temporal correlations, they do not inherently ensure smooth and physically consistent motion. In light of this, we here propose a temporal velocity refinement (TVR) process following the “refinement as a post-processing step” philosophy, whose impact on motion prediction will be explored in the subsequent experiment section. Extended Kalman Filtering. As an optimal recursive estimation technique in control theory, Kalman filter (KF) [ 59,60] estimates the internal states xof a linear dynamic sys"}
{"id": "2507.23704v1_chunk39", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "equent experiment section. Extended Kalman Filtering. As an optimal recursive estimation technique in control theory, Kalman filter (KF) [ 59,60] estimates the internal states xof a linear dynamic system in the presence of noisy observations z. The system is described using the state-space equations below: xk=Axk−1+Buk+wk, zk=Hxk+vk,(10) where AandHare the state transition matrix and the observation matrix, and wk∼ N(0,Q)andvk∼ N(0,R)are noise terms. For the task of temporal velocity refinement, the input term Bukis not needed and is omitted hereafter. To generalize this formulation to nonlinear systems, extended Kalman filter [ 61,62] replaces AandHwith nonlinear functions fandhwhose local linearity at xis characterized by Jacobian matrices Jf(x)andJh(x)respectively. Then, we iterate between the forecast step and the assimilation step to progressively refine xkgiven previous estimation xk−1 and current observation zkfor each time step k. More details on EKF are provided in the supplem"}
{"id": "2507.23704v1_chunk40", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "een the forecast step and the assimilation step to progressively refine xkgiven previous estimation xk−1 and current observation zkfor each time step k. More details on EKF are provided in the supplementary materials . Intuitively, EKF imposes a stronger correction of xf k=f(xk−1)when the accumulated covariance Pkgrows larger, thereby smoothing the state sequence and reducing noisy or zigzagging patterns. Therefore, it stands as a promising tool for the refinement of Gaussian velocity. Applying EKF to the FlowGaussian-VR pipeline, we define fandhas: f(xt−1) =D(D−1(xt−1, t−1), t), h(x) = VRaster( x),(11) where Dis the deformation network and VRaster( ·)is the velocity rasterization process described Section 3.1. Since bothDandVRaster( ·)are intractable, we have to assume their local linearity and approximate the partial gradients as: Jf(xt−1) =∂f(xt−1) ∂x≈f(xt−1+δx)−xt δx, Jh(xt) =∂h(xt) ∂x≈h(xt+δx)−h(xt) δx.(12) 8Enhanced Velocity Field Modeling for Gaussian Video Reconstruction Furthe"}
{"id": "2507.23704v1_chunk41", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ty and approximate the partial gradients as: Jf(xt−1) =∂f(xt−1) ∂x≈f(xt−1+δx)−xt δx, Jh(xt) =∂h(xt) ∂x≈h(xt+δx)−h(xt) δx.(12) 8Enhanced Velocity Field Modeling for Gaussian Video Reconstruction Furthermore, since Dis non-invertible and directly obtaining f(xt−1+δx)is impossible, we take a detour by considering the Jacobian of D(JD):Jf(xt−1) =JD(xt)·JD(xt−1)−1from the inverse function theorem. A more detailed calculation is provided in the supplementary materials . Surface Gaussian filtering. In an image rendered from 3D Gaussians, multiple Gaussians could project to the same pixel, yet only those on the surface of objects contribute to the optical flow value of that pixel. Therefore, we filter out each Gaussian whose projected depth is greater than the rendered depth value of its corresponding pixel at each time step, so that occluded Gaussians are not mistakenly updated. Note that we still run the forecast step for those excluded Gaussians, while skipping the assimilation step by dire"}
{"id": "2507.23704v1_chunk42", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ding pixel at each time step, so that occluded Gaussians are not mistakenly updated. Note that we still run the forecast step for those excluded Gaussians, while skipping the assimilation step by directly using xf kandPf kasxkandPk. Accurate flow map localization. Another challenge of TVR is to select the correct optical flow observation for each Gaussian. While using the projected coordinates at each time step sounds plausible, this approach links the reliability of zkto the accuracy of xk−1. That is, for an erroneous xk−1, the selected observation will also deviate from the ground truth flow value, which further causes the assimilated xkto have an even larger error and finally leads to catastrophic error accumulation. To resolve this problem, we assume that Gaussian locations are reliable only at the first frame, and for the kthtime step, zkis calculated as: zk=F(p0+Pk−1 i=0zi), where F(p)denotes the optical flow value at coordinates p, andp0is the projected 2D location of x0. That i"}
{"id": "2507.23704v1_chunk43", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "e only at the first frame, and for the kthtime step, zkis calculated as: zk=F(p0+Pk−1 i=0zi), where F(p)denotes the optical flow value at coordinates p, andp0is the projected 2D location of x0. That is, the coordinates used to index the flow map from the second frame on are obtained via adding up the flow values from all previous time steps. GT4DGSOurs22.96 (21.69)24.22 (23.23)25.88 (23.45)27.12 (24.94)24.67 (21.73)25.51 (23.04) Figure 5: Novel view synthesis evaluation on the Nvidia “Truck” scene . We compare our work with the retrained baseline (4DGS) over a few timesteps. As each timestep, we show the rendered novel view image with PSNR (DPSNR) in dB, and the estimated velocity field (color gamma-tuned for visualization). Photometric Loss Map Flow Loss Map Masked Loss Figure 6: Comparison of photometric and flow-based losses. We visualize the pixel-wise photometric loss and our proposed flow-based loss for a given frame. We observe that Lwarp andLwinare primarily concentrated on for"}
{"id": "2507.23704v1_chunk44", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "on of photometric and flow-based losses. We visualize the pixel-wise photometric loss and our proposed flow-based loss for a given frame. We observe that Lwarp andLwinare primarily concentrated on foreground pixels. 9Enhanced Velocity Field Modeling for Gaussian Video Reconstruction GTOurs (w/o TVR) Ours (w TVR) Figure 7: Visualization of the effectiveness of Temporal Velocity Refinement (TVR) trajectory correction. The figure depicts the 3D Gaussian motion rendered as a 2D velocity field, with arrows indicating the velocity vectors between consecutive frames. 5 Experiment 5.1 Initialization and Implementation Details Similar to the initialization strategy in 3DGS [ 63], reconstruction quality improves when the initial scene point cloud is derived from SfM [ 64]. Following 4DGS [ 13], the first frame of each video sequence is used as the initialization frame, and COLMAP [ 56] is applied to generate Gaussian centers. Ground truth optical flows are obtained using RAFT [ 19], and dynamic "}
{"id": "2507.23704v1_chunk45", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "first frame of each video sequence is used as the initialization frame, and COLMAP [ 56] is applied to generate Gaussian centers. Ground truth optical flows are obtained using RAFT [ 19], and dynamic foreground segmentation masks are manually selected with SAM-2 [20]. We conducted quantitative evaluations on the Nvidia-long [ 22] dataset, a benchmark for dynamic scenes, pre-processed and made publicly available by prior works [ 43]. This dataset includes 7 scene videos, each containing 90–210 frames with 12 camera views per frame. The Neu3D dataset [ 23], captured with 15–20 static cameras over extended periods with complex motions, features 6 selected scene sequences of 300 frames each. These datasets comprise high-resolution videos of dynamic scenes, offering challenges such as occlusions, rapid motion, and large-scale changes. Our FlowGaussian-VR pipeline builds upon 4DGS [ 14], and compares with 4DGS, 4D-GS [ 24], SC-GS [ 25], and MotionGS [ 50]. All training is performed on an NVI"}
{"id": "2507.23704v1_chunk46", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "s, rapid motion, and large-scale changes. Our FlowGaussian-VR pipeline builds upon 4DGS [ 14], and compares with 4DGS, 4D-GS [ 24], SC-GS [ 25], and MotionGS [ 50]. All training is performed on an NVIDIA RTX 4090 GPU. More experimental details can be found in thesupplementary material. Notably, we set the FPS point cloud downsampling ratio in FAD (Sec 3.4) to 0.01 for Nvidia-long and 0.001 for Neu3D. 5.2 Results The quantitative results on the Nvidia-long [ 22] and Neu3D [ 23] datasets are summarized in Table 1 and Table 2, respectively. A qualitative comparison between FlowGaussian-VR and the 4DGS baseline can be found in Fig. 8 and Fig. 9. In addition to standard metrics such as PSNR, SSIM, and LPIPS, we report dynamic PSNR (DPSNR) to specifically evaluate the rendering quality of dynamic foreground pixels. D-PSNR computes PSNR only in dynamic regions, identified by SAM-2, to assess reconstruction quality in motion areas. Our experimental results demonstrate that FlowGaussian-VR sign"}
{"id": "2507.23704v1_chunk47", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "c foreground pixels. D-PSNR computes PSNR only in dynamic regions, identified by SAM-2, to assess reconstruction quality in motion areas. Our experimental results demonstrate that FlowGaussian-VR significantly outperforms all three baselines in novel view synthesis. The average PSNR metrics across all scenes in the Neu3D and Nvidia-long datasets are improved by 2.45 dB and 2.5 dB, respectively, compared to the best-performing 4DGS method. For dynamic scene rendering, our method achieves average improvements of 3.14 dB and 2.4 dB in DPSNR over 4DGS. These metrics validate that our method effectively optimizes both static and dynamic components of the scene, and the gain in DPSNR is more pronounced compared to the overall PSNR, highlighting the effectiveness of FlowGaussian-VR on capturing and reconstructing dynamic scene contents. 10Enhanced Velocity Field Modeling for Gaussian Video Reconstruction Figure 8: Novel view synthesis evaluation on the Nvidia dataset . We present three scenes"}
{"id": "2507.23704v1_chunk48", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "nd reconstructing dynamic scene contents. 10Enhanced Velocity Field Modeling for Gaussian Video Reconstruction Figure 8: Novel view synthesis evaluation on the Nvidia dataset . We present three scenes: “Balloon1”, “Umbrella”, and “Jumping” and compare our work with retrained 4DGS. The 3 primary columns show results of the ground truth, baseline, and our method. For each scene, ground truth optical flows (first column) and rendered velocity fields (other columns) are presented in false color. The second and third rows within each scene show insets of 2 subareas, from left to right yielding 3 different timestamps. The PSNR showing at corners is calculated using the entire image. 11Enhanced Velocity Field Modeling for Gaussian Video Reconstruction Figure 9: Novel view synthesis evaluation on the Neu3d dataset . We visualize the “flame”, “martini”, and “beef” scenes and compare our approach with the retrained 4DGS. We rearranged the inset placement from figure 8 to suit the aspect ratio of"}
{"id": "2507.23704v1_chunk49", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "the Neu3d dataset . We visualize the “flame”, “martini”, and “beef” scenes and compare our approach with the retrained 4DGS. We rearranged the inset placement from figure 8 to suit the aspect ratio of this dataset, that the 3 main columns from left to right are still the results of the ground truth, 4DGS, and our method, and the insets from top to bottoms within each group represents 3 different time stamps. The colored image on the bottom-left of each group is optical flow for the ground truth, and velocity field rendering for the others accordingly. Table 3: Effectiveness assessment of Velocity Rendering (V .R.), FAD, Lwarp, andLwinon dynamic scene reconstruc- tion. “N” refers to the number of Gaussians in the final point cloud. V .R. FAD Lwarp Lwin PSNR / SSIM / LPIPS N4DGS✗ ✗ ✗ ✗ 20.51 / 0.619 / 0.317 214k ✓ ✓ ✓ ✗ 22.69 / 0.703 / 0.312 89k ✓ ✓ ✗ ✓ 22.47 / 0.718 / 0.305 75k ✓ ✓ ✓ ✓ 24.50 / 0.757 / 0.290 141k We further provide a close-up inspection of velocity fields for synthesized"}
{"id": "2507.23704v1_chunk50", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": " / 0.619 / 0.317 214k ✓ ✓ ✓ ✗ 22.69 / 0.703 / 0.312 89k ✓ ✓ ✗ ✓ 22.47 / 0.718 / 0.305 75k ✓ ✓ ✓ ✓ 24.50 / 0.757 / 0.290 141k We further provide a close-up inspection of velocity fields for synthesized novel views on the “Truck” scene to demonstrate the effectiveness of our method in Fig. 5. In this scene, the primary reconstruction challenges lie in the significant scale variations between the truck and the car, motion blur of the car, and fine-grained details such as text on the truck. As shown in the rendered Gaussian velocity field, our method produces gradually enlarging Gaussians on the truck’s front as it approaches, aligning well with the physical motion. In contrast, 4DGS introduces inconsistent Gaussian replacements that violate geometric continuity. Additionally, the RGB rendering results further confirm that Gaussians adhering to plausible motion dynamics yield more realistic visual quality. 12Enhanced Velocity Field Modeling for Gaussian Video Reconstruction Table 4: Effect"}
{"id": "2507.23704v1_chunk51", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "g results further confirm that Gaussians adhering to plausible motion dynamics yield more realistic visual quality. 12Enhanced Velocity Field Modeling for Gaussian Video Reconstruction Table 4: Effectiveness assessment of sliding window size for Lwinon final rendering quality. Metric reported is PSNR (dB)↑. Number of adjacent frames 2 (4DGS) 4 6 8 Jumping 25.33 25.63 26.45 27.89 Coffee-martini 22.47 22.68 23.23 24.19 5.3 Ablation Study For ablation studies, we chose one representative scene from each dataset based on the level of difficulty: the relative easy “martini” from Neu3D and the hard “jumping” from Nvidia-long. We validate the effectiveness of velocity rendering, Lwarp, andLwinin reconstructing dynamic scenes and report the results in Table 3. Ablation on Sliding Window Size. We study the impact of the sliding window size in Lwinon the reconstruction performance, and present the results in Table 4. We observe that a larger window size results in stronger temporal consistency a"}
{"id": "2507.23704v1_chunk52", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "tudy the impact of the sliding window size in Lwinon the reconstruction performance, and present the results in Table 4. We observe that a larger window size results in stronger temporal consistency and thus incurs more accurate velocity estimation of moving objects. Such benefits are attributed to the temporal window overlapping between adjacent iterations, bringing stability to the training process. Loss Map Visualization. We visualize the loss maps of the L1photometric loss and flow-based losses for a rendered frame in Fig. 6. We note that L1does not emphasize dynamic regions and therefore misleads conventional densification algorithms to prioritize static backgrounds, potentially resulting in inaccurate reconstructions of dynamic elements. On the other hand, our LwinandLwarp generally yield larger loss values for dynamic pixels, making them more suitable for guiding the densification steps in dynamic scenes. By additionally applying a dynamic mask to filter out irrelevant losses fr"}
{"id": "2507.23704v1_chunk53", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "yield larger loss values for dynamic pixels, making them more suitable for guiding the densification steps in dynamic scenes. By additionally applying a dynamic mask to filter out irrelevant losses from the background, we further ensure that all Gaussian candidates in FAD correspond to foreground objects. Evaluation of TVR. By applying the temporal velocity refinement (TVR) with EKF to the “Balloon1” scene from Nvidia-long, we observe that the PSNR on novel views decreases from 24.50 to 24.32 (-0.18 change) and the DPSNR decreases from 24.54 to 24.17 (-0.37 change). While TVR induces a slight deterioration in render quality, we yet observe that the accuracy of Gaussians’ offsets (motion), and subsequently the modeling of scene dynamics, has notably improved, as shown in Fig. 7. We argue that the drop in visual quality is due to TVR not being able to optimize Gaussian attributes other than the center coordinates. 6 Conclusion and Analysis In this work, we present an enhanced flow-based "}
{"id": "2507.23704v1_chunk54", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "at the drop in visual quality is due to TVR not being able to optimize Gaussian attributes other than the center coordinates. 6 Conclusion and Analysis In this work, we present an enhanced flow-based velocity field modeling technique to improve Gaussian deformation for video reconstruction. By expanding the capabilities of the 3DGS rasterizer to accommodate velocity field rendering, we introduce a velocity attribute to each Gaussian and incorporate multiple constraints — Lwin,Lwarp. andLdyn— to ensure temporal consistency and smoothness in both RGB rendering and trajectory predictions. Additionally, we develop FAD, a method that actively adds Gaussians in challenging dynamic regions. Extensive experiments conducted on challenging datasets illustrate that our model demonstrates improved alignment with real motion and reduced motion blur, leading to clearer textures for smoother motions. We believe our approach holds significant potential for achieving more accurate and temporally consis"}
{"id": "2507.23704v1_chunk55", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "gnment with real motion and reduced motion blur, leading to clearer textures for smoother motions. We believe our approach holds significant potential for achieving more accurate and temporally consistent dynamic scene reconstruction in applications such as VR/AR, gaming, and cinematic content creation. Limitations and future work. Our exploration of temporal velocity refinement using EKF is still in its infant stages, yet it has shown promise in refining Gaussian center trajectories alongside other attributes, and is worth further research and experimentation. The current model encounters challenges in accurately predicting motion trajectories under complex and rapid motion, particularly in scenarios involving abrupt object changes (e.g., abrupt appearance or size variations), for example, the “Jumping” scene in Nvidia-long. As shown in Fig. 8, the visual quality and PSNR of reconstruction decrease for later timestamps, when persons in the scene move closer and existing Gaussians are "}
{"id": "2507.23704v1_chunk56", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": ", the “Jumping” scene in Nvidia-long. As shown in Fig. 8, the visual quality and PSNR of reconstruction decrease for later timestamps, when persons in the scene move closer and existing Gaussians are no longer sufficient to fully represent their fine details. Future improvements in dynamic Gaussian control and adaptive optimization strategies could possibly address these issues. Additionally, maintaining Gaussian attribute consistency across views during significant camera perspective changes (e.g., 360-degree rotations [ 65]) remains a challenge. Lastly, the sliding window strategy for loss computation leads to increased training time. Addressing these aspects presents opportunities for future improvements in velocity-based modeling. References [1]Navid Ashrafi, Francesco V ona, Carina Ringsdorf, Christian Hertel, Luca Toni, Sarina Kailer, Alice Bartels, Tanja Kojic, and Jan-Niklas V oigt-Antons. Enhancing job interview preparation through immersive experiences using 13Enhanced Veloci"}
{"id": "2507.23704v1_chunk57", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ngsdorf, Christian Hertel, Luca Toni, Sarina Kailer, Alice Bartels, Tanja Kojic, and Jan-Niklas V oigt-Antons. Enhancing job interview preparation through immersive experiences using 13Enhanced Velocity Field Modeling for Gaussian Video Reconstruction photorealistic, ai-powered metahuman avatars. In 2024 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct) , pages 345–346. IEEE, 2024. [2]Shawheen Alipour, Audrey Rah, Bishnu Karki, Justin Burris, Leslie Coward, and Tony Liao. Enhancing physics education: Designing customized virtual reality for teaching crystalline structures. In 2024 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct) , pages 549–552. IEEE, 2024. [3]Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, et al. Vr-gs: A physical dynamics-aware interactive gaussian splatting system in virtual reality. In ACM SIGGRAPH 2024 Conference Papers , pages 1–1, 2"}
{"id": "2507.23704v1_chunk58", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, et al. Vr-gs: A physical dynamics-aware interactive gaussian splatting system in virtual reality. In ACM SIGGRAPH 2024 Conference Papers , pages 1–1, 2024. [4]Weiliang Chen, Fangfu Liu, Diankun Wu, Haowen Sun, Haixu Song, and Yueqi Duan. Dreamcinema: Cinematic transfer with free camera and 3d character. arXiv preprint arXiv:2408.12601 , 2024. [5]Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 21634–21643, 2024. [6]Xiaokun Pan, Gan Huang, Ziyang Zhang, Jinyu Li, Hujun Bao, and Guofeng Zhang. Robust collaborative visual-inertial slam for mobile augmented reality. IEEE Transactions on Visualization and Computer Graphics , 2024. [7]Zhiming Hu, Zheming Yin, Daniel Haeufle, Syn Schmitt, and Andreas Bulling. Hoimotion: F"}
{"id": "2507.23704v1_chunk59", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "-inertial slam for mobile augmented reality. IEEE Transactions on Visualization and Computer Graphics , 2024. [7]Zhiming Hu, Zheming Yin, Daniel Haeufle, Syn Schmitt, and Andreas Bulling. Hoimotion: Forecasting human motion during human-object interactions using egocentric 3d object bounding boxes. IEEE Transactions on Visualization and Computer Graphics , 2024. [8]Mine Dastan, Michele Fiorentino, Elias D Walter, Christian Diegritz, Antonio E Uva, Ulrich Eck, and Nassir Navab. Co-designing dynamic mixed reality drill positioning widgets: A collaborative approach with dentists in a realistic setup. IEEE Transactions on Visualization and Computer Graphics , 2024. [9]Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph. , 42(4):139–1, 2023. [10] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radian"}
{"id": "2507.23704v1_chunk60", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "endering. ACM Trans. Graph. , 42(4):139–1, 2023. [10] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV , 2020. [11] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713 , 2023. [12] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8508–8520, 2024. [13] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20310–20320, 2024. [14] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xi"}
{"id": "2507.23704v1_chunk61", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "or real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20310–20320, 2024. [14] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 20310–20320, June 2024. [15] Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, and Ulrich Neumann. Gaussianflow: Splatting gaussian dynamics for 4d content creation. arXiv preprint arXiv:2403.12365 , 2024. [16] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from a single video. arXiv preprint arXiv:2407.13764 , 2024. [17] Yanjun Huang, Jiatong Du, Ziru Yang, Zewei Zhou, Lin Zhang, and Hong Chen. A survey on trajectory-prediction methods for autonomous driving. I"}
{"id": "2507.23704v1_chunk62", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ingle video. arXiv preprint arXiv:2407.13764 , 2024. [17] Yanjun Huang, Jiatong Du, Ziru Yang, Zewei Zhou, Lin Zhang, and Hong Chen. A survey on trajectory-prediction methods for autonomous driving. IEEE Transactions on Intelligent Vehicles , 7(3):652–674, 2022. [18] Zihan Zhu, Songyou Peng, Viktor Larsson, Zhaopeng Cui, Martin R Oswald, Andreas Geiger, and Marc Pollefeys. Nicer-slam: Neural implicit scene encoding for rgb slam. arXiv preprint arXiv:2302.03594 , 2023. [19] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16 , pages 402–419. Springer, 2020. [20] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. "}
{"id": "2507.23704v1_chunk63", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "a, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 , 2024. 14Enhanced Velocity Field Modeling for Gaussian Video Reconstruction [21] Keisuke Fujii. Extended kalman filter. Refernce Manual , 14:41, 2013. [22] Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz. Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5336–5345, 2020. [23] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. InProceedings of the IEEE/CVF Conference on Computer Vision and Patt"}
{"id": "2507.23704v1_chunk64", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "l Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5521–5531, 2022. [24] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. In International Conference on Learning Representations (ICLR) , 2024. [25] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4220–4230, 2024. [26] Chaoyang Wang, Xueqian Li, Jhony Kaesemodel Pontes, and Simon Lucey. Neural prior for trajectory estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6532–6542, 2022. [27] Aljaz Bozic, Michael Zollhofer, Christ"}
{"id": "2507.23704v1_chunk65", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": " Lucey. Neural prior for trajectory estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6532–6542, 2022. [27] Aljaz Bozic, Michael Zollhofer, Christian Theobalt, and Matthias Nießner. Deepdeform: Learning non-rigid rgb-d reconstruction with semi-supervised data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7002–7012, 2020. [28] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems , 32, 2019. [29] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip Davidson, Sean Ryan Fanello, Adarsh Kowdle, Sergio Orts Escolano, Christoph Rhemann, David Kim, Jonathan Taylor, et al. Fusion4d: Real-time performance capture of challenging scenes. ACM Transactions on Graphics (ToG) , 35(4):1–13, 2016. [30] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video dep"}
{"id": "2507.23704v1_chunk66", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": ". Fusion4d: Real-time performance capture of challenging scenes. ACM Transactions on Graphics (ToG) , 35(4):1–13, 2016. [30] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1611–1621, 2021. [31] Zhoutong Zhang, Forrester Cole, Richard Tucker, William T Freeman, and Tali Dekel. Consistent depth of moving objects in video. ACM Transactions on Graphics (ToG) , 40(4):1–12, 2021. [32] Seoha Kim, Jeongmin Bae, Youngsik Yun, Hahyun Lee, Gun Bang, and Youngjung Uh. Sync-nerf: Generalizing dynamic nerfs to unsynchronized videos. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 2777–2785, 2024. [33] Guangcong, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for few-shot novel view synthesis. IEEE/CVF International Conference on Computer Vision (ICCV) , 2023. [34] Fabio Tosi, Alessio Tonioni, Dan"}
{"id": "2507.23704v1_chunk67", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "ge Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for few-shot novel view synthesis. IEEE/CVF International Conference on Computer Vision (ICCV) , 2023. [34] Fabio Tosi, Alessio Tonioni, Daniele De Gregorio, and Matteo Poggi. Nerf-supervised deep stereo. In Conference on Computer Vision and Pattern Recognition (CVPR) , pages 855–866, June 2023. [35] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenenbaum, and Jiajun Wu. Neural radiance flow for 4d view synthesis and video processing. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pages 14304–14314. IEEE Computer Society, 2021. [36] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner, and Christian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 12959–12970, 2021. [37] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, an"}
{"id": "2507.23704v1_chunk68", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "nthesis of a dynamic scene from monocular video. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 12959–12970, 2021. [37] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free- viewpoint video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 9421–9431, 2021. [38] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10318–10327, 2021. [39] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 5865–5874, 2021. [40] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Gold"}
{"id": "2507.23704v1_chunk69", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "elds. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 5865–5874, 2021. [40] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228 , 2021. 15Enhanced Velocity Field Modeling for Gaussian Video Reconstruction [41] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 5712–5721, 2021. [42] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6498–6508, 2021. [43] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. D"}
{"id": "2507.23704v1_chunk70", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "cenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6498–6508, 2021. [43] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4273–4284, 2023. [44] Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, and Bo Dai. Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians. arXiv preprint arXiv:2403.17898 , 2024. [45] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 19447–19456, 2024. [46] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF C"}
{"id": "2507.23704v1_chunk71", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "4. [46] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20331–20341, 2024. [47] Ang Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 130–141, 2023. [48] Alvaro Cuno, Claudio Esperança, Antonio Oliveira, and Paulo Roma Cavalcanti. 3d as-rigid-as-possible deforma- tions using mls. In Proceedings of the 27th computer graphics international conference , pages 115–122. Citeseer, 2007. [49] Zhiyang Guo, Wengang Zhou, Li Li, Min Wang, and Houqiang Li. Motion-aware 3d gaussian splatting for efficient dynamic scene reconstruction. arXiv preprint arXiv:2403.11447 , 2024. [50] Ruijie Zhu, Yanzhe Liang, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei Yang, Tianzhu Zhang, and Yong"}
{"id": "2507.23704v1_chunk72", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "platting for efficient dynamic scene reconstruction. arXiv preprint arXiv:2403.11447 , 2024. [50] Ruijie Zhu, Yanzhe Liang, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei Yang, Tianzhu Zhang, and Yongdong Zhang. Motiongs: Exploring explicit motion guidance for deformable 3d gaussian splatting. Advances in Neural Information Processing Systems , 37:101790–101817, 2024. [51] Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. Representing long volumetric video with temporal gaussian hierarchy. ACM Transactions on Graphics (TOG) , 43(6):1–18, 2024. [52] Shizun Wang, Xingyi Yang, Qiuhong Shen, Zhenxiang Jiang, and Xinchao Wang. Gflow: Recovering 4d world from monocular video. arXiv preprint arXiv:2405.18426 , 2024. [53] Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lv, Peng Wang, Wenping Wang, and Junhui Hou. Modgs: Dynamic gaussian splatting from causually-captured monocular videos. arXiv preprint arXiv:2406.00434 , 2024. [54] Xiao Fu, Wei Yin, Mu Hu, K"}
{"id": "2507.23704v1_chunk73", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "anqiang Lv, Peng Wang, Wenping Wang, and Junhui Hou. Modgs: Dynamic gaussian splatting from causually-captured monocular videos. arXiv preprint arXiv:2406.00434 , 2024. [54] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from a single image. In European Conference on Computer Vision , pages 241–258. Springer, 2025. [55] Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandström, Stefano Mattoccia, Martin R Oswald, and Matteo Poggi. How nerfs and 3d gaussian splatting are reshaping slam: a survey. arXiv preprint arXiv:2402.13255 , 4, 2024. [56] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4104–4113, 2016. [57] Yuval Eldar, Michael Lindenbaum, Moshe Porat, and Yehoshua Y Zeevi. The farthest point strategy for progressive image sampling. IEEE transa"}
{"id": "2507.23704v1_chunk74", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "vision and pattern recognition , pages 4104–4113, 2016. [57] Yuval Eldar, Michael Lindenbaum, Moshe Porat, and Yehoshua Y Zeevi. The farthest point strategy for progressive image sampling. IEEE transactions on image processing , 6(9):1305–1315, 1997. [58] Itai Lang, Asaf Manor, and Shai Avidan. Samplenet: Differentiable point cloud sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7578–7588, 2020. [59] Eric A Wan and Rudolph Van Der Merwe. The unscented kalman filter for nonlinear estimation. In Proceedings of the IEEE 2000 adaptive systems for signal processing, communications, and control symposium (Cat. No. 00EX373) , pages 153–158. Ieee, 2000. [60] Yifan Zhan, Zhuoxiao Li, Muyao Niu, Zhihang Zhong, Shohei Nobuhara, Ko Nishino, and Yinqiang Zheng. Kfd-nerf: Rethinking dynamic nerf with kalman filter. arXiv preprint arXiv:2407.13185 , 2024. [61] Gabriel A Terejanu et al. Extended kalman filter tutorial. University at Buffalo , 27, "}
{"id": "2507.23704v1_chunk75", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "nqiang Zheng. Kfd-nerf: Rethinking dynamic nerf with kalman filter. arXiv preprint arXiv:2407.13185 , 2024. [61] Gabriel A Terejanu et al. Extended kalman filter tutorial. University at Buffalo , 27, 2008. [62] Chang Ho Kang, Chan Gook Park, and Jin Woo Song. An adaptive complementary kalman filter using fuzzy logic for a hybrid head tracker system. IEEE Transactions on Instrumentation and Measurement , 65(9):2163–2173, 2016. 16Enhanced Velocity Field Modeling for Gaussian Video Reconstruction [63] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics , 42(4), July 2023. [64] Johannes L Schönberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14 , pages 501–518. Springer, 2016. [65"}
{"id": "2507.23704v1_chunk76", "paper_id": "2507.23704v1", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "text": "r unstructured multi-view stereo. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14 , pages 501–518. Springer, 2016. [65] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin Gui, Sean Banerjee, Timothy Scott Godisart, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: A massively multiview system for social interaction capture. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2017. 17"}
{"id": "2507.23701v1_chunk0", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "TEXTQUESTS : HOWGOOD ARE LLM S AT TEXT-BASED VIDEO GAMES ? Long Phan1Mantas Mazeika1Andy Zou1,2,3Dan Hendrycks1 1Center for AI Safety2Carnegie Mellon University3Gray Swan AI ABSTRACT Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent’s ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TEXTQUESTS , a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on "}
{"id": "2507.23701v1_chunk1", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "active fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agent’s capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release T EXTQUESTS at textquests.ai. 1 Introduction The rapid advancement of Large Language Models (LLMs) has enabled remarkable progress on established academic benchmarks. As academic benchmarks (Hendrycks et al., 2021a,b; Rein et al., 2023) are largely saturated, and frontier models are making significant progress on expert evaluations like HLE (Phan et al., 2025), it is clear that these models pos"}
{"id": "2507.23701v1_chunk2", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "cks et al., 2021a,b; Rein et al., 2023) are largely saturated, and frontier models are making significant progress on expert evaluations like HLE (Phan et al., 2025), it is clear that these models possess the foundational knowledge required to power sophisticated AI agent systems. However, this success in static, knowledge-based tasks does not always translate to effectiveness in dynamic, interactive settings. The development of robust methodologies for evaluating LLMs as autonomous agents, in environments where success demands long-term, adaptive strategies, remains a significant challenge. Current AI agent evaluation frameworks typically prioritize specific skills, such as proficiency in utilizing external tools (Wei et al., 2025; Yao et al., 2024; Mialon et al., 2023), coding-oriented tasks (Jimenez et al., 2024; Starace et al., 2025; Chan et al., 2025), or few-turn conversational interactions (Sirdeshmukh et al., 2025; He et al., 2024). While these benchmarks are effective for thei"}
{"id": "2507.23701v1_chunk3", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "asks (Jimenez et al., 2024; Starace et al., 2025; Chan et al., 2025), or few-turn conversational interactions (Sirdeshmukh et al., 2025; He et al., 2024). While these benchmarks are effective for their stated purpose, they fall short of assessing an agent’s ability to navigate exploratory environments that require sustained, self-directed, long-context reasoning. More recently, demonstrations of agents playing games like Pokémon with Claude (Anthropic, 2025) and Gemini (Gemini Team, 2025) have showcased evaluations of long-horizon reasoning AI agents. However, these gameplay sessions often depend on extensive, task-specific scaffolding, such as history summarization mechanisms, pathfinding tools, or external notepads. This heavy reliance on engineered components makes it difficult to disentangle the base model’s intrinsic capabilities from the contributions of the external scaffolding, hindering fair and direct comparisons across different systems. To enable a more accurate assessment "}
{"id": "2507.23701v1_chunk4", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "tangle the base model’s intrinsic capabilities from the contributions of the external scaffolding, hindering fair and direct comparisons across different systems. To enable a more accurate assessment of AI agents in challenging exploratory environments, we introduce TEXTQUESTS , a benchmark built upon 25 classic Infocom interactive fiction games. These once-popular text-based worlds, which can take human players over 30 hours and require hundreds of precise actions to solve (Smetale, 1983), provide a compelling testbed for the very challenges we have outlined. They demand that an agent: (1) reason over a long and continuously growing history of its actions and observations, (2) learn from experience through trial-and-error, and (3) devise and execute multi-step plans in a self-contained manner, relying solely on its internal reasoning without the aid of external tools. Success in these games requires an agent to build understanding over a long gameplay session, interrogate its own fail"}
{"id": "2507.23701v1_chunk5", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "nner, relying solely on its internal reasoning without the aid of external tools. Success in these games requires an agent to build understanding over a long gameplay session, interrogate its own failures, and make incremental improvements as it explores. This allows for a more direct and accurate assessment of the LLM itself as the reasoning backbone of an AI agent system.arXiv:2507.23701v1 [cs.AI] 31 Jul 2025 Hiding in the office is the best way to observe Monica's actions and collect more evidences.🕰 10:36PM Monica has arrived home. I must be patient and wait.🕰 11:05PM Monica has performed her 'discovery' of the body. I must remain hidden and wait for more evidences I have just witnessed Monica removing the murder weapon. This is the moment I've been waiting for.🕰 11:14PM 🕰 11:54PM4h+ Game Progress:Witness (1983) 6h+ Game Progress:Seastalker (1984) You're now diving at a speed of 5 meters per sea square. (+=you, @=SHIP, *=obstacle) LIGHT:YELLO . . . . . . . . . . . . . . . . @ . . ."}
{"id": "2507.23701v1_chunk6", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "ame Progress:Witness (1983) 6h+ Game Progress:Seastalker (1984) You're now diving at a speed of 5 meters per sea square. (+=you, @=SHIP, *=obstacle) LIGHT:YELLO . . . . . . . . . . . . . . . . @ . . . . . . . . . . . . . . . . . . . . . . . . . . . HDG:N . . . . . . . * . . . ! . . . . . + . * * . . + . . . . . . . . * * . ---------------------- A ship is ahead (marked as @ on the sonarscope) with a collision warning. My current depth is 5m, but its keel is 10m deep, so I need to dive to 15m to pass safely underneath. Given my descent rate of 5m per square and the ship's distance of 3 squares, I can reach the target depth in time. set depth to 15 metersSONARSCOPE: 16h+ Game Progress:The Hitchhiker's Guide to the Galaxy (1984) The sense of 'smell' is missing from the description. I now need to use that sense to proceed.You are in...Dark You can't hear anything, see anything, feel anything, or taste anything, and do not even know where you are or who you are or how you got there.🌑4h+ Gam"}
{"id": "2507.23701v1_chunk7", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": " to use that sense to proceed.You are in...Dark You can't hear anything, see anything, feel anything, or taste anything, and do not even know where you are or who you are or how you got there.🌑4h+ Game Progress:Enchanter (1983) I'm tired, and I've quenched my thirst. Before I sleep, I should use the BLORB spell to protect valuable possessions. I will choose to do it on my spell book. blorb bookBedroom: There is a fourposter feather bed here. Game Progress:Sherlock (1987) This driver is friendly but stupid and won't take me where I want to go. However, I should still try. I need to get to Westminster Abbey to search more thoroughly for the paper. You and Holmes get into the cab. \"Where to, guv'nor?” The growler cab has taken me to The Mall, which is at least in the general Westminster area. I should get out of the cab and navigate to Westminster Abbey from here...[After a wild ride through the streets of London].. >smell>Westminster Abbey >exit cab20h+Zork I (1980) 12h+ Game Progress: C"}
{"id": "2507.23701v1_chunk8", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": " I should get out of the cab and navigate to Westminster Abbey from here...[After a wild ride through the streets of London].. >smell>Westminster Abbey >exit cab20h+Zork I (1980) 12h+ Game Progress: Chimney You can't get up there with what you're carrying. To fit through the narrow chimney, I can only carry the lantern plus one item. I’ll take the bag of coins up first and come back later. >drop painting. drop sword. drop key. climb chimney \"Here we are! The Mall.” It does smell a bit. Your head begins to clear. You can make out a shadow moving in the dark. I've smelled something and can now see a shadow. >examine shadow🌑 hide behind lounge wait wait handcuff monica Figure 1: Examples showing the diverse reasoning challenges in TEXTQUESTS . denotes LLM thinking. denotes the action. 2NOCLUES WITHCLUES Progress (%) ↑Harm ↓Progress (%) ↑Harm ↓ Grok 4 31.2 30.4 61.4 31.4 o3 30.9 18.7 60.4 17.2 Claude Opus 4 26.4 16.5 60.5 19.2 Gemini 2.5 Pro 23.2 15.9 60.6 25.6 Claude Sonnet 4 24.7 16.0 57"}
{"id": "2507.23701v1_chunk9", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "S WITHCLUES Progress (%) ↑Harm ↓Progress (%) ↑Harm ↓ Grok 4 31.2 30.4 61.4 31.4 o3 30.9 18.7 60.4 17.2 Claude Opus 4 26.4 16.5 60.5 19.2 Gemini 2.5 Pro 23.2 15.9 60.6 25.6 Claude Sonnet 4 24.7 16.0 57.2 18.4 GPT-4.1 22.8 11.4 37.5 15.3 Grok 3 mini 22.4 17.8 32.2 18.2 Qwen 3 Thinking 15.1 16.4 29.8 10.8 Gemini 2.5 Flash 14.4 11.7 31.8 16.8 DeepSeek R1 15.2 15.4 23.8 23.0 Kimi K2 10.5 8.3 19.7 9.0 GPT-4.1-mini 10.6 11.7 15.9 12.2 Table 1: LLMs performance on T EXTQUESTS . For complete results and more models, see Table 3. 2 T EXTQUESTS TEXTQUESTS is a benchmark consisting of 25 classic interactive fiction games of varying difficulty (a full list is available in Appendix A.1). These games were developed by Infocom, the preeminent company that pioneered the genre in the 1980s, challenging players to interact with a story-rich world using natural language commands. Our benchmark is built upon the game collections and annotations from Hendrycks et al. (2021c). We extend this foundational wor"}
{"id": "2507.23701v1_chunk10", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "ers to interact with a story-rich world using natural language commands. Our benchmark is built upon the game collections and annotations from Hendrycks et al. (2021c). We extend this foundational work by introducing several enhancements tailored for LLM-based agent evaluation: additional context for clues and guidelines, an autosave/restore mechanism, and a new game progress metric. Clues. We provide a clue-assisted evaluation mode, WITHCLUES , where agents are given the complete set of official \"InvisiClues\" hint booklets directly in their context window. Crucially, these clues do not provide a direct walkthrough of the game. Instead, they consist of tiered, often cryptic hints that an agent must learn to interpret and apply to its current game state, mirroring the challenge human players faced. This setup tests an agent’s ability to reason over long, structured documents and integrate relevant information to solve complex problems. We compare performance in this mode against a N OCL"}
{"id": "2507.23701v1_chunk11", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "s faced. This setup tests an agent’s ability to reason over long, structured documents and integrate relevant information to solve complex problems. We compare performance in this mode against a N OCLUES setting in Table 1, with examples of clues available in Appendix A.1.1. Autosave. To mimic a common human gameplay strategy, we implement an Autosave mechanism in the game environments. At every step an agent takes, the game state is automatically saved. This provides the agent with the ability to freely restore or backtrack to any previous point in the session. This feature mimics the common strategy employed by human players, who regularly save their progress to avoid restarting the entire game upon dying, getting stuck without making progress, or simply to experiment with different puzzle-solving strategies without permanent consequences. We saw a notable improvement in the model’s gameplay when it had access to this autosave and restore feature (more details in Appendix A.1.2). Gam"}
{"id": "2507.23701v1_chunk12", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "le-solving strategies without permanent consequences. We saw a notable improvement in the model’s gameplay when it had access to this autosave and restore feature (more details in Appendix A.1.2). Game Progress. Previous work in text-based game evaluation has often relied on the games’ built-in scoring systems as the primary metric (Hausknecht et al., 2020; Yao et al., 2020). However, these point systems are a weak proxy for actual advancement, as they were often designed to reward exploration or enhance replayability rather than to track progress on the main storyline (for example, in The Witness , as many as 30 different endings are possible). To address these limitations, we introduce a new Game Progress metric based on labeled checkpoints for essential puzzles and game milestones. A visual comparison in Appendix E demonstrates the shortcomings of the original scores and shows how our metric provides a more representative signal of completion. The formal implementation of this metri"}
{"id": "2507.23701v1_chunk13", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "sual comparison in Appendix E demonstrates the shortcomings of the original scores and shows how our metric provides a more representative signal of completion. The formal implementation of this metric is detailed in Section 3.2. 30 100 200 300 400 500 Steps020406080100Average Game Progress (%)Game Progress with Clues Human Optimal Grok 4 Grok 3 Gemini 2.5 Pro Gemini 2.5 FlashFigure 2: Game progress for various models versus an optimal human walkthrough. Capable models sustain progress longer, suggesting better long-horizon reasoning. 3 Evaluation 3.1 Evaluation Setting The evaluation proceeds in a sequence of discrete turns. At each turn, the agent receives the latest observation from the environment, which is appended to the complete history of all previous observations, reasonings, and actions from the current game session. This full, multi-turn history is then provided as input to the model. The model’s task is to generate a brief reasoning for its strategy, followed by a single, e"}
{"id": "2507.23701v1_chunk14", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "actions from the current game session. This full, multi-turn history is then provided as input to the model. The model’s task is to generate a brief reasoning for its strategy, followed by a single, executable command. Further details on the interaction protocol and system prompt are available in Appendix C. For each model, we conduct two distinct evaluation runs: one with access to the game’s official clues ( WITHCLUES ) and one without ( NOCLUES ). Each run is executed for a maximum of 500 steps and stops early if the agent successfully completes the game. To handle the growing context, the full game history is maintained without truncation throughout the run. This long-context evaluation is computationally feasible due to the prompt caching inherent in modern LLM inference frameworks. A detailed token analysis is provided in Table 4. 3.2 Evaluation Metrics Game Progress. TheGame Progress metric is calculated based on a series of labeled checkpoints representing necessary objectives "}
{"id": "2507.23701v1_chunk15", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": " detailed token analysis is provided in Table 4. 3.2 Evaluation Metrics Game Progress. TheGame Progress metric is calculated based on a series of labeled checkpoints representing necessary objectives on the path to finishing a game. For a single game g, its progress score ( Pg) is defined as the maximum percentage of checkpoints the agent completes at any point during its trajectory. The final score for the benchmark is the average of these individual game scores across all Ngames in the suite: Average Game Progress =1 NNX g=1Pg Harm. To evaluate the ethical dimensions of agent behavior, we adopt a metric based on the annotation framework from Hendrycks et al. (2021c), which annotates in-game actions based on their moral implications. This framework assigns a harm score to actions considered harmful to some degree (pro tanto harm). For a given game g, the total Harm score ( Hg) is the sum of the degrees of all such actions taken by the agent. To provide a comprehensive measure of harmf"}
{"id": "2507.23701v1_chunk16", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "ed harmful to some degree (pro tanto harm). For a given game g, the total Harm score ( Hg) is the sum of the degrees of all such actions taken by the agent. To provide a comprehensive measure of harmful behavior across different games, we compute the Average Harm score by taking the mean of total harm scores across all Ngames: Average Harm =1 NNX g=1Hg As this cumulative score is dependent on the number of steps in an episode, it is most suitable for evaluations with a fixed trajectory length, as used in our work. For comparing agents across episodes of varying lengths, we recommend normalizing this score by the number of steps to calculate a rate of harm. 4Zork I (1980) I need to drop something temporarily. The matchbook is a backup light source and I have my lantern. drop match bookYour load is too heavy to pick up the crystal trident Actually, I just realized from the hint guide that there are alternative light sources like matches (matchbook). ❌ I left a matchbook in the Studio Roo"}
{"id": "2507.23701v1_chunk17", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "load is too heavy to pick up the crystal trident Actually, I just realized from the hint guide that there are alternative light sources like matches (matchbook). ❌ I left a matchbook in the Studio Room. I will navigate back. southThe lantern is nearly out. You'd better have more light.....[200 steps later]....Step 223 - Atlantis Room 🔱 🚪 Step 422 - CellarWishbringer(1985) I need to navigate my way down the cliff. I'll try the reverse of the sequence that got me up the cliff. go eastYou're lost in a thick cloud of fog East is a dead end. I'll try north. go northYou'd plummet off the cliff if you went that way. This maze is clearly too dangerous to navigate without assistance. I need an umbrella to use the 'Rain' wish. I must have missed it in Festeron. I will restore to a point before I went up the trail restore 72Oh, no! You lost your footing and tumbled off the cliff! ❌ Figure 4: Examples of long context reasoning failures in TEXTQUESTS .Left: InZork I , tested LLMs failed to correctl"}
{"id": "2507.23701v1_chunk18", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "t up the trail restore 72Oh, no! You lost your footing and tumbled off the cliff! ❌ Figure 4: Examples of long context reasoning failures in TEXTQUESTS .Left: InZork I , tested LLMs failed to correctly recall information from its history, hallucinating that it dropped a matchbook in the Studio instead of the Atlantis Room . Right: InWishbringer , LLMs often fail to retrieve and reverse their own ascent path from in-context history to navigate down a cliff successfully. 4 Results and Analysis 4.1 Quantitative Results OpenAI Google xAI010203040506070Game Progress (%)o3 o4-mini60% 21%Gemini 2.5 Pro Gemini 2.5 Flash61% 32%Grok 4 Grok 3 Mini61% 34%Mini vs Standard Models Game Progress Figure 3: Comparing mini and standard models from differ- ent closed-source providersWe evaluate a range of closed-source and open-weight models on TEXTQUESTS in two modes: with in-game hints ( WITH CLUES ) and without ( NOCLUES ). As shown in Table 1, even state-of-the-art LLMs make min- imal progress in solv"}
{"id": "2507.23701v1_chunk19", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "sed-source and open-weight models on TEXTQUESTS in two modes: with in-game hints ( WITH CLUES ) and without ( NOCLUES ). As shown in Table 1, even state-of-the-art LLMs make min- imal progress in solving the games without assistance. In the WITH CLUES setting, while access to the full hints allows all models to make more substantial progress, most still fail to complete the majority of the games. For instance, Sonnet 4 and Grok-3 each solved two games (Witness andMoonmist ). Gemini 2.5 Pro and o3 solved these two and an additional game, Plunderedhearts . Opus 4 also solved Seastalker , bringing its total to four com- pleted games. Furthermore, the performance differences between model sizes are large (Figure 3), highlighting the importance of model scale for agentic tasks. This difficulty highlights that TEXTQUESTS is a challenging benchmark for measuring the long-horizon reasoning of LLM-based agents in exploratory environments. 4.2 Qualitative Analysis To better understand why even c"}
{"id": "2507.23701v1_chunk20", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "highlights that TEXTQUESTS is a challenging benchmark for measuring the long-horizon reasoning of LLM-based agents in exploratory environments. 4.2 Qualitative Analysis To better understand why even capable models struggle with TEXTQUESTS , we analyze their trajectories to identify common failure modes. Figure 4 illustrates common examples. Long-Context Reasoning. The game progress trajectories in Figure 2 visually represent this challenge. As shown, more capable models sustain progress for longer, suggesting improved long-context reasoning capabilities. During evaluation, the context window can exceed 100K tokens, requiring LLMs to consistently perform precise reasoning and planning over a vast history of observations and clues to effectively progress. As the context length grows, we observe that current models often hallucinate about prior interactions, such as believing they have already picked up an item when they have not or getting stuck navigating in a loop. Furthermore, similar"}
{"id": "2507.23701v1_chunk21", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "rve that current models often hallucinate about prior interactions, such as believing they have already picked up an item when they have not or getting stuck navigating in a loop. Furthermore, similar to observations in Gemini Team (2025), LLM agents show an increased tendency to repeat actions from their history rather than synthesizing novel plans as the context lengthens. These long-context failures are particularly stark in tasks requiring spatial reasoning. For instance, in Wishbringer , most LLMs struggled to navigate back down a cliff after climbing it. The solution simply required reversing the sequence of directions used to ascend—information available in the context history—indicating a fundamental difficulty in building and utilizing a mental map. 564 128 256 512 1,024 Average Output T okens per Step020406080100Average Game Progress (%)Gemini 2.5 Pro Claude Sonnet 4.0 Grok 3 Mini GPT-4.1-miniGrok 3o3-high o3-lowClaude Opus 4.0 Haiku 3.5GPT-4.1 Llama 4 Mavericko4-mini Llama 4"}
{"id": "2507.23701v1_chunk22", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "utput T okens per Step020406080100Average Game Progress (%)Gemini 2.5 Pro Claude Sonnet 4.0 Grok 3 Mini GPT-4.1-miniGrok 3o3-high o3-lowClaude Opus 4.0 Haiku 3.5GPT-4.1 Llama 4 Mavericko4-mini Llama 4 ScoutDeepSeek R1Grok 4 Gemini 2.5 Flash-LiteQwen 3Game Progress vs Output T okens per Step Reasoning Models Non-Reasoning ModelsFigure 5: A comparison of output and reasoning token efficiency across state-of-the-art LLMs on TEXTQUESTS . Since many exploratory steps are intermediate and don’t require a full reasoning budget, an ideal LLM agent should be efficient and dynamic with its reasoning effort while still maintaining consistent performance. Dynamic Thinking. An agent’s overall effectiveness is defined by both its task success and its operational efficiency. For LLM agents, efficiency is closely tied to the number of output or reasoning tokens it generates, which directly impacts inference cost and latency. Figure 5 illustrates the output tokens efficiency for evaluated LLMs relative"}
{"id": "2507.23701v1_chunk23", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": " closely tied to the number of output or reasoning tokens it generates, which directly impacts inference cost and latency. Figure 5 illustrates the output tokens efficiency for evaluated LLMs relative to their performance. Similar to observations from OpenAI (2024), models that utilize more test-time compute generally achieve higher performance on TEXTQUESTS . However, this trend starts to diminish after a certain budget. This consideration is important as many exploratory steps in TEXTQUESTS (for example, navigation steps) are intermediate and can be successfully executed without a large reasoning depth. 5 Related Work and Discussion There has been a long-standing interest in creating AI agents that can navigate and solve problems in interactive, text-based worlds, first as a way to measure language understanding and commonsense reasoning (Hausknecht et al., 2020; Yao et al., 2020; Ammanabrolu and Hausknecht, 2020). As AI capabilities increased, Hendrycks et al. (2021c) revisited thes"}
{"id": "2507.23701v1_chunk24", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "re language understanding and commonsense reasoning (Hausknecht et al., 2020; Yao et al., 2020; Ammanabrolu and Hausknecht, 2020). As AI capabilities increased, Hendrycks et al. (2021c) revisited these games as a testbed to measure harmful behaviors in AI agents, creating an evaluation that jointly measures task progress and ethical compliance through moral-value annotations. Building on these motivations, TEXTQUESTS synthesizes these two goals; we adopt the dual-metric approach of measuring both progress and harm, but we modernize the core objective to evaluate the critical contemporary challenge of long-context, iterative reasoning in LLM agents within an exploratory environment. A parallel thread of research has focused on tool-augmented agents. These benchmarks typically evaluate an agent’s ability to invoke external tools to succeed, ranging from web search (Wei et al., 2025; Mialon et al., 2023) or api calls (Yao et al., 2024) to more complex scientific and engineering workflows "}
{"id": "2507.23701v1_chunk25", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "ent’s ability to invoke external tools to succeed, ranging from web search (Wei et al., 2025; Mialon et al., 2023) or api calls (Yao et al., 2024) to more complex scientific and engineering workflows (Starace et al., 2025; Chan et al., 2025). While these benchmarks offer valuable data on an agent’s ability with external tools, they do not directly assess an LLM’s intrinsic reasoning on long-horizon tasks without scaffolding. Separately, many existing long-context benchmarks use methods like the needle-in-a-haystack (NIAH) test, which involves retrieving a specific piece of information (the “needle”) from a large body of context (the “haystack”) (Bai et al., 2024; OpenAI, 2025; Ahuja et al., 2025; Modarressi et al., 2025). While these evaluations effectively test information retrieval from a long, static context, they do not assess this skill within a dynamic context built by the agent’s own actions. TEXTQUESTS fills this gap by evaluating how well agents combine long-horizon iterative "}
{"id": "2507.23701v1_chunk26", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "a long, static context, they do not assess this skill within a dynamic context built by the agent’s own actions. TEXTQUESTS fills this gap by evaluating how well agents combine long-horizon iterative reasoning with accurate retrieval from a growing context history (Figure 4). In closing, TEXTQUESTS is an evaluation of how well models can consistently progress through a series of classic interactive fiction games that were once popular among human players. We hope that open-sourcing TEXTQUESTS helps researchers better understand and assess the current capabilities of LLM agents in challenging exploratory environments. 6References Kabir Ahuja, Melanie Sclar, and Yulia Tsvetkov. Finding flawed fictions: Evaluating complex reasoning in language models via plot hole detection, 2025. URL https://arxiv.org/abs/2504.11900 . Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces, 2020. URL https://arxiv.org/abs/2001.08837 . Ant"}
{"id": "2507.23701v1_chunk27", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "/arxiv.org/abs/2504.11900 . Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces, 2020. URL https://arxiv.org/abs/2001.08837 . Anthropic. Claude’s extended thinking. Research blog post, Anthropic, February 2025. URL https://www. anthropic.com/research/visible-extended-thinking . Published February 24, 2025. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding, 2024. URL https://arxiv.org/abs/2308.14508 . Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, and Aleksander M ˛ adry. Mle-bench: Evaluating machine learning agents on machine learning engineering, 2025. URL https://arxiv.org/abs/2410.07095 . Gemini Team. Gemini 2.5: Pushing the"}
{"id": "2507.23701v1_chunk28", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": ", Lilian Weng, and Aleksander M ˛ adry. Mle-bench: Evaluating machine learning agents on machine learning engineering, 2025. URL https://arxiv.org/abs/2410.07095 . Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next genera- tion agentic capabilities. Technical report, Google DeepMind, June 2025. URL https://storage.googleapis. com/deepmind-media/gemini/gemini_v2_5_report.pdf . Published June 17, 2025. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive fiction games: A colossal adventure, 2020. URL https://arxiv.org/abs/1909.05398 . Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, Shruti Bhosale, Chenguang Zhu, Karthik Abinav Sankararaman, Eryk Helenowski, Melanie Kambadur, Aditya Tayade, Hao Ma, Han Fang, and Sinong Wang. Multi-if: Benchmarking llms on multi-turn and multilingual instructions following, 2024. URL https://ar"}
{"id": "2507.23701v1_chunk29", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "nkararaman, Eryk Helenowski, Melanie Kambadur, Aditya Tayade, Hao Ma, Han Fang, and Sinong Wang. Multi-if: Benchmarking llms on multi-turn and multilingual instructions following, 2024. URL https://arxiv.org/abs/2410.15553 . Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021a. URL https://arxiv.org/abs/2009.03300 . Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021b. URL https://arxiv.org/ abs/2103.03874 . Dan Hendrycks, Mantas Mazeika, Andy Zou, Sahil Patel, Christine Zhu, Jesus Navarro, Dawn Song, Bo Li, and Jacob Steinhardt. What would jiminy cricket do? towards agents that behave morally. NeurIPS , 2021c. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models re"}
{"id": "2507.23701v1_chunk30", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "t do? towards agents that behave morally. NeurIPS , 2021c. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=VTF8yNQM66 . Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants, 2023. URL https://arxiv.org/abs/2311.12983 . Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, Seunghyun Yoon, and Hinrich Schütze. Nolima: Long-context evaluation beyond literal matching, 2025. URL https://arxiv.org/abs/2502. 05167 . OpenAI. Learning to reason with llms. https://openai.com/index/learning-to-reason-with-llms/ , September 2024. OpenAI. Introducing GPT-4.1 in the api. https://openai.com/index/gpt-4-1/ , April 2025. Long Phan, Alice Gatti, Ziwen Han, "}
{"id": "2507.23701v1_chunk31", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "ms. https://openai.com/index/learning-to-reason-with-llms/ , September 2024. OpenAI. Introducing GPT-4.1 in the api. https://openai.com/index/gpt-4-1/ , April 2025. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Dmitry Dodonov, Tung Nguyen, Jaeho Lee, Daron Anderson, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, John-Clark Levin, Mstyslav Kazakov, Fiona Feng, Steven Y . Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Serguei Popov, Robert Gerbicz, Geoff Galgon, Johannes Schmitt, Will Yeadon, Yongki Lee, Scott Sauers, Alvaro Sanchez, Fabian Giska, Marc Roth, Søren Riis, Saiteja Utpala, Noah Burns, Gashaw M. Goshu, Mohinder Maheshbhai Naiya, Chidozie Agu, Zachary Giboney, Antrell Cheatom, Francesco"}
{"id": "2507.23701v1_chunk32", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "ee, Scott Sauers, Alvaro Sanchez, Fabian Giska, Marc Roth, Søren Riis, Saiteja Utpala, Noah Burns, Gashaw M. Goshu, Mohinder Maheshbhai Naiya, Chidozie Agu, Zachary Giboney, Antrell Cheatom, Francesco Fournier-Facio, Sarah-Jane Crowson, Lennart Finke, Zerui Cheng, Jennifer Zampese, Ryan G. Hoerr, Mark Nandor, Hyunwoo Park, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Alexis C Garretson, Edwin Taylor, Damien Sileo, Qiuyu Ren, Usman Qazi, Lianghui Li, Jungbae Nam, John B. Wydallis, Pavel Arkhipov, Jack Wei Lun Shi, Aras Bacho, Chris G. Willcocks, Hangrui Cao, Sumeet Motwani, Emily de Oliveira Santos, Johannes Veith, Edward Vendrow, Doru Cojoc, Kengo Zenitani, Joshua Robinson, Longke Tang, Yuqi Li, Joshua Vendrow, Natanael Wildner Fraga, Vladyslav Kuchkin, 7Andrey Pupasov Maksimov, Pierre Marion, Denis Efremov, Jayson Lynch, Kaiqu Liang, Aleksandar Mikov, Andrew Gritsevskiy, Julien Guillod, Gözdenur Demir, Dakotah Martinez, Ben Pageler, Kevin Zhou, Saeed Soori, Ori Press, Henry Tang, Paolo Riss"}
{"id": "2507.23701v1_chunk33", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "enis Efremov, Jayson Lynch, Kaiqu Liang, Aleksandar Mikov, Andrew Gritsevskiy, Julien Guillod, Gözdenur Demir, Dakotah Martinez, Ben Pageler, Kevin Zhou, Saeed Soori, Ori Press, Henry Tang, Paolo Rissone, Sean R. Green, Lina Brüssel, Moon Twayana, Aymeric Dieuleveut, Joseph Marvin Imperial, Ameya Prabhu, Jinzhou Yang, Nick Crispino, Arun Rao, Dimitri Zvonkine, Gabriel Loiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Tad Hogg, Carlo Bosio, Brian P Coppola, Julian Salazar, Jaehyeok Jin, Rafael Sayous, Stefan Ivanov, Philippe Schwaller, Shaipranesh Senthilkuma, Andres M Bran, Andres Algaba, Kelsey Van den Houte, Lynn Van Der Sypt, Brecht Verbeken, David Noever, Alexei Kopylov, Benjamin Myklebust, Bikun Li, Lisa Schut, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Tong Yang, John Maar, Julian Wykowski, Martí Oller, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Ariel Ghislain Kemogne Kamdoum, Alvin Jin, Tobias Garcia Vilchis, Yuex"}
{"id": "2507.23701v1_chunk34", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "an, Derek Lim, Richard Stanley, Tong Yang, John Maar, Julian Wykowski, Martí Oller, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Ariel Ghislain Kemogne Kamdoum, Alvin Jin, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner, James Koppel, Gongbo Sun, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene, Joseph M Cavanagh, Daofeng Li, Jiawei Shen, Donato Crisostomi, Wenjin Zhang, Ali Dehghan, Sergey Ivanov, David Perrella, Nurdin Kaparov, Allen Zang, Ilia Sucholutsky, Arina Kharlamova, Daniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster, Daniel Munro, Linh Ho, Shankar Sivarajan, Dan Bar Hava, Aleksey Kuchkin, David Holmes, Alexandra Rodriguez-Romero, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider, Zakayo Kazibwe, Don Clarke, Dae Hyun Kim, Felipe Meneguitti Dias, Sara Fish, Veit Elser, Tobias Kreiman, Victor Efren Guadarrama Vilchis, Immo Klose, Ujjwala Anantheswaran, Adam Zweiger, Kaivalya Rawal, Jeffery Li, Jeremy N"}
{"id": "2507.23701v1_chunk35", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "rke, Dae Hyun Kim, Felipe Meneguitti Dias, Sara Fish, Veit Elser, Tobias Kreiman, Victor Efren Guadarrama Vilchis, Immo Klose, Ujjwala Anantheswaran, Adam Zweiger, Kaivalya Rawal, Jeffery Li, Jeremy Nguyen, Nicolas Daans, Haline Heidinger, Maksim Radionov, Václav Rozho ˇn, Vincent Ginis, Christian Stump, Niv Cohen, Rafał Po ´swiata, Josef Tkadlec, Alan Goldfarb, Chenguang Wang, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Ryan Stendall, Jamie Tucker-Foltz, Jack Stade, T. Ryan Rogers, Tom Goertzen, Declan Grabb, Abhishek Shukla, Alan Givré, John Arnold Ambay, Archan Sen, Muhammad Fayez Aziz, Mark H Inlow, Hao He, Ling Zhang, Younesse Kaddar, Ivar Ängquist, Yanxu Chen, Harrison K Wang, Kalyan Ramakrishnan, Elliott Thornley, Antonio Terpin, Hailey Schoelkopf, Eric Zheng, Avishy Carmi, Ethan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Martin Stehberger, Peter Bradshaw, JP Heimonen, Kaustubh Sridhar, Ido Akov, Jennifer Sandlin, Yury Makarychev, Joanna Tam, Hieu Hoang, Da"}
{"id": "2507.23701v1_chunk36", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "i, Ethan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Martin Stehberger, Peter Bradshaw, JP Heimonen, Kaustubh Sridhar, Ido Akov, Jennifer Sandlin, Yury Makarychev, Joanna Tam, Hieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael Krause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon Lee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Orr Paradise, Jan Hendrik Kirchner, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie, Anna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani, Shreyas Verma, Prashant Joshi, Eli Meril, Ziqiao Ma, Jérémy Andréoletti, Raghav Singhal, Jacob Platnick, V olodymyr Nevirkovets, Luke Basler, Alexander Ivanov, Seri Khoury, Nils Gustafsson, Marco Piccardo, Hamid Mostaghimi, Qijia Chen, Virendra Singh, Tran Quoc Khánh, Paul Rosu, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Aline Menezes, Jonathan Roberts, William Alley, K"}
{"id": "2507.23701v1_chunk37", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "s Gustafsson, Marco Piccardo, Hamid Mostaghimi, Qijia Chen, Virendra Singh, Tran Quoc Khánh, Paul Rosu, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Aline Menezes, Jonathan Roberts, William Alley, Kunyang Sun, Arkil Patel, Max Lamparth, Anka Reuel, Linwei Xin, Hanmeng Xu, Jacob Loader, Freddie Martin, Zixuan Wang, Andrea Achilleos, Thomas Preu, Tomek Korbak, Ida Bosio, Fereshteh Kazemi, Ziye Chen, Biró Bálint, Eve J. Y . Lo, Jiaqi Wang, Maria Inês S. Nunes, Jeremiah Milbauer, M Saiful Bari, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane Durand, Hossam Elgnainy, Guillaume Douville, Daniel Tordera, George Balabanian, Hew Wolff, Lynna Kvistad, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Andrew Favre D. O., Shailesh Shah, Xiaoxiang Zhou, Firuz Kamalov, Sherwin Abdoli, Tim Santens, Shaul Barkan, Allison Tee, Robin Zhang, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt, Jiayi Pan, Emma Rodman, Jacob Drori, Carl J Fossum, Niklas Muennighoff, Milind Jagota"}
{"id": "2507.23701v1_chunk38", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": " Barkan, Allison Tee, Robin Zhang, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt, Jiayi Pan, Emma Rodman, Jacob Drori, Carl J Fossum, Niklas Muennighoff, Milind Jagota, Ronak Pradeep, Honglu Fan, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz Firsching, Carter Harris, Stefan Ciobâc ˘a, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones, Shashank Agnihotri, Pavel Zhelnov, Mohammadreza Mofayezi, Alexander Piperski, David K. Zhang, Kostiantyn Dobarskyi, Roman Leventov, Ignat Soroko, Joshua Duersch, Vage Taamazyan, Andrew Ho, Wenjie Ma, William Held, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed, Julian Noah Leser, Michelle X Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska, Claudio Di Fratta, Edson Oliveira, Joseph W. Jackson, Andy Zou, Muthu Chidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Bita Golshani, David Stap, Egor Kretov, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Nick"}
{"id": "2507.23701v1_chunk39", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": ", Andy Zou, Muthu Chidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Bita Golshani, David Stap, Egor Kretov, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Nick Winter, Miguel Orbegozo Rodriguez, Robert Lauff, Dustin Wehr, Colin Tang, Zaki Hossain, Shaun Phillips, Fortuna Samuele, Fredrik Ekström, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough Mohammadzadeh, Madellene Peñaflor, Haile Kassahun, Alena Friedrich, Rayner Hernandez Perez, Daniel Pyda, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu, Mike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira, Simon Weber, Handoko, Anton Peristyy, Stephen Malina, Mustafa Mehkary, Rami Aly, Frank Reidegeld, Anna-Katharina Dick, Cary Friday, Mukhwinder Singh, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb Gurdogan, Harsh Kumar, Chiara Ceconello, Chao Zhuang, Haon Park, Micah Carroll, Andrew R. Tawfeek, Stefan Steinerberger,"}
{"id": "2507.23701v1_chunk40", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "riday, Mukhwinder Singh, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb Gurdogan, Harsh Kumar, Chiara Ceconello, Chao Zhuang, Haon Park, Micah Carroll, Andrew R. Tawfeek, Stefan Steinerberger, Daattavya Aggarwal, Michael Kirchhof, Linjie Dai, Evan Kim, Johan Ferret, Jainam Shah, Yuzhou Wang, Minghao Yan, Krzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua Robinson, Abram Jackson, Paolo Giordano, Philipp Petersen, Adrian Cosma, Jesus Colino, Colin White, Jacob V otava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M. Shahid, Jean- Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Zheng-Xin Yong, Florencia de la Rosa, Nathan 8Cho, Xiuyu Li, Guillaume Malod, Orion Weller, Guglielmo Albani, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yi ˘git Yalın, Gbenga Daniel Obikoya, Rai, Filippo Bigi, M. C. Boscá, Oleg Shumar, "}
{"id": "2507.23701v1_chunk41", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": ", Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yi ˘git Yalın, Gbenga Daniel Obikoya, Rai, Filippo Bigi, M. C. Boscá, Oleg Shumar, Kaniuar Bacho, Gabriel Recchia, Mara Popescu, Nikita Shulga, Ngefor Mildred Tanwie, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew Brooks, Alesia Yakimchyk, Huanxu, Liu, Stefano Cavalleri, Olle Häggström, Emil Verkama, Joshua Newbould, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover, Ting Wang, Yosi Kratish, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Christian Schroeder de Witt, Pablo Hernández-Cámara, Emanuele Rodolà, Jules Robins, Dominic Williamson, Vincent Cheng, Brad Raynor, Hao Qi, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y . Wang, Kaylie Hausknecht, Michael P. Brenner, Mao Mao, Christoph Demian, Peyman Kassani, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodu"}
{"id": "2507.23701v1_chunk42", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": " Michael P. Brenner, Mao Mao, Christoph Demian, Peyman Kassani, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Yan Carlos Leyva Labrador, Zahra Adoul, Mohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna Liakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Earth Anderson, Rodrigo De Oliveira Pena, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu, Ross Finocchio, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson, Mohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Isaac C. McAlister, Alejandro José Moyano, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Yana Malysheva, Daphiny Pottmaier, Omid Taheri, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali M. R. Minissi, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Ronald Clark, Josh Ducey, Matheus Piza,"}
{"id": "2507.23701v1_chunk43", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "Taheri, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali M. R. Minissi, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Ronald Clark, Josh Ducey, Matheus Piza, Maja Somrak, Eric Vergo, Juehang Qin, Benjámin Borbás, Eric Chu, Jack Lindsey, Antoine Jallon, I. M. J. McInnis, Evan Chen, Avi Semler, Luk Gloor, Tej Shah, Marc Carauleanu, Pascal Lauer, Tran Ðuc Huy, Hossein Shahrtash, Emilien Duc, Lukas Lewark, Assaf Brown, Samuel Albanie, Brian Weber, Warren S. Vaz, Pierre Clavier, Yiyang Fan, Gabriel Poesia Reis e Silva, Long, Lian, Marcus Abramovitch, Xi Jiang, Sandra Mendoza, Murat Islam, Juan Gonzalez, Vasilios Mavroudis, Justin Xu, Pawan Kumar, Laxman Prasad Goswami, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong, Thorben Jansen, Antonella Pinto, Archimedes Apronti, Abdallah Galal, Ng Ze-An, Ankit Singh, Tong Jiang, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani, Gang Zhang, Zhehang Du, Benedito Alves de Oliveira Junior, Dmitry Malishev"}
{"id": "2507.23701v1_chunk44", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "himedes Apronti, Abdallah Galal, Ng Ze-An, Ankit Singh, Tong Jiang, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani, Gang Zhang, Zhehang Du, Benedito Alves de Oliveira Junior, Dmitry Malishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Gautier Abou Loume, Wiktor Morak, Farzad Habibi, Sarah Hoback, Will Cai, Javier Gimenez, Roselynn Grace Montecillo, Jakub Łucki, Russell Campbell, Asankhaya Sharma, Khalida Meer, Shreen Gul, Daniel Espinosa Gonzalez, Xavier Alapont, Alex Hoover, Gunjan Chhablani, Freddie Vargus, Arunim Agarwal, Yibo Jiang, Deepakkumar Patil, David Outevsky, Kevin Joseph Scaria, Rajat Maheshwari, Abdelkader Dendane, Priti Shukla, Ashley Cartwright, Sergei Bogdanov, Niels Mündler, Sören Möller, Luca Arnaboldi, Kunvar Thaman, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Tony Fruhauff, Glen Sherman, Mátyás Vincze, Siranut Usawasutsakorn, Dylan Ler, Anil Radhakrishnan, Innocent Enyekwe, Sk Md Salauddin, Jiang Muzhen, Aleksandr Maksapetyan,"}
{"id": "2507.23701v1_chunk45", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "rajvi Saxena, Himanshu Gupta, Tony Fruhauff, Glen Sherman, Mátyás Vincze, Siranut Usawasutsakorn, Dylan Ler, Anil Radhakrishnan, Innocent Enyekwe, Sk Md Salauddin, Jiang Muzhen, Aleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi, Mohsen Bahaloohoreh, Claire Sparrow, Jasdeep Sidhu, Sam Ali, Song Bian, John Lai, Eric Singer, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling Duclosel, Dario Bezzi, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek, Imad Ali Shah, Jun Jin, Scott Creighton, Denis Peskoff, Zienab EL-Wasif, Ragavendran P V , Michael Richmond, Joseph McGowan, Tejal Patwardhan, Hao-Yu Sun, Ting Sun, Nikola Zubi ´c, Samuele Sala, Stephen Ebert, Jean Kaddour, Manuel Schottdorf, Dianzhuo Wang, Gerol Petruzella, Alex Meiburg, Tilen Medved, Ali ElSheikh, S Ashwin Hebbar, Lorenzo Vaquero, Xianjun Yang, Jason Poulos, Vilém Zouhar, Sergey Bogdanik, Mingfang Zhang, Jorge Sanz-Ros, David Anugraha, Yinwei Dai, Anh N. Nhu, Xue Wang, Ali "}
{"id": "2507.23701v1_chunk46", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "edved, Ali ElSheikh, S Ashwin Hebbar, Lorenzo Vaquero, Xianjun Yang, Jason Poulos, Vilém Zouhar, Sergey Bogdanik, Mingfang Zhang, Jorge Sanz-Ros, David Anugraha, Yinwei Dai, Anh N. Nhu, Xue Wang, Ali Anil Demircali, Zhibai Jia, Yuyin Zhou, Juncheng Wu, Mike He, Nitin Chandok, Aarush Sinha, Gaoxiang Luo, Long Le, Mickaël Noyé, Michał Perełkiewicz, Ioannis Pantidis, Tianbo Qi, Soham Sachin Purohit, Letitia Parcalabescu, Thai-Hoa Nguyen, Genta Indra Winata, Edoardo M. Ponti, Hanchen Li, Kaustubh Dhole, Jongee Park, Dario Abbondanza, Yuanli Wang, Anupam Nayak, Diogo M. Caetano, Antonio A. W. L. Wong, Maria del Rio-Chanona, Dániel Kondor, Pieter Francois, Ed Chalstrey, Jakob Zsambok, Dan Hoyer, Jenny Reddish, Jakob Hauser, Francisco-Javier Rodrigo-Ginés, Suchandra Datta, Maxwell Shepherd, Thom Kamphuis, Qizheng Zhang, Hyunjun Kim, Ruiji Sun, Jianzhu Yao, Franck Dernoncourt, Satyapriya Krishna, Sina Rismanchian, Bonan Pu, Francesco Pinto, Yingheng Wang, Kumar Shridhar, Kalon J. Overholt, Gli"}
{"id": "2507.23701v1_chunk47", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "Kamphuis, Qizheng Zhang, Hyunjun Kim, Ruiji Sun, Jianzhu Yao, Franck Dernoncourt, Satyapriya Krishna, Sina Rismanchian, Bonan Pu, Francesco Pinto, Yingheng Wang, Kumar Shridhar, Kalon J. Overholt, Glib Briia, Hieu Nguyen, David, Soler Bartomeu, Tony CY Pang, Adam Wecker, Yifan Xiong, Fanfei Li, Lukas S. Huber, Joshua Jaeger, Romano De Maddalena, Xing Han Lù, Yuhui Zhang, Claas Beger, Patrick Tser Jern Kon, Sean Li, Vivek Sanker, Ming Yin, Yihao Liang, Xinlu Zhang, Ankit Agrawal, Li S. Yifei, Zechen Zhang, Mu Cai, Yasin Sonmez, Costin Cozianu, Changhao Li, Alex Slen, Shoubin Yu, Hyun Kyu Park, Gabriele Sarti, Marcin Bria´nski, Alessandro Stolfo, Truong An Nguyen, Mike Zhang, Yotam Perlitz, Jose Hernandez-Orallo, Runjia Li, Amin Shabani, Felix Juefei-Xu, Shikhar Dhingra, Orr Zohar, My Chiffon Nguyen, Alexander Pondaven, Abdurrahim Yilmaz, Xuandong Zhao, Chuanyang Jin, Muyan Jiang, Stefan Todoran, Xinyao Han, Jules Kreuer, Brian Rabern, Anna Plassart, Martino Maggetti, Luther Yap, Robert "}
{"id": "2507.23701v1_chunk48", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "n Nguyen, Alexander Pondaven, Abdurrahim Yilmaz, Xuandong Zhao, Chuanyang Jin, Muyan Jiang, Stefan Todoran, Xinyao Han, Jules Kreuer, Brian Rabern, Anna Plassart, Martino Maggetti, Luther Yap, Robert Geirhos, Jonathon Kean, Dingsu Wang, Sina Mollaei, Chenkai Sun, Yifan Yin, Shiqi Wang, Rui Li, Yaowen Chang, Anjiang Wei, Alice Bizeul, Xiaohan Wang, Alexandre Oliveira Arrais, Kushin Mukherjee, Jorge Chamorro-Padial, Jiachen Liu, Xingyu Qu, Junyi Guan, Adam Bouyamourn, Shuyu Wu, Martyna Plomecka, Junda Chen, Mengze Tang, Jiaqi Deng, Shreyas Subramanian, Haocheng Xi, Haoxuan Chen, Weizhi Zhang, Yinuo Ren, Haoqin Tu, Sejong Kim, Yushun Chen, Sara Vera Marjanovi ´c, Junwoo Ha, 9Grzegorz Luczyna, Jeff J. Ma, Zewen Shen, Dawn Song, Cedegao E. Zhang, Zhun Wang, Gaël Gendron, Yunze Xiao, Leo Smucker, Erica Weng, Kwok Hao Lee, Zhe Ye, Stefano Ermon, Ignacio D. Lopez-Miguel, Theo Knights, Anthony Gitter, Namkyu Park, Boyi Wei, Hongzheng Chen, Kunal Pai, Ahmed Elkhanany, Han Lin, Philipp D. Siedler"}
{"id": "2507.23701v1_chunk49", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": ", Erica Weng, Kwok Hao Lee, Zhe Ye, Stefano Ermon, Ignacio D. Lopez-Miguel, Theo Knights, Anthony Gitter, Namkyu Park, Boyi Wei, Hongzheng Chen, Kunal Pai, Ahmed Elkhanany, Han Lin, Philipp D. Siedler, Jichao Fang, Ritwik Mishra, Károly Zsolnai-Fehér, Xilin Jiang, Shadab Khan, Jun Yuan, Rishab Kumar Jain, Xi Lin, Mike Peterson, Zhe Wang, Aditya Malusare, Maosen Tang, Isha Gupta, Ivan Fosin, Timothy Kang, Barbara Dworakowska, Kazuki Matsumoto, Guangyao Zheng, Gerben Sewuster, Jorge Pretel Villanueva, Ivan Rannev, Igor Chernyavsky, Jiale Chen, Deepayan Banik, Ben Racz, Wenchao Dong, Jianxin Wang, Laila Bashmal, Duarte V . Gonçalves, Wei Hu, Kaushik Bar, Ondrej Bohdal, Atharv Singh Patlan, Shehzaad Dhuliawala, Caroline Geirhos, Julien Wist, Yuval Kansal, Bingsen Chen, Kutay Tire, Atak Talay Yücel, Brandon Christof, Veerupaksh Singla, Zijian Song, Sanxing Chen, Jiaxin Ge, Kaustubh Ponkshe, Isaac Park, Tianneng Shi, Martin Q. Ma, Joshua Mak, Sherwin Lai, Antoine Moulin, Zhuo Cheng, Zhanda Z"}
{"id": "2507.23701v1_chunk50", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "ücel, Brandon Christof, Veerupaksh Singla, Zijian Song, Sanxing Chen, Jiaxin Ge, Kaustubh Ponkshe, Isaac Park, Tianneng Shi, Martin Q. Ma, Joshua Mak, Sherwin Lai, Antoine Moulin, Zhuo Cheng, Zhanda Zhu, Ziyi Zhang, Vaidehi Patil, Ketan Jha, Qiutong Men, Jiaxuan Wu, Tianchi Zhang, Bruno Hebling Vieira, Alham Fikri Aji, Jae-Won Chung, Mohammed Mahfoud, Ha Thi Hoang, Marc Sperzel, Wei Hao, Kristof Meding, Sihan Xu, Vassilis Kostakos, Davide Manini, Yueying Liu, Christopher Toukmaji, Jay Paek, Eunmi Yu, Arif Engin Demircali, Zhiyi Sun, Ivan Dewerpe, Hongsen Qin, Roman Pflugfelder, James Bailey, Johnathan Morris, Ville Heilala, Sybille Rosset, Zishun Yu, Peter E. Chen, Woongyeong Yeo, Eeshaan Jain, Ryan Yang, Sreekar Chigurupati, Julia Chernyavsky, Sai Prajwal Reddy, Subhashini Venugopalan, Hunar Batra, Core Francisco Park, Hieu Tran, Guilherme Maximiano, Genghan Zhang, Yizhuo Liang, Hu Shiyu, Rongwu Xu, Rui Pan, Siddharth Suresh, Ziqi Liu, Samaksh Gulati, Songyang Zhang, Peter Turchin, Ch"}
{"id": "2507.23701v1_chunk51", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "nar Batra, Core Francisco Park, Hieu Tran, Guilherme Maximiano, Genghan Zhang, Yizhuo Liang, Hu Shiyu, Rongwu Xu, Rui Pan, Siddharth Suresh, Ziqi Liu, Samaksh Gulati, Songyang Zhang, Peter Turchin, Christopher W. Bartlett, Christopher R. Scotese, Phuong M. Cao, Aakaash Nattanmai, Gordon McKellips, Anish Cheraku, Asim Suhail, Ethan Luo, Marvin Deng, Jason Luo, Ashley Zhang, Kavin Jindel, Jay Paek, Kasper Halevy, Allen Baranov, Michael Liu, Advaith Avadhanam, David Zhang, Vincent Cheng, Brad Ma, Evan Fu, Liam Do, Joshua Lass, Hubert Yang, Surya Sunkari, Vishruth Bharath, Violet Ai, James Leung, Rishit Agrawal, Alan Zhou, Kevin Chen, Tejas Kalpathi, Ziqi Xu, Gavin Wang, Tyler Xiao, Erik Maung, Sam Lee, Ryan Yang, Roy Yue, Ben Zhao, Julia Yoon, Sunny Sun, Aryan Singh, Ethan Luo, Clark Peng, Tyler Osbey, Taozhi Wang, Daryl Echeazu, Hubert Yang, Timothy Wu, Spandan Patel, Vidhi Kulkarni, Vijaykaarti Sundarapandiyan, Ashley Zhang, Andrew Le, Zafir Nasim, Srikar Yalam, Ritesh Kasamsetty, Soham"}
{"id": "2507.23701v1_chunk52", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "ler Osbey, Taozhi Wang, Daryl Echeazu, Hubert Yang, Timothy Wu, Spandan Patel, Vidhi Kulkarni, Vijaykaarti Sundarapandiyan, Ashley Zhang, Andrew Le, Zafir Nasim, Srikar Yalam, Ritesh Kasamsetty, Soham Samal, Hubert Yang, David Sun, Nihar Shah, Abhijeet Saha, Alex Zhang, Leon Nguyen, Laasya Nagumalli, Kaixin Wang, Alan Zhou, Aidan Wu, Jason Luo, Anwith Telluri, Summer Yue, Alexandr Wang, and Dan Hendrycks. Humanity’s last exam, 2025. URL https://arxiv.org/abs/2501.14249 . David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark, 2023. URL https: //arxiv.org/abs/2311.12022 . Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: A realistic multi-turn conversation evaluation benchmark challenging to frontier llms, 2025. URL https://arxiv.org/abs/250"}
{"id": "2507.23701v1_chunk53", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: A realistic multi-turn conversation evaluation benchmark challenging to frontier llms, 2025. URL https://arxiv.org/abs/2501.17399 . Susan Smetale. Through the zorking glass. The Washington Post , December 1983. URL https: //www.washingtonpost.com/archive/lifestyle/1983/12/22/through-the-zorking-glass/ 8f6fc376-0942-4e66-abb9-06f66a05165c/ . Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. Paperbench: Evaluating ai’s ability to replicate ai research, 2025. URL https://arxiv.org/abs/2504.01848 . Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: A simple yet challenging benchmark for browsing agents, 2025. URL https://arxiv.org/abs/2504.12516 . Shuny"}
{"id": "2507.23701v1_chunk54", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": " Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: A simple yet challenging benchmark for browsing agents, 2025. URL https://arxiv.org/abs/2504.12516 . Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep calm and explore: Language models for action generation in text-based games, 2020. URL https://arxiv.org/abs/2010.02903 . Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ-bench: A benchmark for tool-agent-user interaction in real-world domains, 2024. URL https://arxiv.org/abs/2406.12045 . 10A T EXTQUESTS Environments A.1 Environments TEXTQUESTS consists of 25 classic Infocom games. Our benchmark is built upon the game files and annotations collected by Hendrycks et al. (2021c), using the Jericho interface (Hausknecht et al., 2020) to a Frotz interpreter to run the original game files compiled from the Zork Implementation Language (ZIL). The 25 games included in the benchmark are listed in Table 2. Ballyhoo P"}
{"id": "2507.23701v1_chunk55", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "necht et al., 2020) to a Frotz interpreter to run the original game files compiled from the Zork Implementation Language (ZIL). The 25 games included in the benchmark are listed in Table 2. Ballyhoo Planetfall Sherlock Border Zone Plundered Hearts Sorcerer Cutthroats Seastalker Spellbreaker Deadline Starcross Stationfall Enchanter Suspect The Hitchhiker’s Guide to the Galaxy Hollywood Hijinx The Lurking Horror The Witness Infidel Trinity Wishbringer Moonmist Zork I Zork II Zork III Table 2: List of the 25 Infocom text adventure games included in the T EXTQUESTS benchmark. If you use T EXTQUESTS in your research, we ask that you also cite the original work by Hendrycks et al. (2021c): @article{hendrycks2021jiminycricket, title={What Would Jiminy Cricket Do? Towards Agents That Behave Morally}, author={Dan Hendrycks and Mantas Mazeika and Andy Zou and Sahil Patel and Christine Zhu and Jesus Navarro and Dawn Song and Bo Li and Jacob Steinhardt}, journal={NeurIPS}, year={2021} } A.1.1 Feel"}
{"id": "2507.23701v1_chunk56", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "y}, author={Dan Hendrycks and Mantas Mazeika and Andy Zou and Sahil Patel and Christine Zhu and Jesus Navarro and Dawn Song and Bo Li and Jacob Steinhardt}, journal={NeurIPS}, year={2021} } A.1.1 Feelies and InvisiClues Many Infocom games came packaged with physical items known as \"feelies\" or guidelines, which contained information essential for solving puzzles. To ensure all games are solvable, the text from these feelies is provided to the agent in its initial context for both NOCLUES and W ITHCLUES modes. The InvisiClues were separate, official hint booklets that provided a series of progressively more explicit hints for each in-game puzzle. In W ITHCLUES evaluation, the complete text of the InvisiClues booklet is also provided to the agent’s context window. Example of clues in Zork I and WishBringer: Zork I ... The Dam Area ************ How is the control panel operated? A. You can turn the bolt. B. You need the wrench. C. You must activate the panel. (Green bubble lights up.) Wha"}
{"id": "2507.23701v1_chunk57", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "and WishBringer: Zork I ... The Dam Area ************ How is the control panel operated? A. You can turn the bolt. B. You need the wrench. C. You must activate the panel. (Green bubble lights up.) What is the green bubble for? It indicates that the control panel is activated. Use the buttons in the Maintenance Room. What do I do with the tube? A. Read the tube. ....WishBringer ... What should I do with the umbrella? A. It might come in handy if it rains. B. You can’t WISH FOR RAIN unless you have an umbrella. C. So maybe you should hold onto it. How do I get through the locked gate? A. The gravedigger has the only key. B. But he is nowhere to be seen. C. You can’t unlock the gate. To leave, go out the open gate at Creepy Corner. ... 11A.1.2 Autosave TEXTQUESTS environment automatically saves the full game state after each command. An agent can then restore to any of these previous states by issuing the special command restore {step_id} , where restore {step_id} corresponds to the step "}
{"id": "2507.23701v1_chunk58", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "es the full game state after each command. An agent can then restore to any of these previous states by issuing the special command restore {step_id} , where restore {step_id} corresponds to the step it wishes to return to. This functionality is crucial as it allows an agent to explore different strategies or recover from non-optimal paths, such as getting stuck or entering an unwinnable state, without requiring a full restart of the game. This emulates the save/restore patterns common among human players to facilitate exploration and experimentation. As shown in Figure 6, providing agents with this capability resulted in a notable improvement in overall gameplay performance. Wishbringer(1985) We are carrying a gold coin. The simplest way to test whether the troll will accept it is to offer the coin. give coin to trollStanding near the toll gate is an ugly, gnomelike troll We need an alternative way past the troll. We need to cross the troll using the rattlesnake trick, so we must rest"}
{"id": "2507.23701v1_chunk59", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "r the coin. give coin to trollStanding near the toll gate is an ugly, gnomelike troll We need an alternative way past the troll. We need to cross the troll using the rattlesnake trick, so we must restore to step 84, right before we .... restore 84The troll snatches away your gold coin and squints at it suspiciously. “This is a fake!” he cries.... 0 100 200 300 400 500 Steps020406080Average Game Progress (%)Game Progress with and without Autosave Gemini 2.5 Pro Autosave: 59% No Autosave: 47% Claude Sonnet 4.0 Autosave: 57% No Autosave: 45% Grok 3 Mini Beta Autosave: 34% No Autosave: 29% GPT-4.1 Mini Autosave: 15% No Autosave: 15% Figure 6: Adding an AutoSave mechanism to the game environment improves the agent’s exploration efficiency. Left: An example of evaluated LLMs makes use of the autosave and restore features to experiment with different approaches to solve an in-game puzzle. Right: As LLMs’ capabilities increase, the performance difference between runs with and without the Autos"}
{"id": "2507.23701v1_chunk60", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "osave and restore features to experiment with different approaches to solve an in-game puzzle. Right: As LLMs’ capabilities increase, the performance difference between runs with and without the Autosave feature widens, leading to a difference of more than 10% after 500 steps on Gemini 2.5 Pro and Claude Sonnet 4.0 and 6% on Grok 3 Mini. B Full Results No Clues With Clues Progress (%) ↑# Completed (/25) ↑Harm ↓Progress (%) ↑# Completed (/25) ↑Harm ↓ Grok 4 31.2 0 30.4 61.4 3 31.4 o3 30.9 0 18.7 60.4 3 17.2 Claude Opus 4 26.4 0 16.5 60.5 4 19.2 Gemini 2.5 Pro 23.2 0 15.9 60.6 3 25.6 Claude Sonnet 4 24.7 0 16.0 57.2 2 18.4 Grok 3 18.9 0 15.4 41.9 2 21.2 GPT-4.1 22.8 0 11.4 37.5 0 15.3 Grok 3 mini 22.4 0 17.8 32.2 0 18.2 Qwen 3 Thinking 15.1 0 16.4 29.8 1 10.8 Gemini 2.5 Flash 14.4 0 11.7 31.8 0 16.8 DeepSeek R1 15.2 0 15.4 23.8 0 23.0 o4-mini 12.8 0 18.6 20.6 0 20.0 Kimi K2 10.5 0 8.3 19.7 0 9.0 Gemini 2.5 Flash-Lite 11.7 0 22.8 16.6 0 10.6 GPT-4.1-mini 10.6 0 11.7 15.9 0 12.2 Claude Hai"}
{"id": "2507.23701v1_chunk61", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "8 0 16.8 DeepSeek R1 15.2 0 15.4 23.8 0 23.0 o4-mini 12.8 0 18.6 20.6 0 20.0 Kimi K2 10.5 0 8.3 19.7 0 9.0 Gemini 2.5 Flash-Lite 11.7 0 22.8 16.6 0 10.6 GPT-4.1-mini 10.6 0 11.7 15.9 0 12.2 Claude Haiku 3.5 12.3 0 14.9 13.4 0 10.6 Llama 4 Maverick 9.2 0 13.1 16.1 0 12.1 Llama 4 Scout 4.8 0 7.4 7.7 0 7.0 Table 3: LLMs performance on T EXTQUESTS . Reasoning models are evaluated with high reasoning setting. 12C System Prompt and Environment Interaction System Prompt We use the following system prompt with an instruction to output a structured format: System Prompt You are a text-based game player. Your goal is to finish the game. GAME: game_name MAX SCORE: max_score During each game turn, you will be given an observation of the current game state. To take an action, generate a single parser command action. {guidelines_and_clues} [OUTPUT FORMAT DURING GAME TURNS] Return a final json block with the following format: { \"reasoning\": \"<your step-by-step reasoning for action>\", \"action\": \"<acti"}
{"id": "2507.23701v1_chunk62", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "ommand action. {guidelines_and_clues} [OUTPUT FORMAT DURING GAME TURNS] Return a final json block with the following format: { \"reasoning\": \"<your step-by-step reasoning for action>\", \"action\": \"<action to take>\" } SPECIAL ACTION: You can restore the game to previous steps by using ‘restore {step_id}’ for the step that you want to backtrack. Environment Inputs/Outputs As shown in Figure 7, the evaluation proceeds in discrete history turns. For each turn, we parse the brief reasoning and corresponding action from the LLM’s output, adding them to the context history for the subsequent step. To ensure the agent’s decisions are based on its explicit plan, we discard any other ’thinking’ output and do not include it in the context history. /gid00020/gid00052/gid00046/gid00047/gid00032/gid00040 /gid00020/gid00052/gid00046/gid00047/gid00032/gid00040 /gid00022/gid00046/gid00032/gid00045/gid00165/gid00001/gid00016/gid00029/gid00046/gid00032/gid00045/gid00049/gid00028/gid00047/gid00036/gid00042/"}
{"id": "2507.23701v1_chunk63", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "/gid00020/gid00052/gid00046/gid00047/gid00032/gid00040 /gid00022/gid00046/gid00032/gid00045/gid00165/gid00001/gid00016/gid00029/gid00046/gid00032/gid00045/gid00049/gid00028/gid00047/gid00036/gid00042/gid00041/gid00020/gid00052/gid00046/gid00047/gid00032/gid00040 <reasoning> </gid00028/gid00030/gid00047/gid00036/gid00042/gid00041/gid00341/gid00022/gid00046/gid00032/gid00045/gid00165/gid00001/gid00016/gid00029/gid00046/gid00032/gid00045/gid00049/gid00028/gid00047/gid00036/gid00042/gid00041 /gid00340/gid00047/gid00035/gid00036/gid00041/gid00038/gid00036/gid00041/gid00034/gid00341 /gid00191 “reasoning”: <brief reasoning>, “/gid00028/gid00030/gid00047/gid00036/gid00042/gid00041/gid00176/gid00165/gid00001/gid00340/gid00028/gid00030/gid00047/gid00036/gid00042/gid00041/gid00341 /gid00192/gid00016/gid00048/gid00047/gid00043/gid00048/gid00047/gid00010/gid00041/gid00043/gid00048/gid00047 /gid00010/gid00041/gid00043/gid00048/gid00047 /gid00010/gid00041/gid00043/gid00048/gid00047/gid00020/gid00047/"}
{"id": "2507.23701v1_chunk64", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "gid00048/gid00047/gid00043/gid00048/gid00047/gid00010/gid00041/gid00043/gid00048/gid00047 /gid00010/gid00041/gid00043/gid00048/gid00047 /gid00010/gid00041/gid00043/gid00048/gid00047/gid00020/gid00047/gid00032/gid00043/gid00001/gid00422 /gid00020/gid00047/gid00032/gid00043/gid00001/gid00423 /gid00020/gid00047/gid00032/gid00043/gid00001/gid00424 /gid00002/gid00046/gid00046/gid00047/gid00165 /gid00022/gid00046/gid00032/gid00045/gid00165/gid00001/gid00016/gid00029/gid00046/gid00032/gid00045/gid00049/gid00028/gid00047/gid00036/gid00042/gid00041 /gid00340/gid00047/gid00035/gid00036/gid00041/gid00038/gid00036/gid00041/gid00034/gid00341 /gid00191 “reasoning”: <brief reasoning>, “/gid00028/gid00030/gid00047/gid00036/gid00042/gid00041/gid00176/gid00165/gid00001/gid00340/gid00028/gid00030/gid00047/gid00036/gid00042/gid00041/gid00341 /gid00192/gid00016/gid00048/gid00047/gid00043/gid00048/gid00047/gid00022/gid00046/gid00032/gid00045/gid00165/gid00001/gid00016/gid00029/gid00046/gid00032/gid00045/gid"}
{"id": "2507.23701v1_chunk65", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "d00036/gid00042/gid00041/gid00341 /gid00192/gid00016/gid00048/gid00047/gid00043/gid00048/gid00047/gid00022/gid00046/gid00032/gid00045/gid00165/gid00001/gid00016/gid00029/gid00046/gid00032/gid00045/gid00049/gid00028/gid00047/gid00036/gid00042/gid00041 <reasoning> </gid00028/gid00030/gid00047/gid00036/gid00042/gid00041/gid00341/gid00002/gid00046/gid00046/gid00047/gid00165 /gid00022/gid00046/gid00032/gid00045/gid00165/gid00001/gid00016/gid00029/gid00046/gid00032/gid00045/gid00049/gid00028/gid00047/gid00036/gid00042/gid00041 <reasoning> </gid00028/gid00030/gid00047/gid00036/gid00042/gid00041/gid00341/gid00002/gid00046/gid00046/gid00047/gid00165 /gid00022/gid00046/gid00032/gid00045/gid00165/gid00001/gid00016/gid00029/gid00046/gid00032/gid00045/gid00049/gid00028/gid00047/gid00036/gid00042/gid00041 /gid00340/gid00047/gid00035/gid00036/gid00041/gid00038/gid00036/gid00041/gid00034/gid00341 /gid00191 “reasoning”: <brief reasoning>, “/gid00028/gid00030/gid00047/gid00036/gid00042/gid00041/gid00176"}
{"id": "2507.23701v1_chunk66", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "41 /gid00340/gid00047/gid00035/gid00036/gid00041/gid00038/gid00036/gid00041/gid00034/gid00341 /gid00191 “reasoning”: <brief reasoning>, “/gid00028/gid00030/gid00047/gid00036/gid00042/gid00041/gid00176/gid00165/gid00001/gid00340/gid00028/gid00030/gid00047/gid00036/gid00042/gid00041/gid00341 /gid00192/gid00016/gid00048/gid00047/gid00043/gid00048/gid00047 Figure 7: An illustration of an agent’s turn. From the model’s output, the brief reasoning and action are extracted and added to the context history, while any intermediate ‘thinking’ is discarded 13D Token Analysis We report the total input/output tokens cost to evaluate T EXTQUESTS in Table 4. NOCLUES o3 Gemini 2.5 Pro Claude Opus 4.0 Claude Sonnet 4.0 GPT-4.1 GPT-4.1-mini Max Input Tokens 82K 128K 140K 132K 97K 78K Max Output Tokens 6.2K 700 1.4K 1.6K 239 172 Total Input Tokens 471M 562M 524M 569M 460M 428M Cache Tokens 450M 530M 522M 567M 456M 420M Total Output Tokens 10M 2.7M 3.1M 3.3M 0.7M 0.7M WITHCLUES o3 Gemini 2.5 Pro Claude Op"}
{"id": "2507.23701v1_chunk67", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "700 1.4K 1.6K 239 172 Total Input Tokens 471M 562M 524M 569M 460M 428M Cache Tokens 450M 530M 522M 567M 456M 420M Total Output Tokens 10M 2.7M 3.1M 3.3M 0.7M 0.7M WITHCLUES o3 Gemini 2.5 Pro Claude Opus 4.0 Claude Sonnet 4.0 GPT-4.1 GPT-4.1-mini Max Input Tokens 90K 132K 140K 132K 88K 97K Max Output Tokens 6.8K 1.4K 1.7K 1.9K 217 199 Total Input Tokens 531M 675M 585M 569M 509M 539M Cache Tokens 514M 635M 583M 567M 503M 530M Total Output Tokens 9.6M 2.2M 2.8M 3.3M 0.7M 0.7M Table 4: Input and output token costs for evaluating TEXTQUESTS . All models were configured for high reasoning effort (and a 20k token thinking budget for Claude 4 models), though this maximum budget was not always fully utilized. While the majority of the cost is from input tokens, a high cache hit rate (exceeding 95-99%) makes the evaluations significantly cost efficient. E Comparing Game Progress and Game Score As discussed in Section 3.2, the built-in scoring systems of the Infocom games are often a weak proxy f"}
{"id": "2507.23701v1_chunk68", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "9%) makes the evaluations significantly cost efficient. E Comparing Game Progress and Game Score As discussed in Section 3.2, the built-in scoring systems of the Infocom games are often a weak proxy for an agent’s actual advancement toward completing a game. They were designed to reward human players for exploration and cleverness, not to serve as a direct measure of progress along the critical path. To visually illustrate this discrepancy, Figure 8 presents a direct comparison between the traditional Game Score and our checkpoint- based Game Progress metric. The figure highlights how our metric provides a more consistent signal of an agent’s approach to completion and shows clear cases where the game’s score is decoupled from this primary objective. Figure 8: A comparison of our Game Progress metric against the in-game Game Score .Left: The trajectory for an optimal walkthrough of a sample game shows that our Game Progress provides a more representative signal of advancement than the "}
{"id": "2507.23701v1_chunk69", "paper_id": "2507.23701v1", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "text": "ress metric against the in-game Game Score .Left: The trajectory for an optimal walkthrough of a sample game shows that our Game Progress provides a more representative signal of advancement than the built-in score. Right: The final scores for games like Moonmist andWitness demonstrate that game completion (100% progress) is often independent of achieving the maximum possible game score. 14"}
