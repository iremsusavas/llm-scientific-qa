# ğŸ¤– LLM-Based Scientific Q&A App with Inline Citations

A local web app that answers scientific questions using academic papers, providing step-by-step reasoning and inline citations like [1], [2].

---

## âœ¨ Key Features & Enhancements

- ğŸ” **Semantic search** using SentenceTransformers (`all-MiniLM-L6-v2`)
- ğŸ“¥ **arXiv paper fetching** (downloaded, parsed, chunked)
- ğŸ§  **Chunk vectorization** with FAISS for fast nearest neighbor retrieval
- ğŸª„ **LLM-driven summaries** of relevant papers using **Mistral via Ollama**
- ğŸ§© **Chain-of-Thought prompting** for step-by-step reasoning
- ğŸ”— **Inline citation markers** ([1], [2]) matched with clickable bibliography
- ğŸ“š **Structured memory per paper** for fine-grained summarization
- ğŸŒ **Modern FastAPI web interface** with live query handling
- ğŸ¨ **Clean UI** with readable outputs, consistent typography, and responsive design
- âš¡ Optimized prompt formatting and memory grouping to reduce LLM latency

---

## ğŸ“‚ Project Structure

```
llm-scientific-qa/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                         # Raw downloaded arXiv PDFs
â”‚   â”œâ”€â”€ processed/chunks.jsonl       # Paper chunks (jsonl format)
â”‚   â””â”€â”€ vector/faiss.index           # FAISS vector index
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ fetch_arxiv.py              # Script to fetch and parse arXiv papers
â”‚   â”œâ”€â”€ vector/
â”‚   â”‚   â””â”€â”€ query_with_llm.py        # Full pipeline logic
â”‚   â””â”€â”€ web/
â”‚       â”œâ”€â”€ main.py                  # FastAPI backend logic
â”‚       â””â”€â”€ templates/
â”‚           â””â”€â”€ index.html           # Frontend UI template (Jinja2)
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ screenshot.png               # Demo screenshot for README
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1. Clone & Create Environment
```bash
git clone https://github.com/yourusername/llm-scientific-qa.git
cd llm-scientific-qa
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Run Ollama with Mistral
```bash
ollama serve
ollama pull mistral
```
> Ollama must remain running in the background.

---

## ğŸš€ Launch the Web App
```bash
uvicorn src.web.main:app --reload
```
Then visit: [http://127.0.0.1:8000](http://127.0.0.1:8000)

---

## ğŸ’¡ How It Works (Technical Overview)

1. **arXiv papers** are downloaded, parsed, and split into semantically meaningful chunks.
2. **Chunk embeddings** are generated using SentenceTransformers.
3. **FAISS index** is built and queried to find the top-k most relevant chunks for a given question.
4. **Chunks grouped** by `paper_id` for per-paper summarization.
5. **Summarization**: Each group is summarized via Mistral LLM (via Ollama).
6. **Reasoning and citation generation** is done using Chain-of-Thought prompting.
7. **Final answer** is constructed with inline citations like [1], [2] and shown in the UI.

---

## ğŸ–¼ï¸ Demo Screenshot

![LLM QA App Screenshot](assets/screenshot.png)

---

## ğŸ“¤ Example Query

> **Question:** How do LLMs simulate multi-step reasoning and revise decisions?

**Answer Output:**
```
Reasoning:
- Step 1: ... [2]
- Step 2: ... [2]
- Step 3: ... [4]

Final Answer:
LLMs revise decisions and simulate multi-step reasoning via world models, iterative evaluation, and backtracking. [2][4]
```

**References:**
```
[1] Paper Title - https://arxiv.org/abs/xxx
[2] Paper Title - https://arxiv.org/abs/yyy
...
```

---

## ğŸ› ï¸ Requirements

```txt
fastapi
uvicorn
faiss-cpu
sentence-transformers
numpy
jinja2
ollama
```

---

## ğŸ“¦ Technologies Used

- ğŸ§  SentenceTransformers (SBERT)
- ğŸ” FAISS similarity search
- ğŸ“¥ arXiv PDF parsing and chunking
- ğŸ§© Ollama (Mistral LLM backend)
- âš¡ FastAPI for backend routing
- ğŸŒ Jinja2 + HTML + CSS for frontend
- ğŸ§± Chain-of-Thought Prompt Engineering

---

## ğŸ‘¤ Author
Built by [Iremsu Savas] â€“ ML Engineer with deep interest in applied AI, scientific reasoning, and LLM-powered tools.

Feel free to â­ï¸ the repo and reach out if you'd like to collaborate!

